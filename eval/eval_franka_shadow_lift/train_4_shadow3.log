################################################################################
                      [1m Learning iteration 0/2000 [0m                       

                       Computation: 10382 steps/s (collection: 9.178s, learning 0.291s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 31.2573
                       Mean reward: 0.00
               Mean episode length: 21.94
    Episode_Reward/reaching_object: 0.0006
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0002
          Episode_Reward/joint_vel: -0.0003
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 9.47s
                      Time elapsed: 00:00:09
                               ETA: 05:15:37

################################################################################
                      [1m Learning iteration 1/2000 [0m                       

                       Computation: 14639 steps/s (collection: 6.567s, learning 0.149s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 31.3806
                       Mean reward: 0.00
               Mean episode length: 45.00
    Episode_Reward/reaching_object: 0.0018
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0006
          Episode_Reward/joint_vel: -0.0008
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 6.72s
                      Time elapsed: 00:00:16
                               ETA: 04:29:35

################################################################################
                      [1m Learning iteration 2/2000 [0m                       

                       Computation: 14810 steps/s (collection: 6.502s, learning 0.136s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 31.4485
                       Mean reward: 0.01
               Mean episode length: 69.37
    Episode_Reward/reaching_object: 0.0030
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0011
          Episode_Reward/joint_vel: -0.0014
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 6.64s
                      Time elapsed: 00:00:22
                               ETA: 04:13:19

################################################################################
                      [1m Learning iteration 3/2000 [0m                       

                       Computation: 14568 steps/s (collection: 6.578s, learning 0.170s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 31.4372
                       Mean reward: 0.01
               Mean episode length: 93.77
    Episode_Reward/reaching_object: 0.0044
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0015
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 6.75s
                      Time elapsed: 00:00:29
                               ETA: 04:06:02

################################################################################
                      [1m Learning iteration 4/2000 [0m                       

                       Computation: 14070 steps/s (collection: 6.841s, learning 0.146s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 31.4322
                       Mean reward: 0.02
               Mean episode length: 117.34
    Episode_Reward/reaching_object: 0.0062
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 6.99s
                      Time elapsed: 00:00:36
                               ETA: 04:03:12

################################################################################
                      [1m Learning iteration 5/2000 [0m                       

                       Computation: 14120 steps/s (collection: 6.817s, learning 0.145s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 31.4454
                       Mean reward: 0.02
               Mean episode length: 141.43
    Episode_Reward/reaching_object: 0.0082
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0030
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 6.96s
                      Time elapsed: 00:00:43
                               ETA: 04:01:09

################################################################################
                      [1m Learning iteration 6/2000 [0m                       

                       Computation: 14361 steps/s (collection: 6.697s, learning 0.148s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 31.4820
                       Mean reward: 0.04
               Mean episode length: 165.01
    Episode_Reward/reaching_object: 0.0114
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0036
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 6.85s
                      Time elapsed: 00:00:50
                               ETA: 03:59:06

################################################################################
                      [1m Learning iteration 7/2000 [0m                       

                       Computation: 14501 steps/s (collection: 6.636s, learning 0.143s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 31.4706
                       Mean reward: 0.05
               Mean episode length: 189.52
    Episode_Reward/reaching_object: 0.0137
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0041
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 6.78s
                      Time elapsed: 00:00:57
                               ETA: 03:57:15

################################################################################
                      [1m Learning iteration 8/2000 [0m                       

                       Computation: 18016 steps/s (collection: 5.363s, learning 0.094s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 31.4757
                       Mean reward: 0.07
               Mean episode length: 213.81
    Episode_Reward/reaching_object: 0.0181
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0046
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 5.46s
                      Time elapsed: 00:01:02
                               ETA: 03:50:54

################################################################################
                      [1m Learning iteration 9/2000 [0m                       

                       Computation: 58545 steps/s (collection: 1.573s, learning 0.107s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 31.5049
                       Mean reward: 0.09
               Mean episode length: 237.09
    Episode_Reward/reaching_object: 0.0227
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0052
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 1.68s
                      Time elapsed: 00:01:04
                               ETA: 03:33:17

################################################################################
                      [1m Learning iteration 10/2000 [0m                      

                       Computation: 60092 steps/s (collection: 1.544s, learning 0.092s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0069
                 Mean entropy loss: 31.5384
                       Mean reward: 0.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0286
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0056
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 1.64s
                      Time elapsed: 00:01:05
                               ETA: 03:18:44

################################################################################
                      [1m Learning iteration 11/2000 [0m                      

                       Computation: 60589 steps/s (collection: 1.534s, learning 0.089s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 31.5397
                       Mean reward: 0.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0324
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0057
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 1.62s
                      Time elapsed: 00:01:07
                               ETA: 03:06:33

################################################################################
                      [1m Learning iteration 12/2000 [0m                      

                       Computation: 61192 steps/s (collection: 1.521s, learning 0.085s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 31.5502
                       Mean reward: 0.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0427
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0056
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 1.61s
                      Time elapsed: 00:01:09
                               ETA: 02:56:13

################################################################################
                      [1m Learning iteration 13/2000 [0m                      

                       Computation: 57466 steps/s (collection: 1.563s, learning 0.148s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 31.5769
                       Mean reward: 0.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0571
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0056
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 1.71s
                      Time elapsed: 00:01:10
                               ETA: 02:47:35

################################################################################
                      [1m Learning iteration 14/2000 [0m                      

                       Computation: 56061 steps/s (collection: 1.627s, learning 0.127s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.1236
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 31.6693
                       Mean reward: -0.08
               Mean episode length: 248.93
    Episode_Reward/reaching_object: 0.0682
     Episode_Reward/lifting_object: -0.0175
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0056
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 1.75s
                      Time elapsed: 00:01:12
                               ETA: 02:40:12

################################################################################
                      [1m Learning iteration 15/2000 [0m                      

                       Computation: 54892 steps/s (collection: 1.680s, learning 0.111s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.1544
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 31.7573
                       Mean reward: 0.45
               Mean episode length: 246.03
    Episode_Reward/reaching_object: 0.0908
     Episode_Reward/lifting_object: -0.0501
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0056
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 1.79s
                      Time elapsed: 00:01:14
                               ETA: 02:33:49

################################################################################
                      [1m Learning iteration 16/2000 [0m                      

                       Computation: 56190 steps/s (collection: 1.650s, learning 0.100s)
             Mean action noise std: 1.04
          Mean value_function loss: 1.3709
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 31.9695
                       Mean reward: 0.40
               Mean episode length: 244.23
    Episode_Reward/reaching_object: 0.1171
     Episode_Reward/lifting_object: -0.0904
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0055
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 1.75s
                      Time elapsed: 00:01:16
                               ETA: 02:28:06

################################################################################
                      [1m Learning iteration 17/2000 [0m                      

                       Computation: 52161 steps/s (collection: 1.719s, learning 0.165s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.0733
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 32.1230
                       Mean reward: 0.66
               Mean episode length: 245.35
    Episode_Reward/reaching_object: 0.1339
     Episode_Reward/lifting_object: -0.1392
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0055
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 1.88s
                      Time elapsed: 00:01:18
                               ETA: 02:23:16

################################################################################
                      [1m Learning iteration 18/2000 [0m                      

                       Computation: 51879 steps/s (collection: 1.762s, learning 0.133s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.1375
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 32.5212
                       Mean reward: 0.82
               Mean episode length: 240.49
    Episode_Reward/reaching_object: 0.1584
     Episode_Reward/lifting_object: -0.0546
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0055
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 1.89s
                      Time elapsed: 00:01:19
                               ETA: 02:18:57

################################################################################
                      [1m Learning iteration 19/2000 [0m                      

                       Computation: 51846 steps/s (collection: 1.773s, learning 0.124s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.6799
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 32.6329
                       Mean reward: 0.01
               Mean episode length: 238.12
    Episode_Reward/reaching_object: 0.1724
     Episode_Reward/lifting_object: -0.0881
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0055
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 1.90s
                      Time elapsed: 00:01:21
                               ETA: 02:15:04

################################################################################
                      [1m Learning iteration 20/2000 [0m                      

                       Computation: 53475 steps/s (collection: 1.722s, learning 0.117s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.1124
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 32.8116
                       Mean reward: 0.98
               Mean episode length: 233.75
    Episode_Reward/reaching_object: 0.1916
     Episode_Reward/lifting_object: -0.0415
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0055
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 1.84s
                      Time elapsed: 00:01:23
                               ETA: 02:11:27

################################################################################
                      [1m Learning iteration 21/2000 [0m                      

                       Computation: 52933 steps/s (collection: 1.771s, learning 0.087s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.1585
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 32.9481
                       Mean reward: 0.89
               Mean episode length: 238.52
    Episode_Reward/reaching_object: 0.1977
     Episode_Reward/lifting_object: -0.0149
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0056
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 1.86s
                      Time elapsed: 00:01:25
                               ETA: 02:08:12

################################################################################
                      [1m Learning iteration 22/2000 [0m                      

                       Computation: 53896 steps/s (collection: 1.715s, learning 0.109s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 33.0566
                       Mean reward: 0.83
               Mean episode length: 237.30
    Episode_Reward/reaching_object: 0.2057
     Episode_Reward/lifting_object: -0.0250
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0057
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 1.82s
                      Time elapsed: 00:01:27
                               ETA: 02:05:11

################################################################################
                      [1m Learning iteration 23/2000 [0m                      

                       Computation: 53879 steps/s (collection: 1.736s, learning 0.088s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.3300
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 33.1868
                       Mean reward: 0.47
               Mean episode length: 238.77
    Episode_Reward/reaching_object: 0.2113
     Episode_Reward/lifting_object: -0.0278
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0059
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 1.82s
                      Time elapsed: 00:01:29
                               ETA: 02:02:24

################################################################################
                      [1m Learning iteration 24/2000 [0m                      

                       Computation: 52120 steps/s (collection: 1.788s, learning 0.098s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 33.2554
                       Mean reward: 0.96
               Mean episode length: 238.29
    Episode_Reward/reaching_object: 0.1988
     Episode_Reward/lifting_object: -0.0208
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0060
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 1.89s
                      Time elapsed: 00:01:31
                               ETA: 01:59:56

################################################################################
                      [1m Learning iteration 25/2000 [0m                      

                       Computation: 53120 steps/s (collection: 1.691s, learning 0.159s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.4120
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 33.4351
                       Mean reward: 0.99
               Mean episode length: 240.17
    Episode_Reward/reaching_object: 0.2046
     Episode_Reward/lifting_object: -0.0630
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 1.85s
                      Time elapsed: 00:01:32
                               ETA: 01:57:36

################################################################################
                      [1m Learning iteration 26/2000 [0m                      

                       Computation: 54935 steps/s (collection: 1.700s, learning 0.089s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 33.4912
                       Mean reward: 0.90
               Mean episode length: 243.58
    Episode_Reward/reaching_object: 0.1901
     Episode_Reward/lifting_object: -0.0222
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0063
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 1.79s
                      Time elapsed: 00:01:34
                               ETA: 01:55:22

################################################################################
                      [1m Learning iteration 27/2000 [0m                      

                       Computation: 52654 steps/s (collection: 1.780s, learning 0.087s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 33.6479
                       Mean reward: 0.87
               Mean episode length: 246.07
    Episode_Reward/reaching_object: 0.1941
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0064
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 1.87s
                      Time elapsed: 00:01:36
                               ETA: 01:53:23

################################################################################
                      [1m Learning iteration 28/2000 [0m                      

                       Computation: 52592 steps/s (collection: 1.733s, learning 0.136s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.2119
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 33.6798
                       Mean reward: 1.01
               Mean episode length: 244.80
    Episode_Reward/reaching_object: 0.2060
     Episode_Reward/lifting_object: -0.0263
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0065
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 1.87s
                      Time elapsed: 00:01:38
                               ETA: 01:51:33

################################################################################
                      [1m Learning iteration 29/2000 [0m                      

                       Computation: 50874 steps/s (collection: 1.801s, learning 0.131s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 33.7128
                       Mean reward: 0.80
               Mean episode length: 247.66
    Episode_Reward/reaching_object: 0.1871
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0067
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 1.93s
                      Time elapsed: 00:01:40
                               ETA: 01:49:53

################################################################################
                      [1m Learning iteration 30/2000 [0m                      

                       Computation: 53909 steps/s (collection: 1.723s, learning 0.100s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.0012
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 33.8170
                       Mean reward: 0.88
               Mean episode length: 245.44
    Episode_Reward/reaching_object: 0.1834
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0067
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 1.82s
                      Time elapsed: 00:01:42
                               ETA: 01:48:13

################################################################################
                      [1m Learning iteration 31/2000 [0m                      

                       Computation: 53008 steps/s (collection: 1.742s, learning 0.113s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 33.8788
                       Mean reward: 0.90
               Mean episode length: 245.03
    Episode_Reward/reaching_object: 0.1926
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 1.85s
                      Time elapsed: 00:01:44
                               ETA: 01:46:41

################################################################################
                      [1m Learning iteration 32/2000 [0m                      

                       Computation: 51332 steps/s (collection: 1.788s, learning 0.127s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 33.9621
                       Mean reward: 0.98
               Mean episode length: 242.94
    Episode_Reward/reaching_object: 0.1905
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 1.92s
                      Time elapsed: 00:01:45
                               ETA: 01:45:18

################################################################################
                      [1m Learning iteration 33/2000 [0m                      

                       Computation: 54253 steps/s (collection: 1.722s, learning 0.090s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0079
                 Mean entropy loss: 34.0110
                       Mean reward: 0.99
               Mean episode length: 239.84
    Episode_Reward/reaching_object: 0.2049
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 1.81s
                      Time elapsed: 00:01:47
                               ETA: 01:43:54

################################################################################
                      [1m Learning iteration 34/2000 [0m                      

                       Computation: 52346 steps/s (collection: 1.792s, learning 0.086s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.6868
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.0354
                       Mean reward: -0.72
               Mean episode length: 234.02
    Episode_Reward/reaching_object: 0.2076
     Episode_Reward/lifting_object: -0.1030
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 1.88s
                      Time elapsed: 00:01:49
                               ETA: 01:42:38

################################################################################
                      [1m Learning iteration 35/2000 [0m                      

                       Computation: 50129 steps/s (collection: 1.872s, learning 0.089s)
             Mean action noise std: 1.14
          Mean value_function loss: 2.5616
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 34.0765
                       Mean reward: -0.14
               Mean episode length: 225.62
    Episode_Reward/reaching_object: 0.2136
     Episode_Reward/lifting_object: -0.1342
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0067
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 1.96s
                      Time elapsed: 00:01:51
                               ETA: 01:41:31

################################################################################
                      [1m Learning iteration 36/2000 [0m                      

                       Computation: 52680 steps/s (collection: 1.773s, learning 0.093s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.1439
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 34.2062
                       Mean reward: 0.68
               Mean episode length: 218.11
    Episode_Reward/reaching_object: 0.2310
     Episode_Reward/lifting_object: -0.0223
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0066
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.4583
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 1.87s
                      Time elapsed: 00:01:53
                               ETA: 01:40:23

################################################################################
                      [1m Learning iteration 37/2000 [0m                      

                       Computation: 52978 steps/s (collection: 1.761s, learning 0.095s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.1598
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.3369
                       Mean reward: 1.11
               Mean episode length: 208.72
    Episode_Reward/reaching_object: 0.2367
     Episode_Reward/lifting_object: -0.0121
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0064
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 20.2083
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 1.86s
                      Time elapsed: 00:01:55
                               ETA: 01:39:17

################################################################################
                      [1m Learning iteration 38/2000 [0m                      

                       Computation: 52482 steps/s (collection: 1.771s, learning 0.103s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.0917
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.3980
                       Mean reward: 1.26
               Mean episode length: 204.18
    Episode_Reward/reaching_object: 0.2473
     Episode_Reward/lifting_object: -0.0426
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0062
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 19.8750
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 1.87s
                      Time elapsed: 00:01:57
                               ETA: 01:38:15

################################################################################
                      [1m Learning iteration 39/2000 [0m                      

                       Computation: 51550 steps/s (collection: 1.768s, learning 0.139s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 34.4837
                       Mean reward: 1.32
               Mean episode length: 207.71
    Episode_Reward/reaching_object: 0.2590
     Episode_Reward/lifting_object: -0.0076
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0062
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.7917
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 1.91s
                      Time elapsed: 00:01:59
                               ETA: 01:37:19

################################################################################
                      [1m Learning iteration 40/2000 [0m                      

                       Computation: 50840 steps/s (collection: 1.790s, learning 0.144s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.2367
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 34.5752
                       Mean reward: 1.01
               Mean episode length: 198.53
    Episode_Reward/reaching_object: 0.2719
     Episode_Reward/lifting_object: -0.0627
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 1.93s
                      Time elapsed: 00:02:01
                               ETA: 01:36:26

################################################################################
                      [1m Learning iteration 41/2000 [0m                      

                       Computation: 53121 steps/s (collection: 1.759s, learning 0.092s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.1423
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.6154
                       Mean reward: 0.78
               Mean episode length: 210.23
    Episode_Reward/reaching_object: 0.2906
     Episode_Reward/lifting_object: -0.0350
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0064
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 1.85s
                      Time elapsed: 00:02:02
                               ETA: 01:35:31

################################################################################
                      [1m Learning iteration 42/2000 [0m                      

                       Computation: 52623 steps/s (collection: 1.768s, learning 0.100s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0720
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 34.7111
                       Mean reward: 1.54
               Mean episode length: 210.37
    Episode_Reward/reaching_object: 0.3050
     Episode_Reward/lifting_object: -0.0417
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0064
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 1.87s
                      Time elapsed: 00:02:04
                               ETA: 01:34:40

################################################################################
                      [1m Learning iteration 43/2000 [0m                      

                       Computation: 52326 steps/s (collection: 1.782s, learning 0.097s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.2505
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 34.9154
                       Mean reward: 0.96
               Mean episode length: 216.25
    Episode_Reward/reaching_object: 0.3149
     Episode_Reward/lifting_object: -0.0385
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0067
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 1.88s
                      Time elapsed: 00:02:06
                               ETA: 01:33:52

################################################################################
                      [1m Learning iteration 44/2000 [0m                      

                       Computation: 50773 steps/s (collection: 1.830s, learning 0.107s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0468
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 34.9724
                       Mean reward: 1.75
               Mean episode length: 220.15
    Episode_Reward/reaching_object: 0.3221
     Episode_Reward/lifting_object: -0.0199
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 1.94s
                      Time elapsed: 00:02:08
                               ETA: 01:33:08

################################################################################
                      [1m Learning iteration 45/2000 [0m                      

                       Computation: 51455 steps/s (collection: 1.817s, learning 0.094s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.1290
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.0959
                       Mean reward: 1.51
               Mean episode length: 217.81
    Episode_Reward/reaching_object: 0.3165
     Episode_Reward/lifting_object: -0.0316
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0070
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 1.91s
                      Time elapsed: 00:02:10
                               ETA: 01:32:25

################################################################################
                      [1m Learning iteration 46/2000 [0m                      

                       Computation: 52239 steps/s (collection: 1.774s, learning 0.108s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.1351
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.1478
                       Mean reward: 1.13
               Mean episode length: 229.97
    Episode_Reward/reaching_object: 0.3343
     Episode_Reward/lifting_object: -0.0779
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0072
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 1.88s
                      Time elapsed: 00:02:12
                               ETA: 01:31:42

################################################################################
                      [1m Learning iteration 47/2000 [0m                      

                       Computation: 52348 steps/s (collection: 1.782s, learning 0.096s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.4887
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 35.2217
                       Mean reward: 1.22
               Mean episode length: 226.80
    Episode_Reward/reaching_object: 0.3356
     Episode_Reward/lifting_object: -0.0496
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 1.88s
                      Time elapsed: 00:02:14
                               ETA: 01:31:01

################################################################################
                      [1m Learning iteration 48/2000 [0m                      

                       Computation: 49191 steps/s (collection: 1.878s, learning 0.120s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0309
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.2730
                       Mean reward: 1.62
               Mean episode length: 230.55
    Episode_Reward/reaching_object: 0.3268
     Episode_Reward/lifting_object: -0.0076
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 2.00s
                      Time elapsed: 00:02:16
                               ETA: 01:30:27

################################################################################
                      [1m Learning iteration 49/2000 [0m                      

                       Computation: 52145 steps/s (collection: 1.783s, learning 0.102s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0535
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 35.3926
                       Mean reward: 1.47
               Mean episode length: 234.48
    Episode_Reward/reaching_object: 0.3454
     Episode_Reward/lifting_object: -0.0380
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 1.89s
                      Time elapsed: 00:02:18
                               ETA: 01:29:49

################################################################################
                      [1m Learning iteration 50/2000 [0m                      

                       Computation: 52959 steps/s (collection: 1.756s, learning 0.100s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.3143
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.4762
                       Mean reward: 1.66
               Mean episode length: 229.77
    Episode_Reward/reaching_object: 0.3596
     Episode_Reward/lifting_object: -0.0444
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 1.86s
                      Time elapsed: 00:02:19
                               ETA: 01:29:12

################################################################################
                      [1m Learning iteration 51/2000 [0m                      

                       Computation: 53002 steps/s (collection: 1.760s, learning 0.095s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0346
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 35.5137
                       Mean reward: 1.38
               Mean episode length: 222.54
    Episode_Reward/reaching_object: 0.3317
     Episode_Reward/lifting_object: -0.0199
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 1.85s
                      Time elapsed: 00:02:21
                               ETA: 01:28:36

################################################################################
                      [1m Learning iteration 52/2000 [0m                      

                       Computation: 51590 steps/s (collection: 1.818s, learning 0.087s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.1045
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 35.6067
                       Mean reward: 1.58
               Mean episode length: 210.89
    Episode_Reward/reaching_object: 0.3391
     Episode_Reward/lifting_object: -0.0227
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 1.91s
                      Time elapsed: 00:02:23
                               ETA: 01:28:03

################################################################################
                      [1m Learning iteration 53/2000 [0m                      

                       Computation: 51587 steps/s (collection: 1.801s, learning 0.105s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.1209
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.7141
                       Mean reward: 1.48
               Mean episode length: 212.12
    Episode_Reward/reaching_object: 0.3241
     Episode_Reward/lifting_object: -0.0501
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0072
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 1.91s
                      Time elapsed: 00:02:25
                               ETA: 01:27:31

################################################################################
                      [1m Learning iteration 54/2000 [0m                      

                       Computation: 50722 steps/s (collection: 1.840s, learning 0.099s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.5498
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 35.8596
                       Mean reward: 1.75
               Mean episode length: 212.38
    Episode_Reward/reaching_object: 0.3307
     Episode_Reward/lifting_object: -0.0919
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0072
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 1.94s
                      Time elapsed: 00:02:27
                               ETA: 01:27:01

################################################################################
                      [1m Learning iteration 55/2000 [0m                      

                       Computation: 50087 steps/s (collection: 1.802s, learning 0.160s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.1716
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.8954
                       Mean reward: 1.81
               Mean episode length: 223.78
    Episode_Reward/reaching_object: 0.3542
     Episode_Reward/lifting_object: -0.0196
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 1.96s
                      Time elapsed: 00:02:29
                               ETA: 01:26:34

################################################################################
                      [1m Learning iteration 56/2000 [0m                      

                       Computation: 47693 steps/s (collection: 1.886s, learning 0.175s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0357
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 36.0445
                       Mean reward: 1.73
               Mean episode length: 219.12
    Episode_Reward/reaching_object: 0.3797
     Episode_Reward/lifting_object: -0.0180
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 2.06s
                      Time elapsed: 00:02:31
                               ETA: 01:26:10

################################################################################
                      [1m Learning iteration 57/2000 [0m                      

                       Computation: 51274 steps/s (collection: 1.823s, learning 0.095s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.1586
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.1993
                       Mean reward: 1.31
               Mean episode length: 231.89
    Episode_Reward/reaching_object: 0.4086
     Episode_Reward/lifting_object: -0.1144
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 1.92s
                      Time elapsed: 00:02:33
                               ETA: 01:25:43

################################################################################
                      [1m Learning iteration 58/2000 [0m                      

                       Computation: 51406 steps/s (collection: 1.828s, learning 0.084s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.1665
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 36.2508
                       Mean reward: 2.28
               Mean episode length: 234.49
    Episode_Reward/reaching_object: 0.4453
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0083
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 1.91s
                      Time elapsed: 00:02:35
                               ETA: 01:25:16

################################################################################
                      [1m Learning iteration 59/2000 [0m                      

                       Computation: 52188 steps/s (collection: 1.778s, learning 0.106s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0304
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 36.3759
                       Mean reward: 2.28
               Mean episode length: 237.37
    Episode_Reward/reaching_object: 0.4504
     Episode_Reward/lifting_object: -0.0263
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0084
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 1.88s
                      Time elapsed: 00:02:37
                               ETA: 01:24:49

################################################################################
                      [1m Learning iteration 60/2000 [0m                      

                       Computation: 49571 steps/s (collection: 1.878s, learning 0.105s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.4214
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.4575
                       Mean reward: 2.06
               Mean episode length: 238.06
    Episode_Reward/reaching_object: 0.4496
     Episode_Reward/lifting_object: -0.0359
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0085
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 1.98s
                      Time elapsed: 00:02:39
                               ETA: 01:24:26

################################################################################
                      [1m Learning iteration 61/2000 [0m                      

                       Computation: 49949 steps/s (collection: 1.866s, learning 0.102s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.1913
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.4832
                       Mean reward: 1.99
               Mean episode length: 237.18
    Episode_Reward/reaching_object: 0.4927
     Episode_Reward/lifting_object: -0.0626
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 1.97s
                      Time elapsed: 00:02:41
                               ETA: 01:24:03

################################################################################
                      [1m Learning iteration 62/2000 [0m                      

                       Computation: 51014 steps/s (collection: 1.784s, learning 0.143s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.1002
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.5810
                       Mean reward: 2.43
               Mean episode length: 234.31
    Episode_Reward/reaching_object: 0.4839
     Episode_Reward/lifting_object: -0.0320
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 1.93s
                      Time elapsed: 00:02:43
                               ETA: 01:23:40

################################################################################
                      [1m Learning iteration 63/2000 [0m                      

                       Computation: 50763 steps/s (collection: 1.815s, learning 0.121s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0044
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 36.6595
                       Mean reward: 2.42
               Mean episode length: 232.04
    Episode_Reward/reaching_object: 0.4829
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 1.94s
                      Time elapsed: 00:02:45
                               ETA: 01:23:17

################################################################################
                      [1m Learning iteration 64/2000 [0m                      

                       Computation: 49461 steps/s (collection: 1.834s, learning 0.153s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.3306
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 36.6841
                       Mean reward: 2.49
               Mean episode length: 241.03
    Episode_Reward/reaching_object: 0.5131
     Episode_Reward/lifting_object: -0.0444
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 1.99s
                      Time elapsed: 00:02:47
                               ETA: 01:22:57

################################################################################
                      [1m Learning iteration 65/2000 [0m                      

                       Computation: 52633 steps/s (collection: 1.782s, learning 0.086s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0307
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 36.6959
                       Mean reward: 2.71
               Mean episode length: 243.39
    Episode_Reward/reaching_object: 0.5429
     Episode_Reward/lifting_object: -0.0063
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 1.87s
                      Time elapsed: 00:02:48
                               ETA: 01:22:34

################################################################################
                      [1m Learning iteration 66/2000 [0m                      

                       Computation: 51796 steps/s (collection: 1.799s, learning 0.099s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.1050
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 36.7310
                       Mean reward: 2.35
               Mean episode length: 243.55
    Episode_Reward/reaching_object: 0.5427
     Episode_Reward/lifting_object: -0.0159
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 1.90s
                      Time elapsed: 00:02:50
                               ETA: 01:22:12

################################################################################
                      [1m Learning iteration 67/2000 [0m                      

                       Computation: 49360 steps/s (collection: 1.904s, learning 0.088s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.1170
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 36.7973
                       Mean reward: 2.21
               Mean episode length: 229.78
    Episode_Reward/reaching_object: 0.5351
     Episode_Reward/lifting_object: -0.0206
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 1.99s
                      Time elapsed: 00:02:52
                               ETA: 01:21:54

################################################################################
                      [1m Learning iteration 68/2000 [0m                      

                       Computation: 49617 steps/s (collection: 1.878s, learning 0.103s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.7303
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 36.8963
                       Mean reward: 2.43
               Mean episode length: 226.38
    Episode_Reward/reaching_object: 0.5464
     Episode_Reward/lifting_object: -0.0400
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 1.98s
                      Time elapsed: 00:02:54
                               ETA: 01:21:36

################################################################################
                      [1m Learning iteration 69/2000 [0m                      

                       Computation: 49973 steps/s (collection: 1.845s, learning 0.122s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.5260
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 36.9224
                       Mean reward: 2.49
               Mean episode length: 227.34
    Episode_Reward/reaching_object: 0.5622
     Episode_Reward/lifting_object: -0.1104
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 1.97s
                      Time elapsed: 00:02:56
                               ETA: 01:21:17

################################################################################
                      [1m Learning iteration 70/2000 [0m                      

                       Computation: 51060 steps/s (collection: 1.831s, learning 0.094s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.4167
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.9950
                       Mean reward: 2.67
               Mean episode length: 227.02
    Episode_Reward/reaching_object: 0.5598
     Episode_Reward/lifting_object: -0.0900
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 1.93s
                      Time elapsed: 00:02:58
                               ETA: 01:20:59

################################################################################
                      [1m Learning iteration 71/2000 [0m                      

                       Computation: 52414 steps/s (collection: 1.779s, learning 0.096s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0469
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 37.0648
                       Mean reward: 2.84
               Mean episode length: 224.52
    Episode_Reward/reaching_object: 0.5768
     Episode_Reward/lifting_object: -0.0338
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 1.88s
                      Time elapsed: 00:03:00
                               ETA: 01:20:39

################################################################################
                      [1m Learning iteration 72/2000 [0m                      

                       Computation: 52447 steps/s (collection: 1.790s, learning 0.085s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.1697
                       Mean reward: 3.06
               Mean episode length: 230.14
    Episode_Reward/reaching_object: 0.6086
     Episode_Reward/lifting_object: -0.0665
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 1.87s
                      Time elapsed: 00:03:02
                               ETA: 01:20:20

################################################################################
                      [1m Learning iteration 73/2000 [0m                      

                       Computation: 51386 steps/s (collection: 1.806s, learning 0.107s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 37.2226
                       Mean reward: 3.27
               Mean episode length: 238.36
    Episode_Reward/reaching_object: 0.6400
     Episode_Reward/lifting_object: -0.0042
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 1.91s
                      Time elapsed: 00:03:04
                               ETA: 01:20:02

################################################################################
                      [1m Learning iteration 74/2000 [0m                      

                       Computation: 51976 steps/s (collection: 1.780s, learning 0.112s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 37.2503
                       Mean reward: 2.85
               Mean episode length: 231.11
    Episode_Reward/reaching_object: 0.6095
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 1.89s
                      Time elapsed: 00:03:06
                               ETA: 01:19:44

################################################################################
                      [1m Learning iteration 75/2000 [0m                      

                       Computation: 50960 steps/s (collection: 1.823s, learning 0.106s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0114
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 37.3245
                       Mean reward: 2.99
               Mean episode length: 224.04
    Episode_Reward/reaching_object: 0.6050
     Episode_Reward/lifting_object: 0.0042
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 1.93s
                      Time elapsed: 00:03:08
                               ETA: 01:19:27

################################################################################
                      [1m Learning iteration 76/2000 [0m                      

                       Computation: 50551 steps/s (collection: 1.833s, learning 0.112s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0361
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.3953
                       Mean reward: 3.19
               Mean episode length: 235.52
    Episode_Reward/reaching_object: 0.6491
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 1.94s
                      Time elapsed: 00:03:10
                               ETA: 01:19:12

################################################################################
                      [1m Learning iteration 77/2000 [0m                      

                       Computation: 50465 steps/s (collection: 1.815s, learning 0.133s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.7829
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 37.4504
                       Mean reward: 2.83
               Mean episode length: 227.65
    Episode_Reward/reaching_object: 0.6353
     Episode_Reward/lifting_object: -0.0904
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 1.95s
                      Time elapsed: 00:03:12
                               ETA: 01:18:56

################################################################################
                      [1m Learning iteration 78/2000 [0m                      

                       Computation: 51938 steps/s (collection: 1.801s, learning 0.092s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.4362
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 37.4720
                       Mean reward: 2.94
               Mean episode length: 232.98
    Episode_Reward/reaching_object: 0.6712
     Episode_Reward/lifting_object: -0.0431
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 1.89s
                      Time elapsed: 00:03:14
                               ETA: 01:18:40

################################################################################
                      [1m Learning iteration 79/2000 [0m                      

                       Computation: 49818 steps/s (collection: 1.835s, learning 0.139s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0338
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 37.5003
                       Mean reward: 3.29
               Mean episode length: 240.63
    Episode_Reward/reaching_object: 0.7152
     Episode_Reward/lifting_object: -0.0440
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 1.97s
                      Time elapsed: 00:03:15
                               ETA: 01:18:26

################################################################################
                      [1m Learning iteration 80/2000 [0m                      

                       Computation: 43408 steps/s (collection: 2.150s, learning 0.115s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.1912
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 37.5519
                       Mean reward: 3.17
               Mean episode length: 238.61
    Episode_Reward/reaching_object: 0.7165
     Episode_Reward/lifting_object: -0.0485
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 2.26s
                      Time elapsed: 00:03:18
                               ETA: 01:18:19

################################################################################
                      [1m Learning iteration 81/2000 [0m                      

                       Computation: 42518 steps/s (collection: 2.212s, learning 0.100s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0557
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.6147
                       Mean reward: 3.27
               Mean episode length: 232.53
    Episode_Reward/reaching_object: 0.6862
     Episode_Reward/lifting_object: 0.0095
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 2.31s
                      Time elapsed: 00:03:20
                               ETA: 01:18:13

################################################################################
                      [1m Learning iteration 82/2000 [0m                      

                       Computation: 47048 steps/s (collection: 1.965s, learning 0.124s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.2865
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.7009
                       Mean reward: 3.49
               Mean episode length: 240.11
    Episode_Reward/reaching_object: 0.7231
     Episode_Reward/lifting_object: -0.0239
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 2.09s
                      Time elapsed: 00:03:22
                               ETA: 01:18:03

################################################################################
                      [1m Learning iteration 83/2000 [0m                      

                       Computation: 49687 steps/s (collection: 1.853s, learning 0.126s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0905
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 37.7635
                       Mean reward: 3.52
               Mean episode length: 237.20
    Episode_Reward/reaching_object: 0.7328
     Episode_Reward/lifting_object: -0.0303
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 1.98s
                      Time elapsed: 00:03:24
                               ETA: 01:17:50

################################################################################
                      [1m Learning iteration 84/2000 [0m                      

                       Computation: 48769 steps/s (collection: 1.915s, learning 0.101s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0569
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.8950
                       Mean reward: 3.50
               Mean episode length: 234.74
    Episode_Reward/reaching_object: 0.7730
     Episode_Reward/lifting_object: -0.0704
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 2.02s
                      Time elapsed: 00:03:26
                               ETA: 01:17:38

################################################################################
                      [1m Learning iteration 85/2000 [0m                      

                       Computation: 48641 steps/s (collection: 1.898s, learning 0.123s)
             Mean action noise std: 1.36
          Mean value_function loss: 1.5093
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.9960
                       Mean reward: 3.58
               Mean episode length: 229.74
    Episode_Reward/reaching_object: 0.7305
     Episode_Reward/lifting_object: -0.1079
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 2.02s
                      Time elapsed: 00:03:28
                               ETA: 01:17:26

################################################################################
                      [1m Learning iteration 86/2000 [0m                      

                       Computation: 48308 steps/s (collection: 1.937s, learning 0.098s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0278
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.0131
                       Mean reward: 3.89
               Mean episode length: 244.91
    Episode_Reward/reaching_object: 0.7919
     Episode_Reward/lifting_object: -0.0062
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 2.03s
                      Time elapsed: 00:03:30
                               ETA: 01:17:15

################################################################################
                      [1m Learning iteration 87/2000 [0m                      

                       Computation: 49214 steps/s (collection: 1.861s, learning 0.137s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0281
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 38.0713
                       Mean reward: 3.96
               Mean episode length: 242.47
    Episode_Reward/reaching_object: 0.7834
     Episode_Reward/lifting_object: -0.0710
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 2.00s
                      Time elapsed: 00:03:32
                               ETA: 01:17:04

################################################################################
                      [1m Learning iteration 88/2000 [0m                      

                       Computation: 49999 steps/s (collection: 1.874s, learning 0.092s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.0364
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.1552
                       Mean reward: 3.37
               Mean episode length: 241.18
    Episode_Reward/reaching_object: 0.8228
     Episode_Reward/lifting_object: -0.0576
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 1.97s
                      Time elapsed: 00:03:34
                               ETA: 01:16:51

################################################################################
                      [1m Learning iteration 89/2000 [0m                      

                       Computation: 49776 steps/s (collection: 1.885s, learning 0.090s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.2217
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.2185
                       Mean reward: 3.75
               Mean episode length: 238.04
    Episode_Reward/reaching_object: 0.8143
     Episode_Reward/lifting_object: -0.0227
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 1.97s
                      Time elapsed: 00:03:36
                               ETA: 01:16:40

################################################################################
                      [1m Learning iteration 90/2000 [0m                      

                       Computation: 45488 steps/s (collection: 2.069s, learning 0.092s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.1330
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 38.2422
                       Mean reward: 3.90
               Mean episode length: 235.05
    Episode_Reward/reaching_object: 0.8092
     Episode_Reward/lifting_object: -0.0291
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 2.16s
                      Time elapsed: 00:03:38
                               ETA: 01:16:32

################################################################################
                      [1m Learning iteration 91/2000 [0m                      

                       Computation: 49031 steps/s (collection: 1.914s, learning 0.091s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.3274
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.3084
                       Mean reward: 3.77
               Mean episode length: 235.33
    Episode_Reward/reaching_object: 0.7987
     Episode_Reward/lifting_object: -0.0356
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 2.00s
                      Time elapsed: 00:03:40
                               ETA: 01:16:21

################################################################################
                      [1m Learning iteration 92/2000 [0m                      

                       Computation: 49014 steps/s (collection: 1.919s, learning 0.086s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.4013
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.3871
                       Mean reward: 4.17
               Mean episode length: 235.41
    Episode_Reward/reaching_object: 0.8272
     Episode_Reward/lifting_object: 0.0178
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 2.01s
                      Time elapsed: 00:03:42
                               ETA: 01:16:11

################################################################################
                      [1m Learning iteration 93/2000 [0m                      

                       Computation: 50184 steps/s (collection: 1.874s, learning 0.085s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0317
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.4230
                       Mean reward: 4.28
               Mean episode length: 247.06
    Episode_Reward/reaching_object: 0.8282
     Episode_Reward/lifting_object: -0.0091
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 1.96s
                      Time elapsed: 00:03:44
                               ETA: 01:16:00

################################################################################
                      [1m Learning iteration 94/2000 [0m                      

                       Computation: 50914 steps/s (collection: 1.845s, learning 0.086s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.3332
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 38.4698
                       Mean reward: 4.10
               Mean episode length: 238.62
    Episode_Reward/reaching_object: 0.8388
     Episode_Reward/lifting_object: -0.0145
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 1.93s
                      Time elapsed: 00:03:46
                               ETA: 01:15:48

################################################################################
                      [1m Learning iteration 95/2000 [0m                      

                       Computation: 49824 steps/s (collection: 1.888s, learning 0.085s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.1477
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.4857
                       Mean reward: 4.17
               Mean episode length: 243.17
    Episode_Reward/reaching_object: 0.8519
     Episode_Reward/lifting_object: -0.0423
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 1.97s
                      Time elapsed: 00:03:48
                               ETA: 01:15:37

################################################################################
                      [1m Learning iteration 96/2000 [0m                      

                       Computation: 50427 steps/s (collection: 1.851s, learning 0.099s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.2831
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.5249
                       Mean reward: 4.06
               Mean episode length: 238.01
    Episode_Reward/reaching_object: 0.8600
     Episode_Reward/lifting_object: 0.0069
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 1.95s
                      Time elapsed: 00:03:50
                               ETA: 01:15:27

################################################################################
                      [1m Learning iteration 97/2000 [0m                      

                       Computation: 50833 steps/s (collection: 1.840s, learning 0.094s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0121
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 38.5604
                       Mean reward: 4.75
               Mean episode length: 243.16
    Episode_Reward/reaching_object: 0.8821
     Episode_Reward/lifting_object: 0.0247
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 1.93s
                      Time elapsed: 00:03:52
                               ETA: 01:15:16

################################################################################
                      [1m Learning iteration 98/2000 [0m                      

                       Computation: 50607 steps/s (collection: 1.855s, learning 0.087s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.0472
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.6328
                       Mean reward: 4.24
               Mean episode length: 229.60
    Episode_Reward/reaching_object: 0.8519
     Episode_Reward/lifting_object: 0.0183
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 1.94s
                      Time elapsed: 00:03:54
                               ETA: 01:15:05

################################################################################
                      [1m Learning iteration 99/2000 [0m                      

                       Computation: 50666 steps/s (collection: 1.839s, learning 0.101s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.5786
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.6939
                       Mean reward: 4.07
               Mean episode length: 232.12
    Episode_Reward/reaching_object: 0.8498
     Episode_Reward/lifting_object: 0.0134
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 1.94s
                      Time elapsed: 00:03:56
                               ETA: 01:14:54

################################################################################
                     [1m Learning iteration 100/2000 [0m                      

                       Computation: 50116 steps/s (collection: 1.860s, learning 0.102s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.0581
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.7054
                       Mean reward: 3.84
               Mean episode length: 241.23
    Episode_Reward/reaching_object: 0.8874
     Episode_Reward/lifting_object: -0.0342
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 1.96s
                      Time elapsed: 00:03:58
                               ETA: 01:14:45

################################################################################
                     [1m Learning iteration 101/2000 [0m                      

                       Computation: 48733 steps/s (collection: 1.897s, learning 0.121s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.0067
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.7336
                       Mean reward: 4.43
               Mean episode length: 234.16
    Episode_Reward/reaching_object: 0.8906
     Episode_Reward/lifting_object: 0.0153
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 2.02s
                      Time elapsed: 00:04:00
                               ETA: 01:14:36

################################################################################
                     [1m Learning iteration 102/2000 [0m                      

                       Computation: 49270 steps/s (collection: 1.872s, learning 0.124s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.0469
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 38.7547
                       Mean reward: 4.30
               Mean episode length: 238.35
    Episode_Reward/reaching_object: 0.9022
     Episode_Reward/lifting_object: -0.0192
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0119
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 2.00s
                      Time elapsed: 00:04:02
                               ETA: 01:14:27

################################################################################
                     [1m Learning iteration 103/2000 [0m                      

                       Computation: 47961 steps/s (collection: 1.939s, learning 0.111s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0611
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.7931
                       Mean reward: 4.50
               Mean episode length: 244.11
    Episode_Reward/reaching_object: 0.8986
     Episode_Reward/lifting_object: -0.0139
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0119
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 2.05s
                      Time elapsed: 00:04:04
                               ETA: 01:14:19

################################################################################
                     [1m Learning iteration 104/2000 [0m                      

                       Computation: 47998 steps/s (collection: 1.950s, learning 0.099s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.9517
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 38.8411
                       Mean reward: 4.09
               Mean episode length: 240.54
    Episode_Reward/reaching_object: 0.8792
     Episode_Reward/lifting_object: -0.0322
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0119
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 2.05s
                      Time elapsed: 00:04:06
                               ETA: 01:14:11

################################################################################
                     [1m Learning iteration 105/2000 [0m                      

                       Computation: 49759 steps/s (collection: 1.857s, learning 0.119s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0414
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.8548
                       Mean reward: 3.71
               Mean episode length: 246.41
    Episode_Reward/reaching_object: 0.8839
     Episode_Reward/lifting_object: -0.0495
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 1.98s
                      Time elapsed: 00:04:08
                               ETA: 01:14:02

################################################################################
                     [1m Learning iteration 106/2000 [0m                      

                       Computation: 50156 steps/s (collection: 1.847s, learning 0.113s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0370
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 38.8939
                       Mean reward: 4.40
               Mean episode length: 248.32
    Episode_Reward/reaching_object: 0.9139
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 1.96s
                      Time elapsed: 00:04:10
                               ETA: 01:13:53

################################################################################
                     [1m Learning iteration 107/2000 [0m                      

                       Computation: 48928 steps/s (collection: 1.888s, learning 0.121s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.1187
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.9561
                       Mean reward: 4.20
               Mean episode length: 242.91
    Episode_Reward/reaching_object: 0.9099
     Episode_Reward/lifting_object: -0.0339
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 2.01s
                      Time elapsed: 00:04:12
                               ETA: 01:13:45

################################################################################
                     [1m Learning iteration 108/2000 [0m                      

                       Computation: 48400 steps/s (collection: 1.937s, learning 0.094s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.1072
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.0433
                       Mean reward: 4.35
               Mean episode length: 245.69
    Episode_Reward/reaching_object: 0.9519
     Episode_Reward/lifting_object: -0.0174
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 2.03s
                      Time elapsed: 00:04:14
                               ETA: 01:13:37

################################################################################
                     [1m Learning iteration 109/2000 [0m                      

                       Computation: 50062 steps/s (collection: 1.865s, learning 0.099s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.2493
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.1077
                       Mean reward: 4.41
               Mean episode length: 235.64
    Episode_Reward/reaching_object: 0.9259
     Episode_Reward/lifting_object: -0.0970
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 1.96s
                      Time elapsed: 00:04:16
                               ETA: 01:13:28

################################################################################
                     [1m Learning iteration 110/2000 [0m                      

                       Computation: 50035 steps/s (collection: 1.869s, learning 0.096s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.5695
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.1522
                       Mean reward: 4.69
               Mean episode length: 240.27
    Episode_Reward/reaching_object: 0.9252
     Episode_Reward/lifting_object: 0.0071
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 1.96s
                      Time elapsed: 00:04:18
                               ETA: 01:13:20

################################################################################
                     [1m Learning iteration 111/2000 [0m                      

                       Computation: 50657 steps/s (collection: 1.849s, learning 0.092s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.2836
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.1664
                       Mean reward: 4.88
               Mean episode length: 244.52
    Episode_Reward/reaching_object: 0.9231
     Episode_Reward/lifting_object: -0.0208
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 1.94s
                      Time elapsed: 00:04:20
                               ETA: 01:13:11

################################################################################
                     [1m Learning iteration 112/2000 [0m                      

                       Computation: 51157 steps/s (collection: 1.835s, learning 0.087s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.2856
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 39.1824
                       Mean reward: 4.90
               Mean episode length: 245.68
    Episode_Reward/reaching_object: 0.9546
     Episode_Reward/lifting_object: -0.0705
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 1.92s
                      Time elapsed: 00:04:22
                               ETA: 01:13:02

################################################################################
                     [1m Learning iteration 113/2000 [0m                      

                       Computation: 49561 steps/s (collection: 1.895s, learning 0.088s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.0737
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.1907
                       Mean reward: 5.42
               Mean episode length: 246.24
    Episode_Reward/reaching_object: 0.9809
     Episode_Reward/lifting_object: 0.0093
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 1.98s
                      Time elapsed: 00:04:24
                               ETA: 01:12:54

################################################################################
                     [1m Learning iteration 114/2000 [0m                      

                       Computation: 50578 steps/s (collection: 1.848s, learning 0.096s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.0245
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 39.2149
                       Mean reward: 4.74
               Mean episode length: 244.75
    Episode_Reward/reaching_object: 0.9194
     Episode_Reward/lifting_object: 0.0415
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 1.94s
                      Time elapsed: 00:04:26
                               ETA: 01:12:45

################################################################################
                     [1m Learning iteration 115/2000 [0m                      

                       Computation: 50486 steps/s (collection: 1.862s, learning 0.085s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.0240
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.2625
                       Mean reward: 4.59
               Mean episode length: 237.08
    Episode_Reward/reaching_object: 0.9692
     Episode_Reward/lifting_object: 0.0436
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 1.95s
                      Time elapsed: 00:04:28
                               ETA: 01:12:37

################################################################################
                     [1m Learning iteration 116/2000 [0m                      

                       Computation: 49534 steps/s (collection: 1.882s, learning 0.103s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.0878
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.3122
                       Mean reward: 4.25
               Mean episode length: 242.70
    Episode_Reward/reaching_object: 0.9594
     Episode_Reward/lifting_object: -0.0123
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 1.98s
                      Time elapsed: 00:04:30
                               ETA: 01:12:30

################################################################################
                     [1m Learning iteration 117/2000 [0m                      

                       Computation: 50139 steps/s (collection: 1.863s, learning 0.098s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.3139
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 39.3413
                       Mean reward: 3.68
               Mean episode length: 237.51
    Episode_Reward/reaching_object: 0.9543
     Episode_Reward/lifting_object: -0.0441
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 1.96s
                      Time elapsed: 00:04:32
                               ETA: 01:12:22

################################################################################
                     [1m Learning iteration 118/2000 [0m                      

                       Computation: 49273 steps/s (collection: 1.889s, learning 0.106s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.0688
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 39.3776
                       Mean reward: 4.73
               Mean episode length: 239.02
    Episode_Reward/reaching_object: 0.9344
     Episode_Reward/lifting_object: -0.0267
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 2.00s
                      Time elapsed: 00:04:34
                               ETA: 01:12:14

################################################################################
                     [1m Learning iteration 119/2000 [0m                      

                       Computation: 50008 steps/s (collection: 1.875s, learning 0.091s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.1317
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.4323
                       Mean reward: 4.38
               Mean episode length: 243.14
    Episode_Reward/reaching_object: 0.9685
     Episode_Reward/lifting_object: -0.0647
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 1.97s
                      Time elapsed: 00:04:36
                               ETA: 01:12:07

################################################################################
                     [1m Learning iteration 120/2000 [0m                      

                       Computation: 49781 steps/s (collection: 1.870s, learning 0.105s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.4034
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.4776
                       Mean reward: 5.15
               Mean episode length: 243.56
    Episode_Reward/reaching_object: 0.9900
     Episode_Reward/lifting_object: 0.0075
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 1.97s
                      Time elapsed: 00:04:38
                               ETA: 01:12:00

################################################################################
                     [1m Learning iteration 121/2000 [0m                      

                       Computation: 50256 steps/s (collection: 1.857s, learning 0.099s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.1163
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.5516
                       Mean reward: 4.66
               Mean episode length: 237.71
    Episode_Reward/reaching_object: 0.9736
     Episode_Reward/lifting_object: -0.0056
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 1.96s
                      Time elapsed: 00:04:40
                               ETA: 01:11:52

################################################################################
                     [1m Learning iteration 122/2000 [0m                      

                       Computation: 50305 steps/s (collection: 1.848s, learning 0.106s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.0995
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.6489
                       Mean reward: 4.31
               Mean episode length: 236.24
    Episode_Reward/reaching_object: 0.9386
     Episode_Reward/lifting_object: 0.0320
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 1.95s
                      Time elapsed: 00:04:41
                               ETA: 01:11:44

################################################################################
                     [1m Learning iteration 123/2000 [0m                      

                       Computation: 50163 steps/s (collection: 1.867s, learning 0.093s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.0365
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.7220
                       Mean reward: 4.64
               Mean episode length: 240.44
    Episode_Reward/reaching_object: 0.9230
     Episode_Reward/lifting_object: 0.0806
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 1.96s
                      Time elapsed: 00:04:43
                               ETA: 01:11:37

################################################################################
                     [1m Learning iteration 124/2000 [0m                      

                       Computation: 50445 steps/s (collection: 1.851s, learning 0.098s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.0769
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.7870
                       Mean reward: 4.81
               Mean episode length: 239.08
    Episode_Reward/reaching_object: 0.9485
     Episode_Reward/lifting_object: 0.0280
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 1.95s
                      Time elapsed: 00:04:45
                               ETA: 01:11:30

################################################################################
                     [1m Learning iteration 125/2000 [0m                      

                       Computation: 49175 steps/s (collection: 1.905s, learning 0.094s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.0758
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.8644
                       Mean reward: 5.15
               Mean episode length: 240.72
    Episode_Reward/reaching_object: 0.9527
     Episode_Reward/lifting_object: 0.0367
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0131
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 2.00s
                      Time elapsed: 00:04:47
                               ETA: 01:11:23

################################################################################
                     [1m Learning iteration 126/2000 [0m                      

                       Computation: 49734 steps/s (collection: 1.885s, learning 0.092s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.3696
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 39.9200
                       Mean reward: 4.98
               Mean episode length: 244.34
    Episode_Reward/reaching_object: 0.9645
     Episode_Reward/lifting_object: 0.0279
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0132
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 1.98s
                      Time elapsed: 00:04:49
                               ETA: 01:11:16

################################################################################
                     [1m Learning iteration 127/2000 [0m                      

                       Computation: 49594 steps/s (collection: 1.885s, learning 0.097s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.0544
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.9337
                       Mean reward: 4.88
               Mean episode length: 243.65
    Episode_Reward/reaching_object: 0.9812
     Episode_Reward/lifting_object: -0.0248
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0133
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 1.98s
                      Time elapsed: 00:04:51
                               ETA: 01:11:10

################################################################################
                     [1m Learning iteration 128/2000 [0m                      

                       Computation: 49722 steps/s (collection: 1.872s, learning 0.105s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 39.9720
                       Mean reward: 4.56
               Mean episode length: 240.43
    Episode_Reward/reaching_object: 0.9579
     Episode_Reward/lifting_object: 0.0162
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0133
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 1.98s
                      Time elapsed: 00:04:53
                               ETA: 01:11:03

################################################################################
                     [1m Learning iteration 129/2000 [0m                      

                       Computation: 49586 steps/s (collection: 1.869s, learning 0.113s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.3109
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 40.0149
                       Mean reward: 5.08
               Mean episode length: 241.19
    Episode_Reward/reaching_object: 0.9956
     Episode_Reward/lifting_object: -0.0319
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 1.98s
                      Time elapsed: 00:04:55
                               ETA: 01:10:56

################################################################################
                     [1m Learning iteration 130/2000 [0m                      

                       Computation: 50002 steps/s (collection: 1.862s, learning 0.104s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.0835
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.0218
                       Mean reward: 4.90
               Mean episode length: 243.13
    Episode_Reward/reaching_object: 0.9901
     Episode_Reward/lifting_object: -0.0388
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 18.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 1.97s
                      Time elapsed: 00:04:57
                               ETA: 01:10:50

################################################################################
                     [1m Learning iteration 131/2000 [0m                      

                       Computation: 49674 steps/s (collection: 1.877s, learning 0.102s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.1257
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.0343
                       Mean reward: 4.64
               Mean episode length: 239.23
    Episode_Reward/reaching_object: 0.9960
     Episode_Reward/lifting_object: -0.0110
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 1.98s
                      Time elapsed: 00:04:59
                               ETA: 01:10:43

################################################################################
                     [1m Learning iteration 132/2000 [0m                      

                       Computation: 49279 steps/s (collection: 1.901s, learning 0.094s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.0525
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.0727
                       Mean reward: 4.60
               Mean episode length: 236.20
    Episode_Reward/reaching_object: 0.9773
     Episode_Reward/lifting_object: 0.0079
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 1.99s
                      Time elapsed: 00:05:01
                               ETA: 01:10:37

################################################################################
                     [1m Learning iteration 133/2000 [0m                      

                       Computation: 50297 steps/s (collection: 1.867s, learning 0.088s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.0884
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 40.1471
                       Mean reward: 5.00
               Mean episode length: 240.48
    Episode_Reward/reaching_object: 0.9978
     Episode_Reward/lifting_object: -0.0100
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 1.95s
                      Time elapsed: 00:05:03
                               ETA: 01:10:31

################################################################################
                     [1m Learning iteration 134/2000 [0m                      

                       Computation: 49101 steps/s (collection: 1.908s, learning 0.094s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.1860
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 40.1965
                       Mean reward: 4.90
               Mean episode length: 238.05
    Episode_Reward/reaching_object: 1.0123
     Episode_Reward/lifting_object: 0.0182
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 2.00s
                      Time elapsed: 00:05:05
                               ETA: 01:10:25

################################################################################
                     [1m Learning iteration 135/2000 [0m                      

                       Computation: 49423 steps/s (collection: 1.887s, learning 0.102s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.2565
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.2449
                       Mean reward: 5.47
               Mean episode length: 242.64
    Episode_Reward/reaching_object: 0.9872
     Episode_Reward/lifting_object: -0.0185
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 1.99s
                      Time elapsed: 00:05:07
                               ETA: 01:10:19

################################################################################
                     [1m Learning iteration 136/2000 [0m                      

                       Computation: 48962 steps/s (collection: 1.894s, learning 0.114s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.7224
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 40.2951
                       Mean reward: 4.63
               Mean episode length: 235.66
    Episode_Reward/reaching_object: 1.0207
     Episode_Reward/lifting_object: 0.0137
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 2.01s
                      Time elapsed: 00:05:09
                               ETA: 01:10:13

################################################################################
                     [1m Learning iteration 137/2000 [0m                      

                       Computation: 48796 steps/s (collection: 1.904s, learning 0.111s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.3196
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.3438
                       Mean reward: 4.97
               Mean episode length: 243.04
    Episode_Reward/reaching_object: 0.9883
     Episode_Reward/lifting_object: 0.0259
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 2.01s
                      Time elapsed: 00:05:11
                               ETA: 01:10:07

################################################################################
                     [1m Learning iteration 138/2000 [0m                      

                       Computation: 49015 steps/s (collection: 1.908s, learning 0.098s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.3897
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 40.3822
                       Mean reward: 4.89
               Mean episode length: 241.88
    Episode_Reward/reaching_object: 1.0129
     Episode_Reward/lifting_object: -0.0426
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 2.01s
                      Time elapsed: 00:05:13
                               ETA: 01:10:02

################################################################################
                     [1m Learning iteration 139/2000 [0m                      

                       Computation: 48681 steps/s (collection: 1.917s, learning 0.102s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.0501
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 40.3915
                       Mean reward: 5.36
               Mean episode length: 237.33
    Episode_Reward/reaching_object: 1.0090
     Episode_Reward/lifting_object: 0.0003
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 2.02s
                      Time elapsed: 00:05:15
                               ETA: 01:09:56

################################################################################
                     [1m Learning iteration 140/2000 [0m                      

                       Computation: 47637 steps/s (collection: 1.968s, learning 0.096s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.1609
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 40.4243
                       Mean reward: 4.98
               Mean episode length: 235.83
    Episode_Reward/reaching_object: 1.0255
     Episode_Reward/lifting_object: 0.0471
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 2.06s
                      Time elapsed: 00:05:17
                               ETA: 01:09:51

################################################################################
                     [1m Learning iteration 141/2000 [0m                      

                       Computation: 47486 steps/s (collection: 1.976s, learning 0.095s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.6481
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.4833
                       Mean reward: 4.72
               Mean episode length: 236.72
    Episode_Reward/reaching_object: 1.0079
     Episode_Reward/lifting_object: 0.0372
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 2.07s
                      Time elapsed: 00:05:19
                               ETA: 01:09:47

################################################################################
                     [1m Learning iteration 142/2000 [0m                      

                       Computation: 48952 steps/s (collection: 1.916s, learning 0.092s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.1150
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 40.5372
                       Mean reward: 5.08
               Mean episode length: 239.51
    Episode_Reward/reaching_object: 0.9651
     Episode_Reward/lifting_object: 0.0214
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 2.01s
                      Time elapsed: 00:05:21
                               ETA: 01:09:41

################################################################################
                     [1m Learning iteration 143/2000 [0m                      

                       Computation: 49145 steps/s (collection: 1.911s, learning 0.089s)
             Mean action noise std: 1.54
          Mean value_function loss: 1.5544
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 40.6007
                       Mean reward: 4.89
               Mean episode length: 238.21
    Episode_Reward/reaching_object: 1.0080
     Episode_Reward/lifting_object: -0.1228
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 2.00s
                      Time elapsed: 00:05:23
                               ETA: 01:09:36

################################################################################
                     [1m Learning iteration 144/2000 [0m                      

                       Computation: 49658 steps/s (collection: 1.891s, learning 0.089s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.1274
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.6167
                       Mean reward: 5.20
               Mean episode length: 235.41
    Episode_Reward/reaching_object: 1.0389
     Episode_Reward/lifting_object: 0.0716
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 1.98s
                      Time elapsed: 00:05:25
                               ETA: 01:09:30

################################################################################
                     [1m Learning iteration 145/2000 [0m                      

                       Computation: 48977 steps/s (collection: 1.914s, learning 0.093s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.1125
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 40.6640
                       Mean reward: 4.85
               Mean episode length: 234.33
    Episode_Reward/reaching_object: 0.9862
     Episode_Reward/lifting_object: -0.0305
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 2.01s
                      Time elapsed: 00:05:27
                               ETA: 01:09:25

################################################################################
                     [1m Learning iteration 146/2000 [0m                      

                       Computation: 49730 steps/s (collection: 1.885s, learning 0.092s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.6087
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 40.7104
                       Mean reward: 5.08
               Mean episode length: 241.72
    Episode_Reward/reaching_object: 1.0285
     Episode_Reward/lifting_object: 0.0365
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 1.98s
                      Time elapsed: 00:05:29
                               ETA: 01:09:19

################################################################################
                     [1m Learning iteration 147/2000 [0m                      

                       Computation: 49038 steps/s (collection: 1.915s, learning 0.090s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.4142
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.7227
                       Mean reward: 4.75
               Mean episode length: 225.72
    Episode_Reward/reaching_object: 1.0047
     Episode_Reward/lifting_object: 0.0427
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 2.00s
                      Time elapsed: 00:05:31
                               ETA: 01:09:14

################################################################################
                     [1m Learning iteration 148/2000 [0m                      

                       Computation: 49456 steps/s (collection: 1.897s, learning 0.091s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.1088
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 40.7598
                       Mean reward: 5.64
               Mean episode length: 238.29
    Episode_Reward/reaching_object: 1.0290
     Episode_Reward/lifting_object: 0.0401
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 1.99s
                      Time elapsed: 00:05:33
                               ETA: 01:09:09

################################################################################
                     [1m Learning iteration 149/2000 [0m                      

                       Computation: 48808 steps/s (collection: 1.926s, learning 0.088s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.3940
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 40.8108
                       Mean reward: 5.01
               Mean episode length: 238.93
    Episode_Reward/reaching_object: 1.0309
     Episode_Reward/lifting_object: -0.0414
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 2.01s
                      Time elapsed: 00:05:35
                               ETA: 01:09:04

################################################################################
                     [1m Learning iteration 150/2000 [0m                      

                       Computation: 43481 steps/s (collection: 2.153s, learning 0.108s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.1879
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 40.8537
                       Mean reward: 5.11
               Mean episode length: 239.92
    Episode_Reward/reaching_object: 1.0320
     Episode_Reward/lifting_object: 0.0198
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 2.26s
                      Time elapsed: 00:05:38
                               ETA: 01:09:02

################################################################################
                     [1m Learning iteration 151/2000 [0m                      

                       Computation: 45199 steps/s (collection: 2.055s, learning 0.120s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.2632
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 40.9136
                       Mean reward: 5.51
               Mean episode length: 236.01
    Episode_Reward/reaching_object: 1.0254
     Episode_Reward/lifting_object: 0.1078
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 2.17s
                      Time elapsed: 00:05:40
                               ETA: 01:08:59

################################################################################
                     [1m Learning iteration 152/2000 [0m                      

                       Computation: 43095 steps/s (collection: 2.142s, learning 0.140s)
             Mean action noise std: 1.56
          Mean value_function loss: 1.0312
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 40.9623
                       Mean reward: 4.89
               Mean episode length: 221.89
    Episode_Reward/reaching_object: 1.0432
     Episode_Reward/lifting_object: 0.0388
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 2.28s
                      Time elapsed: 00:05:42
                               ETA: 01:08:57

################################################################################
                     [1m Learning iteration 153/2000 [0m                      

                       Computation: 38200 steps/s (collection: 2.469s, learning 0.105s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.2394
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 40.9919
                       Mean reward: 4.34
               Mean episode length: 232.44
    Episode_Reward/reaching_object: 1.0157
     Episode_Reward/lifting_object: -0.0490
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 2.57s
                      Time elapsed: 00:05:45
                               ETA: 01:08:59

################################################################################
                     [1m Learning iteration 154/2000 [0m                      

                       Computation: 42124 steps/s (collection: 2.242s, learning 0.092s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.1254
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 41.0491
                       Mean reward: 4.89
               Mean episode length: 225.74
    Episode_Reward/reaching_object: 1.0144
     Episode_Reward/lifting_object: 0.0107
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 2.33s
                      Time elapsed: 00:05:47
                               ETA: 01:08:58

################################################################################
                     [1m Learning iteration 155/2000 [0m                      

                       Computation: 45606 steps/s (collection: 2.022s, learning 0.134s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.1814
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.1080
                       Mean reward: 4.35
               Mean episode length: 228.91
    Episode_Reward/reaching_object: 1.0276
     Episode_Reward/lifting_object: -0.0534
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 2.16s
                      Time elapsed: 00:05:49
                               ETA: 01:08:54

################################################################################
                     [1m Learning iteration 156/2000 [0m                      

                       Computation: 47511 steps/s (collection: 1.981s, learning 0.089s)
             Mean action noise std: 1.58
          Mean value_function loss: 1.0359
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.1488
                       Mean reward: 4.53
               Mean episode length: 221.58
    Episode_Reward/reaching_object: 1.0014
     Episode_Reward/lifting_object: 0.0682
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 2.07s
                      Time elapsed: 00:05:51
                               ETA: 01:08:50

################################################################################
                     [1m Learning iteration 157/2000 [0m                      

                       Computation: 48361 steps/s (collection: 1.939s, learning 0.094s)
             Mean action noise std: 1.58
          Mean value_function loss: 1.0054
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.1612
                       Mean reward: 5.34
               Mean episode length: 234.53
    Episode_Reward/reaching_object: 1.0335
     Episode_Reward/lifting_object: 0.0464
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 2.03s
                      Time elapsed: 00:05:53
                               ETA: 01:08:45

################################################################################
                     [1m Learning iteration 158/2000 [0m                      

                       Computation: 45801 steps/s (collection: 2.025s, learning 0.121s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.2375
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.1720
                       Mean reward: 5.17
               Mean episode length: 227.16
    Episode_Reward/reaching_object: 1.0279
     Episode_Reward/lifting_object: 0.0990
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 2.15s
                      Time elapsed: 00:05:55
                               ETA: 01:08:42

################################################################################
                     [1m Learning iteration 159/2000 [0m                      

                       Computation: 45996 steps/s (collection: 2.013s, learning 0.124s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.1669
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 41.1988
                       Mean reward: 5.61
               Mean episode length: 237.96
    Episode_Reward/reaching_object: 1.0567
     Episode_Reward/lifting_object: 0.0159
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 2.14s
                      Time elapsed: 00:05:57
                               ETA: 01:08:39

################################################################################
                     [1m Learning iteration 160/2000 [0m                      

                       Computation: 47703 steps/s (collection: 1.950s, learning 0.111s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.7171
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.2393
                       Mean reward: 5.45
               Mean episode length: 230.48
    Episode_Reward/reaching_object: 1.0207
     Episode_Reward/lifting_object: 0.0178
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 2.06s
                      Time elapsed: 00:06:00
                               ETA: 01:08:34

################################################################################
                     [1m Learning iteration 161/2000 [0m                      

                       Computation: 47553 steps/s (collection: 1.974s, learning 0.093s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.1448
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 41.2621
                       Mean reward: 5.37
               Mean episode length: 233.62
    Episode_Reward/reaching_object: 1.0483
     Episode_Reward/lifting_object: 0.0159
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 2.07s
                      Time elapsed: 00:06:02
                               ETA: 01:08:30

################################################################################
                     [1m Learning iteration 162/2000 [0m                      

                       Computation: 47421 steps/s (collection: 1.985s, learning 0.088s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.5264
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.3005
                       Mean reward: 5.19
               Mean episode length: 228.42
    Episode_Reward/reaching_object: 1.0468
     Episode_Reward/lifting_object: 0.0380
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 2.07s
                      Time elapsed: 00:06:04
                               ETA: 01:08:26

################################################################################
                     [1m Learning iteration 163/2000 [0m                      

                       Computation: 45145 steps/s (collection: 2.080s, learning 0.098s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.3516
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 41.3266
                       Mean reward: 4.47
               Mean episode length: 231.27
    Episode_Reward/reaching_object: 0.9959
     Episode_Reward/lifting_object: 0.0679
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 2.18s
                      Time elapsed: 00:06:06
                               ETA: 01:08:23

################################################################################
                     [1m Learning iteration 164/2000 [0m                      

                       Computation: 43808 steps/s (collection: 2.152s, learning 0.092s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.3355
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 41.3792
                       Mean reward: 5.44
               Mean episode length: 223.99
    Episode_Reward/reaching_object: 1.0253
     Episode_Reward/lifting_object: 0.0680
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 2.24s
                      Time elapsed: 00:06:08
                               ETA: 01:08:21

################################################################################
                     [1m Learning iteration 165/2000 [0m                      

                       Computation: 35840 steps/s (collection: 2.574s, learning 0.169s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.2713
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 41.4341
                       Mean reward: 4.67
               Mean episode length: 209.55
    Episode_Reward/reaching_object: 0.9864
     Episode_Reward/lifting_object: 0.0827
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 2.74s
                      Time elapsed: 00:06:11
                               ETA: 01:08:25

################################################################################
                     [1m Learning iteration 166/2000 [0m                      

                       Computation: 38871 steps/s (collection: 2.388s, learning 0.141s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.6672
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 41.4799
                       Mean reward: 5.34
               Mean episode length: 221.50
    Episode_Reward/reaching_object: 0.9888
     Episode_Reward/lifting_object: 0.0998
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 2.53s
                      Time elapsed: 00:06:13
                               ETA: 01:08:26

################################################################################
                     [1m Learning iteration 167/2000 [0m                      

                       Computation: 40290 steps/s (collection: 2.327s, learning 0.113s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.4669
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 41.5184
                       Mean reward: 4.89
               Mean episode length: 217.33
    Episode_Reward/reaching_object: 1.0092
     Episode_Reward/lifting_object: 0.0951
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 2.44s
                      Time elapsed: 00:06:16
                               ETA: 01:08:25

################################################################################
                     [1m Learning iteration 168/2000 [0m                      

                       Computation: 43799 steps/s (collection: 2.153s, learning 0.091s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.4845
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 41.5693
                       Mean reward: 6.06
               Mean episode length: 238.52
    Episode_Reward/reaching_object: 1.0640
     Episode_Reward/lifting_object: 0.1222
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 2.24s
                      Time elapsed: 00:06:18
                               ETA: 01:08:23

################################################################################
                     [1m Learning iteration 169/2000 [0m                      

                       Computation: 42476 steps/s (collection: 2.135s, learning 0.180s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.3740
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 41.5980
                       Mean reward: 6.02
               Mean episode length: 232.75
    Episode_Reward/reaching_object: 1.0180
     Episode_Reward/lifting_object: 0.0287
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 2.31s
                      Time elapsed: 00:06:20
                               ETA: 01:08:22

################################################################################
                     [1m Learning iteration 170/2000 [0m                      

                       Computation: 41092 steps/s (collection: 2.244s, learning 0.148s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.5479
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 41.6423
                       Mean reward: 6.38
               Mean episode length: 231.91
    Episode_Reward/reaching_object: 1.0261
     Episode_Reward/lifting_object: 0.2358
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 2.39s
                      Time elapsed: 00:06:23
                               ETA: 01:08:21

################################################################################
                     [1m Learning iteration 171/2000 [0m                      

                       Computation: 46433 steps/s (collection: 2.017s, learning 0.100s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.5481
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 41.6723
                       Mean reward: 5.83
               Mean episode length: 230.97
    Episode_Reward/reaching_object: 1.0383
     Episode_Reward/lifting_object: 0.1220
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 2.12s
                      Time elapsed: 00:06:25
                               ETA: 01:08:18

################################################################################
                     [1m Learning iteration 172/2000 [0m                      

                       Computation: 46553 steps/s (collection: 1.998s, learning 0.114s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.6178
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 41.7010
                       Mean reward: 5.71
               Mean episode length: 235.78
    Episode_Reward/reaching_object: 0.9956
     Episode_Reward/lifting_object: 0.1763
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 2.11s
                      Time elapsed: 00:06:27
                               ETA: 01:08:14

################################################################################
                     [1m Learning iteration 173/2000 [0m                      

                       Computation: 47766 steps/s (collection: 1.943s, learning 0.115s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.5544
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 41.7555
                       Mean reward: 5.58
               Mean episode length: 229.05
    Episode_Reward/reaching_object: 1.0149
     Episode_Reward/lifting_object: 0.0873
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 2.06s
                      Time elapsed: 00:06:29
                               ETA: 01:08:10

################################################################################
                     [1m Learning iteration 174/2000 [0m                      

                       Computation: 48553 steps/s (collection: 1.934s, learning 0.091s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.2922
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 41.8086
                       Mean reward: 5.43
               Mean episode length: 216.97
    Episode_Reward/reaching_object: 1.0003
     Episode_Reward/lifting_object: 0.0743
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 2.02s
                      Time elapsed: 00:06:31
                               ETA: 01:08:05

################################################################################
                     [1m Learning iteration 175/2000 [0m                      

                       Computation: 47371 steps/s (collection: 1.967s, learning 0.108s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.4914
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 41.8613
                       Mean reward: 6.57
               Mean episode length: 225.76
    Episode_Reward/reaching_object: 0.9810
     Episode_Reward/lifting_object: 0.1798
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 2.08s
                      Time elapsed: 00:06:33
                               ETA: 01:08:01

################################################################################
                     [1m Learning iteration 176/2000 [0m                      

                       Computation: 49519 steps/s (collection: 1.896s, learning 0.089s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.5118
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 41.8891
                       Mean reward: 6.34
               Mean episode length: 226.57
    Episode_Reward/reaching_object: 0.9785
     Episode_Reward/lifting_object: 0.2097
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 1.99s
                      Time elapsed: 00:06:35
                               ETA: 01:07:57

################################################################################
                     [1m Learning iteration 177/2000 [0m                      

                       Computation: 46821 steps/s (collection: 1.990s, learning 0.109s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.6564
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 41.9274
                       Mean reward: 5.80
               Mean episode length: 230.44
    Episode_Reward/reaching_object: 0.9810
     Episode_Reward/lifting_object: 0.2207
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 2.10s
                      Time elapsed: 00:06:37
                               ETA: 01:07:53

################################################################################
                     [1m Learning iteration 178/2000 [0m                      

                       Computation: 46781 steps/s (collection: 2.011s, learning 0.090s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.7192
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 41.9544
                       Mean reward: 4.58
               Mean episode length: 226.38
    Episode_Reward/reaching_object: 0.9845
     Episode_Reward/lifting_object: 0.1333
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 2.10s
                      Time elapsed: 00:06:39
                               ETA: 01:07:49

################################################################################
                     [1m Learning iteration 179/2000 [0m                      

                       Computation: 47552 steps/s (collection: 1.967s, learning 0.101s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.4305
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 41.9752
                       Mean reward: 5.91
               Mean episode length: 230.28
    Episode_Reward/reaching_object: 0.9917
     Episode_Reward/lifting_object: 0.1402
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 2.07s
                      Time elapsed: 00:06:41
                               ETA: 01:07:46

################################################################################
                     [1m Learning iteration 180/2000 [0m                      

                       Computation: 47079 steps/s (collection: 1.986s, learning 0.102s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.7383
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 42.0247
                       Mean reward: 5.20
               Mean episode length: 220.52
    Episode_Reward/reaching_object: 0.9745
     Episode_Reward/lifting_object: 0.0779
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 2.09s
                      Time elapsed: 00:06:44
                               ETA: 01:07:42

################################################################################
                     [1m Learning iteration 181/2000 [0m                      

                       Computation: 45015 steps/s (collection: 2.073s, learning 0.111s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.4504
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 42.0547
                       Mean reward: 5.75
               Mean episode length: 218.22
    Episode_Reward/reaching_object: 0.9722
     Episode_Reward/lifting_object: 0.2305
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 2.18s
                      Time elapsed: 00:06:46
                               ETA: 01:07:39

################################################################################
                     [1m Learning iteration 182/2000 [0m                      

                       Computation: 42700 steps/s (collection: 2.210s, learning 0.092s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.8511
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 42.0855
                       Mean reward: 6.00
               Mean episode length: 220.44
    Episode_Reward/reaching_object: 0.9269
     Episode_Reward/lifting_object: 0.1332
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 2.30s
                      Time elapsed: 00:06:48
                               ETA: 01:07:38

################################################################################
                     [1m Learning iteration 183/2000 [0m                      

                       Computation: 47232 steps/s (collection: 1.981s, learning 0.100s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.7582
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 42.1068
                       Mean reward: 5.31
               Mean episode length: 218.98
    Episode_Reward/reaching_object: 0.9595
     Episode_Reward/lifting_object: 0.2082
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 2.08s
                      Time elapsed: 00:06:50
                               ETA: 01:07:34

################################################################################
                     [1m Learning iteration 184/2000 [0m                      

                       Computation: 45374 steps/s (collection: 2.074s, learning 0.093s)
             Mean action noise std: 1.65
          Mean value_function loss: 1.0654
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 42.1365
                       Mean reward: 5.64
               Mean episode length: 227.46
    Episode_Reward/reaching_object: 0.9926
     Episode_Reward/lifting_object: 0.1862
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 2.17s
                      Time elapsed: 00:06:52
                               ETA: 01:07:31

################################################################################
                     [1m Learning iteration 185/2000 [0m                      

                       Computation: 48626 steps/s (collection: 1.929s, learning 0.093s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.6465
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 42.1690
                       Mean reward: 6.16
               Mean episode length: 225.50
    Episode_Reward/reaching_object: 0.9651
     Episode_Reward/lifting_object: 0.2184
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 2.02s
                      Time elapsed: 00:06:54
                               ETA: 01:07:27

################################################################################
                     [1m Learning iteration 186/2000 [0m                      

                       Computation: 46458 steps/s (collection: 2.002s, learning 0.114s)
             Mean action noise std: 1.66
          Mean value_function loss: 1.7803
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 42.2164
                       Mean reward: 6.65
               Mean episode length: 222.20
    Episode_Reward/reaching_object: 0.9989
     Episode_Reward/lifting_object: 0.1070
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 2.12s
                      Time elapsed: 00:06:56
                               ETA: 01:07:23

################################################################################
                     [1m Learning iteration 187/2000 [0m                      

                       Computation: 48299 steps/s (collection: 1.945s, learning 0.091s)
             Mean action noise std: 1.66
          Mean value_function loss: 1.2240
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 42.2397
                       Mean reward: 5.62
               Mean episode length: 214.90
    Episode_Reward/reaching_object: 0.9693
     Episode_Reward/lifting_object: 0.1693
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 2.04s
                      Time elapsed: 00:06:58
                               ETA: 01:07:19

################################################################################
                     [1m Learning iteration 188/2000 [0m                      

                       Computation: 49361 steps/s (collection: 1.901s, learning 0.090s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.7677
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 42.2551
                       Mean reward: 5.54
               Mean episode length: 212.42
    Episode_Reward/reaching_object: 0.9425
     Episode_Reward/lifting_object: 0.1998
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 1.99s
                      Time elapsed: 00:07:00
                               ETA: 01:07:15

################################################################################
                     [1m Learning iteration 189/2000 [0m                      

                       Computation: 47274 steps/s (collection: 1.960s, learning 0.119s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.6858
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 42.2899
                       Mean reward: 6.66
               Mean episode length: 222.10
    Episode_Reward/reaching_object: 0.9645
     Episode_Reward/lifting_object: 0.2313
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 2.08s
                      Time elapsed: 00:07:02
                               ETA: 01:07:11

################################################################################
                     [1m Learning iteration 190/2000 [0m                      

                       Computation: 45955 steps/s (collection: 2.003s, learning 0.136s)
             Mean action noise std: 1.66
          Mean value_function loss: 1.2259
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 42.3294
                       Mean reward: 7.74
               Mean episode length: 221.56
    Episode_Reward/reaching_object: 0.9418
     Episode_Reward/lifting_object: 0.3877
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 2.14s
                      Time elapsed: 00:07:05
                               ETA: 01:07:08

################################################################################
                     [1m Learning iteration 191/2000 [0m                      

                       Computation: 45596 steps/s (collection: 2.004s, learning 0.152s)
             Mean action noise std: 1.67
          Mean value_function loss: 1.9071
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 42.3568
                       Mean reward: 6.22
               Mean episode length: 229.61
    Episode_Reward/reaching_object: 0.9326
     Episode_Reward/lifting_object: 0.3423
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 2.16s
                      Time elapsed: 00:07:07
                               ETA: 01:07:05

################################################################################
                     [1m Learning iteration 192/2000 [0m                      

                       Computation: 46835 steps/s (collection: 2.004s, learning 0.095s)
             Mean action noise std: 1.67
          Mean value_function loss: 1.2698
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 42.3855
                       Mean reward: 6.63
               Mean episode length: 235.85
    Episode_Reward/reaching_object: 0.9339
     Episode_Reward/lifting_object: 0.1802
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 2.10s
                      Time elapsed: 00:07:09
                               ETA: 01:07:02

################################################################################
                     [1m Learning iteration 193/2000 [0m                      

                       Computation: 46130 steps/s (collection: 2.029s, learning 0.102s)
             Mean action noise std: 1.67
          Mean value_function loss: 1.4637
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 42.4394
                       Mean reward: 5.36
               Mean episode length: 215.98
    Episode_Reward/reaching_object: 0.9507
     Episode_Reward/lifting_object: 0.3272
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 2.13s
                      Time elapsed: 00:07:11
                               ETA: 01:06:59

################################################################################
                     [1m Learning iteration 194/2000 [0m                      

                       Computation: 48886 steps/s (collection: 1.924s, learning 0.087s)
             Mean action noise std: 1.68
          Mean value_function loss: 1.2100
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 42.5024
                       Mean reward: 5.95
               Mean episode length: 224.46
    Episode_Reward/reaching_object: 0.9412
     Episode_Reward/lifting_object: 0.3761
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 2.01s
                      Time elapsed: 00:07:13
                               ETA: 01:06:55

################################################################################
                     [1m Learning iteration 195/2000 [0m                      

                       Computation: 48969 steps/s (collection: 1.919s, learning 0.088s)
             Mean action noise std: 1.68
          Mean value_function loss: 1.4237
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 42.5634
                       Mean reward: 8.22
               Mean episode length: 225.29
    Episode_Reward/reaching_object: 0.9546
     Episode_Reward/lifting_object: 0.4452
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 2.01s
                      Time elapsed: 00:07:15
                               ETA: 01:06:50

################################################################################
                     [1m Learning iteration 196/2000 [0m                      

                       Computation: 46258 steps/s (collection: 2.013s, learning 0.112s)
             Mean action noise std: 1.69
          Mean value_function loss: 1.6408
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 42.6167
                       Mean reward: 6.27
               Mean episode length: 222.62
    Episode_Reward/reaching_object: 0.9669
     Episode_Reward/lifting_object: 0.2663
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 2.13s
                      Time elapsed: 00:07:17
                               ETA: 01:06:47

################################################################################
                     [1m Learning iteration 197/2000 [0m                      

                       Computation: 48630 steps/s (collection: 1.929s, learning 0.092s)
             Mean action noise std: 1.69
          Mean value_function loss: 3.9346
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 42.6477
                       Mean reward: 7.49
               Mean episode length: 226.02
    Episode_Reward/reaching_object: 0.9367
     Episode_Reward/lifting_object: 0.5036
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 2.02s
                      Time elapsed: 00:07:19
                               ETA: 01:06:43

################################################################################
                     [1m Learning iteration 198/2000 [0m                      

                       Computation: 47578 steps/s (collection: 1.944s, learning 0.122s)
             Mean action noise std: 1.69
          Mean value_function loss: 1.2764
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 42.6674
                       Mean reward: 6.61
               Mean episode length: 234.31
    Episode_Reward/reaching_object: 0.9725
     Episode_Reward/lifting_object: 0.5071
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 2.07s
                      Time elapsed: 00:07:21
                               ETA: 01:06:40

################################################################################
                     [1m Learning iteration 199/2000 [0m                      

                       Computation: 49273 steps/s (collection: 1.908s, learning 0.087s)
             Mean action noise std: 1.69
          Mean value_function loss: 1.4137
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 42.7089
                       Mean reward: 6.78
               Mean episode length: 225.60
    Episode_Reward/reaching_object: 0.9443
     Episode_Reward/lifting_object: 0.4236
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 2.00s
                      Time elapsed: 00:07:23
                               ETA: 01:06:35

################################################################################
                     [1m Learning iteration 200/2000 [0m                      

                       Computation: 43912 steps/s (collection: 2.125s, learning 0.114s)
             Mean action noise std: 1.70
          Mean value_function loss: 2.6455
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 42.7529
                       Mean reward: 6.32
               Mean episode length: 229.04
    Episode_Reward/reaching_object: 0.9423
     Episode_Reward/lifting_object: 0.3860
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 2.24s
                      Time elapsed: 00:07:25
                               ETA: 01:06:33

################################################################################
                     [1m Learning iteration 201/2000 [0m                      

                       Computation: 45267 steps/s (collection: 2.076s, learning 0.096s)
             Mean action noise std: 1.70
          Mean value_function loss: 1.7265
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 42.7803
                       Mean reward: 7.50
               Mean episode length: 220.86
    Episode_Reward/reaching_object: 0.8867
     Episode_Reward/lifting_object: 0.4545
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 2.17s
                      Time elapsed: 00:07:28
                               ETA: 01:06:31

################################################################################
                     [1m Learning iteration 202/2000 [0m                      

                       Computation: 47286 steps/s (collection: 1.933s, learning 0.146s)
             Mean action noise std: 1.70
          Mean value_function loss: 1.5500
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 42.8142
                       Mean reward: 7.68
               Mean episode length: 227.16
    Episode_Reward/reaching_object: 0.9140
     Episode_Reward/lifting_object: 0.6373
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 2.08s
                      Time elapsed: 00:07:30
                               ETA: 01:06:27

################################################################################
                     [1m Learning iteration 203/2000 [0m                      

                       Computation: 47270 steps/s (collection: 1.990s, learning 0.090s)
             Mean action noise std: 1.70
          Mean value_function loss: 1.7336
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 42.8507
                       Mean reward: 8.48
               Mean episode length: 218.12
    Episode_Reward/reaching_object: 0.8576
     Episode_Reward/lifting_object: 0.6904
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 2.08s
                      Time elapsed: 00:07:32
                               ETA: 01:06:24

################################################################################
                     [1m Learning iteration 204/2000 [0m                      

                       Computation: 48210 steps/s (collection: 1.890s, learning 0.150s)
             Mean action noise std: 1.71
          Mean value_function loss: 1.5893
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 42.8795
                       Mean reward: 9.06
               Mean episode length: 223.57
    Episode_Reward/reaching_object: 0.8783
     Episode_Reward/lifting_object: 0.5911
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 2.04s
                      Time elapsed: 00:07:34
                               ETA: 01:06:20

################################################################################
                     [1m Learning iteration 205/2000 [0m                      

                       Computation: 47080 steps/s (collection: 1.976s, learning 0.112s)
             Mean action noise std: 1.71
          Mean value_function loss: 2.4931
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 42.9255
                       Mean reward: 7.17
               Mean episode length: 220.78
    Episode_Reward/reaching_object: 0.8533
     Episode_Reward/lifting_object: 0.6163
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 2.09s
                      Time elapsed: 00:07:36
                               ETA: 01:06:17

################################################################################
                     [1m Learning iteration 206/2000 [0m                      

                       Computation: 45025 steps/s (collection: 2.094s, learning 0.090s)
             Mean action noise std: 1.71
          Mean value_function loss: 1.6904
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 42.9704
                       Mean reward: 8.04
               Mean episode length: 216.82
    Episode_Reward/reaching_object: 0.8702
     Episode_Reward/lifting_object: 0.7856
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 2.18s
                      Time elapsed: 00:07:38
                               ETA: 01:06:14

################################################################################
                     [1m Learning iteration 207/2000 [0m                      

                       Computation: 48174 steps/s (collection: 1.953s, learning 0.087s)
             Mean action noise std: 1.72
          Mean value_function loss: 2.9040
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 43.0093
                       Mean reward: 5.81
               Mean episode length: 218.36
    Episode_Reward/reaching_object: 0.8788
     Episode_Reward/lifting_object: 0.7359
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 2.04s
                      Time elapsed: 00:07:40
                               ETA: 01:06:10

################################################################################
                     [1m Learning iteration 208/2000 [0m                      

                       Computation: 46148 steps/s (collection: 2.029s, learning 0.101s)
             Mean action noise std: 1.72
          Mean value_function loss: 3.7512
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 43.0599
                       Mean reward: 9.78
               Mean episode length: 215.73
    Episode_Reward/reaching_object: 0.8638
     Episode_Reward/lifting_object: 0.7511
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 2.13s
                      Time elapsed: 00:07:42
                               ETA: 01:06:07

################################################################################
                     [1m Learning iteration 209/2000 [0m                      

                       Computation: 49107 steps/s (collection: 1.910s, learning 0.092s)
             Mean action noise std: 1.72
          Mean value_function loss: 2.8105
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 43.1099
                       Mean reward: 3.72
               Mean episode length: 209.50
    Episode_Reward/reaching_object: 0.8460
     Episode_Reward/lifting_object: 0.7734
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 2.00s
                      Time elapsed: 00:07:44
                               ETA: 01:06:03

################################################################################
                     [1m Learning iteration 210/2000 [0m                      

                       Computation: 49320 steps/s (collection: 1.898s, learning 0.095s)
             Mean action noise std: 1.73
          Mean value_function loss: 4.7790
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 43.1361
                       Mean reward: 7.57
               Mean episode length: 217.42
    Episode_Reward/reaching_object: 0.8291
     Episode_Reward/lifting_object: 0.8415
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 1.99s
                      Time elapsed: 00:07:46
                               ETA: 01:05:59

################################################################################
                     [1m Learning iteration 211/2000 [0m                      

                       Computation: 44313 steps/s (collection: 2.080s, learning 0.138s)
             Mean action noise std: 1.73
          Mean value_function loss: 16.9905
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 43.1617
                       Mean reward: 7.98
               Mean episode length: 222.39
    Episode_Reward/reaching_object: 0.8327
     Episode_Reward/lifting_object: 0.7304
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 2.22s
                      Time elapsed: 00:07:48
                               ETA: 01:05:57

################################################################################
                     [1m Learning iteration 212/2000 [0m                      

                       Computation: 49263 steps/s (collection: 1.908s, learning 0.088s)
             Mean action noise std: 1.73
          Mean value_function loss: 3.8469
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 43.2158
                       Mean reward: 8.48
               Mean episode length: 220.04
    Episode_Reward/reaching_object: 0.8504
     Episode_Reward/lifting_object: 0.8393
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 2.00s
                      Time elapsed: 00:07:50
                               ETA: 01:05:53

################################################################################
                     [1m Learning iteration 213/2000 [0m                      

                       Computation: 48426 steps/s (collection: 1.922s, learning 0.108s)
             Mean action noise std: 1.74
          Mean value_function loss: 4.2326
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 43.2798
                       Mean reward: 9.52
               Mean episode length: 213.03
    Episode_Reward/reaching_object: 0.8513
     Episode_Reward/lifting_object: 0.8259
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 2.03s
                      Time elapsed: 00:07:53
                               ETA: 01:05:49

################################################################################
                     [1m Learning iteration 214/2000 [0m                      

                       Computation: 46901 steps/s (collection: 2.001s, learning 0.094s)
             Mean action noise std: 1.74
          Mean value_function loss: 2.7567
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 43.3391
                       Mean reward: 9.08
               Mean episode length: 217.41
    Episode_Reward/reaching_object: 0.8235
     Episode_Reward/lifting_object: 0.9600
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 2.10s
                      Time elapsed: 00:07:55
                               ETA: 01:05:46

################################################################################
                     [1m Learning iteration 215/2000 [0m                      

                       Computation: 44909 steps/s (collection: 2.020s, learning 0.169s)
             Mean action noise std: 1.74
          Mean value_function loss: 2.4348
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 43.3719
                       Mean reward: -0.76
               Mean episode length: 208.24
    Episode_Reward/reaching_object: 0.8051
     Episode_Reward/lifting_object: 0.3021
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 2.19s
                      Time elapsed: 00:07:57
                               ETA: 01:05:44

################################################################################
                     [1m Learning iteration 216/2000 [0m                      

                       Computation: 48188 steps/s (collection: 1.924s, learning 0.116s)
             Mean action noise std: 1.75
          Mean value_function loss: 2.7610
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 43.4003
                       Mean reward: 10.75
               Mean episode length: 215.48
    Episode_Reward/reaching_object: 0.8235
     Episode_Reward/lifting_object: 1.2055
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 2.04s
                      Time elapsed: 00:07:59
                               ETA: 01:05:40

################################################################################
                     [1m Learning iteration 217/2000 [0m                      

                       Computation: 49187 steps/s (collection: 1.864s, learning 0.134s)
             Mean action noise std: 1.75
          Mean value_function loss: 4.9585
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 43.4477
                       Mean reward: 8.60
               Mean episode length: 203.29
    Episode_Reward/reaching_object: 0.7929
     Episode_Reward/lifting_object: 0.9278
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 2.00s
                      Time elapsed: 00:08:01
                               ETA: 01:05:36

################################################################################
                     [1m Learning iteration 218/2000 [0m                      

                       Computation: 44974 steps/s (collection: 2.057s, learning 0.129s)
             Mean action noise std: 1.76
          Mean value_function loss: 3.1297
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 43.4982
                       Mean reward: 11.68
               Mean episode length: 215.20
    Episode_Reward/reaching_object: 0.8129
     Episode_Reward/lifting_object: 1.1489
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 2.19s
                      Time elapsed: 00:08:03
                               ETA: 01:05:34

################################################################################
                     [1m Learning iteration 219/2000 [0m                      

                       Computation: 49692 steps/s (collection: 1.891s, learning 0.088s)
             Mean action noise std: 1.76
          Mean value_function loss: 3.7475
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 43.5502
                       Mean reward: 9.12
               Mean episode length: 216.26
    Episode_Reward/reaching_object: 0.8459
     Episode_Reward/lifting_object: 0.9553
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 1.98s
                      Time elapsed: 00:08:05
                               ETA: 01:05:30

################################################################################
                     [1m Learning iteration 220/2000 [0m                      

                       Computation: 43495 steps/s (collection: 2.003s, learning 0.257s)
             Mean action noise std: 1.76
          Mean value_function loss: 3.2991
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 43.5986
                       Mean reward: 7.13
               Mean episode length: 192.71
    Episode_Reward/reaching_object: 0.7994
     Episode_Reward/lifting_object: 1.0679
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 2.26s
                      Time elapsed: 00:08:07
                               ETA: 01:05:28

################################################################################
                     [1m Learning iteration 221/2000 [0m                      

                       Computation: 44228 steps/s (collection: 2.128s, learning 0.095s)
             Mean action noise std: 1.77
          Mean value_function loss: 3.4862
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 43.6307
                       Mean reward: 8.88
               Mean episode length: 208.20
    Episode_Reward/reaching_object: 0.7926
     Episode_Reward/lifting_object: 1.1709
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 2.22s
                      Time elapsed: 00:08:09
                               ETA: 01:05:26

################################################################################
                     [1m Learning iteration 222/2000 [0m                      

                       Computation: 47602 steps/s (collection: 1.961s, learning 0.105s)
             Mean action noise std: 1.77
          Mean value_function loss: 5.0047
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 43.6534
                       Mean reward: 9.48
               Mean episode length: 209.64
    Episode_Reward/reaching_object: 0.8192
     Episode_Reward/lifting_object: 1.3974
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 2.07s
                      Time elapsed: 00:08:12
                               ETA: 01:05:23

################################################################################
                     [1m Learning iteration 223/2000 [0m                      

                       Computation: 42014 steps/s (collection: 2.194s, learning 0.146s)
             Mean action noise std: 1.77
          Mean value_function loss: 3.9116
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 43.6955
                       Mean reward: 11.10
               Mean episode length: 208.26
    Episode_Reward/reaching_object: 0.7988
     Episode_Reward/lifting_object: 1.1964
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 2.34s
                      Time elapsed: 00:08:14
                               ETA: 01:05:22

################################################################################
                     [1m Learning iteration 224/2000 [0m                      

                       Computation: 46566 steps/s (collection: 1.916s, learning 0.195s)
             Mean action noise std: 1.78
          Mean value_function loss: 2.9974
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 43.7331
                       Mean reward: 10.36
               Mean episode length: 208.36
    Episode_Reward/reaching_object: 0.8092
     Episode_Reward/lifting_object: 1.1391
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 2.11s
                      Time elapsed: 00:08:16
                               ETA: 01:05:19

################################################################################
                     [1m Learning iteration 225/2000 [0m                      

                       Computation: 41549 steps/s (collection: 2.274s, learning 0.092s)
             Mean action noise std: 1.78
          Mean value_function loss: 5.0093
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 43.7652
                       Mean reward: 8.98
               Mean episode length: 204.52
    Episode_Reward/reaching_object: 0.7588
     Episode_Reward/lifting_object: 1.0554
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 2.37s
                      Time elapsed: 00:08:18
                               ETA: 01:05:18

################################################################################
                     [1m Learning iteration 226/2000 [0m                      

                       Computation: 43729 steps/s (collection: 2.155s, learning 0.093s)
             Mean action noise std: 1.78
          Mean value_function loss: 3.5455
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 43.7842
                       Mean reward: 11.35
               Mean episode length: 202.92
    Episode_Reward/reaching_object: 0.7930
     Episode_Reward/lifting_object: 1.6373
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 2.25s
                      Time elapsed: 00:08:21
                               ETA: 01:05:16

################################################################################
                     [1m Learning iteration 227/2000 [0m                      

                       Computation: 48033 steps/s (collection: 1.940s, learning 0.106s)
             Mean action noise std: 1.78
          Mean value_function loss: 5.6710
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 43.8073
                       Mean reward: 10.34
               Mean episode length: 208.99
    Episode_Reward/reaching_object: 0.7794
     Episode_Reward/lifting_object: 1.2437
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 2.05s
                      Time elapsed: 00:08:23
                               ETA: 01:05:12

################################################################################
                     [1m Learning iteration 228/2000 [0m                      

                       Computation: 42592 steps/s (collection: 2.195s, learning 0.113s)
             Mean action noise std: 1.78
          Mean value_function loss: 3.8157
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 43.8322
                       Mean reward: 9.50
               Mean episode length: 196.85
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 1.2728
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 2.31s
                      Time elapsed: 00:08:25
                               ETA: 01:05:11

################################################################################
                     [1m Learning iteration 229/2000 [0m                      

                       Computation: 46407 steps/s (collection: 1.946s, learning 0.172s)
             Mean action noise std: 1.79
          Mean value_function loss: 4.6260
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 43.8664
                       Mean reward: 11.21
               Mean episode length: 195.17
    Episode_Reward/reaching_object: 0.7491
     Episode_Reward/lifting_object: 1.1853
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 2.12s
                      Time elapsed: 00:08:27
                               ETA: 01:05:08

################################################################################
                     [1m Learning iteration 230/2000 [0m                      

                       Computation: 41501 steps/s (collection: 2.265s, learning 0.104s)
             Mean action noise std: 1.79
          Mean value_function loss: 4.6467
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 43.9037
                       Mean reward: 11.59
               Mean episode length: 194.40
    Episode_Reward/reaching_object: 0.7447
     Episode_Reward/lifting_object: 1.4387
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 2.37s
                      Time elapsed: 00:08:29
                               ETA: 01:05:07

################################################################################
                     [1m Learning iteration 231/2000 [0m                      

                       Computation: 41245 steps/s (collection: 2.212s, learning 0.171s)
             Mean action noise std: 1.79
          Mean value_function loss: 5.8065
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 43.9300
                       Mean reward: 12.42
               Mean episode length: 207.84
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 1.5563
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 2.38s
                      Time elapsed: 00:08:32
                               ETA: 01:05:06

################################################################################
                     [1m Learning iteration 232/2000 [0m                      

                       Computation: 38775 steps/s (collection: 2.408s, learning 0.128s)
             Mean action noise std: 1.79
          Mean value_function loss: 4.6226
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 43.9536
                       Mean reward: 12.82
               Mean episode length: 194.68
    Episode_Reward/reaching_object: 0.7597
     Episode_Reward/lifting_object: 1.3976
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 2.54s
                      Time elapsed: 00:08:34
                               ETA: 01:05:06

################################################################################
                     [1m Learning iteration 233/2000 [0m                      

                       Computation: 41436 steps/s (collection: 2.202s, learning 0.171s)
             Mean action noise std: 1.80
          Mean value_function loss: 4.7155
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 43.9851
                       Mean reward: 12.87
               Mean episode length: 207.71
    Episode_Reward/reaching_object: 0.7479
     Episode_Reward/lifting_object: 1.6045
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 2.37s
                      Time elapsed: 00:08:37
                               ETA: 01:05:05

################################################################################
                     [1m Learning iteration 234/2000 [0m                      

                       Computation: 41954 steps/s (collection: 2.166s, learning 0.177s)
             Mean action noise std: 1.80
          Mean value_function loss: 4.7477
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 44.0264
                       Mean reward: 10.77
               Mean episode length: 197.41
    Episode_Reward/reaching_object: 0.7387
     Episode_Reward/lifting_object: 1.7280
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 2.34s
                      Time elapsed: 00:08:39
                               ETA: 01:05:04

################################################################################
                     [1m Learning iteration 235/2000 [0m                      

                       Computation: 42813 steps/s (collection: 2.153s, learning 0.143s)
             Mean action noise std: 1.80
          Mean value_function loss: 4.7241
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 44.0759
                       Mean reward: 10.34
               Mean episode length: 207.38
    Episode_Reward/reaching_object: 0.7773
     Episode_Reward/lifting_object: 1.7798
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 2.30s
                      Time elapsed: 00:08:41
                               ETA: 01:05:03

################################################################################
                     [1m Learning iteration 236/2000 [0m                      

                       Computation: 42720 steps/s (collection: 2.178s, learning 0.123s)
             Mean action noise std: 1.80
          Mean value_function loss: 8.9862
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 44.1033
                       Mean reward: 14.28
               Mean episode length: 213.24
    Episode_Reward/reaching_object: 0.7833
     Episode_Reward/lifting_object: 1.9773
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 2.30s
                      Time elapsed: 00:08:44
                               ETA: 01:05:01

################################################################################
                     [1m Learning iteration 237/2000 [0m                      

                       Computation: 42487 steps/s (collection: 2.163s, learning 0.151s)
             Mean action noise std: 1.81
          Mean value_function loss: 6.9991
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 44.1361
                       Mean reward: 12.07
               Mean episode length: 208.92
    Episode_Reward/reaching_object: 0.7457
     Episode_Reward/lifting_object: 1.6286
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 2.31s
                      Time elapsed: 00:08:46
                               ETA: 01:05:00

################################################################################
                     [1m Learning iteration 238/2000 [0m                      

                       Computation: 41512 steps/s (collection: 2.244s, learning 0.125s)
             Mean action noise std: 1.81
          Mean value_function loss: 10.1899
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 44.1676
                       Mean reward: 15.40
               Mean episode length: 204.18
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 1.5407
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 2.37s
                      Time elapsed: 00:08:48
                               ETA: 01:04:59

################################################################################
                     [1m Learning iteration 239/2000 [0m                      

                       Computation: 43488 steps/s (collection: 2.120s, learning 0.140s)
             Mean action noise std: 1.81
          Mean value_function loss: 6.6162
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 44.2054
                       Mean reward: 14.51
               Mean episode length: 211.94
    Episode_Reward/reaching_object: 0.7772
     Episode_Reward/lifting_object: 1.4854
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 2.26s
                      Time elapsed: 00:08:51
                               ETA: 01:04:57

################################################################################
                     [1m Learning iteration 240/2000 [0m                      

                       Computation: 44853 steps/s (collection: 2.095s, learning 0.097s)
             Mean action noise std: 1.82
          Mean value_function loss: 5.8142
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 44.2423
                       Mean reward: 14.86
               Mean episode length: 202.89
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 1.9551
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 2.19s
                      Time elapsed: 00:08:53
                               ETA: 01:04:54

################################################################################
                     [1m Learning iteration 241/2000 [0m                      

                       Computation: 48059 steps/s (collection: 1.932s, learning 0.113s)
             Mean action noise std: 1.82
          Mean value_function loss: 5.8354
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 44.2748
                       Mean reward: 12.56
               Mean episode length: 206.76
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 1.9898
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 2.05s
                      Time elapsed: 00:08:55
                               ETA: 01:04:51

################################################################################
                     [1m Learning iteration 242/2000 [0m                      

                       Computation: 45731 steps/s (collection: 1.994s, learning 0.156s)
             Mean action noise std: 1.82
          Mean value_function loss: 5.6855
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 44.3023
                       Mean reward: 14.20
               Mean episode length: 194.77
    Episode_Reward/reaching_object: 0.7128
     Episode_Reward/lifting_object: 2.0723
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 2.15s
                      Time elapsed: 00:08:57
                               ETA: 01:04:48

################################################################################
                     [1m Learning iteration 243/2000 [0m                      

                       Computation: 47997 steps/s (collection: 1.945s, learning 0.104s)
             Mean action noise std: 1.82
          Mean value_function loss: 6.3294
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 44.3291
                       Mean reward: 13.60
               Mean episode length: 202.77
    Episode_Reward/reaching_object: 0.7377
     Episode_Reward/lifting_object: 2.1369
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 2.05s
                      Time elapsed: 00:08:59
                               ETA: 01:04:45

################################################################################
                     [1m Learning iteration 244/2000 [0m                      

                       Computation: 48461 steps/s (collection: 1.930s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 5.6718
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 44.3573
                       Mean reward: 13.32
               Mean episode length: 204.62
    Episode_Reward/reaching_object: 0.7494
     Episode_Reward/lifting_object: 2.1711
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 2.03s
                      Time elapsed: 00:09:01
                               ETA: 01:04:41

################################################################################
                     [1m Learning iteration 245/2000 [0m                      

                       Computation: 48381 steps/s (collection: 1.930s, learning 0.102s)
             Mean action noise std: 1.83
          Mean value_function loss: 6.3625
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 44.3761
                       Mean reward: 12.82
               Mean episode length: 211.37
    Episode_Reward/reaching_object: 0.7332
     Episode_Reward/lifting_object: 2.2388
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 2.03s
                      Time elapsed: 00:09:03
                               ETA: 01:04:38

################################################################################
                     [1m Learning iteration 246/2000 [0m                      

                       Computation: 47068 steps/s (collection: 1.935s, learning 0.153s)
             Mean action noise std: 1.83
          Mean value_function loss: 5.1266
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 44.3927
                       Mean reward: 14.26
               Mean episode length: 202.06
    Episode_Reward/reaching_object: 0.7084
     Episode_Reward/lifting_object: 1.9305
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 2.09s
                      Time elapsed: 00:09:05
                               ETA: 01:04:35

################################################################################
                     [1m Learning iteration 247/2000 [0m                      

                       Computation: 45438 steps/s (collection: 2.068s, learning 0.096s)
             Mean action noise std: 1.83
          Mean value_function loss: 4.7987
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 44.4151
                       Mean reward: 15.28
               Mean episode length: 200.24
    Episode_Reward/reaching_object: 0.7471
     Episode_Reward/lifting_object: 2.0741
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 2.16s
                      Time elapsed: 00:09:07
                               ETA: 01:04:32

################################################################################
                     [1m Learning iteration 248/2000 [0m                      

                       Computation: 44121 steps/s (collection: 2.111s, learning 0.117s)
             Mean action noise std: 1.83
          Mean value_function loss: 4.5484
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 44.4337
                       Mean reward: 14.65
               Mean episode length: 207.63
    Episode_Reward/reaching_object: 0.7225
     Episode_Reward/lifting_object: 2.3672
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 2.23s
                      Time elapsed: 00:09:10
                               ETA: 01:04:30

################################################################################
                     [1m Learning iteration 249/2000 [0m                      

                       Computation: 47456 steps/s (collection: 1.982s, learning 0.089s)
             Mean action noise std: 1.83
          Mean value_function loss: 8.4637
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 44.4537
                       Mean reward: 15.84
               Mean episode length: 201.70
    Episode_Reward/reaching_object: 0.6987
     Episode_Reward/lifting_object: 1.9915
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 2.07s
                      Time elapsed: 00:09:12
                               ETA: 01:04:27

################################################################################
                     [1m Learning iteration 250/2000 [0m                      

                       Computation: 46693 steps/s (collection: 2.011s, learning 0.095s)
             Mean action noise std: 1.84
          Mean value_function loss: 5.7307
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 44.4863
                       Mean reward: 13.62
               Mean episode length: 204.16
    Episode_Reward/reaching_object: 0.7311
     Episode_Reward/lifting_object: 2.0130
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 2.11s
                      Time elapsed: 00:09:14
                               ETA: 01:04:24

################################################################################
                     [1m Learning iteration 251/2000 [0m                      

                       Computation: 49045 steps/s (collection: 1.914s, learning 0.091s)
             Mean action noise std: 1.84
          Mean value_function loss: 5.3318
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 44.5184
                       Mean reward: 15.12
               Mean episode length: 201.61
    Episode_Reward/reaching_object: 0.7080
     Episode_Reward/lifting_object: 2.2911
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 2.00s
                      Time elapsed: 00:09:16
                               ETA: 01:04:20

################################################################################
                     [1m Learning iteration 252/2000 [0m                      

                       Computation: 48437 steps/s (collection: 1.916s, learning 0.113s)
             Mean action noise std: 1.84
          Mean value_function loss: 6.7232
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 44.5431
                       Mean reward: 13.75
               Mean episode length: 194.28
    Episode_Reward/reaching_object: 0.7264
     Episode_Reward/lifting_object: 2.3829
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 9.9583
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 2.03s
                      Time elapsed: 00:09:18
                               ETA: 01:04:17

################################################################################
                     [1m Learning iteration 253/2000 [0m                      

                       Computation: 42532 steps/s (collection: 2.110s, learning 0.201s)
             Mean action noise std: 1.84
          Mean value_function loss: 6.0559
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 44.5735
                       Mean reward: 12.35
               Mean episode length: 197.40
    Episode_Reward/reaching_object: 0.7174
     Episode_Reward/lifting_object: 2.1125
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 2.31s
                      Time elapsed: 00:09:20
                               ETA: 01:04:16

################################################################################
                     [1m Learning iteration 254/2000 [0m                      

                       Computation: 47710 steps/s (collection: 1.957s, learning 0.103s)
             Mean action noise std: 1.85
          Mean value_function loss: 8.3093
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 44.5975
                       Mean reward: 12.00
               Mean episode length: 194.96
    Episode_Reward/reaching_object: 0.6976
     Episode_Reward/lifting_object: 1.8807
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 2.06s
                      Time elapsed: 00:09:22
                               ETA: 01:04:12

################################################################################
                     [1m Learning iteration 255/2000 [0m                      

                       Computation: 48402 steps/s (collection: 1.939s, learning 0.092s)
             Mean action noise std: 1.85
          Mean value_function loss: 8.3223
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 44.6180
                       Mean reward: 14.04
               Mean episode length: 194.78
    Episode_Reward/reaching_object: 0.6849
     Episode_Reward/lifting_object: 2.2098
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 2.03s
                      Time elapsed: 00:09:24
                               ETA: 01:04:09

################################################################################
                     [1m Learning iteration 256/2000 [0m                      

                       Computation: 46588 steps/s (collection: 2.007s, learning 0.103s)
             Mean action noise std: 1.85
          Mean value_function loss: 7.5431
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 44.6382
                       Mean reward: 15.55
               Mean episode length: 188.75
    Episode_Reward/reaching_object: 0.6781
     Episode_Reward/lifting_object: 2.5355
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 2.11s
                      Time elapsed: 00:09:26
                               ETA: 01:04:06

################################################################################
                     [1m Learning iteration 257/2000 [0m                      

                       Computation: 48586 steps/s (collection: 1.931s, learning 0.092s)
             Mean action noise std: 1.85
          Mean value_function loss: 8.4089
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 44.6504
                       Mean reward: 13.37
               Mean episode length: 195.87
    Episode_Reward/reaching_object: 0.6835
     Episode_Reward/lifting_object: 2.4130
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 2.02s
                      Time elapsed: 00:09:28
                               ETA: 01:04:03

################################################################################
                     [1m Learning iteration 258/2000 [0m                      

                       Computation: 48697 steps/s (collection: 1.901s, learning 0.118s)
             Mean action noise std: 1.85
          Mean value_function loss: 11.3142
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 44.6672
                       Mean reward: 16.47
               Mean episode length: 183.17
    Episode_Reward/reaching_object: 0.6834
     Episode_Reward/lifting_object: 2.5536
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 2.02s
                      Time elapsed: 00:09:30
                               ETA: 01:03:59

################################################################################
                     [1m Learning iteration 259/2000 [0m                      

                       Computation: 47904 steps/s (collection: 1.946s, learning 0.107s)
             Mean action noise std: 1.85
          Mean value_function loss: 10.2853
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 44.6925
                       Mean reward: 12.20
               Mean episode length: 187.00
    Episode_Reward/reaching_object: 0.6525
     Episode_Reward/lifting_object: 2.0374
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 2.05s
                      Time elapsed: 00:09:32
                               ETA: 01:03:56

################################################################################
                     [1m Learning iteration 260/2000 [0m                      

                       Computation: 47500 steps/s (collection: 1.950s, learning 0.119s)
             Mean action noise std: 1.86
          Mean value_function loss: 10.1486
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 44.7145
                       Mean reward: 13.57
               Mean episode length: 185.72
    Episode_Reward/reaching_object: 0.6501
     Episode_Reward/lifting_object: 2.4780
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 2.07s
                      Time elapsed: 00:09:34
                               ETA: 01:03:53

################################################################################
                     [1m Learning iteration 261/2000 [0m                      

                       Computation: 43445 steps/s (collection: 2.005s, learning 0.258s)
             Mean action noise std: 1.86
          Mean value_function loss: 8.4111
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 44.7316
                       Mean reward: 14.55
               Mean episode length: 169.84
    Episode_Reward/reaching_object: 0.6372
     Episode_Reward/lifting_object: 2.6100
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 2.26s
                      Time elapsed: 00:09:37
                               ETA: 01:03:51

################################################################################
                     [1m Learning iteration 262/2000 [0m                      

                       Computation: 44725 steps/s (collection: 2.050s, learning 0.148s)
             Mean action noise std: 1.86
          Mean value_function loss: 9.8292
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 44.7512
                       Mean reward: 14.68
               Mean episode length: 179.99
    Episode_Reward/reaching_object: 0.6343
     Episode_Reward/lifting_object: 2.6641
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 2.20s
                      Time elapsed: 00:09:39
                               ETA: 01:03:49

################################################################################
                     [1m Learning iteration 263/2000 [0m                      

                       Computation: 45761 steps/s (collection: 2.008s, learning 0.141s)
             Mean action noise std: 1.86
          Mean value_function loss: 10.4106
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 44.7818
                       Mean reward: 18.64
               Mean episode length: 167.35
    Episode_Reward/reaching_object: 0.6064
     Episode_Reward/lifting_object: 2.6089
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 2.15s
                      Time elapsed: 00:09:41
                               ETA: 01:03:46

################################################################################
                     [1m Learning iteration 264/2000 [0m                      

                       Computation: 46040 steps/s (collection: 2.012s, learning 0.124s)
             Mean action noise std: 1.86
          Mean value_function loss: 9.0297
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 44.8064
                       Mean reward: 17.17
               Mean episode length: 176.06
    Episode_Reward/reaching_object: 0.5956
     Episode_Reward/lifting_object: 2.4342
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 16.5833
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 2.14s
                      Time elapsed: 00:09:43
                               ETA: 01:03:44

################################################################################
                     [1m Learning iteration 265/2000 [0m                      

                       Computation: 46519 steps/s (collection: 2.012s, learning 0.101s)
             Mean action noise std: 1.87
          Mean value_function loss: 8.1226
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 44.8315
                       Mean reward: 17.25
               Mean episode length: 171.24
    Episode_Reward/reaching_object: 0.6210
     Episode_Reward/lifting_object: 2.8938
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 16.3750
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 2.11s
                      Time elapsed: 00:09:45
                               ETA: 01:03:41

################################################################################
                     [1m Learning iteration 266/2000 [0m                      

                       Computation: 46829 steps/s (collection: 2.004s, learning 0.095s)
             Mean action noise std: 1.87
          Mean value_function loss: 8.5580
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 44.8499
                       Mean reward: 15.28
               Mean episode length: 168.50
    Episode_Reward/reaching_object: 0.5916
     Episode_Reward/lifting_object: 2.4976
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 17.9167
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 2.10s
                      Time elapsed: 00:09:47
                               ETA: 01:03:38

################################################################################
                     [1m Learning iteration 267/2000 [0m                      

                       Computation: 47540 steps/s (collection: 1.957s, learning 0.111s)
             Mean action noise std: 1.87
          Mean value_function loss: 8.9293
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 44.8667
                       Mean reward: 16.51
               Mean episode length: 168.20
    Episode_Reward/reaching_object: 0.5810
     Episode_Reward/lifting_object: 2.6903
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 2.07s
                      Time elapsed: 00:09:50
                               ETA: 01:03:35

################################################################################
                     [1m Learning iteration 268/2000 [0m                      

                       Computation: 47134 steps/s (collection: 1.981s, learning 0.105s)
             Mean action noise std: 1.87
          Mean value_function loss: 9.2127
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 44.8839
                       Mean reward: 14.91
               Mean episode length: 169.26
    Episode_Reward/reaching_object: 0.5988
     Episode_Reward/lifting_object: 2.6195
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 17.2917
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 2.09s
                      Time elapsed: 00:09:52
                               ETA: 01:03:32

################################################################################
                     [1m Learning iteration 269/2000 [0m                      

                       Computation: 48933 steps/s (collection: 1.904s, learning 0.105s)
             Mean action noise std: 1.87
          Mean value_function loss: 8.5952
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 44.9026
                       Mean reward: 20.19
               Mean episode length: 172.84
    Episode_Reward/reaching_object: 0.5828
     Episode_Reward/lifting_object: 3.0021
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 18.4583
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 2.01s
                      Time elapsed: 00:09:54
                               ETA: 01:03:28

################################################################################
                     [1m Learning iteration 270/2000 [0m                      

                       Computation: 48142 steps/s (collection: 1.952s, learning 0.090s)
             Mean action noise std: 1.87
          Mean value_function loss: 10.4086
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 44.9235
                       Mean reward: 17.67
               Mean episode length: 155.79
    Episode_Reward/reaching_object: 0.5603
     Episode_Reward/lifting_object: 2.8607
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 5.3750
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 18.9583
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 2.04s
                      Time elapsed: 00:09:56
                               ETA: 01:03:25

################################################################################
                     [1m Learning iteration 271/2000 [0m                      

                       Computation: 47116 steps/s (collection: 1.988s, learning 0.099s)
             Mean action noise std: 1.88
          Mean value_function loss: 11.5371
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 44.9376
                       Mean reward: 14.69
               Mean episode length: 161.61
    Episode_Reward/reaching_object: 0.5897
     Episode_Reward/lifting_object: 2.8109
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 18.5000
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 2.09s
                      Time elapsed: 00:09:58
                               ETA: 01:03:22

################################################################################
                     [1m Learning iteration 272/2000 [0m                      

                       Computation: 48082 steps/s (collection: 1.936s, learning 0.108s)
             Mean action noise std: 1.88
          Mean value_function loss: 9.8684
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 44.9497
                       Mean reward: 14.80
               Mean episode length: 155.15
    Episode_Reward/reaching_object: 0.5516
     Episode_Reward/lifting_object: 2.7779
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 21.0417
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 2.04s
                      Time elapsed: 00:10:00
                               ETA: 01:03:19

################################################################################
                     [1m Learning iteration 273/2000 [0m                      

                       Computation: 48342 steps/s (collection: 1.928s, learning 0.105s)
             Mean action noise std: 1.88
          Mean value_function loss: 8.9261
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 44.9584
                       Mean reward: 17.37
               Mean episode length: 162.87
    Episode_Reward/reaching_object: 0.5466
     Episode_Reward/lifting_object: 2.8527
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 21.6667
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 2.03s
                      Time elapsed: 00:10:02
                               ETA: 01:03:16

################################################################################
                     [1m Learning iteration 274/2000 [0m                      

                       Computation: 45568 steps/s (collection: 2.064s, learning 0.094s)
             Mean action noise std: 1.88
          Mean value_function loss: 11.5121
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 44.9740
                       Mean reward: 16.44
               Mean episode length: 160.41
    Episode_Reward/reaching_object: 0.5407
     Episode_Reward/lifting_object: 2.7768
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 22.2083
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 2.16s
                      Time elapsed: 00:10:04
                               ETA: 01:03:13

################################################################################
                     [1m Learning iteration 275/2000 [0m                      

                       Computation: 49178 steps/s (collection: 1.905s, learning 0.094s)
             Mean action noise std: 1.88
          Mean value_function loss: 10.9015
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 44.9906
                       Mean reward: 18.17
               Mean episode length: 146.06
    Episode_Reward/reaching_object: 0.5441
     Episode_Reward/lifting_object: 3.0364
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 21.7083
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 2.00s
                      Time elapsed: 00:10:06
                               ETA: 01:03:10

################################################################################
                     [1m Learning iteration 276/2000 [0m                      

                       Computation: 47826 steps/s (collection: 1.966s, learning 0.090s)
             Mean action noise std: 1.88
          Mean value_function loss: 11.5536
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 45.0015
                       Mean reward: 17.50
               Mean episode length: 147.08
    Episode_Reward/reaching_object: 0.5560
     Episode_Reward/lifting_object: 3.2138
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 5.3750
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 22.6667
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 2.06s
                      Time elapsed: 00:10:08
                               ETA: 01:03:07

################################################################################
                     [1m Learning iteration 277/2000 [0m                      

                       Computation: 47376 steps/s (collection: 1.946s, learning 0.129s)
             Mean action noise std: 1.88
          Mean value_function loss: 10.2077
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 45.0125
                       Mean reward: 16.62
               Mean episode length: 140.20
    Episode_Reward/reaching_object: 0.5000
     Episode_Reward/lifting_object: 2.8186
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 24.7500
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 2.07s
                      Time elapsed: 00:10:10
                               ETA: 01:03:04

################################################################################
                     [1m Learning iteration 278/2000 [0m                      

                       Computation: 47079 steps/s (collection: 1.899s, learning 0.189s)
             Mean action noise std: 1.88
          Mean value_function loss: 10.6493
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 45.0264
                       Mean reward: 14.95
               Mean episode length: 139.43
    Episode_Reward/reaching_object: 0.5068
     Episode_Reward/lifting_object: 3.0417
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 23.3750
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 2.09s
                      Time elapsed: 00:10:12
                               ETA: 01:03:01

################################################################################
                     [1m Learning iteration 279/2000 [0m                      

                       Computation: 47239 steps/s (collection: 1.954s, learning 0.127s)
             Mean action noise std: 1.89
          Mean value_function loss: 13.1000
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 45.0402
                       Mean reward: 16.84
               Mean episode length: 142.20
    Episode_Reward/reaching_object: 0.4940
     Episode_Reward/lifting_object: 2.6838
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 23.3750
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 2.08s
                      Time elapsed: 00:10:14
                               ETA: 01:02:58

################################################################################
                     [1m Learning iteration 280/2000 [0m                      

                       Computation: 46736 steps/s (collection: 1.936s, learning 0.167s)
             Mean action noise std: 1.89
          Mean value_function loss: 13.7563
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 45.0508
                       Mean reward: 20.42
               Mean episode length: 147.25
    Episode_Reward/reaching_object: 0.5003
     Episode_Reward/lifting_object: 3.2443
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 25.2917
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 2.10s
                      Time elapsed: 00:10:16
                               ETA: 01:02:55

################################################################################
                     [1m Learning iteration 281/2000 [0m                      

                       Computation: 49285 steps/s (collection: 1.909s, learning 0.086s)
             Mean action noise std: 1.89
          Mean value_function loss: 12.5573
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 45.0574
                       Mean reward: 20.01
               Mean episode length: 141.37
    Episode_Reward/reaching_object: 0.4790
     Episode_Reward/lifting_object: 2.9313
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 2.9583
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 24.4167
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 1.99s
                      Time elapsed: 00:10:18
                               ETA: 01:02:52

################################################################################
                     [1m Learning iteration 282/2000 [0m                      

                       Computation: 46720 steps/s (collection: 2.015s, learning 0.089s)
             Mean action noise std: 1.89
          Mean value_function loss: 11.5947
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 45.0618
                       Mean reward: 14.73
               Mean episode length: 143.02
    Episode_Reward/reaching_object: 0.4949
     Episode_Reward/lifting_object: 2.9856
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 24.8750
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 2.10s
                      Time elapsed: 00:10:20
                               ETA: 01:02:49

################################################################################
                     [1m Learning iteration 283/2000 [0m                      

                       Computation: 48024 steps/s (collection: 1.960s, learning 0.087s)
             Mean action noise std: 1.89
          Mean value_function loss: 14.5618
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 45.0701
                       Mean reward: 15.19
               Mean episode length: 139.26
    Episode_Reward/reaching_object: 0.4620
     Episode_Reward/lifting_object: 2.7611
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 1.7917
     Episode_Termination/robot_out: 25.6250
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 2.05s
                      Time elapsed: 00:10:23
                               ETA: 01:02:46

################################################################################
                     [1m Learning iteration 284/2000 [0m                      

                       Computation: 47708 steps/s (collection: 1.941s, learning 0.120s)
             Mean action noise std: 1.89
          Mean value_function loss: 17.7276
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 45.0838
                       Mean reward: 17.27
               Mean episode length: 139.51
    Episode_Reward/reaching_object: 0.4678
     Episode_Reward/lifting_object: 2.9373
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 3.7083
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 28.2500
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 2.06s
                      Time elapsed: 00:10:25
                               ETA: 01:02:43

################################################################################
                     [1m Learning iteration 285/2000 [0m                      

                       Computation: 48203 steps/s (collection: 1.950s, learning 0.089s)
             Mean action noise std: 1.89
          Mean value_function loss: 12.7130
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 45.0955
                       Mean reward: 17.11
               Mean episode length: 133.85
    Episode_Reward/reaching_object: 0.4575
     Episode_Reward/lifting_object: 3.1632
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 27.3750
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 2.04s
                      Time elapsed: 00:10:27
                               ETA: 01:02:40

################################################################################
                     [1m Learning iteration 286/2000 [0m                      

                       Computation: 47372 steps/s (collection: 1.954s, learning 0.122s)
             Mean action noise std: 1.89
          Mean value_function loss: 11.3024
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 45.1038
                       Mean reward: 15.88
               Mean episode length: 134.58
    Episode_Reward/reaching_object: 0.4466
     Episode_Reward/lifting_object: 3.0718
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 28.0833
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 2.08s
                      Time elapsed: 00:10:29
                               ETA: 01:02:37

################################################################################
                     [1m Learning iteration 287/2000 [0m                      

                       Computation: 47213 steps/s (collection: 1.984s, learning 0.098s)
             Mean action noise std: 1.89
          Mean value_function loss: 18.3166
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 45.1170
                       Mean reward: 8.79
               Mean episode length: 132.07
    Episode_Reward/reaching_object: 0.4395
     Episode_Reward/lifting_object: 3.0857
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 29.5417
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 2.08s
                      Time elapsed: 00:10:31
                               ETA: 01:02:34

################################################################################
                     [1m Learning iteration 288/2000 [0m                      

                       Computation: 47049 steps/s (collection: 1.999s, learning 0.091s)
             Mean action noise std: 1.89
          Mean value_function loss: 12.6390
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 45.1319
                       Mean reward: 18.51
               Mean episode length: 119.26
    Episode_Reward/reaching_object: 0.4374
     Episode_Reward/lifting_object: 3.1726
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 29.2917
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 2.09s
                      Time elapsed: 00:10:33
                               ETA: 01:02:32

################################################################################
                     [1m Learning iteration 289/2000 [0m                      

                       Computation: 46512 steps/s (collection: 2.010s, learning 0.103s)
             Mean action noise std: 1.89
          Mean value_function loss: 17.0601
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 45.1410
                       Mean reward: 18.26
               Mean episode length: 126.43
    Episode_Reward/reaching_object: 0.4250
     Episode_Reward/lifting_object: 3.2016
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 28.2917
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 2.11s
                      Time elapsed: 00:10:35
                               ETA: 01:02:29

################################################################################
                     [1m Learning iteration 290/2000 [0m                      

                       Computation: 42824 steps/s (collection: 2.112s, learning 0.184s)
             Mean action noise std: 1.89
          Mean value_function loss: 12.6645
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 45.1441
                       Mean reward: 17.71
               Mean episode length: 116.56
    Episode_Reward/reaching_object: 0.4309
     Episode_Reward/lifting_object: 3.1966
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 26.9583
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 2.30s
                      Time elapsed: 00:10:37
                               ETA: 01:02:27

################################################################################
                     [1m Learning iteration 291/2000 [0m                      

                       Computation: 47337 steps/s (collection: 1.982s, learning 0.095s)
             Mean action noise std: 1.90
          Mean value_function loss: 11.7647
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 45.1512
                       Mean reward: 20.75
               Mean episode length: 130.31
    Episode_Reward/reaching_object: 0.4378
     Episode_Reward/lifting_object: 3.4707
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 28.3750
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 2.08s
                      Time elapsed: 00:10:39
                               ETA: 01:02:24

################################################################################
                     [1m Learning iteration 292/2000 [0m                      

                       Computation: 44513 steps/s (collection: 2.115s, learning 0.093s)
             Mean action noise std: 1.90
          Mean value_function loss: 13.0648
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 45.1605
                       Mean reward: 19.91
               Mean episode length: 134.35
    Episode_Reward/reaching_object: 0.4415
     Episode_Reward/lifting_object: 3.6651
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 1.7917
     Episode_Termination/robot_out: 26.9583
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 2.21s
                      Time elapsed: 00:10:42
                               ETA: 01:02:22

################################################################################
                     [1m Learning iteration 293/2000 [0m                      

                       Computation: 42649 steps/s (collection: 2.207s, learning 0.097s)
             Mean action noise std: 1.90
          Mean value_function loss: 13.8428
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 45.1720
                       Mean reward: 20.88
               Mean episode length: 134.95
    Episode_Reward/reaching_object: 0.4498
     Episode_Reward/lifting_object: 3.4435
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 26.8333
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 2.30s
                      Time elapsed: 00:10:44
                               ETA: 01:02:21

################################################################################
                     [1m Learning iteration 294/2000 [0m                      

                       Computation: 47221 steps/s (collection: 1.984s, learning 0.098s)
             Mean action noise std: 1.90
          Mean value_function loss: 13.4840
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 45.1904
                       Mean reward: 24.11
               Mean episode length: 139.90
    Episode_Reward/reaching_object: 0.4707
     Episode_Reward/lifting_object: 3.5003
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 27.2083
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 2.08s
                      Time elapsed: 00:10:46
                               ETA: 01:02:18

################################################################################
                     [1m Learning iteration 295/2000 [0m                      

                       Computation: 46654 steps/s (collection: 2.005s, learning 0.102s)
             Mean action noise std: 1.90
          Mean value_function loss: 12.3880
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 45.2016
                       Mean reward: 19.86
               Mean episode length: 138.87
    Episode_Reward/reaching_object: 0.4951
     Episode_Reward/lifting_object: 3.9148
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 25.5417
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 2.11s
                      Time elapsed: 00:10:48
                               ETA: 01:02:15

################################################################################
                     [1m Learning iteration 296/2000 [0m                      

                       Computation: 43502 steps/s (collection: 2.128s, learning 0.132s)
             Mean action noise std: 1.90
          Mean value_function loss: 12.3555
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 45.2114
                       Mean reward: 21.57
               Mean episode length: 127.96
    Episode_Reward/reaching_object: 0.4701
     Episode_Reward/lifting_object: 3.7360
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 26.3333
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 2.26s
                      Time elapsed: 00:10:50
                               ETA: 01:02:14

################################################################################
                     [1m Learning iteration 297/2000 [0m                      

                       Computation: 45766 steps/s (collection: 2.045s, learning 0.103s)
             Mean action noise std: 1.90
          Mean value_function loss: 12.4257
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 45.2228
                       Mean reward: 22.84
               Mean episode length: 131.45
    Episode_Reward/reaching_object: 0.4909
     Episode_Reward/lifting_object: 4.1147
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 25.0833
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 2.15s
                      Time elapsed: 00:10:52
                               ETA: 01:02:11

################################################################################
                     [1m Learning iteration 298/2000 [0m                      

                       Computation: 43318 steps/s (collection: 2.078s, learning 0.191s)
             Mean action noise std: 1.90
          Mean value_function loss: 12.7618
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 45.2366
                       Mean reward: 22.28
               Mean episode length: 143.28
    Episode_Reward/reaching_object: 0.4851
     Episode_Reward/lifting_object: 3.8110
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 23.6250
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 2.27s
                      Time elapsed: 00:10:55
                               ETA: 01:02:09

################################################################################
                     [1m Learning iteration 299/2000 [0m                      

                       Computation: 47342 steps/s (collection: 1.975s, learning 0.102s)
             Mean action noise std: 1.90
          Mean value_function loss: 13.2643
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 45.2491
                       Mean reward: 22.29
               Mean episode length: 136.12
    Episode_Reward/reaching_object: 0.5084
     Episode_Reward/lifting_object: 4.1632
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 2.9583
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 24.1250
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 2.08s
                      Time elapsed: 00:10:57
                               ETA: 01:02:06

################################################################################
                     [1m Learning iteration 300/2000 [0m                      

                       Computation: 44930 steps/s (collection: 2.030s, learning 0.158s)
             Mean action noise std: 1.90
          Mean value_function loss: 11.8889
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 45.2561
                       Mean reward: 24.41
               Mean episode length: 143.25
    Episode_Reward/reaching_object: 0.5079
     Episode_Reward/lifting_object: 4.0513
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 25.8750
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 2.19s
                      Time elapsed: 00:10:59
                               ETA: 01:02:04

################################################################################
                     [1m Learning iteration 301/2000 [0m                      

                       Computation: 45727 steps/s (collection: 2.038s, learning 0.112s)
             Mean action noise std: 1.91
          Mean value_function loss: 14.0125
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 45.2672
                       Mean reward: 23.14
               Mean episode length: 146.84
    Episode_Reward/reaching_object: 0.5084
     Episode_Reward/lifting_object: 4.4089
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 26.0833
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 2.15s
                      Time elapsed: 00:11:01
                               ETA: 01:02:02

################################################################################
                     [1m Learning iteration 302/2000 [0m                      

                       Computation: 46898 steps/s (collection: 1.998s, learning 0.098s)
             Mean action noise std: 1.91
          Mean value_function loss: 14.4796
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 45.2766
                       Mean reward: 20.97
               Mean episode length: 124.31
    Episode_Reward/reaching_object: 0.4802
     Episode_Reward/lifting_object: 4.2000
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 26.6667
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 2.10s
                      Time elapsed: 00:11:03
                               ETA: 01:01:59

################################################################################
                     [1m Learning iteration 303/2000 [0m                      

                       Computation: 48823 steps/s (collection: 1.920s, learning 0.094s)
             Mean action noise std: 1.91
          Mean value_function loss: 15.5285
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 45.2809
                       Mean reward: 21.72
               Mean episode length: 134.28
    Episode_Reward/reaching_object: 0.4985
     Episode_Reward/lifting_object: 4.3284
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 28.4167
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 2.01s
                      Time elapsed: 00:11:05
                               ETA: 01:01:56

################################################################################
                     [1m Learning iteration 304/2000 [0m                      

                       Computation: 45583 steps/s (collection: 1.995s, learning 0.161s)
             Mean action noise std: 1.91
          Mean value_function loss: 14.5444
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 45.2853
                       Mean reward: 26.81
               Mean episode length: 144.75
    Episode_Reward/reaching_object: 0.4805
     Episode_Reward/lifting_object: 4.2233
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 25.9167
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 2.16s
                      Time elapsed: 00:11:07
                               ETA: 01:01:54

################################################################################
                     [1m Learning iteration 305/2000 [0m                      

                       Computation: 48408 steps/s (collection: 1.921s, learning 0.110s)
             Mean action noise std: 1.91
          Mean value_function loss: 15.2047
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 45.2910
                       Mean reward: 22.05
               Mean episode length: 131.01
    Episode_Reward/reaching_object: 0.4710
     Episode_Reward/lifting_object: 4.2356
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 27.7500
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 2.03s
                      Time elapsed: 00:11:09
                               ETA: 01:01:50

################################################################################
                     [1m Learning iteration 306/2000 [0m                      

                       Computation: 46173 steps/s (collection: 1.983s, learning 0.146s)
             Mean action noise std: 1.91
          Mean value_function loss: 15.9998
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 45.2960
                       Mean reward: 23.65
               Mean episode length: 128.63
    Episode_Reward/reaching_object: 0.4715
     Episode_Reward/lifting_object: 4.4050
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 28.1667
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 2.13s
                      Time elapsed: 00:11:12
                               ETA: 01:01:48

################################################################################
                     [1m Learning iteration 307/2000 [0m                      

                       Computation: 46387 steps/s (collection: 2.023s, learning 0.096s)
             Mean action noise std: 1.91
          Mean value_function loss: 14.7427
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 45.3000
                       Mean reward: 22.44
               Mean episode length: 116.11
    Episode_Reward/reaching_object: 0.4403
     Episode_Reward/lifting_object: 4.2688
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 28.0833
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 2.12s
                      Time elapsed: 00:11:14
                               ETA: 01:01:45

################################################################################
                     [1m Learning iteration 308/2000 [0m                      

                       Computation: 44125 steps/s (collection: 2.132s, learning 0.096s)
             Mean action noise std: 1.91
          Mean value_function loss: 17.6220
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 45.3073
                       Mean reward: 21.15
               Mean episode length: 139.09
    Episode_Reward/reaching_object: 0.4472
     Episode_Reward/lifting_object: 4.2329
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 28.9583
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 2.23s
                      Time elapsed: 00:11:16
                               ETA: 01:01:43

################################################################################
                     [1m Learning iteration 309/2000 [0m                      

                       Computation: 46076 steps/s (collection: 2.041s, learning 0.093s)
             Mean action noise std: 1.91
          Mean value_function loss: 19.4490
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 45.3195
                       Mean reward: 23.76
               Mean episode length: 126.99
    Episode_Reward/reaching_object: 0.4439
     Episode_Reward/lifting_object: 4.3186
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 30.7500
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 2.13s
                      Time elapsed: 00:11:18
                               ETA: 01:01:41

################################################################################
                     [1m Learning iteration 310/2000 [0m                      

                       Computation: 41849 steps/s (collection: 2.237s, learning 0.112s)
             Mean action noise std: 1.91
          Mean value_function loss: 14.1928
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 45.3269
                       Mean reward: 26.99
               Mean episode length: 125.01
    Episode_Reward/reaching_object: 0.4427
     Episode_Reward/lifting_object: 4.8341
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 30.0417
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 2.35s
                      Time elapsed: 00:11:20
                               ETA: 01:01:40

################################################################################
                     [1m Learning iteration 311/2000 [0m                      

                       Computation: 44593 steps/s (collection: 2.064s, learning 0.140s)
             Mean action noise std: 1.91
          Mean value_function loss: 16.0262
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 45.3336
                       Mean reward: 23.33
               Mean episode length: 130.94
    Episode_Reward/reaching_object: 0.4499
     Episode_Reward/lifting_object: 4.3113
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 30.5000
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 2.20s
                      Time elapsed: 00:11:23
                               ETA: 01:01:38

################################################################################
                     [1m Learning iteration 312/2000 [0m                      

                       Computation: 44087 steps/s (collection: 2.098s, learning 0.132s)
             Mean action noise std: 1.91
          Mean value_function loss: 15.9259
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 45.3382
                       Mean reward: 27.22
               Mean episode length: 132.41
    Episode_Reward/reaching_object: 0.4407
     Episode_Reward/lifting_object: 4.2266
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 30.6250
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 2.23s
                      Time elapsed: 00:11:25
                               ETA: 01:01:36

################################################################################
                     [1m Learning iteration 313/2000 [0m                      

                       Computation: 46003 steps/s (collection: 2.027s, learning 0.110s)
             Mean action noise std: 1.91
          Mean value_function loss: 14.4276
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 45.3416
                       Mean reward: 26.56
               Mean episode length: 125.45
    Episode_Reward/reaching_object: 0.4401
     Episode_Reward/lifting_object: 4.4563
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 31.1250
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 2.14s
                      Time elapsed: 00:11:27
                               ETA: 01:01:33

################################################################################
                     [1m Learning iteration 314/2000 [0m                      

                       Computation: 44979 steps/s (collection: 2.067s, learning 0.119s)
             Mean action noise std: 1.91
          Mean value_function loss: 16.5271
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 45.3453
                       Mean reward: 25.99
               Mean episode length: 122.49
    Episode_Reward/reaching_object: 0.4277
     Episode_Reward/lifting_object: 4.5449
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 32.6250
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 2.19s
                      Time elapsed: 00:11:29
                               ETA: 01:01:31

################################################################################
                     [1m Learning iteration 315/2000 [0m                      

                       Computation: 46465 steps/s (collection: 2.012s, learning 0.104s)
             Mean action noise std: 1.91
          Mean value_function loss: 16.4315
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 45.3491
                       Mean reward: 23.64
               Mean episode length: 120.08
    Episode_Reward/reaching_object: 0.4362
     Episode_Reward/lifting_object: 4.5174
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 31.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 2.12s
                      Time elapsed: 00:11:31
                               ETA: 01:01:28

################################################################################
                     [1m Learning iteration 316/2000 [0m                      

                       Computation: 46082 steps/s (collection: 2.026s, learning 0.108s)
             Mean action noise std: 1.91
          Mean value_function loss: 16.9995
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 45.3542
                       Mean reward: 26.89
               Mean episode length: 122.11
    Episode_Reward/reaching_object: 0.4143
     Episode_Reward/lifting_object: 4.3549
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 31.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 2.13s
                      Time elapsed: 00:11:33
                               ETA: 01:01:26

################################################################################
                     [1m Learning iteration 317/2000 [0m                      

                       Computation: 43964 steps/s (collection: 2.014s, learning 0.222s)
             Mean action noise std: 1.91
          Mean value_function loss: 18.0968
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 45.3555
                       Mean reward: 23.25
               Mean episode length: 116.16
    Episode_Reward/reaching_object: 0.4218
     Episode_Reward/lifting_object: 4.6189
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 32.9583
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 2.24s
                      Time elapsed: 00:11:36
                               ETA: 01:01:24

################################################################################
                     [1m Learning iteration 318/2000 [0m                      

                       Computation: 45373 steps/s (collection: 2.036s, learning 0.131s)
             Mean action noise std: 1.91
          Mean value_function loss: 15.5438
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 45.3565
                       Mean reward: 21.13
               Mean episode length: 107.72
    Episode_Reward/reaching_object: 0.4251
     Episode_Reward/lifting_object: 4.6992
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 33.9583
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 2.17s
                      Time elapsed: 00:11:38
                               ETA: 01:01:22

################################################################################
                     [1m Learning iteration 319/2000 [0m                      

                       Computation: 46700 steps/s (collection: 1.982s, learning 0.123s)
             Mean action noise std: 1.91
          Mean value_function loss: 17.3747
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 45.3598
                       Mean reward: 25.40
               Mean episode length: 125.94
    Episode_Reward/reaching_object: 0.4278
     Episode_Reward/lifting_object: 4.3941
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 32.7500
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 2.11s
                      Time elapsed: 00:11:40
                               ETA: 01:01:19

################################################################################
                     [1m Learning iteration 320/2000 [0m                      

                       Computation: 46959 steps/s (collection: 1.997s, learning 0.096s)
             Mean action noise std: 1.91
          Mean value_function loss: 17.9732
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 45.3619
                       Mean reward: 22.11
               Mean episode length: 116.67
    Episode_Reward/reaching_object: 0.4208
     Episode_Reward/lifting_object: 4.4792
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 34.2083
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 2.09s
                      Time elapsed: 00:11:42
                               ETA: 01:01:16

################################################################################
                     [1m Learning iteration 321/2000 [0m                      

                       Computation: 46529 steps/s (collection: 2.018s, learning 0.095s)
             Mean action noise std: 1.92
          Mean value_function loss: 21.8756
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 45.3673
                       Mean reward: 24.80
               Mean episode length: 124.18
    Episode_Reward/reaching_object: 0.4361
     Episode_Reward/lifting_object: 4.6536
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 32.8750
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 2.11s
                      Time elapsed: 00:11:44
                               ETA: 01:01:14

################################################################################
                     [1m Learning iteration 322/2000 [0m                      

                       Computation: 46988 steps/s (collection: 1.995s, learning 0.098s)
             Mean action noise std: 1.92
          Mean value_function loss: 20.8094
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 45.3769
                       Mean reward: 26.69
               Mean episode length: 120.74
    Episode_Reward/reaching_object: 0.4100
     Episode_Reward/lifting_object: 4.2387
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 34.6250
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 2.09s
                      Time elapsed: 00:11:46
                               ETA: 01:01:11

################################################################################
                     [1m Learning iteration 323/2000 [0m                      

                       Computation: 47242 steps/s (collection: 1.983s, learning 0.098s)
             Mean action noise std: 1.92
          Mean value_function loss: 22.2671
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 45.3870
                       Mean reward: 18.98
               Mean episode length: 100.66
    Episode_Reward/reaching_object: 0.4067
     Episode_Reward/lifting_object: 4.4644
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 34.2917
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 2.08s
                      Time elapsed: 00:11:48
                               ETA: 01:01:08

################################################################################
                     [1m Learning iteration 324/2000 [0m                      

                       Computation: 47090 steps/s (collection: 1.985s, learning 0.102s)
             Mean action noise std: 1.92
          Mean value_function loss: 20.0623
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 45.3907
                       Mean reward: 25.02
               Mean episode length: 118.79
    Episode_Reward/reaching_object: 0.4223
     Episode_Reward/lifting_object: 4.6384
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 34.1250
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 2.09s
                      Time elapsed: 00:11:50
                               ETA: 01:01:05

################################################################################
                     [1m Learning iteration 325/2000 [0m                      

                       Computation: 46044 steps/s (collection: 2.030s, learning 0.105s)
             Mean action noise std: 1.92
          Mean value_function loss: 20.8943
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 45.3956
                       Mean reward: 23.83
               Mean episode length: 119.84
    Episode_Reward/reaching_object: 0.4136
     Episode_Reward/lifting_object: 4.3848
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 34.4583
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 2.13s
                      Time elapsed: 00:11:53
                               ETA: 01:01:03

################################################################################
                     [1m Learning iteration 326/2000 [0m                      

                       Computation: 44762 steps/s (collection: 1.989s, learning 0.207s)
             Mean action noise std: 1.92
          Mean value_function loss: 18.7576
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 45.4075
                       Mean reward: 26.14
               Mean episode length: 117.61
    Episode_Reward/reaching_object: 0.4064
     Episode_Reward/lifting_object: 4.6024
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 33.5000
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 2.20s
                      Time elapsed: 00:11:55
                               ETA: 01:01:01

################################################################################
                     [1m Learning iteration 327/2000 [0m                      

                       Computation: 44328 steps/s (collection: 2.115s, learning 0.103s)
             Mean action noise std: 1.92
          Mean value_function loss: 18.4740
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 45.4174
                       Mean reward: 18.00
               Mean episode length: 108.44
    Episode_Reward/reaching_object: 0.3974
     Episode_Reward/lifting_object: 4.3384
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 35.0833
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 2.22s
                      Time elapsed: 00:11:57
                               ETA: 01:00:59

################################################################################
                     [1m Learning iteration 328/2000 [0m                      

                       Computation: 46303 steps/s (collection: 2.020s, learning 0.103s)
             Mean action noise std: 1.92
          Mean value_function loss: 20.6005
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 45.4204
                       Mean reward: 23.98
               Mean episode length: 115.72
    Episode_Reward/reaching_object: 0.4124
     Episode_Reward/lifting_object: 4.6863
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 37.2917
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 2.12s
                      Time elapsed: 00:11:59
                               ETA: 01:00:56

################################################################################
                     [1m Learning iteration 329/2000 [0m                      

                       Computation: 46778 steps/s (collection: 1.985s, learning 0.116s)
             Mean action noise std: 1.92
          Mean value_function loss: 19.8252
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 45.4257
                       Mean reward: 25.12
               Mean episode length: 113.62
    Episode_Reward/reaching_object: 0.4036
     Episode_Reward/lifting_object: 4.9322
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 35.0833
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 2.10s
                      Time elapsed: 00:12:01
                               ETA: 01:00:54

################################################################################
                     [1m Learning iteration 330/2000 [0m                      

                       Computation: 46427 steps/s (collection: 2.006s, learning 0.111s)
             Mean action noise std: 1.92
          Mean value_function loss: 18.8675
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 45.4338
                       Mean reward: 24.75
               Mean episode length: 104.56
    Episode_Reward/reaching_object: 0.3912
     Episode_Reward/lifting_object: 4.4917
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 36.4583
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 2.12s
                      Time elapsed: 00:12:03
                               ETA: 01:00:51

################################################################################
                     [1m Learning iteration 331/2000 [0m                      

                       Computation: 46750 steps/s (collection: 2.008s, learning 0.095s)
             Mean action noise std: 1.92
          Mean value_function loss: 19.3147
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 45.4397
                       Mean reward: 27.09
               Mean episode length: 107.54
    Episode_Reward/reaching_object: 0.3871
     Episode_Reward/lifting_object: 4.4701
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 36.9167
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 2.10s
                      Time elapsed: 00:12:05
                               ETA: 01:00:49

################################################################################
                     [1m Learning iteration 332/2000 [0m                      

                       Computation: 47296 steps/s (collection: 1.982s, learning 0.097s)
             Mean action noise std: 1.92
          Mean value_function loss: 18.6794
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 45.4444
                       Mean reward: 26.90
               Mean episode length: 110.81
    Episode_Reward/reaching_object: 0.3868
     Episode_Reward/lifting_object: 4.6221
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 37.0833
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 2.08s
                      Time elapsed: 00:12:07
                               ETA: 01:00:46

################################################################################
                     [1m Learning iteration 333/2000 [0m                      

                       Computation: 18272 steps/s (collection: 5.210s, learning 0.170s)
             Mean action noise std: 1.92
          Mean value_function loss: 17.3674
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 45.4514
                       Mean reward: 23.38
               Mean episode length: 111.72
    Episode_Reward/reaching_object: 0.3950
     Episode_Reward/lifting_object: 4.5996
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 35.0833
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 5.38s
                      Time elapsed: 00:12:13
                               ETA: 01:01:00

################################################################################
                     [1m Learning iteration 334/2000 [0m                      

                       Computation: 13679 steps/s (collection: 7.027s, learning 0.159s)
             Mean action noise std: 1.92
          Mean value_function loss: 20.2237
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 45.4576
                       Mean reward: 27.70
               Mean episode length: 111.33
    Episode_Reward/reaching_object: 0.4000
     Episode_Reward/lifting_object: 4.9442
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 34.7917
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 7.19s
                      Time elapsed: 00:12:20
                               ETA: 01:01:22

################################################################################
                     [1m Learning iteration 335/2000 [0m                      

                       Computation: 13962 steps/s (collection: 6.928s, learning 0.113s)
             Mean action noise std: 1.92
          Mean value_function loss: 19.6198
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 45.4616
                       Mean reward: 22.90
               Mean episode length: 103.25
    Episode_Reward/reaching_object: 0.3801
     Episode_Reward/lifting_object: 4.9551
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 36.6667
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 7.04s
                      Time elapsed: 00:12:27
                               ETA: 01:01:44

################################################################################
                     [1m Learning iteration 336/2000 [0m                      

                       Computation: 14203 steps/s (collection: 6.800s, learning 0.121s)
             Mean action noise std: 1.92
          Mean value_function loss: 23.6777
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 45.4666
                       Mean reward: 27.04
               Mean episode length: 102.62
    Episode_Reward/reaching_object: 0.3725
     Episode_Reward/lifting_object: 4.8330
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 38.5000
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 6.92s
                      Time elapsed: 00:12:34
                               ETA: 01:02:05

################################################################################
                     [1m Learning iteration 337/2000 [0m                      

                       Computation: 14071 steps/s (collection: 6.792s, learning 0.194s)
             Mean action noise std: 1.93
          Mean value_function loss: 21.5894
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 45.4721
                       Mean reward: 26.50
               Mean episode length: 109.24
    Episode_Reward/reaching_object: 0.3859
     Episode_Reward/lifting_object: 5.0987
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 39.5417
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 6.99s
                      Time elapsed: 00:12:41
                               ETA: 01:02:26

################################################################################
                     [1m Learning iteration 338/2000 [0m                      

                       Computation: 13763 steps/s (collection: 6.983s, learning 0.159s)
             Mean action noise std: 1.93
          Mean value_function loss: 22.9668
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 45.4762
                       Mean reward: 26.88
               Mean episode length: 94.12
    Episode_Reward/reaching_object: 0.3716
     Episode_Reward/lifting_object: 4.9322
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 38.5000
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 7.14s
                      Time elapsed: 00:12:48
                               ETA: 01:02:48

################################################################################
                     [1m Learning iteration 339/2000 [0m                      

                       Computation: 13997 steps/s (collection: 6.903s, learning 0.120s)
             Mean action noise std: 1.93
          Mean value_function loss: 20.5213
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 45.4804
                       Mean reward: 26.22
               Mean episode length: 103.24
    Episode_Reward/reaching_object: 0.3644
     Episode_Reward/lifting_object: 4.5609
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 37.0833
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 7.02s
                      Time elapsed: 00:12:55
                               ETA: 01:03:09

################################################################################
                     [1m Learning iteration 340/2000 [0m                      

                       Computation: 13983 steps/s (collection: 6.907s, learning 0.123s)
             Mean action noise std: 1.93
          Mean value_function loss: 21.6747
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 45.4835
                       Mean reward: 21.24
               Mean episode length: 99.37
    Episode_Reward/reaching_object: 0.3717
     Episode_Reward/lifting_object: 5.0707
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 38.4583
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 7.03s
                      Time elapsed: 00:13:02
                               ETA: 01:03:30

################################################################################
                     [1m Learning iteration 341/2000 [0m                      

                       Computation: 12552 steps/s (collection: 7.722s, learning 0.110s)
             Mean action noise std: 1.93
          Mean value_function loss: 19.7853
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 45.4894
                       Mean reward: 28.06
               Mean episode length: 95.75
    Episode_Reward/reaching_object: 0.3749
     Episode_Reward/lifting_object: 4.9178
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 38.6250
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 7.83s
                      Time elapsed: 00:13:10
                               ETA: 01:03:54

################################################################################
                     [1m Learning iteration 342/2000 [0m                      

                       Computation: 47222 steps/s (collection: 1.980s, learning 0.102s)
             Mean action noise std: 1.93
          Mean value_function loss: 20.5422
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 45.4932
                       Mean reward: 29.20
               Mean episode length: 113.98
    Episode_Reward/reaching_object: 0.3717
     Episode_Reward/lifting_object: 5.1898
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 39.1667
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 2.08s
                      Time elapsed: 00:13:12
                               ETA: 01:03:51

################################################################################
                     [1m Learning iteration 343/2000 [0m                      

                       Computation: 49787 steps/s (collection: 1.876s, learning 0.098s)
             Mean action noise std: 1.93
          Mean value_function loss: 19.9220
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 45.4975
                       Mean reward: 25.29
               Mean episode length: 107.98
    Episode_Reward/reaching_object: 0.3690
     Episode_Reward/lifting_object: 5.1384
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 40.6250
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 1.97s
                      Time elapsed: 00:13:14
                               ETA: 01:03:47

################################################################################
                     [1m Learning iteration 344/2000 [0m                      

                       Computation: 49263 steps/s (collection: 1.904s, learning 0.092s)
             Mean action noise std: 1.93
          Mean value_function loss: 23.4139
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 45.5029
                       Mean reward: 32.40
               Mean episode length: 98.20
    Episode_Reward/reaching_object: 0.3537
     Episode_Reward/lifting_object: 5.1527
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 39.2083
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 2.00s
                      Time elapsed: 00:13:16
                               ETA: 01:03:43

################################################################################
                     [1m Learning iteration 345/2000 [0m                      

                       Computation: 48293 steps/s (collection: 1.940s, learning 0.095s)
             Mean action noise std: 1.93
          Mean value_function loss: 26.2844
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 45.5076
                       Mean reward: 26.76
               Mean episode length: 92.49
    Episode_Reward/reaching_object: 0.3652
     Episode_Reward/lifting_object: 5.1318
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 39.7500
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 2.04s
                      Time elapsed: 00:13:18
                               ETA: 01:03:39

################################################################################
                     [1m Learning iteration 346/2000 [0m                      

                       Computation: 45105 steps/s (collection: 2.064s, learning 0.115s)
             Mean action noise std: 1.93
          Mean value_function loss: 26.4540
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 45.5120
                       Mean reward: 25.56
               Mean episode length: 94.34
    Episode_Reward/reaching_object: 0.3550
     Episode_Reward/lifting_object: 5.1910
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 40.6250
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 2.18s
                      Time elapsed: 00:13:20
                               ETA: 01:03:36

################################################################################
                     [1m Learning iteration 347/2000 [0m                      

                       Computation: 37038 steps/s (collection: 2.482s, learning 0.172s)
             Mean action noise std: 1.93
          Mean value_function loss: 21.9122
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 45.5143
                       Mean reward: 29.66
               Mean episode length: 100.88
    Episode_Reward/reaching_object: 0.3687
     Episode_Reward/lifting_object: 5.3030
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 40.2500
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 2.65s
                      Time elapsed: 00:13:23
                               ETA: 01:03:36

################################################################################
                     [1m Learning iteration 348/2000 [0m                      

                       Computation: 35731 steps/s (collection: 2.612s, learning 0.140s)
             Mean action noise std: 1.93
          Mean value_function loss: 29.5520
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 45.5189
                       Mean reward: 29.18
               Mean episode length: 99.15
    Episode_Reward/reaching_object: 0.3522
     Episode_Reward/lifting_object: 5.4815
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 40.4583
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 2.75s
                      Time elapsed: 00:13:26
                               ETA: 01:03:36

################################################################################
                     [1m Learning iteration 349/2000 [0m                      

                       Computation: 42241 steps/s (collection: 2.224s, learning 0.104s)
             Mean action noise std: 1.93
          Mean value_function loss: 28.3332
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 45.5255
                       Mean reward: 30.55
               Mean episode length: 94.15
    Episode_Reward/reaching_object: 0.3466
     Episode_Reward/lifting_object: 5.0692
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 39.9167
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 2.33s
                      Time elapsed: 00:13:28
                               ETA: 01:03:33

################################################################################
                     [1m Learning iteration 350/2000 [0m                      

                       Computation: 46524 steps/s (collection: 1.980s, learning 0.133s)
             Mean action noise std: 1.93
          Mean value_function loss: 25.6176
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 45.5284
                       Mean reward: 25.32
               Mean episode length: 103.25
    Episode_Reward/reaching_object: 0.3485
     Episode_Reward/lifting_object: 5.2503
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 40.4583
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 2.11s
                      Time elapsed: 00:13:30
                               ETA: 01:03:30

################################################################################
                     [1m Learning iteration 351/2000 [0m                      

                       Computation: 48495 steps/s (collection: 1.935s, learning 0.092s)
             Mean action noise std: 1.93
          Mean value_function loss: 20.9099
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 45.5312
                       Mean reward: 30.60
               Mean episode length: 96.64
    Episode_Reward/reaching_object: 0.3461
     Episode_Reward/lifting_object: 5.4495
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 41.6250
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 2.03s
                      Time elapsed: 00:13:32
                               ETA: 01:03:26

################################################################################
                     [1m Learning iteration 352/2000 [0m                      

                       Computation: 49426 steps/s (collection: 1.883s, learning 0.106s)
             Mean action noise std: 1.93
          Mean value_function loss: 22.1915
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 45.5344
                       Mean reward: 27.08
               Mean episode length: 93.93
    Episode_Reward/reaching_object: 0.3526
     Episode_Reward/lifting_object: 5.5632
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 41.6250
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 1.99s
                      Time elapsed: 00:13:34
                               ETA: 01:03:23

################################################################################
                     [1m Learning iteration 353/2000 [0m                      

                       Computation: 44885 steps/s (collection: 2.077s, learning 0.113s)
             Mean action noise std: 1.93
          Mean value_function loss: 25.0929
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 45.5376
                       Mean reward: 27.62
               Mean episode length: 91.78
    Episode_Reward/reaching_object: 0.3408
     Episode_Reward/lifting_object: 5.4986
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 45.2917
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 2.19s
                      Time elapsed: 00:13:36
                               ETA: 01:03:20

################################################################################
                     [1m Learning iteration 354/2000 [0m                      

                       Computation: 45093 steps/s (collection: 2.060s, learning 0.120s)
             Mean action noise std: 1.93
          Mean value_function loss: 22.4035
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 45.5408
                       Mean reward: 32.18
               Mean episode length: 102.69
    Episode_Reward/reaching_object: 0.3351
     Episode_Reward/lifting_object: 5.4098
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 42.0417
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 2.18s
                      Time elapsed: 00:13:38
                               ETA: 01:03:17

################################################################################
                     [1m Learning iteration 355/2000 [0m                      

                       Computation: 46595 steps/s (collection: 2.007s, learning 0.103s)
             Mean action noise std: 1.93
          Mean value_function loss: 26.9552
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 45.5453
                       Mean reward: 28.42
               Mean episode length: 103.57
    Episode_Reward/reaching_object: 0.3322
     Episode_Reward/lifting_object: 5.3656
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 44.1667
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 2.11s
                      Time elapsed: 00:13:41
                               ETA: 01:03:14

################################################################################
                     [1m Learning iteration 356/2000 [0m                      

                       Computation: 45918 steps/s (collection: 2.043s, learning 0.098s)
             Mean action noise std: 1.93
          Mean value_function loss: 28.7981
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 45.5493
                       Mean reward: 29.19
               Mean episode length: 87.69
    Episode_Reward/reaching_object: 0.3295
     Episode_Reward/lifting_object: 5.3236
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 43.9583
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 2.14s
                      Time elapsed: 00:13:43
                               ETA: 01:03:11

################################################################################
                     [1m Learning iteration 357/2000 [0m                      

                       Computation: 48161 steps/s (collection: 1.951s, learning 0.090s)
             Mean action noise std: 1.93
          Mean value_function loss: 23.0236
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 45.5511
                       Mean reward: 33.54
               Mean episode length: 96.84
    Episode_Reward/reaching_object: 0.3276
     Episode_Reward/lifting_object: 5.3147
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 43.3750
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 2.04s
                      Time elapsed: 00:13:45
                               ETA: 01:03:07

################################################################################
                     [1m Learning iteration 358/2000 [0m                      

                       Computation: 49599 steps/s (collection: 1.896s, learning 0.086s)
             Mean action noise std: 1.93
          Mean value_function loss: 24.8929
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 45.5522
                       Mean reward: 25.59
               Mean episode length: 92.56
    Episode_Reward/reaching_object: 0.3208
     Episode_Reward/lifting_object: 5.2556
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 45.5833
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 1.98s
                      Time elapsed: 00:13:47
                               ETA: 01:03:03

################################################################################
                     [1m Learning iteration 359/2000 [0m                      

                       Computation: 50100 steps/s (collection: 1.873s, learning 0.089s)
             Mean action noise std: 1.93
          Mean value_function loss: 29.1384
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 45.5545
                       Mean reward: 27.69
               Mean episode length: 91.23
    Episode_Reward/reaching_object: 0.3194
     Episode_Reward/lifting_object: 5.3310
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 41.8750
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 1.96s
                      Time elapsed: 00:13:49
                               ETA: 01:02:59

################################################################################
                     [1m Learning iteration 360/2000 [0m                      

                       Computation: 50753 steps/s (collection: 1.849s, learning 0.088s)
             Mean action noise std: 1.93
          Mean value_function loss: 25.0123
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 45.5593
                       Mean reward: 32.53
               Mean episode length: 98.53
    Episode_Reward/reaching_object: 0.3203
     Episode_Reward/lifting_object: 5.5466
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 44.9167
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 1.94s
                      Time elapsed: 00:13:51
                               ETA: 01:02:55

################################################################################
                     [1m Learning iteration 361/2000 [0m                      

                       Computation: 48884 steps/s (collection: 1.922s, learning 0.089s)
             Mean action noise std: 1.94
          Mean value_function loss: 32.5344
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 45.5654
                       Mean reward: 30.13
               Mean episode length: 88.24
    Episode_Reward/reaching_object: 0.3249
     Episode_Reward/lifting_object: 5.6132
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 44.8750
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 2.01s
                      Time elapsed: 00:13:53
                               ETA: 01:02:52

################################################################################
                     [1m Learning iteration 362/2000 [0m                      

                       Computation: 49737 steps/s (collection: 1.877s, learning 0.100s)
             Mean action noise std: 1.94
          Mean value_function loss: 26.9396
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 45.5708
                       Mean reward: 28.45
               Mean episode length: 85.54
    Episode_Reward/reaching_object: 0.3221
     Episode_Reward/lifting_object: 5.7262
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 47.2083
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 1.98s
                      Time elapsed: 00:13:55
                               ETA: 01:02:48

################################################################################
                     [1m Learning iteration 363/2000 [0m                      

                       Computation: 49324 steps/s (collection: 1.901s, learning 0.092s)
             Mean action noise std: 1.94
          Mean value_function loss: 24.8740
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 45.5722
                       Mean reward: 27.95
               Mean episode length: 84.88
    Episode_Reward/reaching_object: 0.3152
     Episode_Reward/lifting_object: 5.2794
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 45.7083
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 1.99s
                      Time elapsed: 00:13:57
                               ETA: 01:02:44

################################################################################
                     [1m Learning iteration 364/2000 [0m                      

                       Computation: 49785 steps/s (collection: 1.885s, learning 0.090s)
             Mean action noise std: 1.94
          Mean value_function loss: 23.2490
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 45.5739
                       Mean reward: 31.13
               Mean episode length: 88.55
    Episode_Reward/reaching_object: 0.3263
     Episode_Reward/lifting_object: 5.6544
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 44.8750
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 1.97s
                      Time elapsed: 00:13:59
                               ETA: 01:02:41

################################################################################
                     [1m Learning iteration 365/2000 [0m                      

                       Computation: 50696 steps/s (collection: 1.852s, learning 0.088s)
             Mean action noise std: 1.94
          Mean value_function loss: 31.2634
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 45.5774
                       Mean reward: 27.87
               Mean episode length: 90.36
    Episode_Reward/reaching_object: 0.3100
     Episode_Reward/lifting_object: 5.3040
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 44.3750
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 1.94s
                      Time elapsed: 00:14:01
                               ETA: 01:02:37

################################################################################
                     [1m Learning iteration 366/2000 [0m                      

                       Computation: 49581 steps/s (collection: 1.896s, learning 0.087s)
             Mean action noise std: 1.94
          Mean value_function loss: 38.0906
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 45.5822
                       Mean reward: 21.84
               Mean episode length: 83.93
    Episode_Reward/reaching_object: 0.3077
     Episode_Reward/lifting_object: 5.6153
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 45.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 1.98s
                      Time elapsed: 00:14:03
                               ETA: 01:02:33

################################################################################
                     [1m Learning iteration 367/2000 [0m                      

                       Computation: 50246 steps/s (collection: 1.862s, learning 0.094s)
             Mean action noise std: 1.94
          Mean value_function loss: 26.1270
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 45.5884
                       Mean reward: 31.40
               Mean episode length: 91.69
    Episode_Reward/reaching_object: 0.3150
     Episode_Reward/lifting_object: 5.4719
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 45.1667
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 1.96s
                      Time elapsed: 00:14:05
                               ETA: 01:02:29

################################################################################
                     [1m Learning iteration 368/2000 [0m                      

                       Computation: 49356 steps/s (collection: 1.886s, learning 0.106s)
             Mean action noise std: 1.94
          Mean value_function loss: 29.9696
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 45.5949
                       Mean reward: 37.26
               Mean episode length: 97.41
    Episode_Reward/reaching_object: 0.3260
     Episode_Reward/lifting_object: 5.9852
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 42.4583
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 1.99s
                      Time elapsed: 00:14:06
                               ETA: 01:02:26

################################################################################
                     [1m Learning iteration 369/2000 [0m                      

                       Computation: 49686 steps/s (collection: 1.873s, learning 0.105s)
             Mean action noise std: 1.94
          Mean value_function loss: 29.5083
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 45.6004
                       Mean reward: 31.96
               Mean episode length: 94.84
    Episode_Reward/reaching_object: 0.3326
     Episode_Reward/lifting_object: 6.0015
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 42.2083
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 1.98s
                      Time elapsed: 00:14:08
                               ETA: 01:02:22

################################################################################
                     [1m Learning iteration 370/2000 [0m                      

                       Computation: 50156 steps/s (collection: 1.868s, learning 0.092s)
             Mean action noise std: 1.94
          Mean value_function loss: 28.2266
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 45.6071
                       Mean reward: 29.61
               Mean episode length: 94.71
    Episode_Reward/reaching_object: 0.3319
     Episode_Reward/lifting_object: 6.0188
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 1.7917
     Episode_Termination/robot_out: 43.7500
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 1.96s
                      Time elapsed: 00:14:10
                               ETA: 01:02:18

################################################################################
                     [1m Learning iteration 371/2000 [0m                      

                       Computation: 49919 steps/s (collection: 1.871s, learning 0.098s)
             Mean action noise std: 1.94
          Mean value_function loss: 26.5386
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 45.6100
                       Mean reward: 31.49
               Mean episode length: 95.22
    Episode_Reward/reaching_object: 0.3337
     Episode_Reward/lifting_object: 6.0348
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 44.7500
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 1.97s
                      Time elapsed: 00:14:12
                               ETA: 01:02:14

################################################################################
                     [1m Learning iteration 372/2000 [0m                      

                       Computation: 50450 steps/s (collection: 1.861s, learning 0.088s)
             Mean action noise std: 1.94
          Mean value_function loss: 29.7307
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 45.6104
                       Mean reward: 31.64
               Mean episode length: 92.22
    Episode_Reward/reaching_object: 0.3403
     Episode_Reward/lifting_object: 6.3258
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 43.5833
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 1.95s
                      Time elapsed: 00:14:14
                               ETA: 01:02:11

################################################################################
                     [1m Learning iteration 373/2000 [0m                      

                       Computation: 49656 steps/s (collection: 1.890s, learning 0.090s)
             Mean action noise std: 1.94
          Mean value_function loss: 28.5540
               Mean surrogate loss: 0.0122
                 Mean entropy loss: 45.6108
                       Mean reward: 34.13
               Mean episode length: 87.35
    Episode_Reward/reaching_object: 0.3283
     Episode_Reward/lifting_object: 6.0462
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 44.2083
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 1.98s
                      Time elapsed: 00:14:16
                               ETA: 01:02:07

################################################################################
                     [1m Learning iteration 374/2000 [0m                      

                       Computation: 49811 steps/s (collection: 1.881s, learning 0.093s)
             Mean action noise std: 1.94
          Mean value_function loss: 31.2687
               Mean surrogate loss: 0.0104
                 Mean entropy loss: 45.6110
                       Mean reward: 34.41
               Mean episode length: 91.15
    Episode_Reward/reaching_object: 0.3300
     Episode_Reward/lifting_object: 5.9646
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 47.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 1.97s
                      Time elapsed: 00:14:18
                               ETA: 01:02:03

################################################################################
                     [1m Learning iteration 375/2000 [0m                      

                       Computation: 49401 steps/s (collection: 1.892s, learning 0.098s)
             Mean action noise std: 1.94
          Mean value_function loss: 26.4499
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 45.6112
                       Mean reward: 33.20
               Mean episode length: 90.28
    Episode_Reward/reaching_object: 0.3300
     Episode_Reward/lifting_object: 6.2575
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 45.3333
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 1.99s
                      Time elapsed: 00:14:20
                               ETA: 01:02:00

################################################################################
                     [1m Learning iteration 376/2000 [0m                      

                       Computation: 48751 steps/s (collection: 1.927s, learning 0.090s)
             Mean action noise std: 1.94
          Mean value_function loss: 25.5885
               Mean surrogate loss: 0.0131
                 Mean entropy loss: 45.6114
                       Mean reward: 32.30
               Mean episode length: 91.02
    Episode_Reward/reaching_object: 0.3317
     Episode_Reward/lifting_object: 6.0909
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 43.6250
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 2.02s
                      Time elapsed: 00:14:22
                               ETA: 01:01:56

################################################################################
                     [1m Learning iteration 377/2000 [0m                      

                       Computation: 49426 steps/s (collection: 1.888s, learning 0.101s)
             Mean action noise std: 1.94
          Mean value_function loss: 27.4851
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 45.6116
                       Mean reward: 31.70
               Mean episode length: 88.51
    Episode_Reward/reaching_object: 0.3254
     Episode_Reward/lifting_object: 6.2654
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 43.9583
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 1.99s
                      Time elapsed: 00:14:24
                               ETA: 01:01:53

################################################################################
                     [1m Learning iteration 378/2000 [0m                      

                       Computation: 48536 steps/s (collection: 1.912s, learning 0.113s)
             Mean action noise std: 1.94
          Mean value_function loss: 28.0741
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 45.6125
                       Mean reward: 26.87
               Mean episode length: 82.88
    Episode_Reward/reaching_object: 0.3172
     Episode_Reward/lifting_object: 5.9932
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 43.0417
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 2.03s
                      Time elapsed: 00:14:26
                               ETA: 01:01:49

################################################################################
                     [1m Learning iteration 379/2000 [0m                      

                       Computation: 49299 steps/s (collection: 1.898s, learning 0.097s)
             Mean action noise std: 1.94
          Mean value_function loss: 28.2597
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 45.6149
                       Mean reward: 33.21
               Mean episode length: 90.41
    Episode_Reward/reaching_object: 0.3203
     Episode_Reward/lifting_object: 6.1244
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 44.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 1.99s
                      Time elapsed: 00:14:28
                               ETA: 01:01:46

################################################################################
                     [1m Learning iteration 380/2000 [0m                      

                       Computation: 47478 steps/s (collection: 1.950s, learning 0.120s)
             Mean action noise std: 1.94
          Mean value_function loss: 26.3754
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 45.6179
                       Mean reward: 32.20
               Mean episode length: 81.48
    Episode_Reward/reaching_object: 0.3177
     Episode_Reward/lifting_object: 6.0054
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 46.8333
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 2.07s
                      Time elapsed: 00:14:30
                               ETA: 01:01:42

################################################################################
                     [1m Learning iteration 381/2000 [0m                      

                       Computation: 49451 steps/s (collection: 1.888s, learning 0.100s)
             Mean action noise std: 1.94
          Mean value_function loss: 28.6387
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 45.6238
                       Mean reward: 33.31
               Mean episode length: 96.87
    Episode_Reward/reaching_object: 0.3308
     Episode_Reward/lifting_object: 6.2592
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 43.3750
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 1.99s
                      Time elapsed: 00:14:32
                               ETA: 01:01:39

################################################################################
                     [1m Learning iteration 382/2000 [0m                      

                       Computation: 48015 steps/s (collection: 1.932s, learning 0.115s)
             Mean action noise std: 1.94
          Mean value_function loss: 27.5159
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 45.6313
                       Mean reward: 33.14
               Mean episode length: 87.36
    Episode_Reward/reaching_object: 0.3220
     Episode_Reward/lifting_object: 6.2991
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 45.6250
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 2.05s
                      Time elapsed: 00:14:34
                               ETA: 01:01:36

################################################################################
                     [1m Learning iteration 383/2000 [0m                      

                       Computation: 47299 steps/s (collection: 1.964s, learning 0.114s)
             Mean action noise std: 1.94
          Mean value_function loss: 40.1656
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 45.6343
                       Mean reward: 32.94
               Mean episode length: 86.85
    Episode_Reward/reaching_object: 0.3229
     Episode_Reward/lifting_object: 6.4513
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 45.9167
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 2.08s
                      Time elapsed: 00:14:37
                               ETA: 01:01:33

################################################################################
                     [1m Learning iteration 384/2000 [0m                      

                       Computation: 48898 steps/s (collection: 1.895s, learning 0.115s)
             Mean action noise std: 1.94
          Mean value_function loss: 30.1365
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 45.6391
                       Mean reward: 27.77
               Mean episode length: 84.50
    Episode_Reward/reaching_object: 0.3229
     Episode_Reward/lifting_object: 6.1024
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 48.1250
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 2.01s
                      Time elapsed: 00:14:39
                               ETA: 01:01:29

################################################################################
                     [1m Learning iteration 385/2000 [0m                      

                       Computation: 49863 steps/s (collection: 1.879s, learning 0.093s)
             Mean action noise std: 1.94
          Mean value_function loss: 31.9139
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 45.6443
                       Mean reward: 29.66
               Mean episode length: 78.89
    Episode_Reward/reaching_object: 0.3214
     Episode_Reward/lifting_object: 6.3058
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 45.3750
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 1.97s
                      Time elapsed: 00:14:40
                               ETA: 01:01:25

################################################################################
                     [1m Learning iteration 386/2000 [0m                      

                       Computation: 49289 steps/s (collection: 1.899s, learning 0.095s)
             Mean action noise std: 1.94
          Mean value_function loss: 33.5992
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 45.6483
                       Mean reward: 29.89
               Mean episode length: 89.49
    Episode_Reward/reaching_object: 0.3109
     Episode_Reward/lifting_object: 6.3705
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 44.2917
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 1.99s
                      Time elapsed: 00:14:42
                               ETA: 01:01:22

################################################################################
                     [1m Learning iteration 387/2000 [0m                      

                       Computation: 49311 steps/s (collection: 1.898s, learning 0.096s)
             Mean action noise std: 1.94
          Mean value_function loss: 27.9762
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 45.6532
                       Mean reward: 37.17
               Mean episode length: 81.22
    Episode_Reward/reaching_object: 0.3107
     Episode_Reward/lifting_object: 6.4236
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 45.3750
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 1.99s
                      Time elapsed: 00:14:44
                               ETA: 01:01:19

################################################################################
                     [1m Learning iteration 388/2000 [0m                      

                       Computation: 48074 steps/s (collection: 1.948s, learning 0.097s)
             Mean action noise std: 1.94
          Mean value_function loss: 31.3819
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 45.6586
                       Mean reward: 35.50
               Mean episode length: 84.17
    Episode_Reward/reaching_object: 0.3206
     Episode_Reward/lifting_object: 6.4817
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 46.0833
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 2.04s
                      Time elapsed: 00:14:47
                               ETA: 01:01:15

################################################################################
                     [1m Learning iteration 389/2000 [0m                      

                       Computation: 49800 steps/s (collection: 1.863s, learning 0.111s)
             Mean action noise std: 1.94
          Mean value_function loss: 27.3898
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 45.6625
                       Mean reward: 32.87
               Mean episode length: 84.73
    Episode_Reward/reaching_object: 0.3239
     Episode_Reward/lifting_object: 6.2528
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 46.7917
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 1.97s
                      Time elapsed: 00:14:48
                               ETA: 01:01:12

################################################################################
                     [1m Learning iteration 390/2000 [0m                      

                       Computation: 49423 steps/s (collection: 1.888s, learning 0.101s)
             Mean action noise std: 1.94
          Mean value_function loss: 31.0188
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 45.6637
                       Mean reward: 33.47
               Mean episode length: 93.72
    Episode_Reward/reaching_object: 0.3152
     Episode_Reward/lifting_object: 6.2648
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 46.4583
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 1.99s
                      Time elapsed: 00:14:50
                               ETA: 01:01:08

################################################################################
                     [1m Learning iteration 391/2000 [0m                      

                       Computation: 48602 steps/s (collection: 1.915s, learning 0.108s)
             Mean action noise std: 1.94
          Mean value_function loss: 35.9134
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 45.6640
                       Mean reward: 31.89
               Mean episode length: 89.23
    Episode_Reward/reaching_object: 0.3165
     Episode_Reward/lifting_object: 6.4430
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 48.8750
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 2.02s
                      Time elapsed: 00:14:53
                               ETA: 01:01:05

################################################################################
                     [1m Learning iteration 392/2000 [0m                      

                       Computation: 49228 steps/s (collection: 1.905s, learning 0.092s)
             Mean action noise std: 1.95
          Mean value_function loss: 39.2975
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 45.6652
                       Mean reward: 34.35
               Mean episode length: 88.58
    Episode_Reward/reaching_object: 0.3170
     Episode_Reward/lifting_object: 6.5258
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 48.1667
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 2.00s
                      Time elapsed: 00:14:55
                               ETA: 01:01:01

################################################################################
                     [1m Learning iteration 393/2000 [0m                      

                       Computation: 48111 steps/s (collection: 1.941s, learning 0.103s)
             Mean action noise std: 1.95
          Mean value_function loss: 29.5596
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 45.6680
                       Mean reward: 23.14
               Mean episode length: 78.64
    Episode_Reward/reaching_object: 0.3062
     Episode_Reward/lifting_object: 6.2290
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 49.5417
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 2.04s
                      Time elapsed: 00:14:57
                               ETA: 01:00:58

################################################################################
                     [1m Learning iteration 394/2000 [0m                      

                       Computation: 49044 steps/s (collection: 1.907s, learning 0.098s)
             Mean action noise std: 1.95
          Mean value_function loss: 35.4134
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 45.6671
                       Mean reward: 31.48
               Mean episode length: 84.69
    Episode_Reward/reaching_object: 0.3045
     Episode_Reward/lifting_object: 6.3656
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 49.0417
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 2.00s
                      Time elapsed: 00:14:59
                               ETA: 01:00:55

################################################################################
                     [1m Learning iteration 395/2000 [0m                      

                       Computation: 49928 steps/s (collection: 1.882s, learning 0.087s)
             Mean action noise std: 1.95
          Mean value_function loss: 36.4151
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 45.6675
                       Mean reward: 30.82
               Mean episode length: 79.74
    Episode_Reward/reaching_object: 0.2964
     Episode_Reward/lifting_object: 6.0916
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 46.8750
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 1.97s
                      Time elapsed: 00:15:01
                               ETA: 01:00:51

################################################################################
                     [1m Learning iteration 396/2000 [0m                      

                       Computation: 49561 steps/s (collection: 1.895s, learning 0.089s)
             Mean action noise std: 1.95
          Mean value_function loss: 31.8730
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 45.6715
                       Mean reward: 33.51
               Mean episode length: 86.36
    Episode_Reward/reaching_object: 0.3037
     Episode_Reward/lifting_object: 6.3398
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 48.5833
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 1.98s
                      Time elapsed: 00:15:03
                               ETA: 01:00:48

################################################################################
                     [1m Learning iteration 397/2000 [0m                      

                       Computation: 49317 steps/s (collection: 1.903s, learning 0.090s)
             Mean action noise std: 1.95
          Mean value_function loss: 32.4040
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 45.6753
                       Mean reward: 37.03
               Mean episode length: 87.64
    Episode_Reward/reaching_object: 0.3009
     Episode_Reward/lifting_object: 6.1644
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 47.8333
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 1.99s
                      Time elapsed: 00:15:04
                               ETA: 01:00:44

################################################################################
                     [1m Learning iteration 398/2000 [0m                      

                       Computation: 48803 steps/s (collection: 1.920s, learning 0.094s)
             Mean action noise std: 1.95
          Mean value_function loss: 34.1275
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 45.6780
                       Mean reward: 40.71
               Mean episode length: 97.03
    Episode_Reward/reaching_object: 0.3151
     Episode_Reward/lifting_object: 6.5588
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 48.5000
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 2.01s
                      Time elapsed: 00:15:07
                               ETA: 01:00:41

################################################################################
                     [1m Learning iteration 399/2000 [0m                      

                       Computation: 48549 steps/s (collection: 1.913s, learning 0.112s)
             Mean action noise std: 1.95
          Mean value_function loss: 32.8555
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 45.6813
                       Mean reward: 34.51
               Mean episode length: 82.54
    Episode_Reward/reaching_object: 0.2979
     Episode_Reward/lifting_object: 6.2766
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 48.2083
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 2.02s
                      Time elapsed: 00:15:09
                               ETA: 01:00:38

################################################################################
                     [1m Learning iteration 400/2000 [0m                      

                       Computation: 49777 steps/s (collection: 1.883s, learning 0.092s)
             Mean action noise std: 1.95
          Mean value_function loss: 39.6616
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 45.6871
                       Mean reward: 36.13
               Mean episode length: 86.17
    Episode_Reward/reaching_object: 0.3122
     Episode_Reward/lifting_object: 6.9094
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 45.8333
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 1.97s
                      Time elapsed: 00:15:11
                               ETA: 01:00:34

################################################################################
                     [1m Learning iteration 401/2000 [0m                      

                       Computation: 48993 steps/s (collection: 1.913s, learning 0.094s)
             Mean action noise std: 1.95
          Mean value_function loss: 36.3739
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 45.6964
                       Mean reward: 35.88
               Mean episode length: 82.99
    Episode_Reward/reaching_object: 0.3119
     Episode_Reward/lifting_object: 6.8996
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 47.7083
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 2.01s
                      Time elapsed: 00:15:13
                               ETA: 01:00:31

################################################################################
                     [1m Learning iteration 402/2000 [0m                      

                       Computation: 49532 steps/s (collection: 1.890s, learning 0.095s)
             Mean action noise std: 1.95
          Mean value_function loss: 33.5260
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 45.7059
                       Mean reward: 38.42
               Mean episode length: 87.76
    Episode_Reward/reaching_object: 0.3109
     Episode_Reward/lifting_object: 6.7870
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 49.7083
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 1.98s
                      Time elapsed: 00:15:14
                               ETA: 01:00:28

################################################################################
                     [1m Learning iteration 403/2000 [0m                      

                       Computation: 49421 steps/s (collection: 1.902s, learning 0.088s)
             Mean action noise std: 1.95
          Mean value_function loss: 36.5172
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 45.7094
                       Mean reward: 32.34
               Mean episode length: 82.56
    Episode_Reward/reaching_object: 0.3205
     Episode_Reward/lifting_object: 7.0091
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 47.7083
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 1.99s
                      Time elapsed: 00:15:16
                               ETA: 01:00:24

################################################################################
                     [1m Learning iteration 404/2000 [0m                      

                       Computation: 48837 steps/s (collection: 1.913s, learning 0.100s)
             Mean action noise std: 1.95
          Mean value_function loss: 36.4538
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 45.7133
                       Mean reward: 33.85
               Mean episode length: 85.41
    Episode_Reward/reaching_object: 0.3140
     Episode_Reward/lifting_object: 6.8105
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 46.7083
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 2.01s
                      Time elapsed: 00:15:19
                               ETA: 01:00:21

################################################################################
                     [1m Learning iteration 405/2000 [0m                      

                       Computation: 46504 steps/s (collection: 1.986s, learning 0.128s)
             Mean action noise std: 1.95
          Mean value_function loss: 36.8888
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 45.7158
                       Mean reward: 31.34
               Mean episode length: 79.67
    Episode_Reward/reaching_object: 0.3004
     Episode_Reward/lifting_object: 6.4569
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 52.2917
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 2.11s
                      Time elapsed: 00:15:21
                               ETA: 01:00:18

################################################################################
                     [1m Learning iteration 406/2000 [0m                      

                       Computation: 47828 steps/s (collection: 1.942s, learning 0.114s)
             Mean action noise std: 1.95
          Mean value_function loss: 30.8502
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 45.7197
                       Mean reward: 36.90
               Mean episode length: 91.92
    Episode_Reward/reaching_object: 0.2988
     Episode_Reward/lifting_object: 6.6833
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 47.7083
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 2.06s
                      Time elapsed: 00:15:23
                               ETA: 01:00:15

################################################################################
                     [1m Learning iteration 407/2000 [0m                      

                       Computation: 49269 steps/s (collection: 1.899s, learning 0.097s)
             Mean action noise std: 1.95
          Mean value_function loss: 33.2086
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 45.7247
                       Mean reward: 37.11
               Mean episode length: 85.43
    Episode_Reward/reaching_object: 0.2961
     Episode_Reward/lifting_object: 6.4762
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 50.0417
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 2.00s
                      Time elapsed: 00:15:25
                               ETA: 01:00:12

################################################################################
                     [1m Learning iteration 408/2000 [0m                      

                       Computation: 49748 steps/s (collection: 1.879s, learning 0.097s)
             Mean action noise std: 1.95
          Mean value_function loss: 33.1575
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 45.7302
                       Mean reward: 32.95
               Mean episode length: 81.33
    Episode_Reward/reaching_object: 0.3032
     Episode_Reward/lifting_object: 6.6247
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 47.0417
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 1.98s
                      Time elapsed: 00:15:27
                               ETA: 01:00:08

################################################################################
                     [1m Learning iteration 409/2000 [0m                      

                       Computation: 49046 steps/s (collection: 1.909s, learning 0.095s)
             Mean action noise std: 1.95
          Mean value_function loss: 38.1913
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 45.7314
                       Mean reward: 32.06
               Mean episode length: 77.93
    Episode_Reward/reaching_object: 0.3040
     Episode_Reward/lifting_object: 6.9104
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 51.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 2.00s
                      Time elapsed: 00:15:29
                               ETA: 01:00:05

################################################################################
                     [1m Learning iteration 410/2000 [0m                      

                       Computation: 49058 steps/s (collection: 1.897s, learning 0.107s)
             Mean action noise std: 1.95
          Mean value_function loss: 37.0481
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 45.7336
                       Mean reward: 37.22
               Mean episode length: 79.35
    Episode_Reward/reaching_object: 0.3052
     Episode_Reward/lifting_object: 6.9312
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 49.7917
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 2.00s
                      Time elapsed: 00:15:31
                               ETA: 01:00:02

################################################################################
                     [1m Learning iteration 411/2000 [0m                      

                       Computation: 49141 steps/s (collection: 1.905s, learning 0.095s)
             Mean action noise std: 1.95
          Mean value_function loss: 37.2306
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 45.7361
                       Mean reward: 36.08
               Mean episode length: 83.27
    Episode_Reward/reaching_object: 0.2874
     Episode_Reward/lifting_object: 6.8013
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 50.1250
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 2.00s
                      Time elapsed: 00:15:33
                               ETA: 00:59:58

################################################################################
                     [1m Learning iteration 412/2000 [0m                      

                       Computation: 49109 steps/s (collection: 1.910s, learning 0.091s)
             Mean action noise std: 1.95
          Mean value_function loss: 40.0089
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 45.7371
                       Mean reward: 34.71
               Mean episode length: 78.15
    Episode_Reward/reaching_object: 0.3018
     Episode_Reward/lifting_object: 6.8288
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 51.1667
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 2.00s
                      Time elapsed: 00:15:35
                               ETA: 00:59:55

################################################################################
                     [1m Learning iteration 413/2000 [0m                      

                       Computation: 48379 steps/s (collection: 1.935s, learning 0.097s)
             Mean action noise std: 1.95
          Mean value_function loss: 31.7149
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 45.7374
                       Mean reward: 31.86
               Mean episode length: 78.27
    Episode_Reward/reaching_object: 0.2890
     Episode_Reward/lifting_object: 6.5255
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 50.6250
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 2.03s
                      Time elapsed: 00:15:37
                               ETA: 00:59:52

################################################################################
                     [1m Learning iteration 414/2000 [0m                      

                       Computation: 48861 steps/s (collection: 1.899s, learning 0.113s)
             Mean action noise std: 1.95
          Mean value_function loss: 38.5728
               Mean surrogate loss: 0.0105
                 Mean entropy loss: 45.7376
                       Mean reward: 35.03
               Mean episode length: 80.12
    Episode_Reward/reaching_object: 0.2946
     Episode_Reward/lifting_object: 6.8023
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 49.9167
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 2.01s
                      Time elapsed: 00:15:39
                               ETA: 00:59:49

################################################################################
                     [1m Learning iteration 415/2000 [0m                      

                       Computation: 48912 steps/s (collection: 1.895s, learning 0.115s)
             Mean action noise std: 1.95
          Mean value_function loss: 31.9958
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 45.7378
                       Mean reward: 34.10
               Mean episode length: 80.16
    Episode_Reward/reaching_object: 0.2891
     Episode_Reward/lifting_object: 6.4601
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 50.8333
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 2.01s
                      Time elapsed: 00:15:41
                               ETA: 00:59:46

################################################################################
                     [1m Learning iteration 416/2000 [0m                      

                       Computation: 49208 steps/s (collection: 1.896s, learning 0.102s)
             Mean action noise std: 1.95
          Mean value_function loss: 31.5633
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 45.7387
                       Mean reward: 37.30
               Mean episode length: 79.75
    Episode_Reward/reaching_object: 0.2974
     Episode_Reward/lifting_object: 7.0235
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 48.1250
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 2.00s
                      Time elapsed: 00:15:43
                               ETA: 00:59:42

################################################################################
                     [1m Learning iteration 417/2000 [0m                      

                       Computation: 48879 steps/s (collection: 1.916s, learning 0.096s)
             Mean action noise std: 1.95
          Mean value_function loss: 33.0383
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 45.7414
                       Mean reward: 34.11
               Mean episode length: 78.47
    Episode_Reward/reaching_object: 0.2968
     Episode_Reward/lifting_object: 6.8935
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 47.8750
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 2.01s
                      Time elapsed: 00:15:45
                               ETA: 00:59:39

################################################################################
                     [1m Learning iteration 418/2000 [0m                      

                       Computation: 49337 steps/s (collection: 1.896s, learning 0.096s)
             Mean action noise std: 1.95
          Mean value_function loss: 33.0742
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 45.7453
                       Mean reward: 34.62
               Mean episode length: 81.39
    Episode_Reward/reaching_object: 0.3018
     Episode_Reward/lifting_object: 7.0339
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 47.4583
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 1.99s
                      Time elapsed: 00:15:47
                               ETA: 00:59:36

################################################################################
                     [1m Learning iteration 419/2000 [0m                      

                       Computation: 49717 steps/s (collection: 1.888s, learning 0.089s)
             Mean action noise std: 1.95
          Mean value_function loss: 33.2656
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 45.7505
                       Mean reward: 42.34
               Mean episode length: 89.97
    Episode_Reward/reaching_object: 0.3107
     Episode_Reward/lifting_object: 7.2846
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 48.7500
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 1.98s
                      Time elapsed: 00:15:49
                               ETA: 00:59:32

################################################################################
                     [1m Learning iteration 420/2000 [0m                      

                       Computation: 49194 steps/s (collection: 1.907s, learning 0.092s)
             Mean action noise std: 1.95
          Mean value_function loss: 34.3039
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 45.7547
                       Mean reward: 38.04
               Mean episode length: 84.41
    Episode_Reward/reaching_object: 0.3221
     Episode_Reward/lifting_object: 7.1670
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 47.7917
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 2.00s
                      Time elapsed: 00:15:51
                               ETA: 00:59:29

################################################################################
                     [1m Learning iteration 421/2000 [0m                      

                       Computation: 49345 steps/s (collection: 1.901s, learning 0.091s)
             Mean action noise std: 1.96
          Mean value_function loss: 37.7389
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 45.7617
                       Mean reward: 34.78
               Mean episode length: 80.05
    Episode_Reward/reaching_object: 0.3064
     Episode_Reward/lifting_object: 7.0143
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 49.6250
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 1.99s
                      Time elapsed: 00:15:53
                               ETA: 00:59:26

################################################################################
                     [1m Learning iteration 422/2000 [0m                      

                       Computation: 49193 steps/s (collection: 1.904s, learning 0.094s)
             Mean action noise std: 1.96
          Mean value_function loss: 35.1150
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 45.7716
                       Mean reward: 41.60
               Mean episode length: 85.26
    Episode_Reward/reaching_object: 0.3138
     Episode_Reward/lifting_object: 7.1875
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 49.3750
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 2.00s
                      Time elapsed: 00:15:55
                               ETA: 00:59:23

################################################################################
                     [1m Learning iteration 423/2000 [0m                      

                       Computation: 48453 steps/s (collection: 1.931s, learning 0.098s)
             Mean action noise std: 1.96
          Mean value_function loss: 34.3900
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 45.7764
                       Mean reward: 34.88
               Mean episode length: 84.98
    Episode_Reward/reaching_object: 0.3091
     Episode_Reward/lifting_object: 7.2174
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 48.2083
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 2.03s
                      Time elapsed: 00:15:57
                               ETA: 00:59:20

################################################################################
                     [1m Learning iteration 424/2000 [0m                      

                       Computation: 49114 steps/s (collection: 1.912s, learning 0.090s)
             Mean action noise std: 1.96
          Mean value_function loss: 35.1414
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 45.7775
                       Mean reward: 40.20
               Mean episode length: 84.82
    Episode_Reward/reaching_object: 0.3070
     Episode_Reward/lifting_object: 7.1778
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 48.4583
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 2.00s
                      Time elapsed: 00:15:59
                               ETA: 00:59:16

################################################################################
                     [1m Learning iteration 425/2000 [0m                      

                       Computation: 49391 steps/s (collection: 1.902s, learning 0.089s)
             Mean action noise std: 1.96
          Mean value_function loss: 33.0575
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 45.7797
                       Mean reward: 38.95
               Mean episode length: 81.71
    Episode_Reward/reaching_object: 0.3072
     Episode_Reward/lifting_object: 7.2112
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 51.5417
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 1.99s
                      Time elapsed: 00:16:01
                               ETA: 00:59:13

################################################################################
                     [1m Learning iteration 426/2000 [0m                      

                       Computation: 49078 steps/s (collection: 1.915s, learning 0.088s)
             Mean action noise std: 1.96
          Mean value_function loss: 34.2507
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 45.7838
                       Mean reward: 36.68
               Mean episode length: 79.80
    Episode_Reward/reaching_object: 0.3146
     Episode_Reward/lifting_object: 7.5932
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 45.1250
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 2.00s
                      Time elapsed: 00:16:03
                               ETA: 00:59:10

################################################################################
                     [1m Learning iteration 427/2000 [0m                      

                       Computation: 49271 steps/s (collection: 1.908s, learning 0.087s)
             Mean action noise std: 1.96
          Mean value_function loss: 40.1672
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 45.7870
                       Mean reward: 38.93
               Mean episode length: 83.11
    Episode_Reward/reaching_object: 0.3187
     Episode_Reward/lifting_object: 7.4354
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 50.7500
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 2.00s
                      Time elapsed: 00:16:05
                               ETA: 00:59:07

################################################################################
                     [1m Learning iteration 428/2000 [0m                      

                       Computation: 47708 steps/s (collection: 1.943s, learning 0.118s)
             Mean action noise std: 1.96
          Mean value_function loss: 35.9176
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 45.7903
                       Mean reward: 38.69
               Mean episode length: 86.96
    Episode_Reward/reaching_object: 0.3151
     Episode_Reward/lifting_object: 7.3238
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 45.5000
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 2.06s
                      Time elapsed: 00:16:07
                               ETA: 00:59:04

################################################################################
                     [1m Learning iteration 429/2000 [0m                      

                       Computation: 48505 steps/s (collection: 1.918s, learning 0.109s)
             Mean action noise std: 1.96
          Mean value_function loss: 47.3836
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 45.7925
                       Mean reward: 40.20
               Mean episode length: 87.05
    Episode_Reward/reaching_object: 0.3232
     Episode_Reward/lifting_object: 7.6152
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 47.0833
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 2.03s
                      Time elapsed: 00:16:09
                               ETA: 00:59:01

################################################################################
                     [1m Learning iteration 430/2000 [0m                      

                       Computation: 47336 steps/s (collection: 1.957s, learning 0.120s)
             Mean action noise std: 1.96
          Mean value_function loss: 34.4101
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 45.7974
                       Mean reward: 41.01
               Mean episode length: 86.15
    Episode_Reward/reaching_object: 0.3224
     Episode_Reward/lifting_object: 7.4266
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 47.7500
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 2.08s
                      Time elapsed: 00:16:11
                               ETA: 00:58:58

################################################################################
                     [1m Learning iteration 431/2000 [0m                      

                       Computation: 49550 steps/s (collection: 1.892s, learning 0.092s)
             Mean action noise std: 1.96
          Mean value_function loss: 36.0083
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 45.8022
                       Mean reward: 44.11
               Mean episode length: 85.98
    Episode_Reward/reaching_object: 0.3383
     Episode_Reward/lifting_object: 7.8330
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 45.8333
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 1.98s
                      Time elapsed: 00:16:13
                               ETA: 00:58:55

################################################################################
                     [1m Learning iteration 432/2000 [0m                      

                       Computation: 49488 steps/s (collection: 1.897s, learning 0.089s)
             Mean action noise std: 1.96
          Mean value_function loss: 38.6558
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 45.8058
                       Mean reward: 38.32
               Mean episode length: 88.64
    Episode_Reward/reaching_object: 0.3257
     Episode_Reward/lifting_object: 7.5625
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 49.2917
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 1.99s
                      Time elapsed: 00:16:15
                               ETA: 00:58:51

################################################################################
                     [1m Learning iteration 433/2000 [0m                      

                       Computation: 49164 steps/s (collection: 1.913s, learning 0.087s)
             Mean action noise std: 1.96
          Mean value_function loss: 37.1390
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 45.8116
                       Mean reward: 40.97
               Mean episode length: 85.95
    Episode_Reward/reaching_object: 0.3301
     Episode_Reward/lifting_object: 7.5414
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 49.4167
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 2.00s
                      Time elapsed: 00:16:17
                               ETA: 00:58:48

################################################################################
                     [1m Learning iteration 434/2000 [0m                      

                       Computation: 48460 steps/s (collection: 1.939s, learning 0.090s)
             Mean action noise std: 1.96
          Mean value_function loss: 38.6624
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 45.8174
                       Mean reward: 44.94
               Mean episode length: 88.35
    Episode_Reward/reaching_object: 0.3298
     Episode_Reward/lifting_object: 7.9091
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 49.7500
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 2.03s
                      Time elapsed: 00:16:19
                               ETA: 00:58:45

################################################################################
                     [1m Learning iteration 435/2000 [0m                      

                       Computation: 48329 steps/s (collection: 1.940s, learning 0.094s)
             Mean action noise std: 1.96
          Mean value_function loss: 40.5568
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 45.8228
                       Mean reward: 40.47
               Mean episode length: 86.09
    Episode_Reward/reaching_object: 0.3269
     Episode_Reward/lifting_object: 7.7922
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 48.8333
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 2.03s
                      Time elapsed: 00:16:21
                               ETA: 00:58:42

################################################################################
                     [1m Learning iteration 436/2000 [0m                      

                       Computation: 49603 steps/s (collection: 1.891s, learning 0.091s)
             Mean action noise std: 1.96
          Mean value_function loss: 36.1265
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 45.8266
                       Mean reward: 37.72
               Mean episode length: 79.43
    Episode_Reward/reaching_object: 0.3194
     Episode_Reward/lifting_object: 7.6328
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 46.7500
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 1.98s
                      Time elapsed: 00:16:23
                               ETA: 00:58:39

################################################################################
                     [1m Learning iteration 437/2000 [0m                      

                       Computation: 49452 steps/s (collection: 1.891s, learning 0.097s)
             Mean action noise std: 1.96
          Mean value_function loss: 34.7567
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 45.8291
                       Mean reward: 39.40
               Mean episode length: 82.24
    Episode_Reward/reaching_object: 0.3164
     Episode_Reward/lifting_object: 7.6879
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 50.6250
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 1.99s
                      Time elapsed: 00:16:25
                               ETA: 00:58:36

################################################################################
                     [1m Learning iteration 438/2000 [0m                      

                       Computation: 48414 steps/s (collection: 1.934s, learning 0.096s)
             Mean action noise std: 1.96
          Mean value_function loss: 39.2367
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 45.8326
                       Mean reward: 43.19
               Mean episode length: 84.42
    Episode_Reward/reaching_object: 0.3235
     Episode_Reward/lifting_object: 8.0360
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 51.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 2.03s
                      Time elapsed: 00:16:27
                               ETA: 00:58:33

################################################################################
                     [1m Learning iteration 439/2000 [0m                      

                       Computation: 48477 steps/s (collection: 1.931s, learning 0.097s)
             Mean action noise std: 1.96
          Mean value_function loss: 35.4303
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 45.8387
                       Mean reward: 43.93
               Mean episode length: 86.43
    Episode_Reward/reaching_object: 0.3103
     Episode_Reward/lifting_object: 7.7101
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 51.8750
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 2.03s
                      Time elapsed: 00:16:29
                               ETA: 00:58:30

################################################################################
                     [1m Learning iteration 440/2000 [0m                      

                       Computation: 48837 steps/s (collection: 1.907s, learning 0.106s)
             Mean action noise std: 1.96
          Mean value_function loss: 41.7630
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 45.8425
                       Mean reward: 42.72
               Mean episode length: 77.38
    Episode_Reward/reaching_object: 0.3103
     Episode_Reward/lifting_object: 7.6754
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 48.8750
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 2.01s
                      Time elapsed: 00:16:31
                               ETA: 00:58:27

################################################################################
                     [1m Learning iteration 441/2000 [0m                      

                       Computation: 47904 steps/s (collection: 1.947s, learning 0.106s)
             Mean action noise std: 1.96
          Mean value_function loss: 38.2441
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 45.8432
                       Mean reward: 38.89
               Mean episode length: 75.55
    Episode_Reward/reaching_object: 0.2968
     Episode_Reward/lifting_object: 7.3902
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 55.6250
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 2.05s
                      Time elapsed: 00:16:33
                               ETA: 00:58:24

################################################################################
                     [1m Learning iteration 442/2000 [0m                      

                       Computation: 48272 steps/s (collection: 1.938s, learning 0.099s)
             Mean action noise std: 1.96
          Mean value_function loss: 44.6353
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 45.8437
                       Mean reward: 40.49
               Mean episode length: 78.31
    Episode_Reward/reaching_object: 0.3031
     Episode_Reward/lifting_object: 7.7588
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 52.3333
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 2.04s
                      Time elapsed: 00:16:35
                               ETA: 00:58:21

################################################################################
                     [1m Learning iteration 443/2000 [0m                      

                       Computation: 48375 steps/s (collection: 1.918s, learning 0.114s)
             Mean action noise std: 1.96
          Mean value_function loss: 41.8092
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 45.8460
                       Mean reward: 42.58
               Mean episode length: 79.92
    Episode_Reward/reaching_object: 0.2923
     Episode_Reward/lifting_object: 7.4037
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 54.2083
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 2.03s
                      Time elapsed: 00:16:37
                               ETA: 00:58:18

################################################################################
                     [1m Learning iteration 444/2000 [0m                      

                       Computation: 48789 steps/s (collection: 1.909s, learning 0.106s)
             Mean action noise std: 1.96
          Mean value_function loss: 50.1365
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 45.8501
                       Mean reward: 36.02
               Mean episode length: 72.76
    Episode_Reward/reaching_object: 0.2916
     Episode_Reward/lifting_object: 7.5406
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 51.8333
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 2.01s
                      Time elapsed: 00:16:39
                               ETA: 00:58:15

################################################################################
                     [1m Learning iteration 445/2000 [0m                      

                       Computation: 48694 steps/s (collection: 1.926s, learning 0.093s)
             Mean action noise std: 1.96
          Mean value_function loss: 54.3482
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 45.8534
                       Mean reward: 42.59
               Mean episode length: 77.25
    Episode_Reward/reaching_object: 0.2935
     Episode_Reward/lifting_object: 7.3521
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 53.8750
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 2.02s
                      Time elapsed: 00:16:41
                               ETA: 00:58:12

################################################################################
                     [1m Learning iteration 446/2000 [0m                      

                       Computation: 48871 steps/s (collection: 1.920s, learning 0.092s)
             Mean action noise std: 1.96
          Mean value_function loss: 46.1611
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 45.8567
                       Mean reward: 38.00
               Mean episode length: 73.50
    Episode_Reward/reaching_object: 0.2950
     Episode_Reward/lifting_object: 7.5751
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 55.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 2.01s
                      Time elapsed: 00:16:43
                               ETA: 00:58:09

################################################################################
                     [1m Learning iteration 447/2000 [0m                      

                       Computation: 47874 steps/s (collection: 1.966s, learning 0.088s)
             Mean action noise std: 1.97
          Mean value_function loss: 45.9116
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 45.8626
                       Mean reward: 37.12
               Mean episode length: 75.46
    Episode_Reward/reaching_object: 0.2897
     Episode_Reward/lifting_object: 7.4753
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 52.6667
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 2.05s
                      Time elapsed: 00:16:45
                               ETA: 00:58:06

################################################################################
                     [1m Learning iteration 448/2000 [0m                      

                       Computation: 48953 steps/s (collection: 1.916s, learning 0.092s)
             Mean action noise std: 1.97
          Mean value_function loss: 45.1847
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 45.8678
                       Mean reward: 37.04
               Mean episode length: 75.31
    Episode_Reward/reaching_object: 0.2851
     Episode_Reward/lifting_object: 7.2483
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 52.6667
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 2.01s
                      Time elapsed: 00:16:47
                               ETA: 00:58:03

################################################################################
                     [1m Learning iteration 449/2000 [0m                      

                       Computation: 45678 steps/s (collection: 2.060s, learning 0.092s)
             Mean action noise std: 1.97
          Mean value_function loss: 76.8026
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 45.8708
                       Mean reward: 41.30
               Mean episode length: 77.65
    Episode_Reward/reaching_object: 0.2896
     Episode_Reward/lifting_object: 7.4600
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 53.4167
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 2.15s
                      Time elapsed: 00:16:49
                               ETA: 00:58:00

################################################################################
                     [1m Learning iteration 450/2000 [0m                      

                       Computation: 48058 steps/s (collection: 1.957s, learning 0.088s)
             Mean action noise std: 1.97
          Mean value_function loss: 61.4677
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 45.8752
                       Mean reward: 37.33
               Mean episode length: 78.60
    Episode_Reward/reaching_object: 0.2893
     Episode_Reward/lifting_object: 7.4941
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 53.5833
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 2.05s
                      Time elapsed: 00:16:51
                               ETA: 00:57:57

################################################################################
                     [1m Learning iteration 451/2000 [0m                      

                       Computation: 48224 steps/s (collection: 1.948s, learning 0.091s)
             Mean action noise std: 1.97
          Mean value_function loss: 43.9660
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 45.8791
                       Mean reward: 37.11
               Mean episode length: 78.93
    Episode_Reward/reaching_object: 0.2912
     Episode_Reward/lifting_object: 7.0565
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 53.0833
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 2.04s
                      Time elapsed: 00:16:53
                               ETA: 00:57:54

################################################################################
                     [1m Learning iteration 452/2000 [0m                      

                       Computation: 47581 steps/s (collection: 1.967s, learning 0.100s)
             Mean action noise std: 1.97
          Mean value_function loss: 45.0565
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 45.8811
                       Mean reward: 38.71
               Mean episode length: 78.10
    Episode_Reward/reaching_object: 0.2959
     Episode_Reward/lifting_object: 7.6262
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 51.9583
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 2.07s
                      Time elapsed: 00:16:55
                               ETA: 00:57:51

################################################################################
                     [1m Learning iteration 453/2000 [0m                      

                       Computation: 46931 steps/s (collection: 1.994s, learning 0.101s)
             Mean action noise std: 1.97
          Mean value_function loss: 42.3610
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 45.8824
                       Mean reward: 40.87
               Mean episode length: 75.29
    Episode_Reward/reaching_object: 0.3002
     Episode_Reward/lifting_object: 7.5717
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 51.3750
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 2.09s
                      Time elapsed: 00:16:58
                               ETA: 00:57:49

################################################################################
                     [1m Learning iteration 454/2000 [0m                      

                       Computation: 48333 steps/s (collection: 1.942s, learning 0.092s)
             Mean action noise std: 1.97
          Mean value_function loss: 42.1851
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 45.8838
                       Mean reward: 39.07
               Mean episode length: 76.79
    Episode_Reward/reaching_object: 0.3076
     Episode_Reward/lifting_object: 7.6521
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 48.5833
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 2.03s
                      Time elapsed: 00:17:00
                               ETA: 00:57:46

################################################################################
                     [1m Learning iteration 455/2000 [0m                      

                       Computation: 49451 steps/s (collection: 1.901s, learning 0.087s)
             Mean action noise std: 1.97
          Mean value_function loss: 44.8425
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 45.8855
                       Mean reward: 35.42
               Mean episode length: 73.59
    Episode_Reward/reaching_object: 0.3073
     Episode_Reward/lifting_object: 7.6122
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 48.3750
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 1.99s
                      Time elapsed: 00:17:02
                               ETA: 00:57:42

################################################################################
                     [1m Learning iteration 456/2000 [0m                      

                       Computation: 48307 steps/s (collection: 1.942s, learning 0.093s)
             Mean action noise std: 1.97
          Mean value_function loss: 40.8563
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 45.8880
                       Mean reward: 41.64
               Mean episode length: 80.13
    Episode_Reward/reaching_object: 0.3232
     Episode_Reward/lifting_object: 8.0139
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 49.3750
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 2.03s
                      Time elapsed: 00:17:04
                               ETA: 00:57:40

################################################################################
                     [1m Learning iteration 457/2000 [0m                      

                       Computation: 48716 steps/s (collection: 1.927s, learning 0.091s)
             Mean action noise std: 1.97
          Mean value_function loss: 42.2864
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 45.8918
                       Mean reward: 41.16
               Mean episode length: 81.76
    Episode_Reward/reaching_object: 0.3226
     Episode_Reward/lifting_object: 8.3364
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 51.3333
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 2.02s
                      Time elapsed: 00:17:06
                               ETA: 00:57:37

################################################################################
                     [1m Learning iteration 458/2000 [0m                      

                       Computation: 48528 steps/s (collection: 1.918s, learning 0.108s)
             Mean action noise std: 1.97
          Mean value_function loss: 41.2926
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 45.8947
                       Mean reward: 40.59
               Mean episode length: 78.11
    Episode_Reward/reaching_object: 0.3237
     Episode_Reward/lifting_object: 8.4836
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 50.4583
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 2.03s
                      Time elapsed: 00:17:08
                               ETA: 00:57:34

################################################################################
                     [1m Learning iteration 459/2000 [0m                      

                       Computation: 48504 steps/s (collection: 1.920s, learning 0.107s)
             Mean action noise std: 1.97
          Mean value_function loss: 47.2788
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 45.8955
                       Mean reward: 46.62
               Mean episode length: 86.60
    Episode_Reward/reaching_object: 0.3333
     Episode_Reward/lifting_object: 8.5006
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 49.7917
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 2.03s
                      Time elapsed: 00:17:10
                               ETA: 00:57:31

################################################################################
                     [1m Learning iteration 460/2000 [0m                      

                       Computation: 48486 steps/s (collection: 1.924s, learning 0.104s)
             Mean action noise std: 1.97
          Mean value_function loss: 43.6758
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 45.8978
                       Mean reward: 43.86
               Mean episode length: 80.89
    Episode_Reward/reaching_object: 0.3152
     Episode_Reward/lifting_object: 8.3204
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 51.9167
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 2.03s
                      Time elapsed: 00:17:12
                               ETA: 00:57:28

################################################################################
                     [1m Learning iteration 461/2000 [0m                      

                       Computation: 48665 steps/s (collection: 1.932s, learning 0.088s)
             Mean action noise std: 1.97
          Mean value_function loss: 47.6670
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 45.8997
                       Mean reward: 46.03
               Mean episode length: 81.53
    Episode_Reward/reaching_object: 0.3189
     Episode_Reward/lifting_object: 8.4546
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 49.7500
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 2.02s
                      Time elapsed: 00:17:14
                               ETA: 00:57:25

################################################################################
                     [1m Learning iteration 462/2000 [0m                      

                       Computation: 49238 steps/s (collection: 1.908s, learning 0.088s)
             Mean action noise std: 1.97
          Mean value_function loss: 45.5457
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 45.9028
                       Mean reward: 38.48
               Mean episode length: 69.62
    Episode_Reward/reaching_object: 0.3161
     Episode_Reward/lifting_object: 8.2373
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 50.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 2.00s
                      Time elapsed: 00:17:16
                               ETA: 00:57:22

################################################################################
                     [1m Learning iteration 463/2000 [0m                      

                       Computation: 48780 steps/s (collection: 1.926s, learning 0.089s)
             Mean action noise std: 1.97
          Mean value_function loss: 43.7841
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 45.9056
                       Mean reward: 42.52
               Mean episode length: 78.69
    Episode_Reward/reaching_object: 0.3118
     Episode_Reward/lifting_object: 8.5647
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 51.4167
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 2.02s
                      Time elapsed: 00:17:18
                               ETA: 00:57:19

################################################################################
                     [1m Learning iteration 464/2000 [0m                      

                       Computation: 47210 steps/s (collection: 1.992s, learning 0.091s)
             Mean action noise std: 1.97
          Mean value_function loss: 44.7388
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 45.9081
                       Mean reward: 45.57
               Mean episode length: 78.69
    Episode_Reward/reaching_object: 0.3234
     Episode_Reward/lifting_object: 8.6971
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 51.6250
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 2.08s
                      Time elapsed: 00:17:20
                               ETA: 00:57:16

################################################################################
                     [1m Learning iteration 465/2000 [0m                      

                       Computation: 48946 steps/s (collection: 1.920s, learning 0.088s)
             Mean action noise std: 1.97
          Mean value_function loss: 56.9814
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 45.9101
                       Mean reward: 43.65
               Mean episode length: 79.16
    Episode_Reward/reaching_object: 0.3187
     Episode_Reward/lifting_object: 8.6380
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 53.0833
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 2.01s
                      Time elapsed: 00:17:22
                               ETA: 00:57:13

################################################################################
                     [1m Learning iteration 466/2000 [0m                      

                       Computation: 48001 steps/s (collection: 1.954s, learning 0.094s)
             Mean action noise std: 1.97
          Mean value_function loss: 57.9382
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 45.9156
                       Mean reward: 43.25
               Mean episode length: 79.92
    Episode_Reward/reaching_object: 0.3081
     Episode_Reward/lifting_object: 8.2210
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 56.1667
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 2.05s
                      Time elapsed: 00:17:24
                               ETA: 00:57:10

################################################################################
                     [1m Learning iteration 467/2000 [0m                      

                       Computation: 48306 steps/s (collection: 1.946s, learning 0.089s)
             Mean action noise std: 1.97
          Mean value_function loss: 50.0390
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 45.9190
                       Mean reward: 38.97
               Mean episode length: 76.81
    Episode_Reward/reaching_object: 0.2904
     Episode_Reward/lifting_object: 8.1191
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 51.8750
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 2.03s
                      Time elapsed: 00:17:26
                               ETA: 00:57:07

################################################################################
                     [1m Learning iteration 468/2000 [0m                      

                       Computation: 48584 steps/s (collection: 1.934s, learning 0.090s)
             Mean action noise std: 1.97
          Mean value_function loss: 54.1880
               Mean surrogate loss: 0.0093
                 Mean entropy loss: 45.9225
                       Mean reward: 45.71
               Mean episode length: 80.19
    Episode_Reward/reaching_object: 0.2970
     Episode_Reward/lifting_object: 8.2289
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 56.2917
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 2.02s
                      Time elapsed: 00:17:28
                               ETA: 00:57:04

################################################################################
                     [1m Learning iteration 469/2000 [0m                      

                       Computation: 48943 steps/s (collection: 1.920s, learning 0.089s)
             Mean action noise std: 1.97
          Mean value_function loss: 50.8248
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 45.9232
                       Mean reward: 46.47
               Mean episode length: 78.70
    Episode_Reward/reaching_object: 0.2903
     Episode_Reward/lifting_object: 8.1670
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 52.3333
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 2.01s
                      Time elapsed: 00:17:30
                               ETA: 00:57:01

################################################################################
                     [1m Learning iteration 470/2000 [0m                      

                       Computation: 48698 steps/s (collection: 1.925s, learning 0.094s)
             Mean action noise std: 1.97
          Mean value_function loss: 46.3136
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 45.9240
                       Mean reward: 41.40
               Mean episode length: 76.91
    Episode_Reward/reaching_object: 0.2928
     Episode_Reward/lifting_object: 8.3091
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 52.5000
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 2.02s
                      Time elapsed: 00:17:32
                               ETA: 00:56:58

################################################################################
                     [1m Learning iteration 471/2000 [0m                      

                       Computation: 48894 steps/s (collection: 1.919s, learning 0.091s)
             Mean action noise std: 1.97
          Mean value_function loss: 47.2901
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 45.9252
                       Mean reward: 44.95
               Mean episode length: 79.23
    Episode_Reward/reaching_object: 0.3017
     Episode_Reward/lifting_object: 8.4334
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 51.9583
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 2.01s
                      Time elapsed: 00:17:34
                               ETA: 00:56:55

################################################################################
                     [1m Learning iteration 472/2000 [0m                      

                       Computation: 47880 steps/s (collection: 1.949s, learning 0.104s)
             Mean action noise std: 1.97
          Mean value_function loss: 51.7164
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 45.9277
                       Mean reward: 43.79
               Mean episode length: 76.37
    Episode_Reward/reaching_object: 0.2926
     Episode_Reward/lifting_object: 8.0167
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 53.9167
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 2.05s
                      Time elapsed: 00:17:36
                               ETA: 00:56:53

################################################################################
                     [1m Learning iteration 473/2000 [0m                      

                       Computation: 48224 steps/s (collection: 1.946s, learning 0.093s)
             Mean action noise std: 1.97
          Mean value_function loss: 53.8334
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 45.9322
                       Mean reward: 44.15
               Mean episode length: 77.97
    Episode_Reward/reaching_object: 0.3039
     Episode_Reward/lifting_object: 8.2345
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 53.3333
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 2.04s
                      Time elapsed: 00:17:38
                               ETA: 00:56:50

################################################################################
                     [1m Learning iteration 474/2000 [0m                      

                       Computation: 48991 steps/s (collection: 1.910s, learning 0.097s)
             Mean action noise std: 1.97
          Mean value_function loss: 53.4037
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 45.9357
                       Mean reward: 43.02
               Mean episode length: 74.00
    Episode_Reward/reaching_object: 0.2987
     Episode_Reward/lifting_object: 8.3242
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 50.9583
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 2.01s
                      Time elapsed: 00:17:40
                               ETA: 00:56:47

################################################################################
                     [1m Learning iteration 475/2000 [0m                      

                       Computation: 47390 steps/s (collection: 1.958s, learning 0.116s)
             Mean action noise std: 1.97
          Mean value_function loss: 55.4932
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 45.9379
                       Mean reward: 45.21
               Mean episode length: 78.74
    Episode_Reward/reaching_object: 0.3071
     Episode_Reward/lifting_object: 8.5084
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 55.0833
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 2.07s
                      Time elapsed: 00:17:42
                               ETA: 00:56:44

################################################################################
                     [1m Learning iteration 476/2000 [0m                      

                       Computation: 48657 steps/s (collection: 1.926s, learning 0.094s)
             Mean action noise std: 1.97
          Mean value_function loss: 58.0624
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 45.9417
                       Mean reward: 39.44
               Mean episode length: 71.52
    Episode_Reward/reaching_object: 0.2976
     Episode_Reward/lifting_object: 8.2940
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 53.7917
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 2.02s
                      Time elapsed: 00:17:44
                               ETA: 00:56:41

################################################################################
                     [1m Learning iteration 477/2000 [0m                      

                       Computation: 44821 steps/s (collection: 2.074s, learning 0.119s)
             Mean action noise std: 1.97
          Mean value_function loss: 51.7164
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 45.9463
                       Mean reward: 43.95
               Mean episode length: 74.29
    Episode_Reward/reaching_object: 0.2918
     Episode_Reward/lifting_object: 8.2300
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 54.7500
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 2.19s
                      Time elapsed: 00:17:46
                               ETA: 00:56:39

################################################################################
                     [1m Learning iteration 478/2000 [0m                      

                       Computation: 43841 steps/s (collection: 2.138s, learning 0.104s)
             Mean action noise std: 1.98
          Mean value_function loss: 57.5198
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 45.9522
                       Mean reward: 43.61
               Mean episode length: 79.70
    Episode_Reward/reaching_object: 0.2952
     Episode_Reward/lifting_object: 8.4023
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 51.7500
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 2.24s
                      Time elapsed: 00:17:49
                               ETA: 00:56:37

################################################################################
                     [1m Learning iteration 479/2000 [0m                      

                       Computation: 47652 steps/s (collection: 1.978s, learning 0.085s)
             Mean action noise std: 1.98
          Mean value_function loss: 52.3847
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 45.9578
                       Mean reward: 46.18
               Mean episode length: 74.12
    Episode_Reward/reaching_object: 0.3010
     Episode_Reward/lifting_object: 8.5246
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 50.6250
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 2.06s
                      Time elapsed: 00:17:51
                               ETA: 00:56:34

################################################################################
                     [1m Learning iteration 480/2000 [0m                      

                       Computation: 47517 steps/s (collection: 1.979s, learning 0.090s)
             Mean action noise std: 1.98
          Mean value_function loss: 54.0161
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 45.9603
                       Mean reward: 43.37
               Mean episode length: 72.43
    Episode_Reward/reaching_object: 0.3046
     Episode_Reward/lifting_object: 8.8694
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 53.8750
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 2.07s
                      Time elapsed: 00:17:53
                               ETA: 00:56:31

################################################################################
                     [1m Learning iteration 481/2000 [0m                      

                       Computation: 47298 steps/s (collection: 1.981s, learning 0.098s)
             Mean action noise std: 1.98
          Mean value_function loss: 50.8635
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 45.9633
                       Mean reward: 46.16
               Mean episode length: 84.13
    Episode_Reward/reaching_object: 0.3109
     Episode_Reward/lifting_object: 8.9488
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 52.1250
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 2.08s
                      Time elapsed: 00:17:55
                               ETA: 00:56:28

################################################################################
                     [1m Learning iteration 482/2000 [0m                      

                       Computation: 47849 steps/s (collection: 1.960s, learning 0.095s)
             Mean action noise std: 1.98
          Mean value_function loss: 58.6976
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 45.9662
                       Mean reward: 49.69
               Mean episode length: 82.31
    Episode_Reward/reaching_object: 0.3000
     Episode_Reward/lifting_object: 8.7918
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 50.6667
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 2.05s
                      Time elapsed: 00:17:57
                               ETA: 00:56:26

################################################################################
                     [1m Learning iteration 483/2000 [0m                      

                       Computation: 48447 steps/s (collection: 1.942s, learning 0.087s)
             Mean action noise std: 1.98
          Mean value_function loss: 64.0242
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 45.9680
                       Mean reward: 42.47
               Mean episode length: 79.27
    Episode_Reward/reaching_object: 0.3089
     Episode_Reward/lifting_object: 8.7994
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 53.0833
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 2.03s
                      Time elapsed: 00:17:59
                               ETA: 00:56:23

################################################################################
                     [1m Learning iteration 484/2000 [0m                      

                       Computation: 48224 steps/s (collection: 1.947s, learning 0.091s)
             Mean action noise std: 1.98
          Mean value_function loss: 53.0696
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 45.9694
                       Mean reward: 47.99
               Mean episode length: 79.17
    Episode_Reward/reaching_object: 0.3086
     Episode_Reward/lifting_object: 9.0113
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 52.2917
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 2.04s
                      Time elapsed: 00:18:01
                               ETA: 00:56:20

################################################################################
                     [1m Learning iteration 485/2000 [0m                      

                       Computation: 46771 steps/s (collection: 2.003s, learning 0.099s)
             Mean action noise std: 1.98
          Mean value_function loss: 51.4262
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 45.9711
                       Mean reward: 44.14
               Mean episode length: 73.47
    Episode_Reward/reaching_object: 0.3085
     Episode_Reward/lifting_object: 9.0591
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 52.5000
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 2.10s
                      Time elapsed: 00:18:03
                               ETA: 00:56:17

################################################################################
                     [1m Learning iteration 486/2000 [0m                      

                       Computation: 47151 steps/s (collection: 1.980s, learning 0.105s)
             Mean action noise std: 1.98
          Mean value_function loss: 51.9755
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 45.9726
                       Mean reward: 44.76
               Mean episode length: 73.80
    Episode_Reward/reaching_object: 0.3091
     Episode_Reward/lifting_object: 8.8638
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 54.6667
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 2.08s
                      Time elapsed: 00:18:05
                               ETA: 00:56:15

################################################################################
                     [1m Learning iteration 487/2000 [0m                      

                       Computation: 46605 steps/s (collection: 2.008s, learning 0.102s)
             Mean action noise std: 1.98
          Mean value_function loss: 50.3814
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 45.9735
                       Mean reward: 41.88
               Mean episode length: 72.69
    Episode_Reward/reaching_object: 0.3092
     Episode_Reward/lifting_object: 8.8626
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 51.8333
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 2.11s
                      Time elapsed: 00:18:07
                               ETA: 00:56:12

################################################################################
                     [1m Learning iteration 488/2000 [0m                      

                       Computation: 47376 steps/s (collection: 1.945s, learning 0.130s)
             Mean action noise std: 1.98
          Mean value_function loss: 61.0716
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 45.9741
                       Mean reward: 43.12
               Mean episode length: 75.62
    Episode_Reward/reaching_object: 0.3024
     Episode_Reward/lifting_object: 8.8563
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 52.5000
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 2.07s
                      Time elapsed: 00:18:09
                               ETA: 00:56:09

################################################################################
                     [1m Learning iteration 489/2000 [0m                      

                       Computation: 47544 steps/s (collection: 1.915s, learning 0.153s)
             Mean action noise std: 1.98
          Mean value_function loss: 67.1174
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 45.9782
                       Mean reward: 50.33
               Mean episode length: 77.87
    Episode_Reward/reaching_object: 0.3075
     Episode_Reward/lifting_object: 8.8353
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 51.9583
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 2.07s
                      Time elapsed: 00:18:11
                               ETA: 00:56:06

################################################################################
                     [1m Learning iteration 490/2000 [0m                      

                       Computation: 46988 steps/s (collection: 1.964s, learning 0.128s)
             Mean action noise std: 1.98
          Mean value_function loss: 50.3014
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 45.9828
                       Mean reward: 44.48
               Mean episode length: 75.46
    Episode_Reward/reaching_object: 0.3062
     Episode_Reward/lifting_object: 9.0652
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 57.5833
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 2.09s
                      Time elapsed: 00:18:13
                               ETA: 00:56:04

################################################################################
                     [1m Learning iteration 491/2000 [0m                      

                       Computation: 47328 steps/s (collection: 1.964s, learning 0.113s)
             Mean action noise std: 1.98
          Mean value_function loss: 57.2969
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 45.9866
                       Mean reward: 49.00
               Mean episode length: 82.11
    Episode_Reward/reaching_object: 0.2972
     Episode_Reward/lifting_object: 8.8984
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 53.4167
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 2.08s
                      Time elapsed: 00:18:16
                               ETA: 00:56:01

################################################################################
                     [1m Learning iteration 492/2000 [0m                      

                       Computation: 46477 steps/s (collection: 2.008s, learning 0.107s)
             Mean action noise std: 1.98
          Mean value_function loss: 54.1625
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 45.9900
                       Mean reward: 43.69
               Mean episode length: 77.73
    Episode_Reward/reaching_object: 0.3030
     Episode_Reward/lifting_object: 9.0419
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 52.8333
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 2.12s
                      Time elapsed: 00:18:18
                               ETA: 00:55:59

################################################################################
                     [1m Learning iteration 493/2000 [0m                      

                       Computation: 46096 steps/s (collection: 2.013s, learning 0.120s)
             Mean action noise std: 1.98
          Mean value_function loss: 56.0350
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 45.9969
                       Mean reward: 46.68
               Mean episode length: 78.14
    Episode_Reward/reaching_object: 0.3042
     Episode_Reward/lifting_object: 9.2392
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 51.1250
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 2.13s
                      Time elapsed: 00:18:20
                               ETA: 00:55:56

################################################################################
                     [1m Learning iteration 494/2000 [0m                      

                       Computation: 46282 steps/s (collection: 1.990s, learning 0.134s)
             Mean action noise std: 1.98
          Mean value_function loss: 60.9675
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 46.0006
                       Mean reward: 45.24
               Mean episode length: 74.43
    Episode_Reward/reaching_object: 0.3110
     Episode_Reward/lifting_object: 9.4880
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 53.3750
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 2.12s
                      Time elapsed: 00:18:22
                               ETA: 00:55:54

################################################################################
                     [1m Learning iteration 495/2000 [0m                      

                       Computation: 46257 steps/s (collection: 2.033s, learning 0.092s)
             Mean action noise std: 1.98
          Mean value_function loss: 60.6138
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 46.0034
                       Mean reward: 50.79
               Mean episode length: 80.36
    Episode_Reward/reaching_object: 0.3160
     Episode_Reward/lifting_object: 9.4117
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 57.9167
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 2.13s
                      Time elapsed: 00:18:24
                               ETA: 00:55:51

################################################################################
                     [1m Learning iteration 496/2000 [0m                      

                       Computation: 45570 steps/s (collection: 2.057s, learning 0.100s)
             Mean action noise std: 1.98
          Mean value_function loss: 69.9451
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 46.0070
                       Mean reward: 50.86
               Mean episode length: 73.29
    Episode_Reward/reaching_object: 0.3088
     Episode_Reward/lifting_object: 9.4101
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 53.2500
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 2.16s
                      Time elapsed: 00:18:26
                               ETA: 00:55:49

################################################################################
                     [1m Learning iteration 497/2000 [0m                      

                       Computation: 46271 steps/s (collection: 2.004s, learning 0.120s)
             Mean action noise std: 1.98
          Mean value_function loss: 58.2177
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 46.0126
                       Mean reward: 49.92
               Mean episode length: 74.96
    Episode_Reward/reaching_object: 0.3074
     Episode_Reward/lifting_object: 9.4505
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 55.2917
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 2.12s
                      Time elapsed: 00:18:28
                               ETA: 00:55:46

################################################################################
                     [1m Learning iteration 498/2000 [0m                      

                       Computation: 45268 steps/s (collection: 2.047s, learning 0.125s)
             Mean action noise std: 1.98
          Mean value_function loss: 71.6538
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 46.0170
                       Mean reward: 41.81
               Mean episode length: 75.06
    Episode_Reward/reaching_object: 0.2998
     Episode_Reward/lifting_object: 8.8655
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 56.8333
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 2.17s
                      Time elapsed: 00:18:30
                               ETA: 00:55:44

################################################################################
                     [1m Learning iteration 499/2000 [0m                      

                       Computation: 45331 steps/s (collection: 2.062s, learning 0.107s)
             Mean action noise std: 1.98
          Mean value_function loss: 59.2876
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 46.0213
                       Mean reward: 49.76
               Mean episode length: 74.55
    Episode_Reward/reaching_object: 0.2934
     Episode_Reward/lifting_object: 9.0163
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 56.4583
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 2.17s
                      Time elapsed: 00:18:33
                               ETA: 00:55:41

################################################################################
                     [1m Learning iteration 500/2000 [0m                      

                       Computation: 43970 steps/s (collection: 2.115s, learning 0.121s)
             Mean action noise std: 1.98
          Mean value_function loss: 69.2069
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 46.0247
                       Mean reward: 39.91
               Mean episode length: 65.74
    Episode_Reward/reaching_object: 0.2922
     Episode_Reward/lifting_object: 8.9530
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 57.7083
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 2.24s
                      Time elapsed: 00:18:35
                               ETA: 00:55:39

################################################################################
                     [1m Learning iteration 501/2000 [0m                      

                       Computation: 43219 steps/s (collection: 2.147s, learning 0.128s)
             Mean action noise std: 1.98
          Mean value_function loss: 55.8009
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 46.0290
                       Mean reward: 47.79
               Mean episode length: 71.17
    Episode_Reward/reaching_object: 0.2855
     Episode_Reward/lifting_object: 8.9883
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 58.4583
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 2.27s
                      Time elapsed: 00:18:37
                               ETA: 00:55:37

################################################################################
                     [1m Learning iteration 502/2000 [0m                      

                       Computation: 44904 steps/s (collection: 2.038s, learning 0.151s)
             Mean action noise std: 1.98
          Mean value_function loss: 71.3935
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 46.0341
                       Mean reward: 44.57
               Mean episode length: 69.56
    Episode_Reward/reaching_object: 0.2749
     Episode_Reward/lifting_object: 8.7628
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 58.3750
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 2.19s
                      Time elapsed: 00:18:39
                               ETA: 00:55:35

################################################################################
                     [1m Learning iteration 503/2000 [0m                      

                       Computation: 44995 steps/s (collection: 2.064s, learning 0.121s)
             Mean action noise std: 1.98
          Mean value_function loss: 59.4334
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 46.0385
                       Mean reward: 44.88
               Mean episode length: 70.21
    Episode_Reward/reaching_object: 0.2809
     Episode_Reward/lifting_object: 8.6527
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 61.6667
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 2.18s
                      Time elapsed: 00:18:42
                               ETA: 00:55:32

################################################################################
                     [1m Learning iteration 504/2000 [0m                      

                       Computation: 46957 steps/s (collection: 1.996s, learning 0.097s)
             Mean action noise std: 1.99
          Mean value_function loss: 60.5173
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 46.0409
                       Mean reward: 49.55
               Mean episode length: 70.05
    Episode_Reward/reaching_object: 0.2751
     Episode_Reward/lifting_object: 8.9570
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 57.6667
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 2.09s
                      Time elapsed: 00:18:44
                               ETA: 00:55:30

################################################################################
                     [1m Learning iteration 505/2000 [0m                      

                       Computation: 46955 steps/s (collection: 1.989s, learning 0.104s)
             Mean action noise std: 1.99
          Mean value_function loss: 65.4621
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 46.0453
                       Mean reward: 42.35
               Mean episode length: 72.39
    Episode_Reward/reaching_object: 0.2693
     Episode_Reward/lifting_object: 8.6480
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 59.3333
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 2.09s
                      Time elapsed: 00:18:46
                               ETA: 00:55:27

################################################################################
                     [1m Learning iteration 506/2000 [0m                      

                       Computation: 46547 steps/s (collection: 2.008s, learning 0.104s)
             Mean action noise std: 1.99
          Mean value_function loss: 66.1800
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 46.0497
                       Mean reward: 42.85
               Mean episode length: 70.22
    Episode_Reward/reaching_object: 0.2747
     Episode_Reward/lifting_object: 8.7601
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 60.3750
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 2.11s
                      Time elapsed: 00:18:48
                               ETA: 00:55:24

################################################################################
                     [1m Learning iteration 507/2000 [0m                      

                       Computation: 46839 steps/s (collection: 2.006s, learning 0.093s)
             Mean action noise std: 1.99
          Mean value_function loss: 67.5038
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 46.0541
                       Mean reward: 46.27
               Mean episode length: 71.68
    Episode_Reward/reaching_object: 0.2686
     Episode_Reward/lifting_object: 8.6871
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 57.9583
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 2.10s
                      Time elapsed: 00:18:50
                               ETA: 00:55:22

################################################################################
                     [1m Learning iteration 508/2000 [0m                      

                       Computation: 46337 steps/s (collection: 2.034s, learning 0.087s)
             Mean action noise std: 1.99
          Mean value_function loss: 70.3871
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 46.0576
                       Mean reward: 46.03
               Mean episode length: 70.71
    Episode_Reward/reaching_object: 0.2750
     Episode_Reward/lifting_object: 9.0476
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 58.2083
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 2.12s
                      Time elapsed: 00:18:52
                               ETA: 00:55:19

################################################################################
                     [1m Learning iteration 509/2000 [0m                      

                       Computation: 47338 steps/s (collection: 1.975s, learning 0.102s)
             Mean action noise std: 1.99
          Mean value_function loss: 62.3986
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.0583
                       Mean reward: 47.13
               Mean episode length: 66.59
    Episode_Reward/reaching_object: 0.2734
     Episode_Reward/lifting_object: 9.2118
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 59.7083
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 2.08s
                      Time elapsed: 00:18:54
                               ETA: 00:55:17

################################################################################
                     [1m Learning iteration 510/2000 [0m                      

                       Computation: 48449 steps/s (collection: 1.941s, learning 0.088s)
             Mean action noise std: 1.99
          Mean value_function loss: 91.6826
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 46.0595
                       Mean reward: 46.42
               Mean episode length: 69.39
    Episode_Reward/reaching_object: 0.2770
     Episode_Reward/lifting_object: 9.0486
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 58.3750
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 2.03s
                      Time elapsed: 00:18:56
                               ETA: 00:55:14

################################################################################
                     [1m Learning iteration 511/2000 [0m                      

                       Computation: 48207 steps/s (collection: 1.942s, learning 0.097s)
             Mean action noise std: 1.99
          Mean value_function loss: 83.8293
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 46.0638
                       Mean reward: 47.79
               Mean episode length: 67.19
    Episode_Reward/reaching_object: 0.2695
     Episode_Reward/lifting_object: 8.9417
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 59.4583
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 2.04s
                      Time elapsed: 00:18:58
                               ETA: 00:55:11

################################################################################
                     [1m Learning iteration 512/2000 [0m                      

                       Computation: 47917 steps/s (collection: 1.950s, learning 0.102s)
             Mean action noise std: 1.99
          Mean value_function loss: 71.7130
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 46.0663
                       Mean reward: 44.59
               Mean episode length: 68.58
    Episode_Reward/reaching_object: 0.2773
     Episode_Reward/lifting_object: 8.8910
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 58.4583
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 2.05s
                      Time elapsed: 00:19:00
                               ETA: 00:55:08

################################################################################
                     [1m Learning iteration 513/2000 [0m                      

                       Computation: 47865 steps/s (collection: 1.967s, learning 0.087s)
             Mean action noise std: 1.99
          Mean value_function loss: 70.5881
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 46.0677
                       Mean reward: 48.06
               Mean episode length: 69.35
    Episode_Reward/reaching_object: 0.2735
     Episode_Reward/lifting_object: 9.0128
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 61.2500
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 2.05s
                      Time elapsed: 00:19:02
                               ETA: 00:55:06

################################################################################
                     [1m Learning iteration 514/2000 [0m                      

                       Computation: 47545 steps/s (collection: 1.963s, learning 0.104s)
             Mean action noise std: 1.99
          Mean value_function loss: 61.4451
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 46.0709
                       Mean reward: 47.86
               Mean episode length: 67.95
    Episode_Reward/reaching_object: 0.2696
     Episode_Reward/lifting_object: 9.0933
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 58.5833
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 2.07s
                      Time elapsed: 00:19:04
                               ETA: 00:55:03

################################################################################
                     [1m Learning iteration 515/2000 [0m                      

                       Computation: 47055 steps/s (collection: 1.982s, learning 0.107s)
             Mean action noise std: 1.99
          Mean value_function loss: 74.1620
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 46.0743
                       Mean reward: 45.55
               Mean episode length: 66.42
    Episode_Reward/reaching_object: 0.2685
     Episode_Reward/lifting_object: 9.1438
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 58.1667
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 2.09s
                      Time elapsed: 00:19:06
                               ETA: 00:55:00

################################################################################
                     [1m Learning iteration 516/2000 [0m                      

                       Computation: 47588 steps/s (collection: 1.952s, learning 0.114s)
             Mean action noise std: 1.99
          Mean value_function loss: 72.1113
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 46.0774
                       Mean reward: 52.21
               Mean episode length: 69.76
    Episode_Reward/reaching_object: 0.2691
     Episode_Reward/lifting_object: 8.9717
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 57.0833
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 2.07s
                      Time elapsed: 00:19:09
                               ETA: 00:54:58

################################################################################
                     [1m Learning iteration 517/2000 [0m                      

                       Computation: 47331 steps/s (collection: 1.978s, learning 0.099s)
             Mean action noise std: 1.99
          Mean value_function loss: 70.2990
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 46.0804
                       Mean reward: 48.27
               Mean episode length: 69.96
    Episode_Reward/reaching_object: 0.2822
     Episode_Reward/lifting_object: 9.5550
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 58.7917
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 2.08s
                      Time elapsed: 00:19:11
                               ETA: 00:54:55

################################################################################
                     [1m Learning iteration 518/2000 [0m                      

                       Computation: 46107 steps/s (collection: 2.036s, learning 0.097s)
             Mean action noise std: 1.99
          Mean value_function loss: 64.8814
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 46.0839
                       Mean reward: 46.65
               Mean episode length: 67.84
    Episode_Reward/reaching_object: 0.2833
     Episode_Reward/lifting_object: 9.4672
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 56.3333
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 2.13s
                      Time elapsed: 00:19:13
                               ETA: 00:54:53

################################################################################
                     [1m Learning iteration 519/2000 [0m                      

                       Computation: 48119 steps/s (collection: 1.955s, learning 0.088s)
             Mean action noise std: 1.99
          Mean value_function loss: 69.5872
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 46.0887
                       Mean reward: 49.94
               Mean episode length: 70.72
    Episode_Reward/reaching_object: 0.2901
     Episode_Reward/lifting_object: 9.8448
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 55.2083
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 2.04s
                      Time elapsed: 00:19:15
                               ETA: 00:54:50

################################################################################
                     [1m Learning iteration 520/2000 [0m                      

                       Computation: 47890 steps/s (collection: 1.961s, learning 0.092s)
             Mean action noise std: 1.99
          Mean value_function loss: 76.5008
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 46.0942
                       Mean reward: 52.34
               Mean episode length: 70.13
    Episode_Reward/reaching_object: 0.2892
     Episode_Reward/lifting_object: 9.8169
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 55.1250
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 2.05s
                      Time elapsed: 00:19:17
                               ETA: 00:54:47

################################################################################
                     [1m Learning iteration 521/2000 [0m                      

                       Computation: 47888 steps/s (collection: 1.963s, learning 0.090s)
             Mean action noise std: 1.99
          Mean value_function loss: 66.4367
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 46.0976
                       Mean reward: 49.68
               Mean episode length: 71.36
    Episode_Reward/reaching_object: 0.2955
     Episode_Reward/lifting_object: 9.9900
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 59.7917
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 2.05s
                      Time elapsed: 00:19:19
                               ETA: 00:54:44

################################################################################
                     [1m Learning iteration 522/2000 [0m                      

                       Computation: 47863 steps/s (collection: 1.967s, learning 0.087s)
             Mean action noise std: 1.99
          Mean value_function loss: 66.0746
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 46.0999
                       Mean reward: 50.30
               Mean episode length: 69.64
    Episode_Reward/reaching_object: 0.2885
     Episode_Reward/lifting_object: 9.9239
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 54.1667
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 2.05s
                      Time elapsed: 00:19:21
                               ETA: 00:54:42

################################################################################
                     [1m Learning iteration 523/2000 [0m                      

                       Computation: 47703 steps/s (collection: 1.971s, learning 0.090s)
             Mean action noise std: 1.99
          Mean value_function loss: 67.8367
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 46.1011
                       Mean reward: 51.03
               Mean episode length: 72.73
    Episode_Reward/reaching_object: 0.2904
     Episode_Reward/lifting_object: 9.9278
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 57.8333
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 2.06s
                      Time elapsed: 00:19:23
                               ETA: 00:54:39

################################################################################
                     [1m Learning iteration 524/2000 [0m                      

                       Computation: 48384 steps/s (collection: 1.936s, learning 0.096s)
             Mean action noise std: 1.99
          Mean value_function loss: 61.5913
               Mean surrogate loss: 0.0093
                 Mean entropy loss: 46.1031
                       Mean reward: 53.58
               Mean episode length: 72.27
    Episode_Reward/reaching_object: 0.2952
     Episode_Reward/lifting_object: 10.4001
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 54.0833
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 2.03s
                      Time elapsed: 00:19:25
                               ETA: 00:54:36

################################################################################
                     [1m Learning iteration 525/2000 [0m                      

                       Computation: 47204 steps/s (collection: 1.982s, learning 0.101s)
             Mean action noise std: 1.99
          Mean value_function loss: 65.2076
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 46.1037
                       Mean reward: 57.15
               Mean episode length: 77.88
    Episode_Reward/reaching_object: 0.2953
     Episode_Reward/lifting_object: 10.3787
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 54.9583
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 2.08s
                      Time elapsed: 00:19:27
                               ETA: 00:54:34

################################################################################
                     [1m Learning iteration 526/2000 [0m                      

                       Computation: 48091 steps/s (collection: 1.944s, learning 0.100s)
             Mean action noise std: 1.99
          Mean value_function loss: 71.9039
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 46.1052
                       Mean reward: 49.33
               Mean episode length: 69.02
    Episode_Reward/reaching_object: 0.2942
     Episode_Reward/lifting_object: 10.0020
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 59.9583
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 2.04s
                      Time elapsed: 00:19:29
                               ETA: 00:54:31

################################################################################
                     [1m Learning iteration 527/2000 [0m                      

                       Computation: 47982 steps/s (collection: 1.952s, learning 0.097s)
             Mean action noise std: 1.99
          Mean value_function loss: 67.0458
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 46.1078
                       Mean reward: 52.61
               Mean episode length: 72.92
    Episode_Reward/reaching_object: 0.2927
     Episode_Reward/lifting_object: 9.8637
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 58.4167
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 2.05s
                      Time elapsed: 00:19:31
                               ETA: 00:54:28

################################################################################
                     [1m Learning iteration 528/2000 [0m                      

                       Computation: 47267 steps/s (collection: 1.954s, learning 0.126s)
             Mean action noise std: 1.99
          Mean value_function loss: 70.4393
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 46.1092
                       Mean reward: 46.61
               Mean episode length: 66.90
    Episode_Reward/reaching_object: 0.2897
     Episode_Reward/lifting_object: 9.9740
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 55.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 2.08s
                      Time elapsed: 00:19:33
                               ETA: 00:54:26

################################################################################
                     [1m Learning iteration 529/2000 [0m                      

                       Computation: 46746 steps/s (collection: 2.011s, learning 0.092s)
             Mean action noise std: 1.99
          Mean value_function loss: 66.4139
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 46.1126
                       Mean reward: 50.02
               Mean episode length: 70.65
    Episode_Reward/reaching_object: 0.2881
     Episode_Reward/lifting_object: 10.0721
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 55.8333
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 2.10s
                      Time elapsed: 00:19:35
                               ETA: 00:54:23

################################################################################
                     [1m Learning iteration 530/2000 [0m                      

                       Computation: 47419 steps/s (collection: 1.974s, learning 0.099s)
             Mean action noise std: 1.99
          Mean value_function loss: 61.7844
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 46.1147
                       Mean reward: 47.02
               Mean episode length: 66.89
    Episode_Reward/reaching_object: 0.2939
     Episode_Reward/lifting_object: 10.2397
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 54.2500
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 2.07s
                      Time elapsed: 00:19:37
                               ETA: 00:54:21

################################################################################
                     [1m Learning iteration 531/2000 [0m                      

                       Computation: 48407 steps/s (collection: 1.934s, learning 0.097s)
             Mean action noise std: 1.99
          Mean value_function loss: 64.6938
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 46.1174
                       Mean reward: 52.26
               Mean episode length: 72.16
    Episode_Reward/reaching_object: 0.2926
     Episode_Reward/lifting_object: 10.0080
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 54.2917
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 2.03s
                      Time elapsed: 00:19:40
                               ETA: 00:54:18

################################################################################
                     [1m Learning iteration 532/2000 [0m                      

                       Computation: 47293 steps/s (collection: 1.961s, learning 0.118s)
             Mean action noise std: 1.99
          Mean value_function loss: 67.0251
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 46.1217
                       Mean reward: 54.83
               Mean episode length: 71.81
    Episode_Reward/reaching_object: 0.3039
     Episode_Reward/lifting_object: 10.4191
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 56.1250
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 2.08s
                      Time elapsed: 00:19:42
                               ETA: 00:54:15

################################################################################
                     [1m Learning iteration 533/2000 [0m                      

                       Computation: 47636 steps/s (collection: 1.942s, learning 0.121s)
             Mean action noise std: 2.00
          Mean value_function loss: 79.9863
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 46.1280
                       Mean reward: 53.87
               Mean episode length: 72.61
    Episode_Reward/reaching_object: 0.3061
     Episode_Reward/lifting_object: 10.5569
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 55.4167
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 2.06s
                      Time elapsed: 00:19:44
                               ETA: 00:54:13

################################################################################
                     [1m Learning iteration 534/2000 [0m                      

                       Computation: 47263 steps/s (collection: 1.953s, learning 0.127s)
             Mean action noise std: 2.00
          Mean value_function loss: 90.3101
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 46.1353
                       Mean reward: 54.59
               Mean episode length: 70.46
    Episode_Reward/reaching_object: 0.3040
     Episode_Reward/lifting_object: 10.6942
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 55.7917
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 2.08s
                      Time elapsed: 00:19:46
                               ETA: 00:54:10

################################################################################
                     [1m Learning iteration 535/2000 [0m                      

                       Computation: 47044 steps/s (collection: 1.974s, learning 0.116s)
             Mean action noise std: 2.00
          Mean value_function loss: 76.5619
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 46.1419
                       Mean reward: 54.52
               Mean episode length: 76.83
    Episode_Reward/reaching_object: 0.3068
     Episode_Reward/lifting_object: 10.7203
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 54.7500
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 2.09s
                      Time elapsed: 00:19:48
                               ETA: 00:54:07

################################################################################
                     [1m Learning iteration 536/2000 [0m                      

                       Computation: 46911 steps/s (collection: 2.009s, learning 0.086s)
             Mean action noise std: 2.00
          Mean value_function loss: 71.1590
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 46.1514
                       Mean reward: 55.32
               Mean episode length: 74.06
    Episode_Reward/reaching_object: 0.3000
     Episode_Reward/lifting_object: 10.3742
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 57.1250
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 2.10s
                      Time elapsed: 00:19:50
                               ETA: 00:54:05

################################################################################
                     [1m Learning iteration 537/2000 [0m                      

                       Computation: 47798 steps/s (collection: 1.969s, learning 0.088s)
             Mean action noise std: 2.00
          Mean value_function loss: 76.8685
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 46.1619
                       Mean reward: 52.95
               Mean episode length: 72.74
    Episode_Reward/reaching_object: 0.2997
     Episode_Reward/lifting_object: 10.6192
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 56.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 2.06s
                      Time elapsed: 00:19:52
                               ETA: 00:54:02

################################################################################
                     [1m Learning iteration 538/2000 [0m                      

                       Computation: 48608 steps/s (collection: 1.936s, learning 0.086s)
             Mean action noise std: 2.00
          Mean value_function loss: 73.7839
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 46.1695
                       Mean reward: 60.80
               Mean episode length: 78.05
    Episode_Reward/reaching_object: 0.2979
     Episode_Reward/lifting_object: 10.5367
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 56.7917
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 2.02s
                      Time elapsed: 00:19:54
                               ETA: 00:53:59

################################################################################
                     [1m Learning iteration 539/2000 [0m                      

                       Computation: 48376 steps/s (collection: 1.944s, learning 0.089s)
             Mean action noise std: 2.00
          Mean value_function loss: 72.0306
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 46.1728
                       Mean reward: 58.86
               Mean episode length: 77.32
    Episode_Reward/reaching_object: 0.2978
     Episode_Reward/lifting_object: 10.5533
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 55.5417
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 2.03s
                      Time elapsed: 00:19:56
                               ETA: 00:53:57

################################################################################
                     [1m Learning iteration 540/2000 [0m                      

                       Computation: 48117 steps/s (collection: 1.950s, learning 0.093s)
             Mean action noise std: 2.00
          Mean value_function loss: 72.0657
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 46.1746
                       Mean reward: 50.23
               Mean episode length: 67.61
    Episode_Reward/reaching_object: 0.3016
     Episode_Reward/lifting_object: 10.6527
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 56.7500
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 2.04s
                      Time elapsed: 00:19:58
                               ETA: 00:53:54

################################################################################
                     [1m Learning iteration 541/2000 [0m                      

                       Computation: 46978 steps/s (collection: 1.999s, learning 0.093s)
             Mean action noise std: 2.00
          Mean value_function loss: 69.6898
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 46.1754
                       Mean reward: 53.84
               Mean episode length: 68.99
    Episode_Reward/reaching_object: 0.2927
     Episode_Reward/lifting_object: 10.5629
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 55.1667
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 2.09s
                      Time elapsed: 00:20:00
                               ETA: 00:53:52

################################################################################
                     [1m Learning iteration 542/2000 [0m                      

                       Computation: 47418 steps/s (collection: 1.982s, learning 0.091s)
             Mean action noise std: 2.00
          Mean value_function loss: 72.8248
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 46.1764
                       Mean reward: 57.38
               Mean episode length: 74.33
    Episode_Reward/reaching_object: 0.2938
     Episode_Reward/lifting_object: 10.7931
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 58.9167
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 2.07s
                      Time elapsed: 00:20:02
                               ETA: 00:53:49

################################################################################
                     [1m Learning iteration 543/2000 [0m                      

                       Computation: 47929 steps/s (collection: 1.962s, learning 0.089s)
             Mean action noise std: 2.00
          Mean value_function loss: 81.7652
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 46.1778
                       Mean reward: 50.63
               Mean episode length: 70.80
    Episode_Reward/reaching_object: 0.2906
     Episode_Reward/lifting_object: 10.5464
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 60.1667
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 2.05s
                      Time elapsed: 00:20:04
                               ETA: 00:53:46

################################################################################
                     [1m Learning iteration 544/2000 [0m                      

                       Computation: 47488 steps/s (collection: 1.985s, learning 0.085s)
             Mean action noise std: 2.00
          Mean value_function loss: 91.1997
               Mean surrogate loss: 0.0128
                 Mean entropy loss: 46.1788
                       Mean reward: 55.95
               Mean episode length: 72.16
    Episode_Reward/reaching_object: 0.2801
     Episode_Reward/lifting_object: 10.2966
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 56.7083
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 2.07s
                      Time elapsed: 00:20:06
                               ETA: 00:53:44

################################################################################
                     [1m Learning iteration 545/2000 [0m                      

                       Computation: 48148 steps/s (collection: 1.955s, learning 0.087s)
             Mean action noise std: 2.00
          Mean value_function loss: 77.8669
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 46.1793
                       Mean reward: 50.24
               Mean episode length: 67.77
    Episode_Reward/reaching_object: 0.2838
     Episode_Reward/lifting_object: 10.3849
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 61.6250
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 2.04s
                      Time elapsed: 00:20:08
                               ETA: 00:53:41

################################################################################
                     [1m Learning iteration 546/2000 [0m                      

                       Computation: 47329 steps/s (collection: 1.981s, learning 0.096s)
             Mean action noise std: 2.00
          Mean value_function loss: 77.6542
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 46.1802
                       Mean reward: 46.87
               Mean episode length: 71.13
    Episode_Reward/reaching_object: 0.2719
     Episode_Reward/lifting_object: 9.8704
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 58.1250
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 2.08s
                      Time elapsed: 00:20:10
                               ETA: 00:53:38

################################################################################
                     [1m Learning iteration 547/2000 [0m                      

                       Computation: 47319 steps/s (collection: 1.981s, learning 0.096s)
             Mean action noise std: 2.00
          Mean value_function loss: 80.8885
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 46.1815
                       Mean reward: 47.64
               Mean episode length: 69.38
    Episode_Reward/reaching_object: 0.2719
     Episode_Reward/lifting_object: 10.0193
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 60.3333
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 2.08s
                      Time elapsed: 00:20:13
                               ETA: 00:53:36

################################################################################
                     [1m Learning iteration 548/2000 [0m                      

                       Computation: 46752 steps/s (collection: 2.005s, learning 0.098s)
             Mean action noise std: 2.00
          Mean value_function loss: 89.3446
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 46.1848
                       Mean reward: 50.11
               Mean episode length: 70.17
    Episode_Reward/reaching_object: 0.2712
     Episode_Reward/lifting_object: 9.9548
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 60.8333
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 2.10s
                      Time elapsed: 00:20:15
                               ETA: 00:53:33

################################################################################
                     [1m Learning iteration 549/2000 [0m                      

                       Computation: 46827 steps/s (collection: 1.980s, learning 0.120s)
             Mean action noise std: 2.00
          Mean value_function loss: 87.8993
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 46.1872
                       Mean reward: 53.54
               Mean episode length: 71.24
    Episode_Reward/reaching_object: 0.2657
     Episode_Reward/lifting_object: 9.7628
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 60.1250
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 2.10s
                      Time elapsed: 00:20:17
                               ETA: 00:53:31

################################################################################
                     [1m Learning iteration 550/2000 [0m                      

                       Computation: 47956 steps/s (collection: 1.947s, learning 0.103s)
             Mean action noise std: 2.00
          Mean value_function loss: 72.6234
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 46.1880
                       Mean reward: 52.49
               Mean episode length: 68.19
    Episode_Reward/reaching_object: 0.2668
     Episode_Reward/lifting_object: 9.7809
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 58.0417
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 2.05s
                      Time elapsed: 00:20:19
                               ETA: 00:53:28

################################################################################
                     [1m Learning iteration 551/2000 [0m                      

                       Computation: 47783 steps/s (collection: 1.964s, learning 0.093s)
             Mean action noise std: 2.00
          Mean value_function loss: 111.8398
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 46.1897
                       Mean reward: 56.40
               Mean episode length: 70.86
    Episode_Reward/reaching_object: 0.2728
     Episode_Reward/lifting_object: 9.8974
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 60.2917
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 2.06s
                      Time elapsed: 00:20:21
                               ETA: 00:53:26

################################################################################
                     [1m Learning iteration 552/2000 [0m                      

                       Computation: 46092 steps/s (collection: 2.001s, learning 0.132s)
             Mean action noise std: 2.00
          Mean value_function loss: 85.2941
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 46.1917
                       Mean reward: 53.33
               Mean episode length: 71.67
    Episode_Reward/reaching_object: 0.2719
     Episode_Reward/lifting_object: 9.9601
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 59.0833
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 2.13s
                      Time elapsed: 00:20:23
                               ETA: 00:53:23

################################################################################
                     [1m Learning iteration 553/2000 [0m                      

                       Computation: 47418 steps/s (collection: 1.976s, learning 0.098s)
             Mean action noise std: 2.00
          Mean value_function loss: 75.4381
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 46.1941
                       Mean reward: 49.99
               Mean episode length: 65.09
    Episode_Reward/reaching_object: 0.2656
     Episode_Reward/lifting_object: 9.6534
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 58.9167
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 2.07s
                      Time elapsed: 00:20:25
                               ETA: 00:53:21

################################################################################
                     [1m Learning iteration 554/2000 [0m                      

                       Computation: 46524 steps/s (collection: 2.012s, learning 0.101s)
             Mean action noise std: 2.00
          Mean value_function loss: 82.0092
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 46.1972
                       Mean reward: 49.05
               Mean episode length: 69.05
    Episode_Reward/reaching_object: 0.2739
     Episode_Reward/lifting_object: 10.0933
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 57.2083
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 2.11s
                      Time elapsed: 00:20:27
                               ETA: 00:53:18

################################################################################
                     [1m Learning iteration 555/2000 [0m                      

                       Computation: 46053 steps/s (collection: 2.041s, learning 0.094s)
             Mean action noise std: 2.00
          Mean value_function loss: 78.7680
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 46.2013
                       Mean reward: 53.23
               Mean episode length: 70.11
    Episode_Reward/reaching_object: 0.2755
     Episode_Reward/lifting_object: 10.4306
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 59.4583
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 2.13s
                      Time elapsed: 00:20:29
                               ETA: 00:53:16

################################################################################
                     [1m Learning iteration 556/2000 [0m                      

                       Computation: 46596 steps/s (collection: 2.024s, learning 0.086s)
             Mean action noise std: 2.00
          Mean value_function loss: 82.6097
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 46.2038
                       Mean reward: 53.53
               Mean episode length: 69.01
    Episode_Reward/reaching_object: 0.2860
     Episode_Reward/lifting_object: 10.7295
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 57.2917
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 2.11s
                      Time elapsed: 00:20:31
                               ETA: 00:53:13

################################################################################
                     [1m Learning iteration 557/2000 [0m                      

                       Computation: 46554 steps/s (collection: 2.016s, learning 0.096s)
             Mean action noise std: 2.00
          Mean value_function loss: 83.9655
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 46.2069
                       Mean reward: 50.08
               Mean episode length: 73.65
    Episode_Reward/reaching_object: 0.2795
     Episode_Reward/lifting_object: 10.3579
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 59.0417
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 2.11s
                      Time elapsed: 00:20:34
                               ETA: 00:53:11

################################################################################
                     [1m Learning iteration 558/2000 [0m                      

                       Computation: 46039 steps/s (collection: 2.018s, learning 0.117s)
             Mean action noise std: 2.00
          Mean value_function loss: 101.2541
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 46.2106
                       Mean reward: 53.22
               Mean episode length: 72.40
    Episode_Reward/reaching_object: 0.2848
     Episode_Reward/lifting_object: 10.2986
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 58.9167
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 2.14s
                      Time elapsed: 00:20:36
                               ETA: 00:53:08

################################################################################
                     [1m Learning iteration 559/2000 [0m                      

                       Computation: 44608 steps/s (collection: 2.066s, learning 0.138s)
             Mean action noise std: 2.00
          Mean value_function loss: 81.6492
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 46.2163
                       Mean reward: 48.91
               Mean episode length: 66.15
    Episode_Reward/reaching_object: 0.2763
     Episode_Reward/lifting_object: 10.2291
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 59.5000
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 2.20s
                      Time elapsed: 00:20:38
                               ETA: 00:53:06

################################################################################
                     [1m Learning iteration 560/2000 [0m                      

                       Computation: 45687 steps/s (collection: 2.039s, learning 0.113s)
             Mean action noise std: 2.00
          Mean value_function loss: 79.8455
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 46.2207
                       Mean reward: 57.68
               Mean episode length: 69.95
    Episode_Reward/reaching_object: 0.2774
     Episode_Reward/lifting_object: 10.4312
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 59.2083
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 2.15s
                      Time elapsed: 00:20:40
                               ETA: 00:53:04

################################################################################
                     [1m Learning iteration 561/2000 [0m                      

                       Computation: 46634 steps/s (collection: 2.010s, learning 0.098s)
             Mean action noise std: 2.01
          Mean value_function loss: 102.9167
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 46.2221
                       Mean reward: 53.27
               Mean episode length: 70.15
    Episode_Reward/reaching_object: 0.2771
     Episode_Reward/lifting_object: 10.4753
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 59.2083
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 2.11s
                      Time elapsed: 00:20:42
                               ETA: 00:53:01

################################################################################
                     [1m Learning iteration 562/2000 [0m                      

                       Computation: 47060 steps/s (collection: 2.002s, learning 0.087s)
             Mean action noise std: 2.01
          Mean value_function loss: 89.4406
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 46.2246
                       Mean reward: 56.19
               Mean episode length: 73.76
    Episode_Reward/reaching_object: 0.2758
     Episode_Reward/lifting_object: 10.3233
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 60.3750
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 2.09s
                      Time elapsed: 00:20:44
                               ETA: 00:52:59

################################################################################
                     [1m Learning iteration 563/2000 [0m                      

                       Computation: 46935 steps/s (collection: 2.005s, learning 0.089s)
             Mean action noise std: 2.01
          Mean value_function loss: 90.7346
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 46.2269
                       Mean reward: 52.38
               Mean episode length: 65.01
    Episode_Reward/reaching_object: 0.2751
     Episode_Reward/lifting_object: 10.4078
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 61.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 2.09s
                      Time elapsed: 00:20:46
                               ETA: 00:52:56

################################################################################
                     [1m Learning iteration 564/2000 [0m                      

                       Computation: 47279 steps/s (collection: 1.985s, learning 0.094s)
             Mean action noise std: 2.01
          Mean value_function loss: 93.3393
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 46.2286
                       Mean reward: 57.62
               Mean episode length: 76.10
    Episode_Reward/reaching_object: 0.2767
     Episode_Reward/lifting_object: 10.3878
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 59.7500
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 2.08s
                      Time elapsed: 00:20:48
                               ETA: 00:52:54

################################################################################
                     [1m Learning iteration 565/2000 [0m                      

                       Computation: 47113 steps/s (collection: 1.985s, learning 0.102s)
             Mean action noise std: 2.01
          Mean value_function loss: 82.5193
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 46.2317
                       Mean reward: 65.69
               Mean episode length: 75.32
    Episode_Reward/reaching_object: 0.2721
     Episode_Reward/lifting_object: 10.4519
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 55.7083
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 2.09s
                      Time elapsed: 00:20:50
                               ETA: 00:52:51

################################################################################
                     [1m Learning iteration 566/2000 [0m                      

                       Computation: 46877 steps/s (collection: 2.010s, learning 0.087s)
             Mean action noise std: 2.01
          Mean value_function loss: 87.3967
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 46.2360
                       Mean reward: 51.58
               Mean episode length: 70.19
    Episode_Reward/reaching_object: 0.2771
     Episode_Reward/lifting_object: 10.3195
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 56.7500
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 2.10s
                      Time elapsed: 00:20:53
                               ETA: 00:52:49

################################################################################
                     [1m Learning iteration 567/2000 [0m                      

                       Computation: 47118 steps/s (collection: 2.001s, learning 0.086s)
             Mean action noise std: 2.01
          Mean value_function loss: 92.2041
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 46.2383
                       Mean reward: 59.42
               Mean episode length: 68.73
    Episode_Reward/reaching_object: 0.2776
     Episode_Reward/lifting_object: 10.5391
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 58.8750
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 2.09s
                      Time elapsed: 00:20:55
                               ETA: 00:52:46

################################################################################
                     [1m Learning iteration 568/2000 [0m                      

                       Computation: 47866 steps/s (collection: 1.955s, learning 0.099s)
             Mean action noise std: 2.01
          Mean value_function loss: 79.3282
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 46.2405
                       Mean reward: 60.90
               Mean episode length: 69.90
    Episode_Reward/reaching_object: 0.2905
     Episode_Reward/lifting_object: 11.0620
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 59.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 2.05s
                      Time elapsed: 00:20:57
                               ETA: 00:52:44

################################################################################
                     [1m Learning iteration 569/2000 [0m                      

                       Computation: 46811 steps/s (collection: 2.002s, learning 0.098s)
             Mean action noise std: 2.01
          Mean value_function loss: 99.0959
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 46.2441
                       Mean reward: 50.86
               Mean episode length: 69.61
    Episode_Reward/reaching_object: 0.2864
     Episode_Reward/lifting_object: 10.8428
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 58.5833
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 2.10s
                      Time elapsed: 00:20:59
                               ETA: 00:52:41

################################################################################
                     [1m Learning iteration 570/2000 [0m                      

                       Computation: 47172 steps/s (collection: 1.996s, learning 0.088s)
             Mean action noise std: 2.01
          Mean value_function loss: 86.6695
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 46.2487
                       Mean reward: 58.56
               Mean episode length: 64.83
    Episode_Reward/reaching_object: 0.2783
     Episode_Reward/lifting_object: 10.4655
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 57.9583
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 2.08s
                      Time elapsed: 00:21:01
                               ETA: 00:52:39

################################################################################
                     [1m Learning iteration 571/2000 [0m                      

                       Computation: 46256 steps/s (collection: 2.020s, learning 0.105s)
             Mean action noise std: 2.01
          Mean value_function loss: 85.9181
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 46.2530
                       Mean reward: 57.22
               Mean episode length: 69.24
    Episode_Reward/reaching_object: 0.2888
     Episode_Reward/lifting_object: 11.1935
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 56.1250
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 2.13s
                      Time elapsed: 00:21:03
                               ETA: 00:52:36

################################################################################
                     [1m Learning iteration 572/2000 [0m                      

                       Computation: 46465 steps/s (collection: 2.026s, learning 0.090s)
             Mean action noise std: 2.01
          Mean value_function loss: 96.1436
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 46.2562
                       Mean reward: 52.51
               Mean episode length: 68.01
    Episode_Reward/reaching_object: 0.2859
     Episode_Reward/lifting_object: 11.0605
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 57.8333
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 2.12s
                      Time elapsed: 00:21:05
                               ETA: 00:52:34

################################################################################
                     [1m Learning iteration 573/2000 [0m                      

                       Computation: 47164 steps/s (collection: 1.992s, learning 0.092s)
             Mean action noise std: 2.01
          Mean value_function loss: 93.3110
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 46.2587
                       Mean reward: 64.47
               Mean episode length: 73.50
    Episode_Reward/reaching_object: 0.2874
     Episode_Reward/lifting_object: 11.1506
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 59.3333
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 2.08s
                      Time elapsed: 00:21:07
                               ETA: 00:52:31

################################################################################
                     [1m Learning iteration 574/2000 [0m                      

                       Computation: 47280 steps/s (collection: 1.990s, learning 0.090s)
             Mean action noise std: 2.01
          Mean value_function loss: 84.6542
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 46.2637
                       Mean reward: 56.17
               Mean episode length: 71.11
    Episode_Reward/reaching_object: 0.2899
     Episode_Reward/lifting_object: 10.9915
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 59.6250
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 2.08s
                      Time elapsed: 00:21:09
                               ETA: 00:52:29

################################################################################
                     [1m Learning iteration 575/2000 [0m                      

                       Computation: 47258 steps/s (collection: 1.986s, learning 0.094s)
             Mean action noise std: 2.01
          Mean value_function loss: 87.7688
               Mean surrogate loss: 0.0149
                 Mean entropy loss: 46.2698
                       Mean reward: 62.65
               Mean episode length: 70.71
    Episode_Reward/reaching_object: 0.2827
     Episode_Reward/lifting_object: 11.3363
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 57.1250
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 2.08s
                      Time elapsed: 00:21:11
                               ETA: 00:52:26

################################################################################
                     [1m Learning iteration 576/2000 [0m                      

                       Computation: 46965 steps/s (collection: 1.995s, learning 0.099s)
             Mean action noise std: 2.01
          Mean value_function loss: 118.0515
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 46.2715
                       Mean reward: 57.10
               Mean episode length: 70.55
    Episode_Reward/reaching_object: 0.2898
     Episode_Reward/lifting_object: 11.0859
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 57.7917
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 2.09s
                      Time elapsed: 00:21:13
                               ETA: 00:52:24

################################################################################
                     [1m Learning iteration 577/2000 [0m                      

                       Computation: 46150 steps/s (collection: 2.035s, learning 0.095s)
             Mean action noise std: 2.01
          Mean value_function loss: 105.4265
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 46.2737
                       Mean reward: 59.82
               Mean episode length: 71.33
    Episode_Reward/reaching_object: 0.2832
     Episode_Reward/lifting_object: 10.8650
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 58.3750
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 2.13s
                      Time elapsed: 00:21:16
                               ETA: 00:52:21

################################################################################
                     [1m Learning iteration 578/2000 [0m                      

                       Computation: 46365 steps/s (collection: 2.004s, learning 0.117s)
             Mean action noise std: 2.01
          Mean value_function loss: 89.2784
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 46.2755
                       Mean reward: 54.48
               Mean episode length: 69.13
    Episode_Reward/reaching_object: 0.2883
     Episode_Reward/lifting_object: 11.3334
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 57.0417
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 2.12s
                      Time elapsed: 00:21:18
                               ETA: 00:52:19

################################################################################
                     [1m Learning iteration 579/2000 [0m                      

                       Computation: 43918 steps/s (collection: 2.110s, learning 0.128s)
             Mean action noise std: 2.01
          Mean value_function loss: 84.6559
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 46.2767
                       Mean reward: 69.41
               Mean episode length: 77.50
    Episode_Reward/reaching_object: 0.2953
     Episode_Reward/lifting_object: 11.8962
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 54.8333
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 2.24s
                      Time elapsed: 00:21:20
                               ETA: 00:52:17

################################################################################
                     [1m Learning iteration 580/2000 [0m                      

                       Computation: 45286 steps/s (collection: 2.023s, learning 0.148s)
             Mean action noise std: 2.01
          Mean value_function loss: 96.8205
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 46.2782
                       Mean reward: 53.17
               Mean episode length: 67.78
    Episode_Reward/reaching_object: 0.2896
     Episode_Reward/lifting_object: 11.3647
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 57.9167
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 2.17s
                      Time elapsed: 00:21:22
                               ETA: 00:52:14

################################################################################
                     [1m Learning iteration 581/2000 [0m                      

                       Computation: 46651 steps/s (collection: 2.011s, learning 0.096s)
             Mean action noise std: 2.01
          Mean value_function loss: 94.1298
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 46.2829
                       Mean reward: 63.90
               Mean episode length: 73.56
    Episode_Reward/reaching_object: 0.2909
     Episode_Reward/lifting_object: 11.4577
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 60.2500
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 2.11s
                      Time elapsed: 00:21:24
                               ETA: 00:52:12

################################################################################
                     [1m Learning iteration 582/2000 [0m                      

                       Computation: 46251 steps/s (collection: 2.023s, learning 0.102s)
             Mean action noise std: 2.01
          Mean value_function loss: 105.3311
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 46.2913
                       Mean reward: 57.41
               Mean episode length: 71.52
    Episode_Reward/reaching_object: 0.2874
     Episode_Reward/lifting_object: 11.3164
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 53.1250
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 2.13s
                      Time elapsed: 00:21:26
                               ETA: 00:52:09

################################################################################
                     [1m Learning iteration 583/2000 [0m                      

                       Computation: 47031 steps/s (collection: 2.001s, learning 0.089s)
             Mean action noise std: 2.01
          Mean value_function loss: 97.2502
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 46.2956
                       Mean reward: 61.13
               Mean episode length: 72.20
    Episode_Reward/reaching_object: 0.2890
     Episode_Reward/lifting_object: 11.4821
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 59.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 2.09s
                      Time elapsed: 00:21:28
                               ETA: 00:52:07

################################################################################
                     [1m Learning iteration 584/2000 [0m                      

                       Computation: 46422 steps/s (collection: 2.028s, learning 0.090s)
             Mean action noise std: 2.01
          Mean value_function loss: 97.1191
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 46.2957
                       Mean reward: 59.14
               Mean episode length: 70.85
    Episode_Reward/reaching_object: 0.2930
     Episode_Reward/lifting_object: 11.8139
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 58.0417
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 2.12s
                      Time elapsed: 00:21:31
                               ETA: 00:52:05

################################################################################
                     [1m Learning iteration 585/2000 [0m                      

                       Computation: 45145 steps/s (collection: 2.063s, learning 0.115s)
             Mean action noise std: 2.01
          Mean value_function loss: 118.5248
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 46.2964
                       Mean reward: 52.93
               Mean episode length: 68.66
    Episode_Reward/reaching_object: 0.2899
     Episode_Reward/lifting_object: 11.4174
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 60.3333
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 2.18s
                      Time elapsed: 00:21:33
                               ETA: 00:52:02

################################################################################
                     [1m Learning iteration 586/2000 [0m                      

                       Computation: 46351 steps/s (collection: 2.016s, learning 0.105s)
             Mean action noise std: 2.01
          Mean value_function loss: 87.3891
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 46.2980
                       Mean reward: 63.44
               Mean episode length: 73.26
    Episode_Reward/reaching_object: 0.2813
     Episode_Reward/lifting_object: 11.0594
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 57.1667
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 2.12s
                      Time elapsed: 00:21:35
                               ETA: 00:52:00

################################################################################
                     [1m Learning iteration 587/2000 [0m                      

                       Computation: 45060 steps/s (collection: 2.038s, learning 0.143s)
             Mean action noise std: 2.01
          Mean value_function loss: 89.2238
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 46.3028
                       Mean reward: 56.43
               Mean episode length: 68.59
    Episode_Reward/reaching_object: 0.2889
     Episode_Reward/lifting_object: 11.6874
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 58.1250
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 2.18s
                      Time elapsed: 00:21:37
                               ETA: 00:51:58

################################################################################
                     [1m Learning iteration 588/2000 [0m                      

                       Computation: 45029 steps/s (collection: 2.045s, learning 0.139s)
             Mean action noise std: 2.02
          Mean value_function loss: 99.9803
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 46.3098
                       Mean reward: 61.58
               Mean episode length: 68.50
    Episode_Reward/reaching_object: 0.2874
     Episode_Reward/lifting_object: 11.8000
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 59.9167
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 2.18s
                      Time elapsed: 00:21:39
                               ETA: 00:51:55

################################################################################
                     [1m Learning iteration 589/2000 [0m                      

                       Computation: 46330 steps/s (collection: 2.010s, learning 0.112s)
             Mean action noise std: 2.02
          Mean value_function loss: 96.2062
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 46.3152
                       Mean reward: 60.91
               Mean episode length: 67.15
    Episode_Reward/reaching_object: 0.2828
     Episode_Reward/lifting_object: 11.4442
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 56.0417
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 2.12s
                      Time elapsed: 00:21:41
                               ETA: 00:51:53

################################################################################
                     [1m Learning iteration 590/2000 [0m                      

                       Computation: 45820 steps/s (collection: 2.047s, learning 0.099s)
             Mean action noise std: 2.02
          Mean value_function loss: 103.8786
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 46.3180
                       Mean reward: 63.27
               Mean episode length: 68.99
    Episode_Reward/reaching_object: 0.2882
     Episode_Reward/lifting_object: 11.7249
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 58.3750
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 2.15s
                      Time elapsed: 00:21:44
                               ETA: 00:51:51

################################################################################
                     [1m Learning iteration 591/2000 [0m                      

                       Computation: 46399 steps/s (collection: 2.028s, learning 0.091s)
             Mean action noise std: 2.02
          Mean value_function loss: 99.6866
               Mean surrogate loss: 0.0128
                 Mean entropy loss: 46.3200
                       Mean reward: 67.39
               Mean episode length: 73.99
    Episode_Reward/reaching_object: 0.2987
     Episode_Reward/lifting_object: 12.3767
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 57.8750
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 2.12s
                      Time elapsed: 00:21:46
                               ETA: 00:51:48

################################################################################
                     [1m Learning iteration 592/2000 [0m                      

                       Computation: 46878 steps/s (collection: 2.001s, learning 0.096s)
             Mean action noise std: 2.02
          Mean value_function loss: 95.9111
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 46.3204
                       Mean reward: 54.06
               Mean episode length: 69.65
    Episode_Reward/reaching_object: 0.2925
     Episode_Reward/lifting_object: 11.7310
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 56.0833
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 2.10s
                      Time elapsed: 00:21:48
                               ETA: 00:51:46

################################################################################
                     [1m Learning iteration 593/2000 [0m                      

                       Computation: 46735 steps/s (collection: 2.009s, learning 0.094s)
             Mean action noise std: 2.02
          Mean value_function loss: 85.0678
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 46.3210
                       Mean reward: 60.66
               Mean episode length: 70.10
    Episode_Reward/reaching_object: 0.2963
     Episode_Reward/lifting_object: 11.9780
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 54.4583
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 2.10s
                      Time elapsed: 00:21:50
                               ETA: 00:51:43

################################################################################
                     [1m Learning iteration 594/2000 [0m                      

                       Computation: 45847 steps/s (collection: 2.042s, learning 0.103s)
             Mean action noise std: 2.02
          Mean value_function loss: 84.9561
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 46.3240
                       Mean reward: 65.24
               Mean episode length: 71.90
    Episode_Reward/reaching_object: 0.2992
     Episode_Reward/lifting_object: 12.2255
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 57.6250
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 2.14s
                      Time elapsed: 00:21:52
                               ETA: 00:51:41

################################################################################
                     [1m Learning iteration 595/2000 [0m                      

                       Computation: 45792 steps/s (collection: 2.043s, learning 0.104s)
             Mean action noise std: 2.02
          Mean value_function loss: 88.7272
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 46.3270
                       Mean reward: 63.65
               Mean episode length: 71.27
    Episode_Reward/reaching_object: 0.3036
     Episode_Reward/lifting_object: 12.2754
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 52.8333
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 2.15s
                      Time elapsed: 00:21:54
                               ETA: 00:51:39

################################################################################
                     [1m Learning iteration 596/2000 [0m                      

                       Computation: 42146 steps/s (collection: 2.224s, learning 0.108s)
             Mean action noise std: 2.02
          Mean value_function loss: 102.6251
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 46.3303
                       Mean reward: 61.36
               Mean episode length: 75.44
    Episode_Reward/reaching_object: 0.3115
     Episode_Reward/lifting_object: 12.2190
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 57.4167
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 2.33s
                      Time elapsed: 00:21:56
                               ETA: 00:51:37

################################################################################
                     [1m Learning iteration 597/2000 [0m                      

                       Computation: 46224 steps/s (collection: 2.022s, learning 0.105s)
             Mean action noise std: 2.02
          Mean value_function loss: 93.2289
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 46.3342
                       Mean reward: 62.01
               Mean episode length: 71.24
    Episode_Reward/reaching_object: 0.3072
     Episode_Reward/lifting_object: 12.2443
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 55.6667
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 2.13s
                      Time elapsed: 00:21:59
                               ETA: 00:51:34

################################################################################
                     [1m Learning iteration 598/2000 [0m                      

                       Computation: 45453 steps/s (collection: 2.057s, learning 0.106s)
             Mean action noise std: 2.02
          Mean value_function loss: 104.6489
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 46.3393
                       Mean reward: 58.71
               Mean episode length: 66.77
    Episode_Reward/reaching_object: 0.3031
     Episode_Reward/lifting_object: 12.2419
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 58.8333
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 2.16s
                      Time elapsed: 00:22:01
                               ETA: 00:51:32

################################################################################
                     [1m Learning iteration 599/2000 [0m                      

                       Computation: 45202 steps/s (collection: 2.077s, learning 0.098s)
             Mean action noise std: 2.02
          Mean value_function loss: 98.9154
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 46.3443
                       Mean reward: 60.99
               Mean episode length: 75.69
    Episode_Reward/reaching_object: 0.2955
     Episode_Reward/lifting_object: 12.0289
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 59.4583
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 2.17s
                      Time elapsed: 00:22:03
                               ETA: 00:51:30

################################################################################
                     [1m Learning iteration 600/2000 [0m                      

                       Computation: 45051 steps/s (collection: 2.068s, learning 0.114s)
             Mean action noise std: 2.02
          Mean value_function loss: 92.1722
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 46.3484
                       Mean reward: 60.09
               Mean episode length: 68.88
    Episode_Reward/reaching_object: 0.2885
     Episode_Reward/lifting_object: 12.1125
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 56.8750
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 2.18s
                      Time elapsed: 00:22:05
                               ETA: 00:51:27

################################################################################
                     [1m Learning iteration 601/2000 [0m                      

                       Computation: 44446 steps/s (collection: 2.091s, learning 0.121s)
             Mean action noise std: 2.02
          Mean value_function loss: 97.7180
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 46.3531
                       Mean reward: 63.10
               Mean episode length: 68.98
    Episode_Reward/reaching_object: 0.2957
     Episode_Reward/lifting_object: 12.2670
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 58.2083
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 2.21s
                      Time elapsed: 00:22:07
                               ETA: 00:51:25

################################################################################
                     [1m Learning iteration 602/2000 [0m                      

                       Computation: 46210 steps/s (collection: 2.033s, learning 0.094s)
             Mean action noise std: 2.02
          Mean value_function loss: 104.7873
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 46.3576
                       Mean reward: 63.14
               Mean episode length: 69.67
    Episode_Reward/reaching_object: 0.2908
     Episode_Reward/lifting_object: 12.3949
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 59.0417
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 2.13s
                      Time elapsed: 00:22:09
                               ETA: 00:51:23

################################################################################
                     [1m Learning iteration 603/2000 [0m                      

                       Computation: 45653 steps/s (collection: 2.053s, learning 0.100s)
             Mean action noise std: 2.02
          Mean value_function loss: 92.7075
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 46.3619
                       Mean reward: 58.09
               Mean episode length: 65.57
    Episode_Reward/reaching_object: 0.2887
     Episode_Reward/lifting_object: 12.2326
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 60.5833
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 2.15s
                      Time elapsed: 00:22:12
                               ETA: 00:51:20

################################################################################
                     [1m Learning iteration 604/2000 [0m                      

                       Computation: 44104 steps/s (collection: 2.117s, learning 0.112s)
             Mean action noise std: 2.02
          Mean value_function loss: 109.2370
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 46.3665
                       Mean reward: 63.65
               Mean episode length: 66.34
    Episode_Reward/reaching_object: 0.2866
     Episode_Reward/lifting_object: 12.2469
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 57.9167
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 2.23s
                      Time elapsed: 00:22:14
                               ETA: 00:51:18

################################################################################
                     [1m Learning iteration 605/2000 [0m                      

                       Computation: 45194 steps/s (collection: 2.087s, learning 0.089s)
             Mean action noise std: 2.02
          Mean value_function loss: 103.6065
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 46.3695
                       Mean reward: 51.88
               Mean episode length: 67.55
    Episode_Reward/reaching_object: 0.2832
     Episode_Reward/lifting_object: 11.9630
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 59.4583
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 2.18s
                      Time elapsed: 00:22:16
                               ETA: 00:51:16

################################################################################
                     [1m Learning iteration 606/2000 [0m                      

                       Computation: 45068 steps/s (collection: 2.049s, learning 0.132s)
             Mean action noise std: 2.02
          Mean value_function loss: 106.4062
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 46.3747
                       Mean reward: 68.21
               Mean episode length: 69.90
    Episode_Reward/reaching_object: 0.2880
     Episode_Reward/lifting_object: 12.1868
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 56.9167
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 2.18s
                      Time elapsed: 00:22:18
                               ETA: 00:51:14

################################################################################
                     [1m Learning iteration 607/2000 [0m                      

                       Computation: 44855 steps/s (collection: 2.070s, learning 0.121s)
             Mean action noise std: 2.02
          Mean value_function loss: 97.3702
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 46.3816
                       Mean reward: 65.93
               Mean episode length: 71.20
    Episode_Reward/reaching_object: 0.2909
     Episode_Reward/lifting_object: 12.5521
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 58.0417
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 2.19s
                      Time elapsed: 00:22:20
                               ETA: 00:51:12

################################################################################
                     [1m Learning iteration 608/2000 [0m                      

                       Computation: 46241 steps/s (collection: 2.036s, learning 0.090s)
             Mean action noise std: 2.02
          Mean value_function loss: 100.1720
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 46.3866
                       Mean reward: 63.36
               Mean episode length: 66.86
    Episode_Reward/reaching_object: 0.2824
     Episode_Reward/lifting_object: 12.4298
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 61.0417
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 2.13s
                      Time elapsed: 00:22:22
                               ETA: 00:51:09

################################################################################
                     [1m Learning iteration 609/2000 [0m                      

                       Computation: 44708 steps/s (collection: 2.108s, learning 0.091s)
             Mean action noise std: 2.02
          Mean value_function loss: 98.6409
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 46.3899
                       Mean reward: 59.21
               Mean episode length: 68.00
    Episode_Reward/reaching_object: 0.2862
     Episode_Reward/lifting_object: 12.5202
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 58.5000
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 2.20s
                      Time elapsed: 00:22:25
                               ETA: 00:51:07

################################################################################
                     [1m Learning iteration 610/2000 [0m                      

                       Computation: 44753 steps/s (collection: 2.074s, learning 0.123s)
             Mean action noise std: 2.02
          Mean value_function loss: 109.5684
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 46.3943
                       Mean reward: 60.04
               Mean episode length: 65.60
    Episode_Reward/reaching_object: 0.2881
     Episode_Reward/lifting_object: 12.6723
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 58.8333
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 2.20s
                      Time elapsed: 00:22:27
                               ETA: 00:51:05

################################################################################
                     [1m Learning iteration 611/2000 [0m                      

                       Computation: 45423 steps/s (collection: 2.073s, learning 0.092s)
             Mean action noise std: 2.02
          Mean value_function loss: 117.1633
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 46.3966
                       Mean reward: 60.73
               Mean episode length: 69.69
    Episode_Reward/reaching_object: 0.2872
     Episode_Reward/lifting_object: 12.4374
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 56.9167
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 2.16s
                      Time elapsed: 00:22:29
                               ETA: 00:51:02

################################################################################
                     [1m Learning iteration 612/2000 [0m                      

                       Computation: 43101 steps/s (collection: 2.100s, learning 0.181s)
             Mean action noise std: 2.03
          Mean value_function loss: 120.1702
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 46.3990
                       Mean reward: 59.35
               Mean episode length: 70.91
    Episode_Reward/reaching_object: 0.2909
     Episode_Reward/lifting_object: 12.7063
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 62.2917
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 2.28s
                      Time elapsed: 00:22:31
                               ETA: 00:51:00

################################################################################
                     [1m Learning iteration 613/2000 [0m                      

                       Computation: 43896 steps/s (collection: 2.129s, learning 0.110s)
             Mean action noise std: 2.03
          Mean value_function loss: 113.0505
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 46.4033
                       Mean reward: 61.87
               Mean episode length: 70.76
    Episode_Reward/reaching_object: 0.2832
     Episode_Reward/lifting_object: 12.3416
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 57.9167
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 2.24s
                      Time elapsed: 00:22:34
                               ETA: 00:50:58

################################################################################
                     [1m Learning iteration 614/2000 [0m                      

                       Computation: 45261 steps/s (collection: 2.075s, learning 0.097s)
             Mean action noise std: 2.03
          Mean value_function loss: 102.9125
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 46.4081
                       Mean reward: 58.68
               Mean episode length: 67.20
    Episode_Reward/reaching_object: 0.2841
     Episode_Reward/lifting_object: 12.1138
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 60.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 2.17s
                      Time elapsed: 00:22:36
                               ETA: 00:50:56

################################################################################
                     [1m Learning iteration 615/2000 [0m                      

                       Computation: 46025 steps/s (collection: 2.038s, learning 0.098s)
             Mean action noise std: 2.03
          Mean value_function loss: 102.5667
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 46.4094
                       Mean reward: 69.51
               Mean episode length: 68.30
    Episode_Reward/reaching_object: 0.2841
     Episode_Reward/lifting_object: 12.7292
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 58.2500
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 2.14s
                      Time elapsed: 00:22:38
                               ETA: 00:50:54

################################################################################
                     [1m Learning iteration 616/2000 [0m                      

                       Computation: 45162 steps/s (collection: 2.083s, learning 0.094s)
             Mean action noise std: 2.03
          Mean value_function loss: 100.6002
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 46.4119
                       Mean reward: 72.82
               Mean episode length: 70.13
    Episode_Reward/reaching_object: 0.2865
     Episode_Reward/lifting_object: 12.8688
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 56.3333
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 2.18s
                      Time elapsed: 00:22:40
                               ETA: 00:50:51

################################################################################
                     [1m Learning iteration 617/2000 [0m                      

                       Computation: 41563 steps/s (collection: 2.129s, learning 0.236s)
             Mean action noise std: 2.03
          Mean value_function loss: 108.7124
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 46.4150
                       Mean reward: 55.05
               Mean episode length: 65.84
    Episode_Reward/reaching_object: 0.2900
     Episode_Reward/lifting_object: 12.8797
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 62.9583
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 2.37s
                      Time elapsed: 00:22:42
                               ETA: 00:50:50

################################################################################
                     [1m Learning iteration 618/2000 [0m                      

                       Computation: 43481 steps/s (collection: 2.157s, learning 0.104s)
             Mean action noise std: 2.03
          Mean value_function loss: 109.3374
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 46.4166
                       Mean reward: 68.23
               Mean episode length: 68.80
    Episode_Reward/reaching_object: 0.2870
     Episode_Reward/lifting_object: 12.9049
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 59.6250
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 2.26s
                      Time elapsed: 00:22:45
                               ETA: 00:50:47

################################################################################
                     [1m Learning iteration 619/2000 [0m                      

                       Computation: 40405 steps/s (collection: 2.243s, learning 0.190s)
             Mean action noise std: 2.03
          Mean value_function loss: 115.9652
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 46.4196
                       Mean reward: 59.06
               Mean episode length: 65.92
    Episode_Reward/reaching_object: 0.2740
     Episode_Reward/lifting_object: 12.4305
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 59.6667
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 2.43s
                      Time elapsed: 00:22:47
                               ETA: 00:50:46

################################################################################
                     [1m Learning iteration 620/2000 [0m                      

                       Computation: 41982 steps/s (collection: 2.233s, learning 0.109s)
             Mean action noise std: 2.03
          Mean value_function loss: 127.2510
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 46.4238
                       Mean reward: 71.72
               Mean episode length: 71.44
    Episode_Reward/reaching_object: 0.2751
     Episode_Reward/lifting_object: 12.7169
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 62.1250
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 2.34s
                      Time elapsed: 00:22:49
                               ETA: 00:50:44

################################################################################
                     [1m Learning iteration 621/2000 [0m                      

                       Computation: 44763 steps/s (collection: 2.086s, learning 0.110s)
             Mean action noise std: 2.03
          Mean value_function loss: 120.2374
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 46.4258
                       Mean reward: 68.29
               Mean episode length: 68.87
    Episode_Reward/reaching_object: 0.2796
     Episode_Reward/lifting_object: 12.7143
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 58.7500
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 2.20s
                      Time elapsed: 00:22:52
                               ETA: 00:50:42

################################################################################
                     [1m Learning iteration 622/2000 [0m                      

                       Computation: 44833 steps/s (collection: 2.100s, learning 0.093s)
             Mean action noise std: 2.03
          Mean value_function loss: 114.2845
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.4296
                       Mean reward: 71.47
               Mean episode length: 69.65
    Episode_Reward/reaching_object: 0.2764
     Episode_Reward/lifting_object: 12.6172
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 57.5000
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 2.19s
                      Time elapsed: 00:22:54
                               ETA: 00:50:39

################################################################################
                     [1m Learning iteration 623/2000 [0m                      

                       Computation: 43022 steps/s (collection: 2.119s, learning 0.166s)
             Mean action noise std: 2.03
          Mean value_function loss: 139.6672
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 46.4339
                       Mean reward: 63.57
               Mean episode length: 68.58
    Episode_Reward/reaching_object: 0.2838
     Episode_Reward/lifting_object: 12.8303
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 58.7917
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 2.28s
                      Time elapsed: 00:22:56
                               ETA: 00:50:37

################################################################################
                     [1m Learning iteration 624/2000 [0m                      

                       Computation: 44107 steps/s (collection: 2.096s, learning 0.133s)
             Mean action noise std: 2.03
          Mean value_function loss: 104.5551
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.4358
                       Mean reward: 73.37
               Mean episode length: 70.80
    Episode_Reward/reaching_object: 0.2959
     Episode_Reward/lifting_object: 13.4754
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 56.5000
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 2.23s
                      Time elapsed: 00:22:58
                               ETA: 00:50:35

################################################################################
                     [1m Learning iteration 625/2000 [0m                      

                       Computation: 44440 steps/s (collection: 2.101s, learning 0.111s)
             Mean action noise std: 2.03
          Mean value_function loss: 105.8687
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 46.4359
                       Mean reward: 73.98
               Mean episode length: 72.86
    Episode_Reward/reaching_object: 0.2912
     Episode_Reward/lifting_object: 13.6478
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 58.3750
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 2.21s
                      Time elapsed: 00:23:01
                               ETA: 00:50:33

################################################################################
                     [1m Learning iteration 626/2000 [0m                      

                       Computation: 44958 steps/s (collection: 2.080s, learning 0.106s)
             Mean action noise std: 2.03
          Mean value_function loss: 108.9118
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 46.4381
                       Mean reward: 64.12
               Mean episode length: 64.11
    Episode_Reward/reaching_object: 0.2958
     Episode_Reward/lifting_object: 13.8417
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 58.3333
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 2.19s
                      Time elapsed: 00:23:03
                               ETA: 00:50:31

################################################################################
                     [1m Learning iteration 627/2000 [0m                      

                       Computation: 44742 steps/s (collection: 2.097s, learning 0.100s)
             Mean action noise std: 2.03
          Mean value_function loss: 115.5197
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 46.4426
                       Mean reward: 70.34
               Mean episode length: 71.13
    Episode_Reward/reaching_object: 0.2958
     Episode_Reward/lifting_object: 13.6246
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 59.7917
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 2.20s
                      Time elapsed: 00:23:05
                               ETA: 00:50:29

################################################################################
                     [1m Learning iteration 628/2000 [0m                      

                       Computation: 44658 steps/s (collection: 2.109s, learning 0.093s)
             Mean action noise std: 2.03
          Mean value_function loss: 121.1880
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 46.4465
                       Mean reward: 66.08
               Mean episode length: 67.48
    Episode_Reward/reaching_object: 0.2899
     Episode_Reward/lifting_object: 13.6400
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 57.3333
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 2.20s
                      Time elapsed: 00:23:07
                               ETA: 00:50:26

################################################################################
                     [1m Learning iteration 629/2000 [0m                      

                       Computation: 45434 steps/s (collection: 2.066s, learning 0.098s)
             Mean action noise std: 2.03
          Mean value_function loss: 109.9144
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 46.4518
                       Mean reward: 68.73
               Mean episode length: 67.39
    Episode_Reward/reaching_object: 0.2892
     Episode_Reward/lifting_object: 13.3856
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 59.0417
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 2.16s
                      Time elapsed: 00:23:09
                               ETA: 00:50:24

################################################################################
                     [1m Learning iteration 630/2000 [0m                      

                       Computation: 43941 steps/s (collection: 2.098s, learning 0.140s)
             Mean action noise std: 2.03
          Mean value_function loss: 110.2328
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 46.4536
                       Mean reward: 73.19
               Mean episode length: 69.81
    Episode_Reward/reaching_object: 0.2961
     Episode_Reward/lifting_object: 13.6659
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 58.1250
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 2.24s
                      Time elapsed: 00:23:12
                               ETA: 00:50:22

################################################################################
                     [1m Learning iteration 631/2000 [0m                      

                       Computation: 44939 steps/s (collection: 2.087s, learning 0.101s)
             Mean action noise std: 2.03
          Mean value_function loss: 112.7962
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 46.4542
                       Mean reward: 62.40
               Mean episode length: 66.96
    Episode_Reward/reaching_object: 0.2954
     Episode_Reward/lifting_object: 13.6000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 56.2917
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 2.19s
                      Time elapsed: 00:23:14
                               ETA: 00:50:20

################################################################################
                     [1m Learning iteration 632/2000 [0m                      

                       Computation: 44797 steps/s (collection: 2.094s, learning 0.100s)
             Mean action noise std: 2.03
          Mean value_function loss: 108.9037
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 46.4554
                       Mean reward: 68.92
               Mean episode length: 70.26
    Episode_Reward/reaching_object: 0.2977
     Episode_Reward/lifting_object: 13.6689
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 55.6667
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 2.19s
                      Time elapsed: 00:23:16
                               ETA: 00:50:17

################################################################################
                     [1m Learning iteration 633/2000 [0m                      

                       Computation: 43256 steps/s (collection: 2.114s, learning 0.159s)
             Mean action noise std: 2.03
          Mean value_function loss: 121.9765
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 46.4567
                       Mean reward: 72.27
               Mean episode length: 73.26
    Episode_Reward/reaching_object: 0.3002
     Episode_Reward/lifting_object: 13.9120
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 58.7917
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 2.27s
                      Time elapsed: 00:23:18
                               ETA: 00:50:15

################################################################################
                     [1m Learning iteration 634/2000 [0m                      

                       Computation: 42359 steps/s (collection: 2.163s, learning 0.158s)
             Mean action noise std: 2.03
          Mean value_function loss: 109.0777
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 46.4582
                       Mean reward: 67.22
               Mean episode length: 68.53
    Episode_Reward/reaching_object: 0.2991
     Episode_Reward/lifting_object: 13.8794
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 55.5000
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 2.32s
                      Time elapsed: 00:23:21
                               ETA: 00:50:13

################################################################################
                     [1m Learning iteration 635/2000 [0m                      

                       Computation: 45721 steps/s (collection: 2.055s, learning 0.096s)
             Mean action noise std: 2.03
          Mean value_function loss: 128.2585
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 46.4598
                       Mean reward: 69.87
               Mean episode length: 69.13
    Episode_Reward/reaching_object: 0.3097
     Episode_Reward/lifting_object: 14.6679
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 56.1250
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 2.15s
                      Time elapsed: 00:23:23
                               ETA: 00:50:11

################################################################################
                     [1m Learning iteration 636/2000 [0m                      

                       Computation: 45691 steps/s (collection: 2.056s, learning 0.095s)
             Mean action noise std: 2.03
          Mean value_function loss: 136.6520
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 46.4618
                       Mean reward: 73.07
               Mean episode length: 78.70
    Episode_Reward/reaching_object: 0.3095
     Episode_Reward/lifting_object: 14.2041
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 57.8333
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 2.15s
                      Time elapsed: 00:23:25
                               ETA: 00:50:09

################################################################################
                     [1m Learning iteration 637/2000 [0m                      

                       Computation: 44162 steps/s (collection: 2.114s, learning 0.112s)
             Mean action noise std: 2.03
          Mean value_function loss: 123.5349
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 46.4627
                       Mean reward: 68.89
               Mean episode length: 66.10
    Episode_Reward/reaching_object: 0.3005
     Episode_Reward/lifting_object: 14.0761
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 58.2500
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 2.23s
                      Time elapsed: 00:23:27
                               ETA: 00:50:07

################################################################################
                     [1m Learning iteration 638/2000 [0m                      

                       Computation: 44953 steps/s (collection: 2.090s, learning 0.097s)
             Mean action noise std: 2.03
          Mean value_function loss: 118.9084
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 46.4631
                       Mean reward: 71.14
               Mean episode length: 70.22
    Episode_Reward/reaching_object: 0.3025
     Episode_Reward/lifting_object: 14.1882
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 56.2500
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 2.19s
                      Time elapsed: 00:23:29
                               ETA: 00:50:04

################################################################################
                     [1m Learning iteration 639/2000 [0m                      

                       Computation: 42793 steps/s (collection: 2.145s, learning 0.152s)
             Mean action noise std: 2.03
          Mean value_function loss: 268.0284
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 46.4637
                       Mean reward: 70.10
               Mean episode length: 69.74
    Episode_Reward/reaching_object: 0.3076
     Episode_Reward/lifting_object: 14.4585
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 56.9583
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 2.30s
                      Time elapsed: 00:23:32
                               ETA: 00:50:02

################################################################################
                     [1m Learning iteration 640/2000 [0m                      

                       Computation: 43928 steps/s (collection: 2.145s, learning 0.093s)
             Mean action noise std: 2.03
          Mean value_function loss: 219.6480
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 46.4663
                       Mean reward: 67.81
               Mean episode length: 71.07
    Episode_Reward/reaching_object: 0.3020
     Episode_Reward/lifting_object: 14.1716
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 54.8333
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 2.24s
                      Time elapsed: 00:23:34
                               ETA: 00:50:00

################################################################################
                     [1m Learning iteration 641/2000 [0m                      

                       Computation: 44426 steps/s (collection: 2.123s, learning 0.090s)
             Mean action noise std: 2.03
          Mean value_function loss: 124.2235
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 46.4701
                       Mean reward: 84.28
               Mean episode length: 75.11
    Episode_Reward/reaching_object: 0.3088
     Episode_Reward/lifting_object: 14.6155
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 57.3333
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 2.21s
                      Time elapsed: 00:23:36
                               ETA: 00:49:58

################################################################################
                     [1m Learning iteration 642/2000 [0m                      

                       Computation: 40455 steps/s (collection: 2.290s, learning 0.140s)
             Mean action noise std: 2.03
          Mean value_function loss: 122.2611
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 46.4735
                       Mean reward: 74.08
               Mean episode length: 70.80
    Episode_Reward/reaching_object: 0.3121
     Episode_Reward/lifting_object: 14.2829
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 57.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 2.43s
                      Time elapsed: 00:23:38
                               ETA: 00:49:56

################################################################################
                     [1m Learning iteration 643/2000 [0m                      

                       Computation: 42943 steps/s (collection: 2.191s, learning 0.099s)
             Mean action noise std: 2.04
          Mean value_function loss: 127.2621
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 46.4793
                       Mean reward: 69.18
               Mean episode length: 68.99
    Episode_Reward/reaching_object: 0.3045
     Episode_Reward/lifting_object: 14.4578
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 58.0417
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 2.29s
                      Time elapsed: 00:23:41
                               ETA: 00:49:54

################################################################################
                     [1m Learning iteration 644/2000 [0m                      

                       Computation: 45510 steps/s (collection: 2.067s, learning 0.093s)
             Mean action noise std: 2.04
          Mean value_function loss: 140.2521
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 46.4831
                       Mean reward: 77.44
               Mean episode length: 71.31
    Episode_Reward/reaching_object: 0.3061
     Episode_Reward/lifting_object: 14.4233
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 56.8333
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 2.16s
                      Time elapsed: 00:23:43
                               ETA: 00:49:52

################################################################################
                     [1m Learning iteration 645/2000 [0m                      

                       Computation: 43615 steps/s (collection: 2.148s, learning 0.106s)
             Mean action noise std: 2.04
          Mean value_function loss: 152.0722
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 46.4862
                       Mean reward: 66.87
               Mean episode length: 67.01
    Episode_Reward/reaching_object: 0.2973
     Episode_Reward/lifting_object: 13.9989
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 59.1250
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 2.25s
                      Time elapsed: 00:23:45
                               ETA: 00:49:50

################################################################################
                     [1m Learning iteration 646/2000 [0m                      

                       Computation: 45690 steps/s (collection: 2.058s, learning 0.093s)
             Mean action noise std: 2.04
          Mean value_function loss: 127.6015
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 46.4876
                       Mean reward: 72.51
               Mean episode length: 69.48
    Episode_Reward/reaching_object: 0.3055
     Episode_Reward/lifting_object: 14.8729
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 63.1667
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 2.15s
                      Time elapsed: 00:23:47
                               ETA: 00:49:47

################################################################################
                     [1m Learning iteration 647/2000 [0m                      

                       Computation: 39683 steps/s (collection: 2.348s, learning 0.130s)
             Mean action noise std: 2.04
          Mean value_function loss: 129.5957
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 46.4902
                       Mean reward: 71.32
               Mean episode length: 63.25
    Episode_Reward/reaching_object: 0.2875
     Episode_Reward/lifting_object: 14.1346
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 61.8750
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 2.48s
                      Time elapsed: 00:23:50
                               ETA: 00:49:46

################################################################################
                     [1m Learning iteration 648/2000 [0m                      

                       Computation: 44472 steps/s (collection: 2.116s, learning 0.095s)
             Mean action noise std: 2.04
          Mean value_function loss: 136.6845
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 46.4945
                       Mean reward: 76.89
               Mean episode length: 66.96
    Episode_Reward/reaching_object: 0.2816
     Episode_Reward/lifting_object: 14.0945
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 62.6250
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 2.21s
                      Time elapsed: 00:23:52
                               ETA: 00:49:44

################################################################################
                     [1m Learning iteration 649/2000 [0m                      

                       Computation: 40841 steps/s (collection: 2.264s, learning 0.143s)
             Mean action noise std: 2.04
          Mean value_function loss: 139.5216
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 46.4987
                       Mean reward: 76.87
               Mean episode length: 67.81
    Episode_Reward/reaching_object: 0.2740
     Episode_Reward/lifting_object: 13.8698
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 58.7917
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 2.41s
                      Time elapsed: 00:23:54
                               ETA: 00:49:42

################################################################################
                     [1m Learning iteration 650/2000 [0m                      

                       Computation: 37946 steps/s (collection: 2.470s, learning 0.121s)
             Mean action noise std: 2.04
          Mean value_function loss: 133.3249
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 46.5010
                       Mean reward: 65.21
               Mean episode length: 64.36
    Episode_Reward/reaching_object: 0.2830
     Episode_Reward/lifting_object: 14.0319
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 61.2917
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 2.59s
                      Time elapsed: 00:23:57
                               ETA: 00:49:40

################################################################################
                     [1m Learning iteration 651/2000 [0m                      

                       Computation: 40879 steps/s (collection: 2.280s, learning 0.125s)
             Mean action noise std: 2.04
          Mean value_function loss: 131.7570
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 46.5015
                       Mean reward: 71.45
               Mean episode length: 66.98
    Episode_Reward/reaching_object: 0.2861
     Episode_Reward/lifting_object: 14.3371
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 61.2083
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 2.40s
                      Time elapsed: 00:23:59
                               ETA: 00:49:39

################################################################################
                     [1m Learning iteration 652/2000 [0m                      

                       Computation: 42818 steps/s (collection: 2.146s, learning 0.150s)
             Mean action noise std: 2.04
          Mean value_function loss: 136.1772
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 46.5004
                       Mean reward: 72.74
               Mean episode length: 69.26
    Episode_Reward/reaching_object: 0.2813
     Episode_Reward/lifting_object: 13.8231
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 62.5417
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 2.30s
                      Time elapsed: 00:24:02
                               ETA: 00:49:37

################################################################################
                     [1m Learning iteration 653/2000 [0m                      

                       Computation: 44002 steps/s (collection: 2.127s, learning 0.107s)
             Mean action noise std: 2.04
          Mean value_function loss: 127.9817
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 46.5016
                       Mean reward: 79.76
               Mean episode length: 70.46
    Episode_Reward/reaching_object: 0.2868
     Episode_Reward/lifting_object: 14.4379
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 61.7083
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 2.23s
                      Time elapsed: 00:24:04
                               ETA: 00:49:34

################################################################################
                     [1m Learning iteration 654/2000 [0m                      

                       Computation: 44839 steps/s (collection: 2.099s, learning 0.094s)
             Mean action noise std: 2.04
          Mean value_function loss: 136.7936
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 46.5038
                       Mean reward: 68.50
               Mean episode length: 60.30
    Episode_Reward/reaching_object: 0.2770
     Episode_Reward/lifting_object: 14.2403
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 60.9167
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 2.19s
                      Time elapsed: 00:24:06
                               ETA: 00:49:32

################################################################################
                     [1m Learning iteration 655/2000 [0m                      

                       Computation: 45591 steps/s (collection: 2.060s, learning 0.096s)
             Mean action noise std: 2.04
          Mean value_function loss: 133.1442
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 46.5073
                       Mean reward: 67.92
               Mean episode length: 66.35
    Episode_Reward/reaching_object: 0.2806
     Episode_Reward/lifting_object: 14.1468
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 60.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 2.16s
                      Time elapsed: 00:24:08
                               ETA: 00:49:30

################################################################################
                     [1m Learning iteration 656/2000 [0m                      

                       Computation: 44822 steps/s (collection: 2.057s, learning 0.136s)
             Mean action noise std: 2.04
          Mean value_function loss: 163.4812
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 46.5119
                       Mean reward: 74.90
               Mean episode length: 67.25
    Episode_Reward/reaching_object: 0.2805
     Episode_Reward/lifting_object: 14.1522
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 64.5833
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 2.19s
                      Time elapsed: 00:24:10
                               ETA: 00:49:28

################################################################################
                     [1m Learning iteration 657/2000 [0m                      

                       Computation: 42924 steps/s (collection: 2.113s, learning 0.177s)
             Mean action noise std: 2.04
          Mean value_function loss: 141.7446
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 46.5188
                       Mean reward: 71.67
               Mean episode length: 64.40
    Episode_Reward/reaching_object: 0.2739
     Episode_Reward/lifting_object: 13.8867
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 60.5000
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 2.29s
                      Time elapsed: 00:24:13
                               ETA: 00:49:26

################################################################################
                     [1m Learning iteration 658/2000 [0m                      

                       Computation: 41572 steps/s (collection: 2.245s, learning 0.120s)
             Mean action noise std: 2.04
          Mean value_function loss: 127.1687
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 46.5220
                       Mean reward: 76.01
               Mean episode length: 68.58
    Episode_Reward/reaching_object: 0.2848
     Episode_Reward/lifting_object: 14.9382
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 56.4583
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 2.36s
                      Time elapsed: 00:24:15
                               ETA: 00:49:24

################################################################################
                     [1m Learning iteration 659/2000 [0m                      

                       Computation: 44578 steps/s (collection: 2.106s, learning 0.099s)
             Mean action noise std: 2.04
          Mean value_function loss: 139.8953
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 46.5224
                       Mean reward: 79.03
               Mean episode length: 66.95
    Episode_Reward/reaching_object: 0.2907
     Episode_Reward/lifting_object: 14.7691
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 58.0833
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 2.21s
                      Time elapsed: 00:24:17
                               ETA: 00:49:21

################################################################################
                     [1m Learning iteration 660/2000 [0m                      

                       Computation: 43525 steps/s (collection: 2.122s, learning 0.137s)
             Mean action noise std: 2.04
          Mean value_function loss: 135.1640
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 46.5208
                       Mean reward: 74.18
               Mean episode length: 68.90
    Episode_Reward/reaching_object: 0.3042
     Episode_Reward/lifting_object: 15.4476
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 60.5833
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 2.26s
                      Time elapsed: 00:24:20
                               ETA: 00:49:19

################################################################################
                     [1m Learning iteration 661/2000 [0m                      

                       Computation: 43080 steps/s (collection: 2.155s, learning 0.127s)
             Mean action noise std: 2.04
          Mean value_function loss: 136.2472
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 46.5223
                       Mean reward: 81.71
               Mean episode length: 71.62
    Episode_Reward/reaching_object: 0.2989
     Episode_Reward/lifting_object: 15.0841
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 58.6250
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 2.28s
                      Time elapsed: 00:24:22
                               ETA: 00:49:17

################################################################################
                     [1m Learning iteration 662/2000 [0m                      

                       Computation: 44395 steps/s (collection: 2.115s, learning 0.099s)
             Mean action noise std: 2.04
          Mean value_function loss: 135.3873
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 46.5232
                       Mean reward: 74.78
               Mean episode length: 66.21
    Episode_Reward/reaching_object: 0.2928
     Episode_Reward/lifting_object: 15.0518
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 61.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 2.21s
                      Time elapsed: 00:24:24
                               ETA: 00:49:15

################################################################################
                     [1m Learning iteration 663/2000 [0m                      

                       Computation: 45289 steps/s (collection: 2.075s, learning 0.095s)
             Mean action noise std: 2.04
          Mean value_function loss: 139.4118
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 46.5238
                       Mean reward: 78.95
               Mean episode length: 68.52
    Episode_Reward/reaching_object: 0.2921
     Episode_Reward/lifting_object: 15.0337
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 58.2917
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 2.17s
                      Time elapsed: 00:24:26
                               ETA: 00:49:13

################################################################################
                     [1m Learning iteration 664/2000 [0m                      

                       Computation: 45356 steps/s (collection: 2.071s, learning 0.097s)
             Mean action noise std: 2.04
          Mean value_function loss: 168.0039
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 46.5247
                       Mean reward: 76.99
               Mean episode length: 67.70
    Episode_Reward/reaching_object: 0.2918
     Episode_Reward/lifting_object: 14.9764
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 58.4167
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 2.17s
                      Time elapsed: 00:24:28
                               ETA: 00:49:11

################################################################################
                     [1m Learning iteration 665/2000 [0m                      

                       Computation: 44396 steps/s (collection: 2.086s, learning 0.129s)
             Mean action noise std: 2.04
          Mean value_function loss: 148.0704
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 46.5277
                       Mean reward: 66.75
               Mean episode length: 64.34
    Episode_Reward/reaching_object: 0.2950
     Episode_Reward/lifting_object: 15.1030
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 61.7917
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 2.21s
                      Time elapsed: 00:24:31
                               ETA: 00:49:08

################################################################################
                     [1m Learning iteration 666/2000 [0m                      

                       Computation: 27484 steps/s (collection: 3.476s, learning 0.101s)
             Mean action noise std: 2.04
          Mean value_function loss: 163.7480
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.5333
                       Mean reward: 73.86
               Mean episode length: 68.20
    Episode_Reward/reaching_object: 0.2903
     Episode_Reward/lifting_object: 14.9581
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 58.8750
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 3.58s
                      Time elapsed: 00:24:34
                               ETA: 00:49:09

################################################################################
                     [1m Learning iteration 667/2000 [0m                      

                       Computation: 13826 steps/s (collection: 6.941s, learning 0.169s)
             Mean action noise std: 2.04
          Mean value_function loss: 150.0657
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 46.5374
                       Mean reward: 77.57
               Mean episode length: 70.73
    Episode_Reward/reaching_object: 0.2855
     Episode_Reward/lifting_object: 14.8502
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 61.6250
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 7.11s
                      Time elapsed: 00:24:41
                               ETA: 00:49:16

################################################################################
                     [1m Learning iteration 668/2000 [0m                      

                       Computation: 13953 steps/s (collection: 6.924s, learning 0.121s)
             Mean action noise std: 2.04
          Mean value_function loss: 153.1608
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 46.5392
                       Mean reward: 80.60
               Mean episode length: 67.93
    Episode_Reward/reaching_object: 0.2868
     Episode_Reward/lifting_object: 15.0859
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 63.6250
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 7.05s
                      Time elapsed: 00:24:48
                               ETA: 00:49:24

################################################################################
                     [1m Learning iteration 669/2000 [0m                      

                       Computation: 14004 steps/s (collection: 6.899s, learning 0.120s)
             Mean action noise std: 2.04
          Mean value_function loss: 164.4159
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 46.5426
                       Mean reward: 66.48
               Mean episode length: 63.07
    Episode_Reward/reaching_object: 0.2862
     Episode_Reward/lifting_object: 15.1468
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 60.8333
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 7.02s
                      Time elapsed: 00:24:55
                               ETA: 00:49:31

################################################################################
                     [1m Learning iteration 670/2000 [0m                      

                       Computation: 13947 steps/s (collection: 6.911s, learning 0.138s)
             Mean action noise std: 2.04
          Mean value_function loss: 155.0835
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 46.5444
                       Mean reward: 77.28
               Mean episode length: 66.68
    Episode_Reward/reaching_object: 0.2800
     Episode_Reward/lifting_object: 14.6626
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 63.0417
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 7.05s
                      Time elapsed: 00:25:02
                               ETA: 00:49:38

################################################################################
                     [1m Learning iteration 671/2000 [0m                      

                       Computation: 14186 steps/s (collection: 6.794s, learning 0.136s)
             Mean action noise std: 2.04
          Mean value_function loss: 145.0993
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 46.5462
                       Mean reward: 77.47
               Mean episode length: 66.68
    Episode_Reward/reaching_object: 0.2781
     Episode_Reward/lifting_object: 14.8917
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 59.6667
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 6.93s
                      Time elapsed: 00:25:09
                               ETA: 00:49:45

################################################################################
                     [1m Learning iteration 672/2000 [0m                      

                       Computation: 13997 steps/s (collection: 6.904s, learning 0.119s)
             Mean action noise std: 2.05
          Mean value_function loss: 162.6669
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 46.5506
                       Mean reward: 76.53
               Mean episode length: 67.18
    Episode_Reward/reaching_object: 0.2853
     Episode_Reward/lifting_object: 15.3323
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 61.1250
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 7.02s
                      Time elapsed: 00:25:16
                               ETA: 00:49:53

################################################################################
                     [1m Learning iteration 673/2000 [0m                      

                       Computation: 14138 steps/s (collection: 6.824s, learning 0.130s)
             Mean action noise std: 2.05
          Mean value_function loss: 147.7000
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 46.5552
                       Mean reward: 76.28
               Mean episode length: 65.11
    Episode_Reward/reaching_object: 0.2776
     Episode_Reward/lifting_object: 14.9801
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 62.5000
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 6.95s
                      Time elapsed: 00:25:23
                               ETA: 00:50:00

################################################################################
                     [1m Learning iteration 674/2000 [0m                      

                       Computation: 13957 steps/s (collection: 6.908s, learning 0.135s)
             Mean action noise std: 2.05
          Mean value_function loss: 165.7247
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 46.5603
                       Mean reward: 72.05
               Mean episode length: 66.23
    Episode_Reward/reaching_object: 0.2905
     Episode_Reward/lifting_object: 15.5969
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 60.2083
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 7.04s
                      Time elapsed: 00:25:30
                               ETA: 00:50:07

################################################################################
                     [1m Learning iteration 675/2000 [0m                      

                       Computation: 21429 steps/s (collection: 4.492s, learning 0.096s)
             Mean action noise std: 2.05
          Mean value_function loss: 162.6575
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 46.5636
                       Mean reward: 79.75
               Mean episode length: 64.98
    Episode_Reward/reaching_object: 0.2813
     Episode_Reward/lifting_object: 15.6476
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 63.4167
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 4.59s
                      Time elapsed: 00:25:35
                               ETA: 00:50:09

################################################################################
                     [1m Learning iteration 676/2000 [0m                      

                       Computation: 47747 steps/s (collection: 1.971s, learning 0.088s)
             Mean action noise std: 2.05
          Mean value_function loss: 158.8051
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 46.5647
                       Mean reward: 83.39
               Mean episode length: 68.43
    Episode_Reward/reaching_object: 0.2881
     Episode_Reward/lifting_object: 15.9682
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 60.6667
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 2.06s
                      Time elapsed: 00:25:37
                               ETA: 00:50:06

################################################################################
                     [1m Learning iteration 677/2000 [0m                      

                       Computation: 45971 steps/s (collection: 2.030s, learning 0.109s)
             Mean action noise std: 2.05
          Mean value_function loss: 161.2150
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 46.5670
                       Mean reward: 76.22
               Mean episode length: 65.92
    Episode_Reward/reaching_object: 0.2829
     Episode_Reward/lifting_object: 15.7304
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 61.2500
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 2.14s
                      Time elapsed: 00:25:39
                               ETA: 00:50:04

################################################################################
                     [1m Learning iteration 678/2000 [0m                      

                       Computation: 43807 steps/s (collection: 2.086s, learning 0.158s)
             Mean action noise std: 2.05
          Mean value_function loss: 178.8502
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 46.5692
                       Mean reward: 78.26
               Mean episode length: 68.64
    Episode_Reward/reaching_object: 0.2826
     Episode_Reward/lifting_object: 15.5059
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 61.0833
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 2.24s
                      Time elapsed: 00:25:41
                               ETA: 00:50:02

################################################################################
                     [1m Learning iteration 679/2000 [0m                      

                       Computation: 43257 steps/s (collection: 2.183s, learning 0.089s)
             Mean action noise std: 2.05
          Mean value_function loss: 181.6237
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 46.5720
                       Mean reward: 80.99
               Mean episode length: 67.20
    Episode_Reward/reaching_object: 0.2816
     Episode_Reward/lifting_object: 15.6132
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 62.3333
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 2.27s
                      Time elapsed: 00:25:44
                               ETA: 00:49:59

################################################################################
                     [1m Learning iteration 680/2000 [0m                      

                       Computation: 45649 steps/s (collection: 2.065s, learning 0.088s)
             Mean action noise std: 2.05
          Mean value_function loss: 180.3667
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 46.5779
                       Mean reward: 88.76
               Mean episode length: 71.16
    Episode_Reward/reaching_object: 0.2899
     Episode_Reward/lifting_object: 16.0964
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 60.8333
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 2.15s
                      Time elapsed: 00:25:46
                               ETA: 00:49:57

################################################################################
                     [1m Learning iteration 681/2000 [0m                      

                       Computation: 43150 steps/s (collection: 2.112s, learning 0.166s)
             Mean action noise std: 2.05
          Mean value_function loss: 177.1463
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 46.5807
                       Mean reward: 72.37
               Mean episode length: 63.63
    Episode_Reward/reaching_object: 0.2796
     Episode_Reward/lifting_object: 15.5305
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 62.2083
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 2.28s
                      Time elapsed: 00:25:48
                               ETA: 00:49:54

################################################################################
                     [1m Learning iteration 682/2000 [0m                      

                       Computation: 47025 steps/s (collection: 1.994s, learning 0.096s)
             Mean action noise std: 2.05
          Mean value_function loss: 169.8003
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 46.5824
                       Mean reward: 84.52
               Mean episode length: 66.25
    Episode_Reward/reaching_object: 0.2844
     Episode_Reward/lifting_object: 15.8759
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 61.6250
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 2.09s
                      Time elapsed: 00:25:50
                               ETA: 00:49:52

################################################################################
                     [1m Learning iteration 683/2000 [0m                      

                       Computation: 48016 steps/s (collection: 1.941s, learning 0.107s)
             Mean action noise std: 2.05
          Mean value_function loss: 179.7477
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 46.5848
                       Mean reward: 82.50
               Mean episode length: 64.75
    Episode_Reward/reaching_object: 0.2814
     Episode_Reward/lifting_object: 16.1514
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 60.1250
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 2.05s
                      Time elapsed: 00:25:52
                               ETA: 00:49:49

################################################################################
                     [1m Learning iteration 684/2000 [0m                      

                       Computation: 48824 steps/s (collection: 1.925s, learning 0.089s)
             Mean action noise std: 2.05
          Mean value_function loss: 178.4068
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 46.5886
                       Mean reward: 85.70
               Mean episode length: 66.90
    Episode_Reward/reaching_object: 0.2814
     Episode_Reward/lifting_object: 16.2531
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 61.9167
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 2.01s
                      Time elapsed: 00:25:54
                               ETA: 00:49:46

################################################################################
                     [1m Learning iteration 685/2000 [0m                      

                       Computation: 44260 steps/s (collection: 2.110s, learning 0.112s)
             Mean action noise std: 2.05
          Mean value_function loss: 195.2777
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 46.5929
                       Mean reward: 76.31
               Mean episode length: 63.74
    Episode_Reward/reaching_object: 0.2843
     Episode_Reward/lifting_object: 16.2309
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 61.5833
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 2.22s
                      Time elapsed: 00:25:56
                               ETA: 00:49:44

################################################################################
                     [1m Learning iteration 686/2000 [0m                      

                       Computation: 46659 steps/s (collection: 1.981s, learning 0.126s)
             Mean action noise std: 2.05
          Mean value_function loss: 191.2979
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 46.5959
                       Mean reward: 82.65
               Mean episode length: 65.35
    Episode_Reward/reaching_object: 0.2808
     Episode_Reward/lifting_object: 16.3320
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 62.4583
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 2.11s
                      Time elapsed: 00:25:59
                               ETA: 00:49:41

################################################################################
                     [1m Learning iteration 687/2000 [0m                      

                       Computation: 47233 steps/s (collection: 1.979s, learning 0.102s)
             Mean action noise std: 2.05
          Mean value_function loss: 172.7067
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 46.6011
                       Mean reward: 89.70
               Mean episode length: 67.12
    Episode_Reward/reaching_object: 0.2908
     Episode_Reward/lifting_object: 17.1004
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 61.7917
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 2.08s
                      Time elapsed: 00:26:01
                               ETA: 00:49:39

################################################################################
                     [1m Learning iteration 688/2000 [0m                      

                       Computation: 47901 steps/s (collection: 1.932s, learning 0.121s)
             Mean action noise std: 2.05
          Mean value_function loss: 192.2993
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 46.6031
                       Mean reward: 90.05
               Mean episode length: 66.87
    Episode_Reward/reaching_object: 0.2763
     Episode_Reward/lifting_object: 15.9270
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 62.6250
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 2.05s
                      Time elapsed: 00:26:03
                               ETA: 00:49:36

################################################################################
                     [1m Learning iteration 689/2000 [0m                      

                       Computation: 46919 steps/s (collection: 1.953s, learning 0.143s)
             Mean action noise std: 2.05
          Mean value_function loss: 184.4980
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 46.6053
                       Mean reward: 85.50
               Mean episode length: 65.24
    Episode_Reward/reaching_object: 0.2736
     Episode_Reward/lifting_object: 16.2276
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 61.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 2.10s
                      Time elapsed: 00:26:05
                               ETA: 00:49:34

################################################################################
                     [1m Learning iteration 690/2000 [0m                      

                       Computation: 47673 steps/s (collection: 1.966s, learning 0.096s)
             Mean action noise std: 2.05
          Mean value_function loss: 195.2497
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 46.6086
                       Mean reward: 80.11
               Mean episode length: 64.45
    Episode_Reward/reaching_object: 0.2820
     Episode_Reward/lifting_object: 16.8794
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 61.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 2.06s
                      Time elapsed: 00:26:07
                               ETA: 00:49:31

################################################################################
                     [1m Learning iteration 691/2000 [0m                      

                       Computation: 47360 steps/s (collection: 1.955s, learning 0.121s)
             Mean action noise std: 2.05
          Mean value_function loss: 212.8900
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 46.6131
                       Mean reward: 84.95
               Mean episode length: 68.54
    Episode_Reward/reaching_object: 0.2900
     Episode_Reward/lifting_object: 17.1526
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 62.6250
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 2.08s
                      Time elapsed: 00:26:09
                               ETA: 00:49:28

################################################################################
                     [1m Learning iteration 692/2000 [0m                      

                       Computation: 47064 steps/s (collection: 1.983s, learning 0.106s)
             Mean action noise std: 2.05
          Mean value_function loss: 196.0345
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 46.6186
                       Mean reward: 87.11
               Mean episode length: 64.86
    Episode_Reward/reaching_object: 0.2824
     Episode_Reward/lifting_object: 17.0237
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 61.7500
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 2.09s
                      Time elapsed: 00:26:11
                               ETA: 00:49:26

################################################################################
                     [1m Learning iteration 693/2000 [0m                      

                       Computation: 47430 steps/s (collection: 1.979s, learning 0.094s)
             Mean action noise std: 2.05
          Mean value_function loss: 198.5016
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 46.6221
                       Mean reward: 83.52
               Mean episode length: 64.64
    Episode_Reward/reaching_object: 0.2785
     Episode_Reward/lifting_object: 16.9229
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 62.5417
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 2.07s
                      Time elapsed: 00:26:13
                               ETA: 00:49:23

################################################################################
                     [1m Learning iteration 694/2000 [0m                      

                       Computation: 47890 steps/s (collection: 1.967s, learning 0.086s)
             Mean action noise std: 2.05
          Mean value_function loss: 210.4061
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 46.6253
                       Mean reward: 97.16
               Mean episode length: 67.74
    Episode_Reward/reaching_object: 0.2786
     Episode_Reward/lifting_object: 17.0912
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 62.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 2.05s
                      Time elapsed: 00:26:15
                               ETA: 00:49:20

################################################################################
                     [1m Learning iteration 695/2000 [0m                      

                       Computation: 48517 steps/s (collection: 1.942s, learning 0.085s)
             Mean action noise std: 2.05
          Mean value_function loss: 199.2947
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 46.6306
                       Mean reward: 82.47
               Mean episode length: 63.37
    Episode_Reward/reaching_object: 0.2799
     Episode_Reward/lifting_object: 17.1354
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 63.3750
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 2.03s
                      Time elapsed: 00:26:17
                               ETA: 00:49:18

################################################################################
                     [1m Learning iteration 696/2000 [0m                      

                       Computation: 44409 steps/s (collection: 2.096s, learning 0.118s)
             Mean action noise std: 2.06
          Mean value_function loss: 201.7015
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 46.6372
                       Mean reward: 89.57
               Mean episode length: 68.48
    Episode_Reward/reaching_object: 0.2798
     Episode_Reward/lifting_object: 17.2688
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 61.7500
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 2.21s
                      Time elapsed: 00:26:19
                               ETA: 00:49:15

################################################################################
                     [1m Learning iteration 697/2000 [0m                      

                       Computation: 46944 steps/s (collection: 1.976s, learning 0.118s)
             Mean action noise std: 2.06
          Mean value_function loss: 212.7562
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 46.6442
                       Mean reward: 73.48
               Mean episode length: 64.15
    Episode_Reward/reaching_object: 0.2729
     Episode_Reward/lifting_object: 16.4163
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 62.0417
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 2.09s
                      Time elapsed: 00:26:21
                               ETA: 00:49:13

################################################################################
                     [1m Learning iteration 698/2000 [0m                      

                       Computation: 47406 steps/s (collection: 1.978s, learning 0.096s)
             Mean action noise std: 2.06
          Mean value_function loss: 207.5408
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 46.6498
                       Mean reward: 88.46
               Mean episode length: 63.00
    Episode_Reward/reaching_object: 0.2792
     Episode_Reward/lifting_object: 17.1480
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 64.0417
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 2.07s
                      Time elapsed: 00:26:24
                               ETA: 00:49:10

################################################################################
                     [1m Learning iteration 699/2000 [0m                      

                       Computation: 47568 steps/s (collection: 1.983s, learning 0.084s)
             Mean action noise std: 2.06
          Mean value_function loss: 234.1076
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 46.6529
                       Mean reward: 97.95
               Mean episode length: 67.40
    Episode_Reward/reaching_object: 0.2868
     Episode_Reward/lifting_object: 18.0729
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 64.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 2.07s
                      Time elapsed: 00:26:26
                               ETA: 00:49:07

################################################################################
                     [1m Learning iteration 700/2000 [0m                      

                       Computation: 47024 steps/s (collection: 1.985s, learning 0.105s)
             Mean action noise std: 2.06
          Mean value_function loss: 225.8809
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 46.6581
                       Mean reward: 93.47
               Mean episode length: 65.53
    Episode_Reward/reaching_object: 0.2774
     Episode_Reward/lifting_object: 17.4760
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 63.7083
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 2.09s
                      Time elapsed: 00:26:28
                               ETA: 00:49:05

################################################################################
                     [1m Learning iteration 701/2000 [0m                      

                       Computation: 44287 steps/s (collection: 2.021s, learning 0.199s)
             Mean action noise std: 2.06
          Mean value_function loss: 226.1181
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 46.6649
                       Mean reward: 75.69
               Mean episode length: 61.29
    Episode_Reward/reaching_object: 0.2746
     Episode_Reward/lifting_object: 17.1470
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 64.1250
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 2.22s
                      Time elapsed: 00:26:30
                               ETA: 00:49:02

################################################################################
                     [1m Learning iteration 702/2000 [0m                      

                       Computation: 44759 steps/s (collection: 2.072s, learning 0.125s)
             Mean action noise std: 2.06
          Mean value_function loss: 240.9380
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 46.6710
                       Mean reward: 89.75
               Mean episode length: 63.54
    Episode_Reward/reaching_object: 0.2745
     Episode_Reward/lifting_object: 17.6203
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 62.2500
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 2.20s
                      Time elapsed: 00:26:32
                               ETA: 00:49:00

################################################################################
                     [1m Learning iteration 703/2000 [0m                      

                       Computation: 47428 steps/s (collection: 1.962s, learning 0.111s)
             Mean action noise std: 2.06
          Mean value_function loss: 223.9622
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 46.6757
                       Mean reward: 84.26
               Mean episode length: 60.10
    Episode_Reward/reaching_object: 0.2732
     Episode_Reward/lifting_object: 17.4808
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 63.9583
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 2.07s
                      Time elapsed: 00:26:34
                               ETA: 00:48:57

################################################################################
                     [1m Learning iteration 704/2000 [0m                      

                       Computation: 47583 steps/s (collection: 1.975s, learning 0.091s)
             Mean action noise std: 2.06
          Mean value_function loss: 228.0398
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 46.6795
                       Mean reward: 89.89
               Mean episode length: 64.02
    Episode_Reward/reaching_object: 0.2786
     Episode_Reward/lifting_object: 18.3960
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 64.1667
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 2.07s
                      Time elapsed: 00:26:36
                               ETA: 00:48:55

################################################################################
                     [1m Learning iteration 705/2000 [0m                      

                       Computation: 47409 steps/s (collection: 1.971s, learning 0.102s)
             Mean action noise std: 2.06
          Mean value_function loss: 228.1412
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 46.6815
                       Mean reward: 87.34
               Mean episode length: 63.78
    Episode_Reward/reaching_object: 0.2746
     Episode_Reward/lifting_object: 17.6436
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 64.9583
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 2.07s
                      Time elapsed: 00:26:38
                               ETA: 00:48:52

################################################################################
                     [1m Learning iteration 706/2000 [0m                      

                       Computation: 46859 steps/s (collection: 2.001s, learning 0.097s)
             Mean action noise std: 2.06
          Mean value_function loss: 216.2107
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 46.6822
                       Mean reward: 84.07
               Mean episode length: 60.34
    Episode_Reward/reaching_object: 0.2734
     Episode_Reward/lifting_object: 17.9822
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 60.6667
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 2.10s
                      Time elapsed: 00:26:40
                               ETA: 00:48:50

################################################################################
                     [1m Learning iteration 707/2000 [0m                      

                       Computation: 47568 steps/s (collection: 1.981s, learning 0.086s)
             Mean action noise std: 2.06
          Mean value_function loss: 222.0825
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 46.6835
                       Mean reward: 100.11
               Mean episode length: 65.27
    Episode_Reward/reaching_object: 0.2765
     Episode_Reward/lifting_object: 18.3832
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 62.7917
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 2.07s
                      Time elapsed: 00:26:43
                               ETA: 00:48:47

################################################################################
                     [1m Learning iteration 708/2000 [0m                      

                       Computation: 46022 steps/s (collection: 2.016s, learning 0.120s)
             Mean action noise std: 2.06
          Mean value_function loss: 229.3851
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 46.6871
                       Mean reward: 95.48
               Mean episode length: 65.26
    Episode_Reward/reaching_object: 0.2829
     Episode_Reward/lifting_object: 19.0035
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 62.0833
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 2.14s
                      Time elapsed: 00:26:45
                               ETA: 00:48:45

################################################################################
                     [1m Learning iteration 709/2000 [0m                      

                       Computation: 47897 steps/s (collection: 1.955s, learning 0.098s)
             Mean action noise std: 2.06
          Mean value_function loss: 231.3037
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 46.6918
                       Mean reward: 97.11
               Mean episode length: 65.99
    Episode_Reward/reaching_object: 0.2825
     Episode_Reward/lifting_object: 18.2334
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 61.9167
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 2.05s
                      Time elapsed: 00:26:47
                               ETA: 00:48:42

################################################################################
                     [1m Learning iteration 710/2000 [0m                      

                       Computation: 46968 steps/s (collection: 1.992s, learning 0.101s)
             Mean action noise std: 2.06
          Mean value_function loss: 223.5203
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 46.6964
                       Mean reward: 92.72
               Mean episode length: 66.55
    Episode_Reward/reaching_object: 0.2834
     Episode_Reward/lifting_object: 19.2051
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 61.3750
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 2.09s
                      Time elapsed: 00:26:49
                               ETA: 00:48:39

################################################################################
                     [1m Learning iteration 711/2000 [0m                      

                       Computation: 46330 steps/s (collection: 1.973s, learning 0.149s)
             Mean action noise std: 2.06
          Mean value_function loss: 223.3709
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 46.6996
                       Mean reward: 100.38
               Mean episode length: 65.73
    Episode_Reward/reaching_object: 0.2773
     Episode_Reward/lifting_object: 18.9984
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 63.5000
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 2.12s
                      Time elapsed: 00:26:51
                               ETA: 00:48:37

################################################################################
                     [1m Learning iteration 712/2000 [0m                      

                       Computation: 47746 steps/s (collection: 1.972s, learning 0.087s)
             Mean action noise std: 2.06
          Mean value_function loss: 226.0266
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 46.7026
                       Mean reward: 96.22
               Mean episode length: 66.87
    Episode_Reward/reaching_object: 0.2780
     Episode_Reward/lifting_object: 18.8703
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 66.0417
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 2.06s
                      Time elapsed: 00:26:53
                               ETA: 00:48:34

################################################################################
                     [1m Learning iteration 713/2000 [0m                      

                       Computation: 47336 steps/s (collection: 1.965s, learning 0.112s)
             Mean action noise std: 2.06
          Mean value_function loss: 245.2527
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 46.7059
                       Mean reward: 95.11
               Mean episode length: 65.99
    Episode_Reward/reaching_object: 0.2713
     Episode_Reward/lifting_object: 18.0874
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 62.4583
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 2.08s
                      Time elapsed: 00:26:55
                               ETA: 00:48:32

################################################################################
                     [1m Learning iteration 714/2000 [0m                      

                       Computation: 47851 steps/s (collection: 1.954s, learning 0.100s)
             Mean action noise std: 2.06
          Mean value_function loss: 245.3366
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 46.7094
                       Mean reward: 101.68
               Mean episode length: 65.93
    Episode_Reward/reaching_object: 0.2792
     Episode_Reward/lifting_object: 18.8541
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 61.7500
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 2.05s
                      Time elapsed: 00:26:57
                               ETA: 00:48:29

################################################################################
                     [1m Learning iteration 715/2000 [0m                      

                       Computation: 47938 steps/s (collection: 1.954s, learning 0.097s)
             Mean action noise std: 2.06
          Mean value_function loss: 255.8083
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 46.7128
                       Mean reward: 98.94
               Mean episode length: 63.76
    Episode_Reward/reaching_object: 0.2807
     Episode_Reward/lifting_object: 19.4825
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 64.3750
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 2.05s
                      Time elapsed: 00:26:59
                               ETA: 00:48:26

################################################################################
                     [1m Learning iteration 716/2000 [0m                      

                       Computation: 47123 steps/s (collection: 1.970s, learning 0.116s)
             Mean action noise std: 2.06
          Mean value_function loss: 273.6525
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 46.7150
                       Mean reward: 90.57
               Mean episode length: 62.68
    Episode_Reward/reaching_object: 0.2763
     Episode_Reward/lifting_object: 19.3264
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 63.7083
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 2.09s
                      Time elapsed: 00:27:01
                               ETA: 00:48:24

################################################################################
                     [1m Learning iteration 717/2000 [0m                      

                       Computation: 47041 steps/s (collection: 1.990s, learning 0.100s)
             Mean action noise std: 2.06
          Mean value_function loss: 264.2954
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 46.7184
                       Mean reward: 92.57
               Mean episode length: 61.20
    Episode_Reward/reaching_object: 0.2692
     Episode_Reward/lifting_object: 18.9639
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 63.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 2.09s
                      Time elapsed: 00:27:03
                               ETA: 00:48:21

################################################################################
                     [1m Learning iteration 718/2000 [0m                      

                       Computation: 43853 steps/s (collection: 2.138s, learning 0.104s)
             Mean action noise std: 2.07
          Mean value_function loss: 272.5324
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 46.7226
                       Mean reward: 100.75
               Mean episode length: 62.38
    Episode_Reward/reaching_object: 0.2801
     Episode_Reward/lifting_object: 19.9804
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 62.8750
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 2.24s
                      Time elapsed: 00:27:06
                               ETA: 00:48:19

################################################################################
                     [1m Learning iteration 719/2000 [0m                      

                       Computation: 47497 steps/s (collection: 1.965s, learning 0.105s)
             Mean action noise std: 2.07
          Mean value_function loss: 288.0831
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 46.7256
                       Mean reward: 101.98
               Mean episode length: 66.06
    Episode_Reward/reaching_object: 0.2804
     Episode_Reward/lifting_object: 20.1310
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 60.7500
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 2.07s
                      Time elapsed: 00:27:08
                               ETA: 00:48:16

################################################################################
                     [1m Learning iteration 720/2000 [0m                      

                       Computation: 44127 steps/s (collection: 2.066s, learning 0.162s)
             Mean action noise std: 2.07
          Mean value_function loss: 278.2313
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 46.7286
                       Mean reward: 106.52
               Mean episode length: 67.64
    Episode_Reward/reaching_object: 0.2842
     Episode_Reward/lifting_object: 20.4523
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 63.2917
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 2.23s
                      Time elapsed: 00:27:10
                               ETA: 00:48:14

################################################################################
                     [1m Learning iteration 721/2000 [0m                      

                       Computation: 47043 steps/s (collection: 1.977s, learning 0.113s)
             Mean action noise std: 2.07
          Mean value_function loss: 271.8219
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.7318
                       Mean reward: 94.24
               Mean episode length: 63.39
    Episode_Reward/reaching_object: 0.2803
     Episode_Reward/lifting_object: 20.1338
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 60.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 2.09s
                      Time elapsed: 00:27:12
                               ETA: 00:48:11

################################################################################
                     [1m Learning iteration 722/2000 [0m                      

                       Computation: 46547 steps/s (collection: 1.957s, learning 0.155s)
             Mean action noise std: 2.07
          Mean value_function loss: 264.9147
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 46.7365
                       Mean reward: 91.94
               Mean episode length: 63.29
    Episode_Reward/reaching_object: 0.2838
     Episode_Reward/lifting_object: 20.2083
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 62.5417
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 2.11s
                      Time elapsed: 00:27:14
                               ETA: 00:48:09

################################################################################
                     [1m Learning iteration 723/2000 [0m                      

                       Computation: 42372 steps/s (collection: 2.141s, learning 0.179s)
             Mean action noise std: 2.07
          Mean value_function loss: 274.2149
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 46.7402
                       Mean reward: 93.76
               Mean episode length: 63.66
    Episode_Reward/reaching_object: 0.2836
     Episode_Reward/lifting_object: 19.7016
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 59.7500
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 2.32s
                      Time elapsed: 00:27:16
                               ETA: 00:48:07

################################################################################
                     [1m Learning iteration 724/2000 [0m                      

                       Computation: 46064 steps/s (collection: 2.017s, learning 0.117s)
             Mean action noise std: 2.07
          Mean value_function loss: 278.1224
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 46.7439
                       Mean reward: 93.39
               Mean episode length: 67.64
    Episode_Reward/reaching_object: 0.2820
     Episode_Reward/lifting_object: 19.7309
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 60.4167
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 2.13s
                      Time elapsed: 00:27:19
                               ETA: 00:48:04

################################################################################
                     [1m Learning iteration 725/2000 [0m                      

                       Computation: 44083 steps/s (collection: 2.126s, learning 0.104s)
             Mean action noise std: 2.07
          Mean value_function loss: 273.3303
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 46.7474
                       Mean reward: 103.64
               Mean episode length: 65.50
    Episode_Reward/reaching_object: 0.2876
     Episode_Reward/lifting_object: 21.2060
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 60.6667
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 2.23s
                      Time elapsed: 00:27:21
                               ETA: 00:48:02

################################################################################
                     [1m Learning iteration 726/2000 [0m                      

                       Computation: 45580 steps/s (collection: 2.040s, learning 0.117s)
             Mean action noise std: 2.07
          Mean value_function loss: 277.7106
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 46.7492
                       Mean reward: 113.06
               Mean episode length: 68.16
    Episode_Reward/reaching_object: 0.2859
     Episode_Reward/lifting_object: 21.3140
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 60.9583
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 2.16s
                      Time elapsed: 00:27:23
                               ETA: 00:47:59

################################################################################
                     [1m Learning iteration 727/2000 [0m                      

                       Computation: 46235 steps/s (collection: 2.024s, learning 0.102s)
             Mean action noise std: 2.07
          Mean value_function loss: 273.4445
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 46.7517
                       Mean reward: 116.37
               Mean episode length: 68.00
    Episode_Reward/reaching_object: 0.2842
     Episode_Reward/lifting_object: 20.9778
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 63.2917
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 2.13s
                      Time elapsed: 00:27:25
                               ETA: 00:47:57

################################################################################
                     [1m Learning iteration 728/2000 [0m                      

                       Computation: 47060 steps/s (collection: 1.987s, learning 0.102s)
             Mean action noise std: 2.07
          Mean value_function loss: 278.4385
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 46.7564
                       Mean reward: 106.08
               Mean episode length: 63.44
    Episode_Reward/reaching_object: 0.2906
     Episode_Reward/lifting_object: 21.6215
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 58.0417
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 2.09s
                      Time elapsed: 00:27:27
                               ETA: 00:47:54

################################################################################
                     [1m Learning iteration 729/2000 [0m                      

                       Computation: 47023 steps/s (collection: 1.986s, learning 0.105s)
             Mean action noise std: 2.07
          Mean value_function loss: 281.5213
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 46.7600
                       Mean reward: 126.70
               Mean episode length: 70.26
    Episode_Reward/reaching_object: 0.2786
     Episode_Reward/lifting_object: 20.7209
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 61.1250
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 2.09s
                      Time elapsed: 00:27:29
                               ETA: 00:47:52

################################################################################
                     [1m Learning iteration 730/2000 [0m                      

                       Computation: 46896 steps/s (collection: 1.991s, learning 0.105s)
             Mean action noise std: 2.07
          Mean value_function loss: 303.6423
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 46.7639
                       Mean reward: 104.06
               Mean episode length: 64.69
    Episode_Reward/reaching_object: 0.2870
     Episode_Reward/lifting_object: 21.8528
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 62.9167
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 2.10s
                      Time elapsed: 00:27:31
                               ETA: 00:47:49

################################################################################
                     [1m Learning iteration 731/2000 [0m                      

                       Computation: 45199 steps/s (collection: 2.056s, learning 0.119s)
             Mean action noise std: 2.07
          Mean value_function loss: 281.0915
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 46.7691
                       Mean reward: 79.20
               Mean episode length: 69.63
    Episode_Reward/reaching_object: 0.2800
     Episode_Reward/lifting_object: 21.0961
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 62.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 2.17s
                      Time elapsed: 00:27:33
                               ETA: 00:47:47

################################################################################
                     [1m Learning iteration 732/2000 [0m                      

                       Computation: 45880 steps/s (collection: 2.024s, learning 0.119s)
             Mean action noise std: 2.07
          Mean value_function loss: 292.2443
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 46.7727
                       Mean reward: 108.20
               Mean episode length: 67.29
    Episode_Reward/reaching_object: 0.2789
     Episode_Reward/lifting_object: 21.0645
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 63.6250
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 2.14s
                      Time elapsed: 00:27:36
                               ETA: 00:47:44

################################################################################
                     [1m Learning iteration 733/2000 [0m                      

                       Computation: 46761 steps/s (collection: 1.986s, learning 0.117s)
             Mean action noise std: 2.07
          Mean value_function loss: 309.4303
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 46.7760
                       Mean reward: 121.37
               Mean episode length: 68.86
    Episode_Reward/reaching_object: 0.2775
     Episode_Reward/lifting_object: 21.6236
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 59.3333
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 2.10s
                      Time elapsed: 00:27:38
                               ETA: 00:47:42

################################################################################
                     [1m Learning iteration 734/2000 [0m                      

                       Computation: 44910 steps/s (collection: 2.048s, learning 0.141s)
             Mean action noise std: 2.07
          Mean value_function loss: 317.5164
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 46.7794
                       Mean reward: 115.62
               Mean episode length: 65.71
    Episode_Reward/reaching_object: 0.2833
     Episode_Reward/lifting_object: 21.9354
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 61.0417
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 2.19s
                      Time elapsed: 00:27:40
                               ETA: 00:47:39

################################################################################
                     [1m Learning iteration 735/2000 [0m                      

                       Computation: 46072 steps/s (collection: 2.022s, learning 0.112s)
             Mean action noise std: 2.07
          Mean value_function loss: 346.9155
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 46.7842
                       Mean reward: 109.98
               Mean episode length: 69.87
    Episode_Reward/reaching_object: 0.2914
     Episode_Reward/lifting_object: 22.1055
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 60.5000
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 2.13s
                      Time elapsed: 00:27:42
                               ETA: 00:47:37

################################################################################
                     [1m Learning iteration 736/2000 [0m                      

                       Computation: 46863 steps/s (collection: 2.001s, learning 0.097s)
             Mean action noise std: 2.07
          Mean value_function loss: 335.0172
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 46.7877
                       Mean reward: 107.60
               Mean episode length: 65.56
    Episode_Reward/reaching_object: 0.2882
     Episode_Reward/lifting_object: 22.6970
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 61.6667
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 2.10s
                      Time elapsed: 00:27:44
                               ETA: 00:47:34

################################################################################
                     [1m Learning iteration 737/2000 [0m                      

                       Computation: 46781 steps/s (collection: 1.983s, learning 0.118s)
             Mean action noise std: 2.07
          Mean value_function loss: 318.4825
               Mean surrogate loss: 0.0109
                 Mean entropy loss: 46.7889
                       Mean reward: 119.50
               Mean episode length: 65.26
    Episode_Reward/reaching_object: 0.2841
     Episode_Reward/lifting_object: 21.8959
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 60.9583
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 2.10s
                      Time elapsed: 00:27:46
                               ETA: 00:47:32

################################################################################
                     [1m Learning iteration 738/2000 [0m                      

                       Computation: 45819 steps/s (collection: 2.033s, learning 0.113s)
             Mean action noise std: 2.07
          Mean value_function loss: 321.7666
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 46.7896
                       Mean reward: 99.30
               Mean episode length: 61.20
    Episode_Reward/reaching_object: 0.2873
     Episode_Reward/lifting_object: 22.6786
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 62.2083
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 2.15s
                      Time elapsed: 00:27:48
                               ETA: 00:47:29

################################################################################
                     [1m Learning iteration 739/2000 [0m                      

                       Computation: 44925 steps/s (collection: 2.081s, learning 0.108s)
             Mean action noise std: 2.07
          Mean value_function loss: 315.1005
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 46.7902
                       Mean reward: 115.94
               Mean episode length: 65.36
    Episode_Reward/reaching_object: 0.2875
     Episode_Reward/lifting_object: 23.3846
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 56.5833
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 2.19s
                      Time elapsed: 00:27:51
                               ETA: 00:47:27

################################################################################
                     [1m Learning iteration 740/2000 [0m                      

                       Computation: 47247 steps/s (collection: 1.984s, learning 0.097s)
             Mean action noise std: 2.07
          Mean value_function loss: 299.3593
               Mean surrogate loss: 0.0098
                 Mean entropy loss: 46.7915
                       Mean reward: 108.19
               Mean episode length: 71.25
    Episode_Reward/reaching_object: 0.2877
     Episode_Reward/lifting_object: 22.6722
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 61.7083
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 2.08s
                      Time elapsed: 00:27:53
                               ETA: 00:47:25

################################################################################
                     [1m Learning iteration 741/2000 [0m                      

                       Computation: 46686 steps/s (collection: 2.013s, learning 0.093s)
             Mean action noise std: 2.07
          Mean value_function loss: 322.6641
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 46.7922
                       Mean reward: 124.55
               Mean episode length: 69.93
    Episode_Reward/reaching_object: 0.2986
     Episode_Reward/lifting_object: 24.3788
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 63.4167
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 2.11s
                      Time elapsed: 00:27:55
                               ETA: 00:47:22

################################################################################
                     [1m Learning iteration 742/2000 [0m                      

                       Computation: 46189 steps/s (collection: 2.021s, learning 0.108s)
             Mean action noise std: 2.07
          Mean value_function loss: 320.0475
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 46.7932
                       Mean reward: 111.09
               Mean episode length: 61.35
    Episode_Reward/reaching_object: 0.2835
     Episode_Reward/lifting_object: 21.8763
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 62.6250
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 2.13s
                      Time elapsed: 00:27:57
                               ETA: 00:47:20

################################################################################
                     [1m Learning iteration 743/2000 [0m                      

                       Computation: 47385 steps/s (collection: 1.988s, learning 0.087s)
             Mean action noise std: 2.07
          Mean value_function loss: 329.1735
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 46.7939
                       Mean reward: 123.68
               Mean episode length: 66.20
    Episode_Reward/reaching_object: 0.2857
     Episode_Reward/lifting_object: 23.1292
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 59.3750
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 2.07s
                      Time elapsed: 00:27:59
                               ETA: 00:47:17

################################################################################
                     [1m Learning iteration 744/2000 [0m                      

                       Computation: 46518 steps/s (collection: 2.000s, learning 0.114s)
             Mean action noise std: 2.07
          Mean value_function loss: 313.2018
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 46.7944
                       Mean reward: 126.57
               Mean episode length: 67.77
    Episode_Reward/reaching_object: 0.2888
     Episode_Reward/lifting_object: 24.0174
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 58.5000
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 2.11s
                      Time elapsed: 00:28:01
                               ETA: 00:47:14

################################################################################
                     [1m Learning iteration 745/2000 [0m                      

                       Computation: 45760 steps/s (collection: 2.002s, learning 0.147s)
             Mean action noise std: 2.07
          Mean value_function loss: 314.4301
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 46.7954
                       Mean reward: 118.99
               Mean episode length: 67.20
    Episode_Reward/reaching_object: 0.2863
     Episode_Reward/lifting_object: 23.0027
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 62.6250
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 2.15s
                      Time elapsed: 00:28:03
                               ETA: 00:47:12

################################################################################
                     [1m Learning iteration 746/2000 [0m                      

                       Computation: 46631 steps/s (collection: 2.001s, learning 0.108s)
             Mean action noise std: 2.08
          Mean value_function loss: 354.5634
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 46.7983
                       Mean reward: 120.29
               Mean episode length: 68.41
    Episode_Reward/reaching_object: 0.2898
     Episode_Reward/lifting_object: 23.4186
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 64.1250
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 2.11s
                      Time elapsed: 00:28:05
                               ETA: 00:47:10

################################################################################
                     [1m Learning iteration 747/2000 [0m                      

                       Computation: 48338 steps/s (collection: 1.941s, learning 0.093s)
             Mean action noise std: 2.08
          Mean value_function loss: 331.7879
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 46.8043
                       Mean reward: 105.63
               Mean episode length: 64.01
    Episode_Reward/reaching_object: 0.2837
     Episode_Reward/lifting_object: 22.3075
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 59.7083
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 2.03s
                      Time elapsed: 00:28:07
                               ETA: 00:47:07

################################################################################
                     [1m Learning iteration 748/2000 [0m                      

                       Computation: 47407 steps/s (collection: 1.969s, learning 0.105s)
             Mean action noise std: 2.08
          Mean value_function loss: 333.4978
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 46.8047
                       Mean reward: 108.29
               Mean episode length: 65.14
    Episode_Reward/reaching_object: 0.2870
     Episode_Reward/lifting_object: 23.6451
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 62.0833
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 2.07s
                      Time elapsed: 00:28:09
                               ETA: 00:47:04

################################################################################
                     [1m Learning iteration 749/2000 [0m                      

                       Computation: 46154 steps/s (collection: 2.035s, learning 0.095s)
             Mean action noise std: 2.08
          Mean value_function loss: 355.6411
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 46.8040
                       Mean reward: 108.82
               Mean episode length: 62.26
    Episode_Reward/reaching_object: 0.2831
     Episode_Reward/lifting_object: 23.0012
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 60.6250
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 2.13s
                      Time elapsed: 00:28:12
                               ETA: 00:47:02

################################################################################
                     [1m Learning iteration 750/2000 [0m                      

                       Computation: 47626 steps/s (collection: 1.970s, learning 0.094s)
             Mean action noise std: 2.08
          Mean value_function loss: 340.7142
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 46.8096
                       Mean reward: 106.38
               Mean episode length: 66.12
    Episode_Reward/reaching_object: 0.2887
     Episode_Reward/lifting_object: 23.9741
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 61.2917
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 2.06s
                      Time elapsed: 00:28:14
                               ETA: 00:46:59

################################################################################
                     [1m Learning iteration 751/2000 [0m                      

                       Computation: 43713 steps/s (collection: 2.149s, learning 0.100s)
             Mean action noise std: 2.08
          Mean value_function loss: 371.3848
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 46.8157
                       Mean reward: 116.52
               Mean episode length: 63.77
    Episode_Reward/reaching_object: 0.2899
     Episode_Reward/lifting_object: 24.4785
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 62.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 2.25s
                      Time elapsed: 00:28:16
                               ETA: 00:46:57

################################################################################
                     [1m Learning iteration 752/2000 [0m                      

                       Computation: 47212 steps/s (collection: 1.987s, learning 0.095s)
             Mean action noise std: 2.08
          Mean value_function loss: 349.5914
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 46.8196
                       Mean reward: 144.23
               Mean episode length: 72.22
    Episode_Reward/reaching_object: 0.2883
     Episode_Reward/lifting_object: 24.9186
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 56.8750
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 2.08s
                      Time elapsed: 00:28:18
                               ETA: 00:46:54

################################################################################
                     [1m Learning iteration 753/2000 [0m                      

                       Computation: 46752 steps/s (collection: 2.007s, learning 0.096s)
             Mean action noise std: 2.08
          Mean value_function loss: 399.4915
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 46.8271
                       Mean reward: 129.68
               Mean episode length: 68.99
    Episode_Reward/reaching_object: 0.2949
     Episode_Reward/lifting_object: 25.0586
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 60.4167
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 2.10s
                      Time elapsed: 00:28:20
                               ETA: 00:46:52

################################################################################
                     [1m Learning iteration 754/2000 [0m                      

                       Computation: 46705 steps/s (collection: 1.996s, learning 0.109s)
             Mean action noise std: 2.08
          Mean value_function loss: 370.5650
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 46.8318
                       Mean reward: 147.22
               Mean episode length: 75.63
    Episode_Reward/reaching_object: 0.3005
     Episode_Reward/lifting_object: 25.7566
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 58.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 2.10s
                      Time elapsed: 00:28:22
                               ETA: 00:46:49

################################################################################
                     [1m Learning iteration 755/2000 [0m                      

                       Computation: 46668 steps/s (collection: 1.988s, learning 0.118s)
             Mean action noise std: 2.08
          Mean value_function loss: 382.5439
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 46.8322
                       Mean reward: 144.56
               Mean episode length: 72.43
    Episode_Reward/reaching_object: 0.3015
     Episode_Reward/lifting_object: 26.6627
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 58.2500
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 2.11s
                      Time elapsed: 00:28:24
                               ETA: 00:46:47

################################################################################
                     [1m Learning iteration 756/2000 [0m                      

                       Computation: 45035 steps/s (collection: 2.027s, learning 0.156s)
             Mean action noise std: 2.08
          Mean value_function loss: 393.8778
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 46.8330
                       Mean reward: 147.02
               Mean episode length: 71.81
    Episode_Reward/reaching_object: 0.3053
     Episode_Reward/lifting_object: 27.0733
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 58.6667
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 2.18s
                      Time elapsed: 00:28:26
                               ETA: 00:46:45

################################################################################
                     [1m Learning iteration 757/2000 [0m                      

                       Computation: 47592 steps/s (collection: 1.979s, learning 0.087s)
             Mean action noise std: 2.08
          Mean value_function loss: 394.5242
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 46.8350
                       Mean reward: 135.18
               Mean episode length: 70.07
    Episode_Reward/reaching_object: 0.3004
     Episode_Reward/lifting_object: 27.0298
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 59.0833
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 2.07s
                      Time elapsed: 00:28:29
                               ETA: 00:46:42

################################################################################
                     [1m Learning iteration 758/2000 [0m                      

                       Computation: 47398 steps/s (collection: 1.977s, learning 0.097s)
             Mean action noise std: 2.08
          Mean value_function loss: 415.6870
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 46.8384
                       Mean reward: 133.79
               Mean episode length: 67.69
    Episode_Reward/reaching_object: 0.3014
     Episode_Reward/lifting_object: 27.2018
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 58.7083
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 2.07s
                      Time elapsed: 00:28:31
                               ETA: 00:46:39

################################################################################
                     [1m Learning iteration 759/2000 [0m                      

                       Computation: 45485 steps/s (collection: 2.071s, learning 0.091s)
             Mean action noise std: 2.08
          Mean value_function loss: 403.0163
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 46.8421
                       Mean reward: 118.33
               Mean episode length: 64.76
    Episode_Reward/reaching_object: 0.3066
     Episode_Reward/lifting_object: 27.9711
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 58.0833
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 2.16s
                      Time elapsed: 00:28:33
                               ETA: 00:46:37

################################################################################
                     [1m Learning iteration 760/2000 [0m                      

                       Computation: 47526 steps/s (collection: 1.958s, learning 0.110s)
             Mean action noise std: 2.08
          Mean value_function loss: 403.5314
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 46.8441
                       Mean reward: 140.11
               Mean episode length: 66.62
    Episode_Reward/reaching_object: 0.3004
     Episode_Reward/lifting_object: 27.6651
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 56.8750
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 2.07s
                      Time elapsed: 00:28:35
                               ETA: 00:46:35

################################################################################
                     [1m Learning iteration 761/2000 [0m                      

                       Computation: 46987 steps/s (collection: 2.003s, learning 0.089s)
             Mean action noise std: 2.08
          Mean value_function loss: 400.3853
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 46.8453
                       Mean reward: 142.87
               Mean episode length: 70.94
    Episode_Reward/reaching_object: 0.3093
     Episode_Reward/lifting_object: 28.3122
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 56.6667
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 2.09s
                      Time elapsed: 00:28:37
                               ETA: 00:46:32

################################################################################
                     [1m Learning iteration 762/2000 [0m                      

                       Computation: 46966 steps/s (collection: 1.995s, learning 0.098s)
             Mean action noise std: 2.08
          Mean value_function loss: 450.3028
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 46.8467
                       Mean reward: 137.00
               Mean episode length: 67.33
    Episode_Reward/reaching_object: 0.3078
     Episode_Reward/lifting_object: 28.2144
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 55.4167
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 2.09s
                      Time elapsed: 00:28:39
                               ETA: 00:46:29

################################################################################
                     [1m Learning iteration 763/2000 [0m                      

                       Computation: 46982 steps/s (collection: 1.992s, learning 0.100s)
             Mean action noise std: 2.08
          Mean value_function loss: 474.2943
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 46.8492
                       Mean reward: 167.58
               Mean episode length: 76.12
    Episode_Reward/reaching_object: 0.3181
     Episode_Reward/lifting_object: 30.5860
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 55.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 2.09s
                      Time elapsed: 00:28:41
                               ETA: 00:46:27

################################################################################
                     [1m Learning iteration 764/2000 [0m                      

                       Computation: 48010 steps/s (collection: 1.960s, learning 0.088s)
             Mean action noise std: 2.08
          Mean value_function loss: 418.0214
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 46.8547
                       Mean reward: 149.07
               Mean episode length: 71.08
    Episode_Reward/reaching_object: 0.3191
     Episode_Reward/lifting_object: 30.3717
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 54.3750
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 2.05s
                      Time elapsed: 00:28:43
                               ETA: 00:46:24

################################################################################
                     [1m Learning iteration 765/2000 [0m                      

                       Computation: 44144 steps/s (collection: 2.089s, learning 0.138s)
             Mean action noise std: 2.08
          Mean value_function loss: 424.2568
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 46.8563
                       Mean reward: 145.79
               Mean episode length: 74.54
    Episode_Reward/reaching_object: 0.3178
     Episode_Reward/lifting_object: 30.3944
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 53.6667
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 2.23s
                      Time elapsed: 00:28:45
                               ETA: 00:46:22

################################################################################
                     [1m Learning iteration 766/2000 [0m                      

                       Computation: 47053 steps/s (collection: 1.995s, learning 0.095s)
             Mean action noise std: 2.08
          Mean value_function loss: 419.7752
               Mean surrogate loss: 0.0144
                 Mean entropy loss: 46.8566
                       Mean reward: 171.01
               Mean episode length: 75.75
    Episode_Reward/reaching_object: 0.3332
     Episode_Reward/lifting_object: 32.5395
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 53.2500
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 2.09s
                      Time elapsed: 00:28:47
                               ETA: 00:46:20

################################################################################
                     [1m Learning iteration 767/2000 [0m                      

                       Computation: 43173 steps/s (collection: 2.121s, learning 0.156s)
             Mean action noise std: 2.08
          Mean value_function loss: 443.0000
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 46.8574
                       Mean reward: 162.24
               Mean episode length: 73.28
    Episode_Reward/reaching_object: 0.3378
     Episode_Reward/lifting_object: 33.5000
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 53.4167
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 2.28s
                      Time elapsed: 00:28:50
                               ETA: 00:46:17

################################################################################
                     [1m Learning iteration 768/2000 [0m                      

                       Computation: 46340 steps/s (collection: 2.006s, learning 0.116s)
             Mean action noise std: 2.08
          Mean value_function loss: 479.3229
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 46.8608
                       Mean reward: 191.59
               Mean episode length: 80.90
    Episode_Reward/reaching_object: 0.3297
     Episode_Reward/lifting_object: 32.1313
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 51.8333
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 2.12s
                      Time elapsed: 00:28:52
                               ETA: 00:46:15

################################################################################
                     [1m Learning iteration 769/2000 [0m                      

                       Computation: 46862 steps/s (collection: 1.975s, learning 0.123s)
             Mean action noise std: 2.08
          Mean value_function loss: 464.5234
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 46.8648
                       Mean reward: 167.38
               Mean episode length: 74.50
    Episode_Reward/reaching_object: 0.3359
     Episode_Reward/lifting_object: 32.6984
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 54.2500
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 2.10s
                      Time elapsed: 00:28:54
                               ETA: 00:46:12

################################################################################
                     [1m Learning iteration 770/2000 [0m                      

                       Computation: 47727 steps/s (collection: 1.957s, learning 0.103s)
             Mean action noise std: 2.08
          Mean value_function loss: 434.4406
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 46.8675
                       Mean reward: 150.35
               Mean episode length: 76.29
    Episode_Reward/reaching_object: 0.3380
     Episode_Reward/lifting_object: 33.4186
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 53.2917
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 2.06s
                      Time elapsed: 00:28:56
                               ETA: 00:46:10

################################################################################
                     [1m Learning iteration 771/2000 [0m                      

                       Computation: 46893 steps/s (collection: 1.984s, learning 0.113s)
             Mean action noise std: 2.08
          Mean value_function loss: 415.4488
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 46.8695
                       Mean reward: 159.81
               Mean episode length: 74.13
    Episode_Reward/reaching_object: 0.3363
     Episode_Reward/lifting_object: 33.0917
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 49.2917
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 2.10s
                      Time elapsed: 00:28:58
                               ETA: 00:46:07

################################################################################
                     [1m Learning iteration 772/2000 [0m                      

                       Computation: 44345 steps/s (collection: 2.106s, learning 0.111s)
             Mean action noise std: 2.09
          Mean value_function loss: 436.4857
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 46.8724
                       Mean reward: 207.84
               Mean episode length: 84.89
    Episode_Reward/reaching_object: 0.3469
     Episode_Reward/lifting_object: 34.8745
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 48.6250
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 2.22s
                      Time elapsed: 00:29:00
                               ETA: 00:46:05

################################################################################
                     [1m Learning iteration 773/2000 [0m                      

                       Computation: 46173 steps/s (collection: 1.987s, learning 0.142s)
             Mean action noise std: 2.09
          Mean value_function loss: 455.1734
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 46.8750
                       Mean reward: 191.35
               Mean episode length: 80.07
    Episode_Reward/reaching_object: 0.3510
     Episode_Reward/lifting_object: 36.1873
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 51.7500
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 2.13s
                      Time elapsed: 00:29:02
                               ETA: 00:46:03

################################################################################
                     [1m Learning iteration 774/2000 [0m                      

                       Computation: 47117 steps/s (collection: 1.986s, learning 0.101s)
             Mean action noise std: 2.09
          Mean value_function loss: 425.8819
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 46.8721
                       Mean reward: 199.84
               Mean episode length: 81.25
    Episode_Reward/reaching_object: 0.3461
     Episode_Reward/lifting_object: 35.2257
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 46.8750
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 2.09s
                      Time elapsed: 00:29:05
                               ETA: 00:46:00

################################################################################
                     [1m Learning iteration 775/2000 [0m                      

                       Computation: 47224 steps/s (collection: 1.986s, learning 0.096s)
             Mean action noise std: 2.09
          Mean value_function loss: 432.7680
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 46.8706
                       Mean reward: 180.71
               Mean episode length: 78.90
    Episode_Reward/reaching_object: 0.3672
     Episode_Reward/lifting_object: 37.8231
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 49.5417
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 2.08s
                      Time elapsed: 00:29:07
                               ETA: 00:45:58

################################################################################
                     [1m Learning iteration 776/2000 [0m                      

                       Computation: 48476 steps/s (collection: 1.936s, learning 0.092s)
             Mean action noise std: 2.09
          Mean value_function loss: 445.2330
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 46.8737
                       Mean reward: 211.99
               Mean episode length: 88.33
    Episode_Reward/reaching_object: 0.3687
     Episode_Reward/lifting_object: 37.7539
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 49.6250
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 2.03s
                      Time elapsed: 00:29:09
                               ETA: 00:45:55

################################################################################
                     [1m Learning iteration 777/2000 [0m                      

                       Computation: 46923 steps/s (collection: 1.999s, learning 0.096s)
             Mean action noise std: 2.09
          Mean value_function loss: 435.1272
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 46.8757
                       Mean reward: 173.27
               Mean episode length: 83.77
    Episode_Reward/reaching_object: 0.3722
     Episode_Reward/lifting_object: 39.0992
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 48.5417
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 2.09s
                      Time elapsed: 00:29:11
                               ETA: 00:45:52

################################################################################
                     [1m Learning iteration 778/2000 [0m                      

                       Computation: 47138 steps/s (collection: 1.972s, learning 0.114s)
             Mean action noise std: 2.09
          Mean value_function loss: 478.1892
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 46.8786
                       Mean reward: 220.28
               Mean episode length: 87.73
    Episode_Reward/reaching_object: 0.3766
     Episode_Reward/lifting_object: 39.3507
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 46.6250
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 2.09s
                      Time elapsed: 00:29:13
                               ETA: 00:45:50

################################################################################
                     [1m Learning iteration 779/2000 [0m                      

                       Computation: 47485 steps/s (collection: 1.974s, learning 0.096s)
             Mean action noise std: 2.09
          Mean value_function loss: 451.9296
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 46.8813
                       Mean reward: 180.61
               Mean episode length: 76.98
    Episode_Reward/reaching_object: 0.3633
     Episode_Reward/lifting_object: 37.3860
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 45.4167
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 2.07s
                      Time elapsed: 00:29:15
                               ETA: 00:45:47

################################################################################
                     [1m Learning iteration 780/2000 [0m                      

                       Computation: 46931 steps/s (collection: 1.984s, learning 0.111s)
             Mean action noise std: 2.09
          Mean value_function loss: 447.0482
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 46.8882
                       Mean reward: 236.16
               Mean episode length: 93.12
    Episode_Reward/reaching_object: 0.3924
     Episode_Reward/lifting_object: 42.3993
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 43.6667
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 2.09s
                      Time elapsed: 00:29:17
                               ETA: 00:45:45

################################################################################
                     [1m Learning iteration 781/2000 [0m                      

                       Computation: 45329 steps/s (collection: 2.036s, learning 0.133s)
             Mean action noise std: 2.09
          Mean value_function loss: 457.6470
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 46.8912
                       Mean reward: 203.76
               Mean episode length: 84.70
    Episode_Reward/reaching_object: 0.3822
     Episode_Reward/lifting_object: 40.9017
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 46.5833
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 2.17s
                      Time elapsed: 00:29:19
                               ETA: 00:45:43

################################################################################
                     [1m Learning iteration 782/2000 [0m                      

                       Computation: 46997 steps/s (collection: 2.000s, learning 0.092s)
             Mean action noise std: 2.09
          Mean value_function loss: 458.8617
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 46.8912
                       Mean reward: 243.49
               Mean episode length: 89.66
    Episode_Reward/reaching_object: 0.3974
     Episode_Reward/lifting_object: 43.6644
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 44.4167
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 2.09s
                      Time elapsed: 00:29:21
                               ETA: 00:45:40

################################################################################
                     [1m Learning iteration 783/2000 [0m                      

                       Computation: 48079 steps/s (collection: 1.958s, learning 0.087s)
             Mean action noise std: 2.09
          Mean value_function loss: 453.9734
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 46.8954
                       Mean reward: 247.28
               Mean episode length: 96.35
    Episode_Reward/reaching_object: 0.4089
     Episode_Reward/lifting_object: 45.0179
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 45.8333
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 2.04s
                      Time elapsed: 00:29:23
                               ETA: 00:45:37

################################################################################
                     [1m Learning iteration 784/2000 [0m                      

                       Computation: 47693 steps/s (collection: 1.964s, learning 0.097s)
             Mean action noise std: 2.09
          Mean value_function loss: 456.9814
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 46.8998
                       Mean reward: 208.81
               Mean episode length: 87.74
    Episode_Reward/reaching_object: 0.3938
     Episode_Reward/lifting_object: 43.0631
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 44.1667
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 2.06s
                      Time elapsed: 00:29:25
                               ETA: 00:45:35

################################################################################
                     [1m Learning iteration 785/2000 [0m                      

                       Computation: 47872 steps/s (collection: 1.937s, learning 0.116s)
             Mean action noise std: 2.09
          Mean value_function loss: 439.0297
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 46.9019
                       Mean reward: 258.55
               Mean episode length: 95.78
    Episode_Reward/reaching_object: 0.3988
     Episode_Reward/lifting_object: 44.0852
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 41.7917
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 2.05s
                      Time elapsed: 00:29:27
                               ETA: 00:45:32

################################################################################
                     [1m Learning iteration 786/2000 [0m                      

                       Computation: 44707 steps/s (collection: 2.098s, learning 0.101s)
             Mean action noise std: 2.09
          Mean value_function loss: 499.1494
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 46.9030
                       Mean reward: 234.44
               Mean episode length: 94.40
    Episode_Reward/reaching_object: 0.4143
     Episode_Reward/lifting_object: 46.3685
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 41.6250
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 2.20s
                      Time elapsed: 00:29:30
                               ETA: 00:45:30

################################################################################
                     [1m Learning iteration 787/2000 [0m                      

                       Computation: 44714 steps/s (collection: 2.065s, learning 0.134s)
             Mean action noise std: 2.09
          Mean value_function loss: 500.1353
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 46.9066
                       Mean reward: 273.78
               Mean episode length: 103.62
    Episode_Reward/reaching_object: 0.4055
     Episode_Reward/lifting_object: 45.2883
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 43.6250
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 2.20s
                      Time elapsed: 00:29:32
                               ETA: 00:45:28

################################################################################
                     [1m Learning iteration 788/2000 [0m                      

                       Computation: 48113 steps/s (collection: 1.957s, learning 0.086s)
             Mean action noise std: 2.09
          Mean value_function loss: 454.4146
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 46.9126
                       Mean reward: 211.81
               Mean episode length: 87.15
    Episode_Reward/reaching_object: 0.4152
     Episode_Reward/lifting_object: 46.6174
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 42.5000
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 2.04s
                      Time elapsed: 00:29:34
                               ETA: 00:45:25

################################################################################
                     [1m Learning iteration 789/2000 [0m                      

                       Computation: 47636 steps/s (collection: 1.961s, learning 0.103s)
             Mean action noise std: 2.09
          Mean value_function loss: 443.6732
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 46.9149
                       Mean reward: 244.35
               Mean episode length: 100.37
    Episode_Reward/reaching_object: 0.4021
     Episode_Reward/lifting_object: 44.6179
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 39.1667
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 2.06s
                      Time elapsed: 00:29:36
                               ETA: 00:45:23

################################################################################
                     [1m Learning iteration 790/2000 [0m                      

                       Computation: 47407 steps/s (collection: 1.971s, learning 0.103s)
             Mean action noise std: 2.09
          Mean value_function loss: 460.3172
               Mean surrogate loss: 0.0165
                 Mean entropy loss: 46.9155
                       Mean reward: 260.29
               Mean episode length: 102.18
    Episode_Reward/reaching_object: 0.4215
     Episode_Reward/lifting_object: 47.8440
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 43.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 2.07s
                      Time elapsed: 00:29:38
                               ETA: 00:45:20

################################################################################
                     [1m Learning iteration 791/2000 [0m                      

                       Computation: 46378 steps/s (collection: 2.032s, learning 0.088s)
             Mean action noise std: 2.09
          Mean value_function loss: 458.6149
               Mean surrogate loss: 0.0104
                 Mean entropy loss: 46.9157
                       Mean reward: 273.10
               Mean episode length: 101.31
    Episode_Reward/reaching_object: 0.4322
     Episode_Reward/lifting_object: 49.5025
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 39.8750
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 2.12s
                      Time elapsed: 00:29:40
                               ETA: 00:45:18

################################################################################
                     [1m Learning iteration 792/2000 [0m                      

                       Computation: 47969 steps/s (collection: 1.962s, learning 0.087s)
             Mean action noise std: 2.09
          Mean value_function loss: 461.8760
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 46.9160
                       Mean reward: 275.68
               Mean episode length: 103.33
    Episode_Reward/reaching_object: 0.4372
     Episode_Reward/lifting_object: 51.1821
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 40.8750
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 2.05s
                      Time elapsed: 00:29:42
                               ETA: 00:45:15

################################################################################
                     [1m Learning iteration 793/2000 [0m                      

                       Computation: 45314 steps/s (collection: 2.019s, learning 0.151s)
             Mean action noise std: 2.09
          Mean value_function loss: 487.2805
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 46.9197
                       Mean reward: 240.56
               Mean episode length: 89.32
    Episode_Reward/reaching_object: 0.4304
     Episode_Reward/lifting_object: 50.4121
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 39.5833
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 2.17s
                      Time elapsed: 00:29:44
                               ETA: 00:45:13

################################################################################
                     [1m Learning iteration 794/2000 [0m                      

                       Computation: 46600 steps/s (collection: 1.988s, learning 0.121s)
             Mean action noise std: 2.09
          Mean value_function loss: 487.0818
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 46.9277
                       Mean reward: 256.71
               Mean episode length: 96.39
    Episode_Reward/reaching_object: 0.4535
     Episode_Reward/lifting_object: 54.0962
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 39.8333
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 2.11s
                      Time elapsed: 00:29:46
                               ETA: 00:45:10

################################################################################
                     [1m Learning iteration 795/2000 [0m                      

                       Computation: 47435 steps/s (collection: 1.985s, learning 0.087s)
             Mean action noise std: 2.09
          Mean value_function loss: 487.8792
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 46.9336
                       Mean reward: 298.42
               Mean episode length: 105.33
    Episode_Reward/reaching_object: 0.4365
     Episode_Reward/lifting_object: 51.8226
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 38.3333
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 2.07s
                      Time elapsed: 00:29:49
                               ETA: 00:45:08

################################################################################
                     [1m Learning iteration 796/2000 [0m                      

                       Computation: 48211 steps/s (collection: 1.950s, learning 0.089s)
             Mean action noise std: 2.09
          Mean value_function loss: 464.8620
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 46.9369
                       Mean reward: 298.99
               Mean episode length: 104.78
    Episode_Reward/reaching_object: 0.4641
     Episode_Reward/lifting_object: 56.4145
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 37.7500
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 2.04s
                      Time elapsed: 00:29:51
                               ETA: 00:45:05

################################################################################
                     [1m Learning iteration 797/2000 [0m                      

                       Computation: 46928 steps/s (collection: 1.970s, learning 0.125s)
             Mean action noise std: 2.09
          Mean value_function loss: 481.1750
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 46.9389
                       Mean reward: 306.69
               Mean episode length: 108.97
    Episode_Reward/reaching_object: 0.4378
     Episode_Reward/lifting_object: 52.6638
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 37.1667
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 2.09s
                      Time elapsed: 00:29:53
                               ETA: 00:45:03

################################################################################
                     [1m Learning iteration 798/2000 [0m                      

                       Computation: 47332 steps/s (collection: 1.941s, learning 0.136s)
             Mean action noise std: 2.09
          Mean value_function loss: 470.3031
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 46.9407
                       Mean reward: 281.10
               Mean episode length: 98.94
    Episode_Reward/reaching_object: 0.4568
     Episode_Reward/lifting_object: 54.8867
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 36.1250
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 2.08s
                      Time elapsed: 00:29:55
                               ETA: 00:45:00

################################################################################
                     [1m Learning iteration 799/2000 [0m                      

                       Computation: 47761 steps/s (collection: 1.965s, learning 0.094s)
             Mean action noise std: 2.10
          Mean value_function loss: 497.8733
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.9425
                       Mean reward: 256.37
               Mean episode length: 97.58
    Episode_Reward/reaching_object: 0.4647
     Episode_Reward/lifting_object: 56.8318
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 33.2500
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 2.06s
                      Time elapsed: 00:29:57
                               ETA: 00:44:58

################################################################################
                     [1m Learning iteration 800/2000 [0m                      

                       Computation: 48438 steps/s (collection: 1.938s, learning 0.091s)
             Mean action noise std: 2.10
          Mean value_function loss: 496.9184
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 46.9489
                       Mean reward: 288.79
               Mean episode length: 101.20
    Episode_Reward/reaching_object: 0.4730
     Episode_Reward/lifting_object: 58.1910
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 36.2083
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 2.03s
                      Time elapsed: 00:29:59
                               ETA: 00:44:55

################################################################################
                     [1m Learning iteration 801/2000 [0m                      

                       Computation: 47250 steps/s (collection: 1.975s, learning 0.106s)
             Mean action noise std: 2.10
          Mean value_function loss: 460.6628
               Mean surrogate loss: 0.0108
                 Mean entropy loss: 46.9544
                       Mean reward: 285.28
               Mean episode length: 102.98
    Episode_Reward/reaching_object: 0.4671
     Episode_Reward/lifting_object: 56.6407
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 33.4167
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 2.08s
                      Time elapsed: 00:30:01
                               ETA: 00:44:53

################################################################################
                     [1m Learning iteration 802/2000 [0m                      

                       Computation: 47452 steps/s (collection: 1.967s, learning 0.105s)
             Mean action noise std: 2.10
          Mean value_function loss: 437.7308
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 46.9574
                       Mean reward: 259.41
               Mean episode length: 97.85
    Episode_Reward/reaching_object: 0.4975
     Episode_Reward/lifting_object: 61.7406
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 32.2500
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 2.07s
                      Time elapsed: 00:30:03
                               ETA: 00:44:50

################################################################################
                     [1m Learning iteration 803/2000 [0m                      

                       Computation: 48513 steps/s (collection: 1.930s, learning 0.097s)
             Mean action noise std: 2.10
          Mean value_function loss: 590.6938
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 46.9685
                       Mean reward: 345.37
               Mean episode length: 120.04
    Episode_Reward/reaching_object: 0.4960
     Episode_Reward/lifting_object: 62.1421
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 31.9167
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 2.03s
                      Time elapsed: 00:30:05
                               ETA: 00:44:48

################################################################################
                     [1m Learning iteration 804/2000 [0m                      

                       Computation: 48555 steps/s (collection: 1.938s, learning 0.087s)
             Mean action noise std: 2.10
          Mean value_function loss: 461.7271
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 46.9762
                       Mean reward: 376.39
               Mean episode length: 125.47
    Episode_Reward/reaching_object: 0.5143
     Episode_Reward/lifting_object: 64.2280
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 30.2500
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 2.02s
                      Time elapsed: 00:30:07
                               ETA: 00:44:45

################################################################################
                     [1m Learning iteration 805/2000 [0m                      

                       Computation: 45772 steps/s (collection: 1.982s, learning 0.166s)
             Mean action noise std: 2.10
          Mean value_function loss: 457.0504
               Mean surrogate loss: 0.0116
                 Mean entropy loss: 46.9783
                       Mean reward: 295.05
               Mean episode length: 107.08
    Episode_Reward/reaching_object: 0.5287
     Episode_Reward/lifting_object: 66.3975
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 31.2500
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 2.15s
                      Time elapsed: 00:30:09
                               ETA: 00:44:43

################################################################################
                     [1m Learning iteration 806/2000 [0m                      

                       Computation: 47165 steps/s (collection: 1.977s, learning 0.107s)
             Mean action noise std: 2.10
          Mean value_function loss: 442.4217
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 46.9793
                       Mean reward: 373.35
               Mean episode length: 124.51
    Episode_Reward/reaching_object: 0.5444
     Episode_Reward/lifting_object: 69.5063
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 27.7083
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 2.08s
                      Time elapsed: 00:30:11
                               ETA: 00:44:40

################################################################################
                     [1m Learning iteration 807/2000 [0m                      

                       Computation: 48612 steps/s (collection: 1.934s, learning 0.088s)
             Mean action noise std: 2.10
          Mean value_function loss: 479.0993
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 46.9816
                       Mean reward: 334.16
               Mean episode length: 113.60
    Episode_Reward/reaching_object: 0.5325
     Episode_Reward/lifting_object: 66.9489
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 3.8750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 28.4583
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 2.02s
                      Time elapsed: 00:30:13
                               ETA: 00:44:38

################################################################################
                     [1m Learning iteration 808/2000 [0m                      

                       Computation: 47010 steps/s (collection: 1.981s, learning 0.110s)
             Mean action noise std: 2.10
          Mean value_function loss: 442.8034
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 46.9828
                       Mean reward: 391.14
               Mean episode length: 132.16
    Episode_Reward/reaching_object: 0.5560
     Episode_Reward/lifting_object: 70.2137
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 29.0833
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 2.09s
                      Time elapsed: 00:30:15
                               ETA: 00:44:35

################################################################################
                     [1m Learning iteration 809/2000 [0m                      

                       Computation: 47647 steps/s (collection: 1.975s, learning 0.089s)
             Mean action noise std: 2.10
          Mean value_function loss: 427.4371
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 46.9852
                       Mean reward: 341.95
               Mean episode length: 119.54
    Episode_Reward/reaching_object: 0.5371
     Episode_Reward/lifting_object: 67.4559
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 29.3750
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 2.06s
                      Time elapsed: 00:30:17
                               ETA: 00:44:33

################################################################################
                     [1m Learning iteration 810/2000 [0m                      

                       Computation: 48414 steps/s (collection: 1.942s, learning 0.089s)
             Mean action noise std: 2.10
          Mean value_function loss: 436.0450
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 46.9852
                       Mean reward: 366.99
               Mean episode length: 127.18
    Episode_Reward/reaching_object: 0.5681
     Episode_Reward/lifting_object: 73.2575
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.7917
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 2.03s
                      Time elapsed: 00:30:19
                               ETA: 00:44:30

################################################################################
                     [1m Learning iteration 811/2000 [0m                      

                       Computation: 48534 steps/s (collection: 1.940s, learning 0.086s)
             Mean action noise std: 2.10
          Mean value_function loss: 435.4470
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 46.9852
                       Mean reward: 323.57
               Mean episode length: 113.48
    Episode_Reward/reaching_object: 0.5460
     Episode_Reward/lifting_object: 69.4794
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 25.0417
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 2.03s
                      Time elapsed: 00:30:21
                               ETA: 00:44:27

################################################################################
                     [1m Learning iteration 812/2000 [0m                      

                       Computation: 44247 steps/s (collection: 2.126s, learning 0.096s)
             Mean action noise std: 2.10
          Mean value_function loss: 485.6981
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 46.9860
                       Mean reward: 362.22
               Mean episode length: 124.55
    Episode_Reward/reaching_object: 0.5767
     Episode_Reward/lifting_object: 74.4576
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 26.9167
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 2.22s
                      Time elapsed: 00:30:24
                               ETA: 00:44:25

################################################################################
                     [1m Learning iteration 813/2000 [0m                      

                       Computation: 47961 steps/s (collection: 1.957s, learning 0.093s)
             Mean action noise std: 2.10
          Mean value_function loss: 454.7274
               Mean surrogate loss: 0.0114
                 Mean entropy loss: 46.9865
                       Mean reward: 333.07
               Mean episode length: 116.29
    Episode_Reward/reaching_object: 0.5700
     Episode_Reward/lifting_object: 73.9107
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 23.6250
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 2.05s
                      Time elapsed: 00:30:26
                               ETA: 00:44:23

################################################################################
                     [1m Learning iteration 814/2000 [0m                      

                       Computation: 46116 steps/s (collection: 1.973s, learning 0.159s)
             Mean action noise std: 2.10
          Mean value_function loss: 423.5738
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 46.9879
                       Mean reward: 387.61
               Mean episode length: 128.21
    Episode_Reward/reaching_object: 0.5587
     Episode_Reward/lifting_object: 72.2522
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 24.5833
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 2.13s
                      Time elapsed: 00:30:28
                               ETA: 00:44:20

################################################################################
                     [1m Learning iteration 815/2000 [0m                      

                       Computation: 46628 steps/s (collection: 1.981s, learning 0.127s)
             Mean action noise std: 2.10
          Mean value_function loss: 537.5804
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 46.9960
                       Mean reward: 330.94
               Mean episode length: 118.51
    Episode_Reward/reaching_object: 0.6031
     Episode_Reward/lifting_object: 79.2073
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 24.6250
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 2.11s
                      Time elapsed: 00:30:30
                               ETA: 00:44:18

################################################################################
                     [1m Learning iteration 816/2000 [0m                      

                       Computation: 45824 steps/s (collection: 2.028s, learning 0.118s)
             Mean action noise std: 2.10
          Mean value_function loss: 619.6958
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 47.0094
                       Mean reward: 396.94
               Mean episode length: 132.15
    Episode_Reward/reaching_object: 0.5793
     Episode_Reward/lifting_object: 75.7520
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 23.7500
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 2.15s
                      Time elapsed: 00:30:32
                               ETA: 00:44:15

################################################################################
                     [1m Learning iteration 817/2000 [0m                      

                       Computation: 48242 steps/s (collection: 1.944s, learning 0.094s)
             Mean action noise std: 2.10
          Mean value_function loss: 407.0808
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 47.0181
                       Mean reward: 372.75
               Mean episode length: 127.88
    Episode_Reward/reaching_object: 0.6010
     Episode_Reward/lifting_object: 79.1678
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 24.7500
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 2.04s
                      Time elapsed: 00:30:34
                               ETA: 00:44:13

################################################################################
                     [1m Learning iteration 818/2000 [0m                      

                       Computation: 46867 steps/s (collection: 1.979s, learning 0.119s)
             Mean action noise std: 2.10
          Mean value_function loss: 437.4083
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 47.0202
                       Mean reward: 376.69
               Mean episode length: 128.16
    Episode_Reward/reaching_object: 0.5918
     Episode_Reward/lifting_object: 77.2449
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 24.3750
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 2.10s
                      Time elapsed: 00:30:36
                               ETA: 00:44:10

################################################################################
                     [1m Learning iteration 819/2000 [0m                      

                       Computation: 47708 steps/s (collection: 1.954s, learning 0.107s)
             Mean action noise std: 2.10
          Mean value_function loss: 439.1024
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.0224
                       Mean reward: 367.73
               Mean episode length: 126.37
    Episode_Reward/reaching_object: 0.6090
     Episode_Reward/lifting_object: 79.7250
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 24.1667
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 2.06s
                      Time elapsed: 00:30:38
                               ETA: 00:44:08

################################################################################
                     [1m Learning iteration 820/2000 [0m                      

                       Computation: 47703 steps/s (collection: 1.971s, learning 0.090s)
             Mean action noise std: 2.11
          Mean value_function loss: 548.1617
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 47.0332
                       Mean reward: 386.02
               Mean episode length: 127.01
    Episode_Reward/reaching_object: 0.6115
     Episode_Reward/lifting_object: 81.0561
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.5000
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 2.06s
                      Time elapsed: 00:30:40
                               ETA: 00:44:05

################################################################################
                     [1m Learning iteration 821/2000 [0m                      

                       Computation: 46575 steps/s (collection: 2.011s, learning 0.100s)
             Mean action noise std: 2.11
          Mean value_function loss: 517.6984
               Mean surrogate loss: 0.0115
                 Mean entropy loss: 47.0404
                       Mean reward: 411.35
               Mean episode length: 134.15
    Episode_Reward/reaching_object: 0.6101
     Episode_Reward/lifting_object: 81.1031
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 23.6250
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 2.11s
                      Time elapsed: 00:30:43
                               ETA: 00:44:03

################################################################################
                     [1m Learning iteration 822/2000 [0m                      

                       Computation: 48507 steps/s (collection: 1.936s, learning 0.091s)
             Mean action noise std: 2.11
          Mean value_function loss: 430.6426
               Mean surrogate loss: 0.0140
                 Mean entropy loss: 47.0412
                       Mean reward: 392.70
               Mean episode length: 127.37
    Episode_Reward/reaching_object: 0.6063
     Episode_Reward/lifting_object: 80.7782
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 21.7917
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 2.03s
                      Time elapsed: 00:30:45
                               ETA: 00:44:00

################################################################################
                     [1m Learning iteration 823/2000 [0m                      

                       Computation: 45149 steps/s (collection: 2.073s, learning 0.105s)
             Mean action noise std: 2.11
          Mean value_function loss: 443.4733
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 47.0417
                       Mean reward: 379.54
               Mean episode length: 122.12
    Episode_Reward/reaching_object: 0.6366
     Episode_Reward/lifting_object: 87.0179
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 23.6667
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 2.18s
                      Time elapsed: 00:30:47
                               ETA: 00:43:58

################################################################################
                     [1m Learning iteration 824/2000 [0m                      

                       Computation: 47249 steps/s (collection: 1.950s, learning 0.130s)
             Mean action noise std: 2.11
          Mean value_function loss: 423.0617
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 47.0422
                       Mean reward: 431.66
               Mean episode length: 138.50
    Episode_Reward/reaching_object: 0.6084
     Episode_Reward/lifting_object: 82.1385
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.7917
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 2.08s
                      Time elapsed: 00:30:49
                               ETA: 00:43:56

################################################################################
                     [1m Learning iteration 825/2000 [0m                      

                       Computation: 47203 steps/s (collection: 1.993s, learning 0.090s)
             Mean action noise std: 2.11
          Mean value_function loss: 434.4744
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 47.0433
                       Mean reward: 458.25
               Mean episode length: 145.49
    Episode_Reward/reaching_object: 0.6060
     Episode_Reward/lifting_object: 82.1912
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 24.0417
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 2.08s
                      Time elapsed: 00:30:51
                               ETA: 00:43:53

################################################################################
                     [1m Learning iteration 826/2000 [0m                      

                       Computation: 45150 steps/s (collection: 2.081s, learning 0.097s)
             Mean action noise std: 2.11
          Mean value_function loss: 429.4038
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 47.0484
                       Mean reward: 488.22
               Mean episode length: 152.47
    Episode_Reward/reaching_object: 0.6485
     Episode_Reward/lifting_object: 88.9840
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 22.5833
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 2.18s
                      Time elapsed: 00:30:53
                               ETA: 00:43:51

################################################################################
                     [1m Learning iteration 827/2000 [0m                      

                       Computation: 44746 steps/s (collection: 2.051s, learning 0.146s)
             Mean action noise std: 2.11
          Mean value_function loss: 442.6576
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 47.0537
                       Mean reward: 351.82
               Mean episode length: 123.26
    Episode_Reward/reaching_object: 0.6259
     Episode_Reward/lifting_object: 85.0350
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 23.7083
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 2.20s
                      Time elapsed: 00:30:55
                               ETA: 00:43:48

################################################################################
                     [1m Learning iteration 828/2000 [0m                      

                       Computation: 46664 steps/s (collection: 1.988s, learning 0.119s)
             Mean action noise std: 2.11
          Mean value_function loss: 415.1915
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 47.0595
                       Mean reward: 475.36
               Mean episode length: 146.24
    Episode_Reward/reaching_object: 0.6076
     Episode_Reward/lifting_object: 82.1048
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 21.6250
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 2.11s
                      Time elapsed: 00:30:57
                               ETA: 00:43:46

################################################################################
                     [1m Learning iteration 829/2000 [0m                      

                       Computation: 47702 steps/s (collection: 1.944s, learning 0.117s)
             Mean action noise std: 2.11
          Mean value_function loss: 409.0276
               Mean surrogate loss: 0.0141
                 Mean entropy loss: 47.0631
                       Mean reward: 378.11
               Mean episode length: 126.61
    Episode_Reward/reaching_object: 0.6228
     Episode_Reward/lifting_object: 84.9020
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 21.0833
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 2.06s
                      Time elapsed: 00:30:59
                               ETA: 00:43:44

################################################################################
                     [1m Learning iteration 830/2000 [0m                      

                       Computation: 47394 steps/s (collection: 1.973s, learning 0.102s)
             Mean action noise std: 2.11
          Mean value_function loss: 410.8558
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 47.0634
                       Mean reward: 426.77
               Mean episode length: 135.03
    Episode_Reward/reaching_object: 0.6486
     Episode_Reward/lifting_object: 90.7316
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 2.07s
                      Time elapsed: 00:31:02
                               ETA: 00:43:41

################################################################################
                     [1m Learning iteration 831/2000 [0m                      

                       Computation: 44153 steps/s (collection: 2.124s, learning 0.102s)
             Mean action noise std: 2.11
          Mean value_function loss: 467.6950
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 47.0647
                       Mean reward: 435.96
               Mean episode length: 138.73
    Episode_Reward/reaching_object: 0.6306
     Episode_Reward/lifting_object: 85.1304
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.5417
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 2.23s
                      Time elapsed: 00:31:04
                               ETA: 00:43:39

################################################################################
                     [1m Learning iteration 832/2000 [0m                      

                       Computation: 46575 steps/s (collection: 2.015s, learning 0.096s)
             Mean action noise std: 2.11
          Mean value_function loss: 444.1133
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 47.0681
                       Mean reward: 458.31
               Mean episode length: 147.74
    Episode_Reward/reaching_object: 0.6592
     Episode_Reward/lifting_object: 91.3608
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 21.3750
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 2.11s
                      Time elapsed: 00:31:06
                               ETA: 00:43:36

################################################################################
                     [1m Learning iteration 833/2000 [0m                      

                       Computation: 45665 steps/s (collection: 2.014s, learning 0.139s)
             Mean action noise std: 2.11
          Mean value_function loss: 419.4177
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 47.0700
                       Mean reward: 508.70
               Mean episode length: 156.89
    Episode_Reward/reaching_object: 0.6656
     Episode_Reward/lifting_object: 93.1190
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.3333
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 2.15s
                      Time elapsed: 00:31:08
                               ETA: 00:43:34

################################################################################
                     [1m Learning iteration 834/2000 [0m                      

                       Computation: 47417 steps/s (collection: 1.984s, learning 0.089s)
             Mean action noise std: 2.11
          Mean value_function loss: 421.5465
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 47.0761
                       Mean reward: 481.72
               Mean episode length: 147.03
    Episode_Reward/reaching_object: 0.6866
     Episode_Reward/lifting_object: 96.4624
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 19.6250
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 2.07s
                      Time elapsed: 00:31:10
                               ETA: 00:43:32

################################################################################
                     [1m Learning iteration 835/2000 [0m                      

                       Computation: 45925 steps/s (collection: 1.997s, learning 0.143s)
             Mean action noise std: 2.11
          Mean value_function loss: 428.8183
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 47.0848
                       Mean reward: 460.85
               Mean episode length: 144.05
    Episode_Reward/reaching_object: 0.6391
     Episode_Reward/lifting_object: 87.8580
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.8333
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 2.14s
                      Time elapsed: 00:31:12
                               ETA: 00:43:29

################################################################################
                     [1m Learning iteration 836/2000 [0m                      

                       Computation: 47563 steps/s (collection: 1.979s, learning 0.088s)
             Mean action noise std: 2.11
          Mean value_function loss: 412.6402
               Mean surrogate loss: 0.0096
                 Mean entropy loss: 47.0899
                       Mean reward: 493.09
               Mean episode length: 153.30
    Episode_Reward/reaching_object: 0.6625
     Episode_Reward/lifting_object: 92.0509
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.6250
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 2.07s
                      Time elapsed: 00:31:14
                               ETA: 00:43:27

################################################################################
                     [1m Learning iteration 837/2000 [0m                      

                       Computation: 47515 steps/s (collection: 1.980s, learning 0.089s)
             Mean action noise std: 2.11
          Mean value_function loss: 396.9246
               Mean surrogate loss: 0.0138
                 Mean entropy loss: 47.0909
                       Mean reward: 506.93
               Mean episode length: 158.94
    Episode_Reward/reaching_object: 0.6849
     Episode_Reward/lifting_object: 95.9335
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.8750
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 2.07s
                      Time elapsed: 00:31:16
                               ETA: 00:43:24

################################################################################
                     [1m Learning iteration 838/2000 [0m                      

                       Computation: 43232 steps/s (collection: 2.078s, learning 0.196s)
             Mean action noise std: 2.11
          Mean value_function loss: 420.7338
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 47.0915
                       Mean reward: 462.38
               Mean episode length: 145.95
    Episode_Reward/reaching_object: 0.6808
     Episode_Reward/lifting_object: 95.1590
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.5417
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 2.27s
                      Time elapsed: 00:31:19
                               ETA: 00:43:22

################################################################################
                     [1m Learning iteration 839/2000 [0m                      

                       Computation: 48552 steps/s (collection: 1.926s, learning 0.099s)
             Mean action noise std: 2.11
          Mean value_function loss: 394.4704
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 47.0935
                       Mean reward: 509.33
               Mean episode length: 156.93
    Episode_Reward/reaching_object: 0.6768
     Episode_Reward/lifting_object: 94.8239
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 2.02s
                      Time elapsed: 00:31:21
                               ETA: 00:43:20

################################################################################
                     [1m Learning iteration 840/2000 [0m                      

                       Computation: 48147 steps/s (collection: 1.938s, learning 0.104s)
             Mean action noise std: 2.11
          Mean value_function loss: 371.5269
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 47.1007
                       Mean reward: 367.61
               Mean episode length: 128.28
    Episode_Reward/reaching_object: 0.6665
     Episode_Reward/lifting_object: 91.7634
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.2500
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 2.04s
                      Time elapsed: 00:31:23
                               ETA: 00:43:17

################################################################################
                     [1m Learning iteration 841/2000 [0m                      

                       Computation: 45686 steps/s (collection: 2.054s, learning 0.097s)
             Mean action noise std: 2.11
          Mean value_function loss: 457.6467
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 47.1091
                       Mean reward: 450.84
               Mean episode length: 151.10
    Episode_Reward/reaching_object: 0.6643
     Episode_Reward/lifting_object: 90.9084
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 18.7917
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 2.15s
                      Time elapsed: 00:31:25
                               ETA: 00:43:15

################################################################################
                     [1m Learning iteration 842/2000 [0m                      

                       Computation: 48008 steps/s (collection: 1.957s, learning 0.091s)
             Mean action noise std: 2.11
          Mean value_function loss: 472.3354
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 47.1113
                       Mean reward: 528.61
               Mean episode length: 163.11
    Episode_Reward/reaching_object: 0.6962
     Episode_Reward/lifting_object: 96.1796
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.5417
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 2.05s
                      Time elapsed: 00:31:27
                               ETA: 00:43:12

################################################################################
                     [1m Learning iteration 843/2000 [0m                      

                       Computation: 47476 steps/s (collection: 1.954s, learning 0.117s)
             Mean action noise std: 2.11
          Mean value_function loss: 394.7589
               Mean surrogate loss: 0.0114
                 Mean entropy loss: 47.1117
                       Mean reward: 434.26
               Mean episode length: 141.67
    Episode_Reward/reaching_object: 0.6790
     Episode_Reward/lifting_object: 93.3258
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 18.2083
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 2.07s
                      Time elapsed: 00:31:29
                               ETA: 00:43:10

################################################################################
                     [1m Learning iteration 844/2000 [0m                      

                       Computation: 47855 steps/s (collection: 1.960s, learning 0.094s)
             Mean action noise std: 2.11
          Mean value_function loss: 380.5075
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 47.1118
                       Mean reward: 493.97
               Mean episode length: 156.30
    Episode_Reward/reaching_object: 0.6554
     Episode_Reward/lifting_object: 89.3077
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 17.7500
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 2.05s
                      Time elapsed: 00:31:31
                               ETA: 00:43:07

################################################################################
                     [1m Learning iteration 845/2000 [0m                      

                       Computation: 47361 steps/s (collection: 1.958s, learning 0.118s)
             Mean action noise std: 2.11
          Mean value_function loss: 381.9478
               Mean surrogate loss: 0.0134
                 Mean entropy loss: 47.1120
                       Mean reward: 531.22
               Mean episode length: 165.12
    Episode_Reward/reaching_object: 0.6956
     Episode_Reward/lifting_object: 96.4469
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 2.08s
                      Time elapsed: 00:31:33
                               ETA: 00:43:05

################################################################################
                     [1m Learning iteration 846/2000 [0m                      

                       Computation: 48447 steps/s (collection: 1.900s, learning 0.129s)
             Mean action noise std: 2.11
          Mean value_function loss: 390.3509
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 47.1122
                       Mean reward: 442.02
               Mean episode length: 139.02
    Episode_Reward/reaching_object: 0.6502
     Episode_Reward/lifting_object: 88.7667
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.2917
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 2.03s
                      Time elapsed: 00:31:35
                               ETA: 00:43:02

################################################################################
                     [1m Learning iteration 847/2000 [0m                      

                       Computation: 48328 steps/s (collection: 1.923s, learning 0.111s)
             Mean action noise std: 2.11
          Mean value_function loss: 365.0847
               Mean surrogate loss: 0.0117
                 Mean entropy loss: 47.1123
                       Mean reward: 338.05
               Mean episode length: 118.63
    Episode_Reward/reaching_object: 0.6502
     Episode_Reward/lifting_object: 89.2557
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 2.03s
                      Time elapsed: 00:31:37
                               ETA: 00:43:00

################################################################################
                     [1m Learning iteration 848/2000 [0m                      

                       Computation: 46684 steps/s (collection: 1.979s, learning 0.127s)
             Mean action noise std: 2.11
          Mean value_function loss: 389.1733
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 47.1129
                       Mean reward: 522.45
               Mean episode length: 159.63
    Episode_Reward/reaching_object: 0.6886
     Episode_Reward/lifting_object: 95.9806
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 2.11s
                      Time elapsed: 00:31:39
                               ETA: 00:42:57

################################################################################
                     [1m Learning iteration 849/2000 [0m                      

                       Computation: 47218 steps/s (collection: 1.978s, learning 0.104s)
             Mean action noise std: 2.11
          Mean value_function loss: 387.8209
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 47.1151
                       Mean reward: 435.80
               Mean episode length: 138.66
    Episode_Reward/reaching_object: 0.7006
     Episode_Reward/lifting_object: 99.3196
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.5417
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 2.08s
                      Time elapsed: 00:31:41
                               ETA: 00:42:55

################################################################################
                     [1m Learning iteration 850/2000 [0m                      

                       Computation: 48182 steps/s (collection: 1.948s, learning 0.093s)
             Mean action noise std: 2.11
          Mean value_function loss: 389.9899
               Mean surrogate loss: 0.0104
                 Mean entropy loss: 47.1159
                       Mean reward: 567.69
               Mean episode length: 168.44
    Episode_Reward/reaching_object: 0.7091
     Episode_Reward/lifting_object: 100.8325
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 2.04s
                      Time elapsed: 00:31:43
                               ETA: 00:42:52

################################################################################
                     [1m Learning iteration 851/2000 [0m                      

                       Computation: 48863 steps/s (collection: 1.926s, learning 0.086s)
             Mean action noise std: 2.11
          Mean value_function loss: 368.5771
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 47.1170
                       Mean reward: 545.44
               Mean episode length: 163.17
    Episode_Reward/reaching_object: 0.7254
     Episode_Reward/lifting_object: 104.9941
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 2.01s
                      Time elapsed: 00:31:45
                               ETA: 00:42:50

################################################################################
                     [1m Learning iteration 852/2000 [0m                      

                       Computation: 48934 steps/s (collection: 1.907s, learning 0.102s)
             Mean action noise std: 2.11
          Mean value_function loss: 373.0464
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 47.1212
                       Mean reward: 617.35
               Mean episode length: 181.59
    Episode_Reward/reaching_object: 0.7610
     Episode_Reward/lifting_object: 110.5239
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.7083
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 2.01s
                      Time elapsed: 00:31:47
                               ETA: 00:42:47

################################################################################
                     [1m Learning iteration 853/2000 [0m                      

                       Computation: 47511 steps/s (collection: 1.968s, learning 0.102s)
             Mean action noise std: 2.11
          Mean value_function loss: 386.7437
               Mean surrogate loss: 0.0111
                 Mean entropy loss: 47.1231
                       Mean reward: 562.01
               Mean episode length: 166.08
    Episode_Reward/reaching_object: 0.7021
     Episode_Reward/lifting_object: 100.4713
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 2.07s
                      Time elapsed: 00:31:49
                               ETA: 00:42:45

################################################################################
                     [1m Learning iteration 854/2000 [0m                      

                       Computation: 48581 steps/s (collection: 1.921s, learning 0.102s)
             Mean action noise std: 2.11
          Mean value_function loss: 368.7859
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 47.1242
                       Mean reward: 501.04
               Mean episode length: 153.76
    Episode_Reward/reaching_object: 0.7178
     Episode_Reward/lifting_object: 103.1520
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.2500
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 2.02s
                      Time elapsed: 00:31:51
                               ETA: 00:42:42

################################################################################
                     [1m Learning iteration 855/2000 [0m                      

                       Computation: 44289 steps/s (collection: 2.067s, learning 0.153s)
             Mean action noise std: 2.12
          Mean value_function loss: 377.9950
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 47.1266
                       Mean reward: 502.69
               Mean episode length: 152.74
    Episode_Reward/reaching_object: 0.7104
     Episode_Reward/lifting_object: 102.1121
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 2.22s
                      Time elapsed: 00:31:54
                               ETA: 00:42:40

################################################################################
                     [1m Learning iteration 856/2000 [0m                      

                       Computation: 47449 steps/s (collection: 1.972s, learning 0.100s)
             Mean action noise std: 2.12
          Mean value_function loss: 362.6666
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 47.1298
                       Mean reward: 512.77
               Mean episode length: 155.84
    Episode_Reward/reaching_object: 0.7001
     Episode_Reward/lifting_object: 99.9704
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 2.07s
                      Time elapsed: 00:31:56
                               ETA: 00:42:38

################################################################################
                     [1m Learning iteration 857/2000 [0m                      

                       Computation: 48420 steps/s (collection: 1.921s, learning 0.109s)
             Mean action noise std: 2.12
          Mean value_function loss: 362.3249
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 47.1370
                       Mean reward: 505.88
               Mean episode length: 154.95
    Episode_Reward/reaching_object: 0.6813
     Episode_Reward/lifting_object: 97.0204
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 16.0417
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 2.03s
                      Time elapsed: 00:31:58
                               ETA: 00:42:35

################################################################################
                     [1m Learning iteration 858/2000 [0m                      

                       Computation: 49613 steps/s (collection: 1.888s, learning 0.093s)
             Mean action noise std: 2.12
          Mean value_function loss: 366.8162
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 47.1438
                       Mean reward: 507.75
               Mean episode length: 155.99
    Episode_Reward/reaching_object: 0.6814
     Episode_Reward/lifting_object: 96.6347
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.2917
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 1.98s
                      Time elapsed: 00:32:00
                               ETA: 00:42:32

################################################################################
                     [1m Learning iteration 859/2000 [0m                      

                       Computation: 48331 steps/s (collection: 1.946s, learning 0.088s)
             Mean action noise std: 2.12
          Mean value_function loss: 364.8553
               Mean surrogate loss: 0.0115
                 Mean entropy loss: 47.1463
                       Mean reward: 517.89
               Mean episode length: 157.60
    Episode_Reward/reaching_object: 0.7160
     Episode_Reward/lifting_object: 102.9408
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.9583
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 2.03s
                      Time elapsed: 00:32:02
                               ETA: 00:42:30

################################################################################
                     [1m Learning iteration 860/2000 [0m                      

                       Computation: 46482 steps/s (collection: 1.993s, learning 0.122s)
             Mean action noise std: 2.12
          Mean value_function loss: 370.3501
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 47.1472
                       Mean reward: 474.41
               Mean episode length: 148.52
    Episode_Reward/reaching_object: 0.6967
     Episode_Reward/lifting_object: 99.5888
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 2.11s
                      Time elapsed: 00:32:04
                               ETA: 00:42:28

################################################################################
                     [1m Learning iteration 861/2000 [0m                      

                       Computation: 46771 steps/s (collection: 2.003s, learning 0.099s)
             Mean action noise std: 2.12
          Mean value_function loss: 363.1432
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 47.1499
                       Mean reward: 506.61
               Mean episode length: 154.88
    Episode_Reward/reaching_object: 0.7313
     Episode_Reward/lifting_object: 105.3737
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 2.10s
                      Time elapsed: 00:32:06
                               ETA: 00:42:25

################################################################################
                     [1m Learning iteration 862/2000 [0m                      

                       Computation: 48581 steps/s (collection: 1.934s, learning 0.089s)
             Mean action noise std: 2.12
          Mean value_function loss: 454.4124
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 47.1566
                       Mean reward: 571.98
               Mean episode length: 170.96
    Episode_Reward/reaching_object: 0.7269
     Episode_Reward/lifting_object: 105.8568
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 2.02s
                      Time elapsed: 00:32:08
                               ETA: 00:42:23

################################################################################
                     [1m Learning iteration 863/2000 [0m                      

                       Computation: 48223 steps/s (collection: 1.939s, learning 0.099s)
             Mean action noise std: 2.12
          Mean value_function loss: 364.0190
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 47.1638
                       Mean reward: 496.55
               Mean episode length: 149.86
    Episode_Reward/reaching_object: 0.7183
     Episode_Reward/lifting_object: 104.8593
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 2.04s
                      Time elapsed: 00:32:10
                               ETA: 00:42:20

################################################################################
                     [1m Learning iteration 864/2000 [0m                      

                       Computation: 49211 steps/s (collection: 1.907s, learning 0.091s)
             Mean action noise std: 2.12
          Mean value_function loss: 358.0776
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 47.1677
                       Mean reward: 531.99
               Mean episode length: 157.70
    Episode_Reward/reaching_object: 0.7439
     Episode_Reward/lifting_object: 109.9257
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 2.00s
                      Time elapsed: 00:32:12
                               ETA: 00:42:18

################################################################################
                     [1m Learning iteration 865/2000 [0m                      

                       Computation: 48293 steps/s (collection: 1.951s, learning 0.085s)
             Mean action noise std: 2.12
          Mean value_function loss: 350.5888
               Mean surrogate loss: 0.0096
                 Mean entropy loss: 47.1717
                       Mean reward: 549.92
               Mean episode length: 164.61
    Episode_Reward/reaching_object: 0.7386
     Episode_Reward/lifting_object: 108.1385
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 2.04s
                      Time elapsed: 00:32:14
                               ETA: 00:42:15

################################################################################
                     [1m Learning iteration 866/2000 [0m                      

                       Computation: 48233 steps/s (collection: 1.922s, learning 0.116s)
             Mean action noise std: 2.12
          Mean value_function loss: 351.4168
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 47.1733
                       Mean reward: 567.38
               Mean episode length: 170.27
    Episode_Reward/reaching_object: 0.7296
     Episode_Reward/lifting_object: 106.9752
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 2.04s
                      Time elapsed: 00:32:16
                               ETA: 00:42:13

################################################################################
                     [1m Learning iteration 867/2000 [0m                      

                       Computation: 48011 steps/s (collection: 1.948s, learning 0.100s)
             Mean action noise std: 2.12
          Mean value_function loss: 344.0966
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 47.1777
                       Mean reward: 591.25
               Mean episode length: 174.37
    Episode_Reward/reaching_object: 0.7657
     Episode_Reward/lifting_object: 113.1718
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.9583
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 2.05s
                      Time elapsed: 00:32:18
                               ETA: 00:42:10

################################################################################
                     [1m Learning iteration 868/2000 [0m                      

                       Computation: 49575 steps/s (collection: 1.893s, learning 0.090s)
             Mean action noise std: 2.12
          Mean value_function loss: 354.8839
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 47.1871
                       Mean reward: 462.33
               Mean episode length: 145.51
    Episode_Reward/reaching_object: 0.7422
     Episode_Reward/lifting_object: 109.3690
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 1.98s
                      Time elapsed: 00:32:20
                               ETA: 00:42:08

################################################################################
                     [1m Learning iteration 869/2000 [0m                      

                       Computation: 48558 steps/s (collection: 1.911s, learning 0.113s)
             Mean action noise std: 2.12
          Mean value_function loss: 352.8260
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 47.1912
                       Mean reward: 525.47
               Mean episode length: 155.47
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 113.0832
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 2.02s
                      Time elapsed: 00:32:22
                               ETA: 00:42:05

################################################################################
                     [1m Learning iteration 870/2000 [0m                      

                       Computation: 47220 steps/s (collection: 1.932s, learning 0.150s)
             Mean action noise std: 2.12
          Mean value_function loss: 344.8222
               Mean surrogate loss: 0.0135
                 Mean entropy loss: 47.1919
                       Mean reward: 521.01
               Mean episode length: 157.99
    Episode_Reward/reaching_object: 0.7532
     Episode_Reward/lifting_object: 111.8006
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 2.08s
                      Time elapsed: 00:32:24
                               ETA: 00:42:03

################################################################################
                     [1m Learning iteration 871/2000 [0m                      

                       Computation: 47583 steps/s (collection: 1.970s, learning 0.096s)
             Mean action noise std: 2.12
          Mean value_function loss: 342.6073
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 47.1922
                       Mean reward: 476.96
               Mean episode length: 147.51
    Episode_Reward/reaching_object: 0.7534
     Episode_Reward/lifting_object: 112.4931
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 2.07s
                      Time elapsed: 00:32:26
                               ETA: 00:42:00

################################################################################
                     [1m Learning iteration 872/2000 [0m                      

                       Computation: 44885 steps/s (collection: 2.091s, learning 0.100s)
             Mean action noise std: 2.12
          Mean value_function loss: 344.7235
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 47.1924
                       Mean reward: 594.48
               Mean episode length: 174.46
    Episode_Reward/reaching_object: 0.7215
     Episode_Reward/lifting_object: 105.9700
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 2.19s
                      Time elapsed: 00:32:29
                               ETA: 00:41:58

################################################################################
                     [1m Learning iteration 873/2000 [0m                      

                       Computation: 45740 steps/s (collection: 1.981s, learning 0.168s)
             Mean action noise std: 2.12
          Mean value_function loss: 347.0443
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.1932
                       Mean reward: 478.47
               Mean episode length: 150.49
    Episode_Reward/reaching_object: 0.7101
     Episode_Reward/lifting_object: 102.9683
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 2.15s
                      Time elapsed: 00:32:31
                               ETA: 00:41:56

################################################################################
                     [1m Learning iteration 874/2000 [0m                      

                       Computation: 49517 steps/s (collection: 1.891s, learning 0.095s)
             Mean action noise std: 2.12
          Mean value_function loss: 347.9572
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 47.2002
                       Mean reward: 471.89
               Mean episode length: 150.74
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 109.7503
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 1.99s
                      Time elapsed: 00:32:33
                               ETA: 00:41:53

################################################################################
                     [1m Learning iteration 875/2000 [0m                      

                       Computation: 50246 steps/s (collection: 1.873s, learning 0.084s)
             Mean action noise std: 2.12
          Mean value_function loss: 341.8126
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 47.2040
                       Mean reward: 549.73
               Mean episode length: 165.01
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 111.9601
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 1.96s
                      Time elapsed: 00:32:35
                               ETA: 00:41:50

################################################################################
                     [1m Learning iteration 876/2000 [0m                      

                       Computation: 48553 steps/s (collection: 1.938s, learning 0.087s)
             Mean action noise std: 2.12
          Mean value_function loss: 335.9609
               Mean surrogate loss: 0.0121
                 Mean entropy loss: 47.2045
                       Mean reward: 520.06
               Mean episode length: 156.09
    Episode_Reward/reaching_object: 0.7358
     Episode_Reward/lifting_object: 108.0806
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 2.02s
                      Time elapsed: 00:32:37
                               ETA: 00:41:48

################################################################################
                     [1m Learning iteration 877/2000 [0m                      

                       Computation: 47198 steps/s (collection: 1.977s, learning 0.106s)
             Mean action noise std: 2.12
          Mean value_function loss: 323.3980
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 47.2049
                       Mean reward: 548.50
               Mean episode length: 165.14
    Episode_Reward/reaching_object: 0.7378
     Episode_Reward/lifting_object: 108.1570
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 2.08s
                      Time elapsed: 00:32:39
                               ETA: 00:41:45

################################################################################
                     [1m Learning iteration 878/2000 [0m                      

                       Computation: 45296 steps/s (collection: 2.059s, learning 0.112s)
             Mean action noise std: 2.12
          Mean value_function loss: 324.9271
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 47.2061
                       Mean reward: 477.06
               Mean episode length: 147.76
    Episode_Reward/reaching_object: 0.7532
     Episode_Reward/lifting_object: 112.2294
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 2.17s
                      Time elapsed: 00:32:41
                               ETA: 00:41:43

################################################################################
                     [1m Learning iteration 879/2000 [0m                      

                       Computation: 48253 steps/s (collection: 1.935s, learning 0.103s)
             Mean action noise std: 2.12
          Mean value_function loss: 342.2539
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 47.2071
                       Mean reward: 530.33
               Mean episode length: 159.63
    Episode_Reward/reaching_object: 0.7608
     Episode_Reward/lifting_object: 112.9531
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 2.04s
                      Time elapsed: 00:32:43
                               ETA: 00:41:41

################################################################################
                     [1m Learning iteration 880/2000 [0m                      

                       Computation: 46256 steps/s (collection: 1.968s, learning 0.157s)
             Mean action noise std: 2.13
          Mean value_function loss: 344.4264
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 47.2107
                       Mean reward: 559.50
               Mean episode length: 167.19
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 113.9004
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 2.13s
                      Time elapsed: 00:32:45
                               ETA: 00:41:38

################################################################################
                     [1m Learning iteration 881/2000 [0m                      

                       Computation: 46733 steps/s (collection: 1.980s, learning 0.123s)
             Mean action noise std: 2.13
          Mean value_function loss: 337.7051
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 47.2222
                       Mean reward: 551.92
               Mean episode length: 164.14
    Episode_Reward/reaching_object: 0.7293
     Episode_Reward/lifting_object: 107.5570
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 2.10s
                      Time elapsed: 00:32:47
                               ETA: 00:41:36

################################################################################
                     [1m Learning iteration 882/2000 [0m                      

                       Computation: 47157 steps/s (collection: 1.978s, learning 0.107s)
             Mean action noise std: 2.13
          Mean value_function loss: 334.5951
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 47.2310
                       Mean reward: 586.98
               Mean episode length: 176.60
    Episode_Reward/reaching_object: 0.7935
     Episode_Reward/lifting_object: 118.8661
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 2.08s
                      Time elapsed: 00:32:49
                               ETA: 00:41:34

################################################################################
                     [1m Learning iteration 883/2000 [0m                      

                       Computation: 47708 steps/s (collection: 1.951s, learning 0.109s)
             Mean action noise std: 2.13
          Mean value_function loss: 336.8993
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 47.2353
                       Mean reward: 546.63
               Mean episode length: 162.14
    Episode_Reward/reaching_object: 0.7865
     Episode_Reward/lifting_object: 117.7434
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 2.06s
                      Time elapsed: 00:32:51
                               ETA: 00:41:31

################################################################################
                     [1m Learning iteration 884/2000 [0m                      

                       Computation: 47296 steps/s (collection: 1.972s, learning 0.107s)
             Mean action noise std: 2.13
          Mean value_function loss: 335.8943
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 47.2443
                       Mean reward: 573.65
               Mean episode length: 168.97
    Episode_Reward/reaching_object: 0.7483
     Episode_Reward/lifting_object: 111.6525
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 2.08s
                      Time elapsed: 00:32:53
                               ETA: 00:41:29

################################################################################
                     [1m Learning iteration 885/2000 [0m                      

                       Computation: 47845 steps/s (collection: 1.932s, learning 0.123s)
             Mean action noise std: 2.13
          Mean value_function loss: 333.6140
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 47.2537
                       Mean reward: 580.96
               Mean episode length: 172.27
    Episode_Reward/reaching_object: 0.7397
     Episode_Reward/lifting_object: 109.9491
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.0417
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 2.05s
                      Time elapsed: 00:32:55
                               ETA: 00:41:26

################################################################################
                     [1m Learning iteration 886/2000 [0m                      

                       Computation: 48281 steps/s (collection: 1.934s, learning 0.102s)
             Mean action noise std: 2.13
          Mean value_function loss: 341.7048
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 47.2561
                       Mean reward: 537.21
               Mean episode length: 160.74
    Episode_Reward/reaching_object: 0.7453
     Episode_Reward/lifting_object: 112.0780
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 2.04s
                      Time elapsed: 00:32:58
                               ETA: 00:41:24

################################################################################
                     [1m Learning iteration 887/2000 [0m                      

                       Computation: 47190 steps/s (collection: 1.957s, learning 0.127s)
             Mean action noise std: 2.13
          Mean value_function loss: 339.7032
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 47.2594
                       Mean reward: 463.97
               Mean episode length: 144.73
    Episode_Reward/reaching_object: 0.7086
     Episode_Reward/lifting_object: 104.3933
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 2.08s
                      Time elapsed: 00:33:00
                               ETA: 00:41:21

################################################################################
                     [1m Learning iteration 888/2000 [0m                      

                       Computation: 47916 steps/s (collection: 1.946s, learning 0.106s)
             Mean action noise std: 2.13
          Mean value_function loss: 319.8763
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 47.2630
                       Mean reward: 555.49
               Mean episode length: 168.74
    Episode_Reward/reaching_object: 0.7525
     Episode_Reward/lifting_object: 112.0352
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 2.05s
                      Time elapsed: 00:33:02
                               ETA: 00:41:19

################################################################################
                     [1m Learning iteration 889/2000 [0m                      

                       Computation: 47928 steps/s (collection: 1.951s, learning 0.100s)
             Mean action noise std: 2.13
          Mean value_function loss: 307.3754
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 47.2711
                       Mean reward: 675.10
               Mean episode length: 194.20
    Episode_Reward/reaching_object: 0.7761
     Episode_Reward/lifting_object: 116.4713
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 2.05s
                      Time elapsed: 00:33:04
                               ETA: 00:41:16

################################################################################
                     [1m Learning iteration 890/2000 [0m                      

                       Computation: 46930 steps/s (collection: 1.994s, learning 0.101s)
             Mean action noise std: 2.13
          Mean value_function loss: 323.6450
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 47.2798
                       Mean reward: 611.85
               Mean episode length: 180.28
    Episode_Reward/reaching_object: 0.7753
     Episode_Reward/lifting_object: 117.2883
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 2.09s
                      Time elapsed: 00:33:06
                               ETA: 00:41:14

################################################################################
                     [1m Learning iteration 891/2000 [0m                      

                       Computation: 47744 steps/s (collection: 1.950s, learning 0.109s)
             Mean action noise std: 2.13
          Mean value_function loss: 319.6882
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 47.2815
                       Mean reward: 567.76
               Mean episode length: 169.65
    Episode_Reward/reaching_object: 0.7759
     Episode_Reward/lifting_object: 116.4123
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 2.06s
                      Time elapsed: 00:33:08
                               ETA: 00:41:12

################################################################################
                     [1m Learning iteration 892/2000 [0m                      

                       Computation: 47732 steps/s (collection: 1.969s, learning 0.090s)
             Mean action noise std: 2.13
          Mean value_function loss: 323.9178
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 47.2821
                       Mean reward: 543.82
               Mean episode length: 164.63
    Episode_Reward/reaching_object: 0.7778
     Episode_Reward/lifting_object: 117.3979
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 2.06s
                      Time elapsed: 00:33:10
                               ETA: 00:41:09

################################################################################
                     [1m Learning iteration 893/2000 [0m                      

                       Computation: 48655 steps/s (collection: 1.911s, learning 0.109s)
             Mean action noise std: 2.13
          Mean value_function loss: 328.4058
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 47.2839
                       Mean reward: 629.48
               Mean episode length: 182.97
    Episode_Reward/reaching_object: 0.7945
     Episode_Reward/lifting_object: 119.3760
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 2.02s
                      Time elapsed: 00:33:12
                               ETA: 00:41:07

################################################################################
                     [1m Learning iteration 894/2000 [0m                      

                       Computation: 47638 steps/s (collection: 1.970s, learning 0.094s)
             Mean action noise std: 2.13
          Mean value_function loss: 337.9436
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 47.2917
                       Mean reward: 622.07
               Mean episode length: 180.50
    Episode_Reward/reaching_object: 0.7842
     Episode_Reward/lifting_object: 118.5063
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 2.06s
                      Time elapsed: 00:33:14
                               ETA: 00:41:04

################################################################################
                     [1m Learning iteration 895/2000 [0m                      

                       Computation: 48040 steps/s (collection: 1.935s, learning 0.112s)
             Mean action noise std: 2.14
          Mean value_function loss: 356.1430
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 47.2960
                       Mean reward: 606.80
               Mean episode length: 175.93
    Episode_Reward/reaching_object: 0.7841
     Episode_Reward/lifting_object: 119.2176
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 2.05s
                      Time elapsed: 00:33:16
                               ETA: 00:41:02

################################################################################
                     [1m Learning iteration 896/2000 [0m                      

                       Computation: 47737 steps/s (collection: 1.952s, learning 0.107s)
             Mean action noise std: 2.14
          Mean value_function loss: 356.8093
               Mean surrogate loss: 0.0100
                 Mean entropy loss: 47.2972
                       Mean reward: 549.59
               Mean episode length: 163.26
    Episode_Reward/reaching_object: 0.7294
     Episode_Reward/lifting_object: 108.8473
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 2.06s
                      Time elapsed: 00:33:18
                               ETA: 00:40:59

################################################################################
                     [1m Learning iteration 897/2000 [0m                      

                       Computation: 48624 steps/s (collection: 1.921s, learning 0.101s)
             Mean action noise std: 2.14
          Mean value_function loss: 327.7328
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 47.2986
                       Mean reward: 590.00
               Mean episode length: 172.64
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 116.4082
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 2.02s
                      Time elapsed: 00:33:20
                               ETA: 00:40:57

################################################################################
                     [1m Learning iteration 898/2000 [0m                      

                       Computation: 48087 steps/s (collection: 1.928s, learning 0.117s)
             Mean action noise std: 2.14
          Mean value_function loss: 348.5349
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 47.3066
                       Mean reward: 595.90
               Mean episode length: 176.16
    Episode_Reward/reaching_object: 0.7392
     Episode_Reward/lifting_object: 109.9170
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.6667
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 2.04s
                      Time elapsed: 00:33:22
                               ETA: 00:40:54

################################################################################
                     [1m Learning iteration 899/2000 [0m                      

                       Computation: 47658 steps/s (collection: 1.958s, learning 0.104s)
             Mean action noise std: 2.14
          Mean value_function loss: 354.5055
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 47.3223
                       Mean reward: 575.73
               Mean episode length: 169.22
    Episode_Reward/reaching_object: 0.7560
     Episode_Reward/lifting_object: 114.9777
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 2.06s
                      Time elapsed: 00:33:24
                               ETA: 00:40:52

################################################################################
                     [1m Learning iteration 900/2000 [0m                      

                       Computation: 48238 steps/s (collection: 1.948s, learning 0.090s)
             Mean action noise std: 2.14
          Mean value_function loss: 382.6943
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 47.3351
                       Mean reward: 555.88
               Mean episode length: 163.96
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 115.7690
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 2.04s
                      Time elapsed: 00:33:26
                               ETA: 00:40:49

################################################################################
                     [1m Learning iteration 901/2000 [0m                      

                       Computation: 46336 steps/s (collection: 2.013s, learning 0.108s)
             Mean action noise std: 2.14
          Mean value_function loss: 370.7952
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 47.3383
                       Mean reward: 582.49
               Mean episode length: 170.35
    Episode_Reward/reaching_object: 0.7196
     Episode_Reward/lifting_object: 109.4052
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 2.12s
                      Time elapsed: 00:33:28
                               ETA: 00:40:47

################################################################################
                     [1m Learning iteration 902/2000 [0m                      

                       Computation: 45907 steps/s (collection: 1.995s, learning 0.147s)
             Mean action noise std: 2.14
          Mean value_function loss: 348.7171
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 47.3428
                       Mean reward: 501.70
               Mean episode length: 149.40
    Episode_Reward/reaching_object: 0.7227
     Episode_Reward/lifting_object: 110.1309
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.0417
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 2.14s
                      Time elapsed: 00:33:31
                               ETA: 00:40:45

################################################################################
                     [1m Learning iteration 903/2000 [0m                      

                       Computation: 46713 steps/s (collection: 1.980s, learning 0.125s)
             Mean action noise std: 2.14
          Mean value_function loss: 352.4311
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 47.3461
                       Mean reward: 631.88
               Mean episode length: 180.69
    Episode_Reward/reaching_object: 0.7601
     Episode_Reward/lifting_object: 117.0260
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 2.10s
                      Time elapsed: 00:33:33
                               ETA: 00:40:42

################################################################################
                     [1m Learning iteration 904/2000 [0m                      

                       Computation: 48080 steps/s (collection: 1.947s, learning 0.097s)
             Mean action noise std: 2.14
          Mean value_function loss: 350.7979
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 47.3478
                       Mean reward: 605.18
               Mean episode length: 175.94
    Episode_Reward/reaching_object: 0.7682
     Episode_Reward/lifting_object: 117.7819
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 2.04s
                      Time elapsed: 00:33:35
                               ETA: 00:40:40

################################################################################
                     [1m Learning iteration 905/2000 [0m                      

                       Computation: 48375 steps/s (collection: 1.942s, learning 0.090s)
             Mean action noise std: 2.14
          Mean value_function loss: 364.1403
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 47.3485
                       Mean reward: 535.08
               Mean episode length: 160.99
    Episode_Reward/reaching_object: 0.7532
     Episode_Reward/lifting_object: 115.6942
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 2.03s
                      Time elapsed: 00:33:37
                               ETA: 00:40:38

################################################################################
                     [1m Learning iteration 906/2000 [0m                      

                       Computation: 48471 steps/s (collection: 1.931s, learning 0.097s)
             Mean action noise std: 2.14
          Mean value_function loss: 401.5652
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.3498
                       Mean reward: 557.05
               Mean episode length: 163.92
    Episode_Reward/reaching_object: 0.7269
     Episode_Reward/lifting_object: 110.2035
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.6667
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 2.03s
                      Time elapsed: 00:33:39
                               ETA: 00:40:35

################################################################################
                     [1m Learning iteration 907/2000 [0m                      

                       Computation: 45100 steps/s (collection: 2.046s, learning 0.134s)
             Mean action noise std: 2.14
          Mean value_function loss: 347.8106
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 47.3593
                       Mean reward: 556.78
               Mean episode length: 163.78
    Episode_Reward/reaching_object: 0.7183
     Episode_Reward/lifting_object: 108.8536
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 2.18s
                      Time elapsed: 00:33:41
                               ETA: 00:40:33

################################################################################
                     [1m Learning iteration 908/2000 [0m                      

                       Computation: 45087 steps/s (collection: 2.033s, learning 0.148s)
             Mean action noise std: 2.14
          Mean value_function loss: 344.7040
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 47.3663
                       Mean reward: 667.03
               Mean episode length: 188.33
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 116.2850
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 2.18s
                      Time elapsed: 00:33:43
                               ETA: 00:40:30

################################################################################
                     [1m Learning iteration 909/2000 [0m                      

                       Computation: 47356 steps/s (collection: 1.979s, learning 0.097s)
             Mean action noise std: 2.14
          Mean value_function loss: 365.0954
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 47.3683
                       Mean reward: 608.77
               Mean episode length: 175.18
    Episode_Reward/reaching_object: 0.7265
     Episode_Reward/lifting_object: 110.1313
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 2.08s
                      Time elapsed: 00:33:45
                               ETA: 00:40:28

################################################################################
                     [1m Learning iteration 910/2000 [0m                      

                       Computation: 48556 steps/s (collection: 1.932s, learning 0.093s)
             Mean action noise std: 2.15
          Mean value_function loss: 364.6610
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 47.3766
                       Mean reward: 581.87
               Mean episode length: 169.35
    Episode_Reward/reaching_object: 0.7150
     Episode_Reward/lifting_object: 108.0229
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 2.02s
                      Time elapsed: 00:33:47
                               ETA: 00:40:26

################################################################################
                     [1m Learning iteration 911/2000 [0m                      

                       Computation: 46953 steps/s (collection: 1.997s, learning 0.097s)
             Mean action noise std: 2.15
          Mean value_function loss: 390.9974
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 47.3913
                       Mean reward: 467.17
               Mean episode length: 142.53
    Episode_Reward/reaching_object: 0.7086
     Episode_Reward/lifting_object: 106.8952
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 16.2917
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 2.09s
                      Time elapsed: 00:33:49
                               ETA: 00:40:23

################################################################################
                     [1m Learning iteration 912/2000 [0m                      

                       Computation: 47227 steps/s (collection: 1.978s, learning 0.104s)
             Mean action noise std: 2.15
          Mean value_function loss: 399.1792
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 47.4024
                       Mean reward: 514.98
               Mean episode length: 154.77
    Episode_Reward/reaching_object: 0.7102
     Episode_Reward/lifting_object: 106.9650
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 2.08s
                      Time elapsed: 00:33:51
                               ETA: 00:40:21

################################################################################
                     [1m Learning iteration 913/2000 [0m                      

                       Computation: 46827 steps/s (collection: 1.984s, learning 0.115s)
             Mean action noise std: 2.15
          Mean value_function loss: 454.7311
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 47.4101
                       Mean reward: 427.01
               Mean episode length: 133.65
    Episode_Reward/reaching_object: 0.6461
     Episode_Reward/lifting_object: 95.3834
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 2.10s
                      Time elapsed: 00:33:53
                               ETA: 00:40:18

################################################################################
                     [1m Learning iteration 914/2000 [0m                      

                       Computation: 47411 steps/s (collection: 1.982s, learning 0.091s)
             Mean action noise std: 2.15
          Mean value_function loss: 436.5452
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 47.4154
                       Mean reward: 438.43
               Mean episode length: 135.23
    Episode_Reward/reaching_object: 0.6414
     Episode_Reward/lifting_object: 94.6919
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 2.07s
                      Time elapsed: 00:33:56
                               ETA: 00:40:16

################################################################################
                     [1m Learning iteration 915/2000 [0m                      

                       Computation: 48677 steps/s (collection: 1.921s, learning 0.098s)
             Mean action noise std: 2.15
          Mean value_function loss: 435.8199
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 47.4184
                       Mean reward: 502.70
               Mean episode length: 152.15
    Episode_Reward/reaching_object: 0.6707
     Episode_Reward/lifting_object: 99.1580
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.9583
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 2.02s
                      Time elapsed: 00:33:58
                               ETA: 00:40:14

################################################################################
                     [1m Learning iteration 916/2000 [0m                      

                       Computation: 46695 steps/s (collection: 1.965s, learning 0.140s)
             Mean action noise std: 2.15
          Mean value_function loss: 435.8359
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 47.4208
                       Mean reward: 442.81
               Mean episode length: 139.76
    Episode_Reward/reaching_object: 0.6478
     Episode_Reward/lifting_object: 94.9647
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.2917
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 2.11s
                      Time elapsed: 00:34:00
                               ETA: 00:40:11

################################################################################
                     [1m Learning iteration 917/2000 [0m                      

                       Computation: 47785 steps/s (collection: 1.948s, learning 0.109s)
             Mean action noise std: 2.15
          Mean value_function loss: 441.4730
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 47.4233
                       Mean reward: 460.08
               Mean episode length: 142.66
    Episode_Reward/reaching_object: 0.6260
     Episode_Reward/lifting_object: 90.0149
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 21.5417
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 2.06s
                      Time elapsed: 00:34:02
                               ETA: 00:40:09

################################################################################
                     [1m Learning iteration 918/2000 [0m                      

                       Computation: 46140 steps/s (collection: 1.989s, learning 0.141s)
             Mean action noise std: 2.15
          Mean value_function loss: 459.3209
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.4300
                       Mean reward: 337.77
               Mean episode length: 113.58
    Episode_Reward/reaching_object: 0.6006
     Episode_Reward/lifting_object: 86.1492
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.7083
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 2.13s
                      Time elapsed: 00:34:04
                               ETA: 00:40:06

################################################################################
                     [1m Learning iteration 919/2000 [0m                      

                       Computation: 46441 steps/s (collection: 1.984s, learning 0.133s)
             Mean action noise std: 2.15
          Mean value_function loss: 413.5213
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 47.4399
                       Mean reward: 542.39
               Mean episode length: 160.25
    Episode_Reward/reaching_object: 0.6303
     Episode_Reward/lifting_object: 92.2518
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.4583
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 2.12s
                      Time elapsed: 00:34:06
                               ETA: 00:40:04

################################################################################
                     [1m Learning iteration 920/2000 [0m                      

                       Computation: 47914 steps/s (collection: 1.955s, learning 0.097s)
             Mean action noise std: 2.15
          Mean value_function loss: 423.3742
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 47.4413
                       Mean reward: 497.92
               Mean episode length: 150.28
    Episode_Reward/reaching_object: 0.6193
     Episode_Reward/lifting_object: 90.5507
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 18.0417
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 2.05s
                      Time elapsed: 00:34:08
                               ETA: 00:40:02

################################################################################
                     [1m Learning iteration 921/2000 [0m                      

                       Computation: 48319 steps/s (collection: 1.941s, learning 0.094s)
             Mean action noise std: 2.15
          Mean value_function loss: 415.5890
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 47.4449
                       Mean reward: 444.88
               Mean episode length: 140.88
    Episode_Reward/reaching_object: 0.6434
     Episode_Reward/lifting_object: 94.0197
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.5833
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 2.03s
                      Time elapsed: 00:34:10
                               ETA: 00:39:59

################################################################################
                     [1m Learning iteration 922/2000 [0m                      

                       Computation: 48119 steps/s (collection: 1.955s, learning 0.088s)
             Mean action noise std: 2.15
          Mean value_function loss: 404.3337
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 47.4480
                       Mean reward: 497.17
               Mean episode length: 149.14
    Episode_Reward/reaching_object: 0.6509
     Episode_Reward/lifting_object: 95.7107
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 2.04s
                      Time elapsed: 00:34:12
                               ETA: 00:39:57

################################################################################
                     [1m Learning iteration 923/2000 [0m                      

                       Computation: 47609 steps/s (collection: 1.958s, learning 0.107s)
             Mean action noise std: 2.15
          Mean value_function loss: 406.3039
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 47.4487
                       Mean reward: 424.25
               Mean episode length: 131.69
    Episode_Reward/reaching_object: 0.6197
     Episode_Reward/lifting_object: 90.5144
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.1250
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 2.06s
                      Time elapsed: 00:34:14
                               ETA: 00:39:54

################################################################################
                     [1m Learning iteration 924/2000 [0m                      

                       Computation: 47630 steps/s (collection: 1.952s, learning 0.112s)
             Mean action noise std: 2.15
          Mean value_function loss: 402.0166
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 47.4501
                       Mean reward: 440.42
               Mean episode length: 134.22
    Episode_Reward/reaching_object: 0.6571
     Episode_Reward/lifting_object: 98.0242
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 16.8750
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 2.06s
                      Time elapsed: 00:34:16
                               ETA: 00:39:52

################################################################################
                     [1m Learning iteration 925/2000 [0m                      

                       Computation: 47415 steps/s (collection: 1.967s, learning 0.106s)
             Mean action noise std: 2.15
          Mean value_function loss: 415.3546
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 47.4534
                       Mean reward: 472.09
               Mean episode length: 141.75
    Episode_Reward/reaching_object: 0.6880
     Episode_Reward/lifting_object: 103.4180
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.9583
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 2.07s
                      Time elapsed: 00:34:18
                               ETA: 00:39:50

################################################################################
                     [1m Learning iteration 926/2000 [0m                      

                       Computation: 45824 steps/s (collection: 2.052s, learning 0.094s)
             Mean action noise std: 2.16
          Mean value_function loss: 447.5043
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 47.4654
                       Mean reward: 484.62
               Mean episode length: 145.00
    Episode_Reward/reaching_object: 0.7155
     Episode_Reward/lifting_object: 109.1527
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 2.15s
                      Time elapsed: 00:34:20
                               ETA: 00:39:47

################################################################################
                     [1m Learning iteration 927/2000 [0m                      

                       Computation: 48493 steps/s (collection: 1.930s, learning 0.098s)
             Mean action noise std: 2.16
          Mean value_function loss: 505.0662
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 47.4766
                       Mean reward: 486.94
               Mean episode length: 145.43
    Episode_Reward/reaching_object: 0.7206
     Episode_Reward/lifting_object: 109.7722
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.9167
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 2.03s
                      Time elapsed: 00:34:22
                               ETA: 00:39:45

################################################################################
                     [1m Learning iteration 928/2000 [0m                      

                       Computation: 48621 steps/s (collection: 1.928s, learning 0.094s)
             Mean action noise std: 2.16
          Mean value_function loss: 437.7539
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 47.4827
                       Mean reward: 552.59
               Mean episode length: 160.05
    Episode_Reward/reaching_object: 0.7191
     Episode_Reward/lifting_object: 110.1548
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.1667
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 2.02s
                      Time elapsed: 00:34:25
                               ETA: 00:39:42

################################################################################
                     [1m Learning iteration 929/2000 [0m                      

                       Computation: 47959 steps/s (collection: 1.949s, learning 0.101s)
             Mean action noise std: 2.16
          Mean value_function loss: 426.0704
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 47.4904
                       Mean reward: 521.85
               Mean episode length: 151.41
    Episode_Reward/reaching_object: 0.6695
     Episode_Reward/lifting_object: 101.4985
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.7500
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 2.05s
                      Time elapsed: 00:34:27
                               ETA: 00:39:40

################################################################################
                     [1m Learning iteration 930/2000 [0m                      

                       Computation: 48245 steps/s (collection: 1.940s, learning 0.098s)
             Mean action noise std: 2.16
          Mean value_function loss: 414.8476
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 47.4978
                       Mean reward: 475.20
               Mean episode length: 141.28
    Episode_Reward/reaching_object: 0.6882
     Episode_Reward/lifting_object: 104.7591
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 2.04s
                      Time elapsed: 00:34:29
                               ETA: 00:39:38

################################################################################
                     [1m Learning iteration 931/2000 [0m                      

                       Computation: 46631 steps/s (collection: 1.975s, learning 0.133s)
             Mean action noise std: 2.16
          Mean value_function loss: 344.5058
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 47.5024
                       Mean reward: 570.34
               Mean episode length: 165.41
    Episode_Reward/reaching_object: 0.7381
     Episode_Reward/lifting_object: 113.3726
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 2.11s
                      Time elapsed: 00:34:31
                               ETA: 00:39:35

################################################################################
                     [1m Learning iteration 932/2000 [0m                      

                       Computation: 47343 steps/s (collection: 1.945s, learning 0.132s)
             Mean action noise std: 2.16
          Mean value_function loss: 336.8961
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 47.5059
                       Mean reward: 535.35
               Mean episode length: 159.42
    Episode_Reward/reaching_object: 0.7345
     Episode_Reward/lifting_object: 112.5938
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 2.08s
                      Time elapsed: 00:34:33
                               ETA: 00:39:33

################################################################################
                     [1m Learning iteration 933/2000 [0m                      

                       Computation: 46968 steps/s (collection: 1.948s, learning 0.145s)
             Mean action noise std: 2.16
          Mean value_function loss: 325.3900
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 47.5090
                       Mean reward: 605.00
               Mean episode length: 173.33
    Episode_Reward/reaching_object: 0.7381
     Episode_Reward/lifting_object: 113.6945
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.3750
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 2.09s
                      Time elapsed: 00:34:35
                               ETA: 00:39:30

################################################################################
                     [1m Learning iteration 934/2000 [0m                      

                       Computation: 46524 steps/s (collection: 1.964s, learning 0.149s)
             Mean action noise std: 2.16
          Mean value_function loss: 365.9491
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 47.5119
                       Mean reward: 530.12
               Mean episode length: 159.95
    Episode_Reward/reaching_object: 0.7116
     Episode_Reward/lifting_object: 107.4651
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 2.11s
                      Time elapsed: 00:34:37
                               ETA: 00:39:28

################################################################################
                     [1m Learning iteration 935/2000 [0m                      

                       Computation: 48062 steps/s (collection: 1.945s, learning 0.101s)
             Mean action noise std: 2.16
          Mean value_function loss: 375.3041
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 47.5182
                       Mean reward: 603.65
               Mean episode length: 175.50
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 118.1573
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 2.05s
                      Time elapsed: 00:34:39
                               ETA: 00:39:26

################################################################################
                     [1m Learning iteration 936/2000 [0m                      

                       Computation: 47671 steps/s (collection: 1.954s, learning 0.109s)
             Mean action noise std: 2.16
          Mean value_function loss: 380.4435
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 47.5247
                       Mean reward: 561.78
               Mean episode length: 163.39
    Episode_Reward/reaching_object: 0.7350
     Episode_Reward/lifting_object: 112.3845
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.4583
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 2.06s
                      Time elapsed: 00:34:41
                               ETA: 00:39:23

################################################################################
                     [1m Learning iteration 937/2000 [0m                      

                       Computation: 46669 steps/s (collection: 1.997s, learning 0.110s)
             Mean action noise std: 2.17
          Mean value_function loss: 376.9751
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 47.5405
                       Mean reward: 561.04
               Mean episode length: 162.31
    Episode_Reward/reaching_object: 0.7823
     Episode_Reward/lifting_object: 121.9578
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 2.11s
                      Time elapsed: 00:34:43
                               ETA: 00:39:21

################################################################################
                     [1m Learning iteration 938/2000 [0m                      

                       Computation: 47038 steps/s (collection: 1.973s, learning 0.117s)
             Mean action noise std: 2.17
          Mean value_function loss: 384.4254
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 47.5557
                       Mean reward: 628.84
               Mean episode length: 180.22
    Episode_Reward/reaching_object: 0.7856
     Episode_Reward/lifting_object: 122.2177
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 2.09s
                      Time elapsed: 00:34:45
                               ETA: 00:39:19

################################################################################
                     [1m Learning iteration 939/2000 [0m                      

                       Computation: 47558 steps/s (collection: 1.961s, learning 0.106s)
             Mean action noise std: 2.17
          Mean value_function loss: 405.2021
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 47.5681
                       Mean reward: 545.46
               Mean episode length: 160.08
    Episode_Reward/reaching_object: 0.7710
     Episode_Reward/lifting_object: 119.4622
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 2.07s
                      Time elapsed: 00:34:47
                               ETA: 00:39:16

################################################################################
                     [1m Learning iteration 940/2000 [0m                      

                       Computation: 47580 steps/s (collection: 1.968s, learning 0.098s)
             Mean action noise std: 2.17
          Mean value_function loss: 379.6829
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 47.5820
                       Mean reward: 531.15
               Mean episode length: 156.82
    Episode_Reward/reaching_object: 0.7344
     Episode_Reward/lifting_object: 112.3620
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 2.07s
                      Time elapsed: 00:34:49
                               ETA: 00:39:14

################################################################################
                     [1m Learning iteration 941/2000 [0m                      

                       Computation: 48129 steps/s (collection: 1.948s, learning 0.095s)
             Mean action noise std: 2.17
          Mean value_function loss: 416.8363
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 47.5873
                       Mean reward: 522.05
               Mean episode length: 155.94
    Episode_Reward/reaching_object: 0.7362
     Episode_Reward/lifting_object: 112.8898
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 2.04s
                      Time elapsed: 00:34:51
                               ETA: 00:39:11

################################################################################
                     [1m Learning iteration 942/2000 [0m                      

                       Computation: 48081 steps/s (collection: 1.949s, learning 0.095s)
             Mean action noise std: 2.17
          Mean value_function loss: 391.7169
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 47.5942
                       Mean reward: 575.71
               Mean episode length: 166.54
    Episode_Reward/reaching_object: 0.7065
     Episode_Reward/lifting_object: 107.4957
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 2.04s
                      Time elapsed: 00:34:54
                               ETA: 00:39:09

################################################################################
                     [1m Learning iteration 943/2000 [0m                      

                       Computation: 47896 steps/s (collection: 1.945s, learning 0.108s)
             Mean action noise std: 2.17
          Mean value_function loss: 386.1467
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 47.6006
                       Mean reward: 569.27
               Mean episode length: 166.19
    Episode_Reward/reaching_object: 0.7159
     Episode_Reward/lifting_object: 108.8479
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 2.05s
                      Time elapsed: 00:34:56
                               ETA: 00:39:06

################################################################################
                     [1m Learning iteration 944/2000 [0m                      

                       Computation: 48542 steps/s (collection: 1.932s, learning 0.094s)
             Mean action noise std: 2.17
          Mean value_function loss: 331.5303
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 47.6120
                       Mean reward: 550.29
               Mean episode length: 160.53
    Episode_Reward/reaching_object: 0.7471
     Episode_Reward/lifting_object: 114.9912
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 2.03s
                      Time elapsed: 00:34:58
                               ETA: 00:39:04

################################################################################
                     [1m Learning iteration 945/2000 [0m                      

                       Computation: 48726 steps/s (collection: 1.926s, learning 0.092s)
             Mean action noise std: 2.18
          Mean value_function loss: 339.1056
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 47.6188
                       Mean reward: 648.01
               Mean episode length: 184.73
    Episode_Reward/reaching_object: 0.7547
     Episode_Reward/lifting_object: 116.4886
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 2.02s
                      Time elapsed: 00:35:00
                               ETA: 00:39:02

################################################################################
                     [1m Learning iteration 946/2000 [0m                      

                       Computation: 46925 steps/s (collection: 2.002s, learning 0.093s)
             Mean action noise std: 2.18
          Mean value_function loss: 345.3644
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 47.6205
                       Mean reward: 607.33
               Mean episode length: 174.21
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 117.2594
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 2.09s
                      Time elapsed: 00:35:02
                               ETA: 00:38:59

################################################################################
                     [1m Learning iteration 947/2000 [0m                      

                       Computation: 48183 steps/s (collection: 1.936s, learning 0.105s)
             Mean action noise std: 2.18
          Mean value_function loss: 345.5541
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.6251
                       Mean reward: 594.03
               Mean episode length: 173.42
    Episode_Reward/reaching_object: 0.8081
     Episode_Reward/lifting_object: 125.0583
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 2.04s
                      Time elapsed: 00:35:04
                               ETA: 00:38:57

################################################################################
                     [1m Learning iteration 948/2000 [0m                      

                       Computation: 47166 steps/s (collection: 1.974s, learning 0.110s)
             Mean action noise std: 2.18
          Mean value_function loss: 322.3823
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 47.6329
                       Mean reward: 682.35
               Mean episode length: 192.32
    Episode_Reward/reaching_object: 0.7905
     Episode_Reward/lifting_object: 122.8488
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 2.08s
                      Time elapsed: 00:35:06
                               ETA: 00:38:54

################################################################################
                     [1m Learning iteration 949/2000 [0m                      

                       Computation: 43855 steps/s (collection: 2.047s, learning 0.194s)
             Mean action noise std: 2.18
          Mean value_function loss: 308.2214
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 47.6388
                       Mean reward: 624.20
               Mean episode length: 179.30
    Episode_Reward/reaching_object: 0.8003
     Episode_Reward/lifting_object: 124.7258
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 2.24s
                      Time elapsed: 00:35:08
                               ETA: 00:38:52

################################################################################
                     [1m Learning iteration 950/2000 [0m                      

                       Computation: 45383 steps/s (collection: 2.040s, learning 0.126s)
             Mean action noise std: 2.18
          Mean value_function loss: 310.2784
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 47.6416
                       Mean reward: 647.61
               Mean episode length: 183.84
    Episode_Reward/reaching_object: 0.8117
     Episode_Reward/lifting_object: 126.3412
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 2.17s
                      Time elapsed: 00:35:10
                               ETA: 00:38:50

################################################################################
                     [1m Learning iteration 951/2000 [0m                      

                       Computation: 45884 steps/s (collection: 1.997s, learning 0.145s)
             Mean action noise std: 2.18
          Mean value_function loss: 337.1086
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 47.6446
                       Mean reward: 635.78
               Mean episode length: 181.79
    Episode_Reward/reaching_object: 0.8019
     Episode_Reward/lifting_object: 124.5184
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 2.14s
                      Time elapsed: 00:35:12
                               ETA: 00:38:48

################################################################################
                     [1m Learning iteration 952/2000 [0m                      

                       Computation: 47272 steps/s (collection: 1.954s, learning 0.125s)
             Mean action noise std: 2.18
          Mean value_function loss: 353.4504
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 47.6450
                       Mean reward: 683.39
               Mean episode length: 192.29
    Episode_Reward/reaching_object: 0.8359
     Episode_Reward/lifting_object: 131.6176
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 2.08s
                      Time elapsed: 00:35:14
                               ETA: 00:38:45

################################################################################
                     [1m Learning iteration 953/2000 [0m                      

                       Computation: 46374 steps/s (collection: 2.001s, learning 0.119s)
             Mean action noise std: 2.18
          Mean value_function loss: 309.2991
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 47.6451
                       Mean reward: 617.64
               Mean episode length: 176.98
    Episode_Reward/reaching_object: 0.7705
     Episode_Reward/lifting_object: 119.3602
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 2.12s
                      Time elapsed: 00:35:17
                               ETA: 00:38:43

################################################################################
                     [1m Learning iteration 954/2000 [0m                      

                       Computation: 47317 steps/s (collection: 1.984s, learning 0.094s)
             Mean action noise std: 2.18
          Mean value_function loss: 316.8411
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 47.6457
                       Mean reward: 587.76
               Mean episode length: 170.71
    Episode_Reward/reaching_object: 0.8013
     Episode_Reward/lifting_object: 124.8411
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 2.08s
                      Time elapsed: 00:35:19
                               ETA: 00:38:41

################################################################################
                     [1m Learning iteration 955/2000 [0m                      

                       Computation: 47825 steps/s (collection: 1.957s, learning 0.099s)
             Mean action noise std: 2.18
          Mean value_function loss: 346.8756
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 47.6469
                       Mean reward: 671.15
               Mean episode length: 189.00
    Episode_Reward/reaching_object: 0.8120
     Episode_Reward/lifting_object: 126.5385
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 2.06s
                      Time elapsed: 00:35:21
                               ETA: 00:38:38

################################################################################
                     [1m Learning iteration 956/2000 [0m                      

                       Computation: 42703 steps/s (collection: 2.100s, learning 0.202s)
             Mean action noise std: 2.18
          Mean value_function loss: 311.6033
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 47.6481
                       Mean reward: 597.49
               Mean episode length: 172.02
    Episode_Reward/reaching_object: 0.8041
     Episode_Reward/lifting_object: 126.0876
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 2.30s
                      Time elapsed: 00:35:23
                               ETA: 00:38:36

################################################################################
                     [1m Learning iteration 957/2000 [0m                      

                       Computation: 45641 steps/s (collection: 2.006s, learning 0.148s)
             Mean action noise std: 2.18
          Mean value_function loss: 315.1583
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.6471
                       Mean reward: 646.52
               Mean episode length: 183.99
    Episode_Reward/reaching_object: 0.8283
     Episode_Reward/lifting_object: 130.3239
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 2.15s
                      Time elapsed: 00:35:25
                               ETA: 00:38:34

################################################################################
                     [1m Learning iteration 958/2000 [0m                      

                       Computation: 47069 steps/s (collection: 1.979s, learning 0.109s)
             Mean action noise std: 2.18
          Mean value_function loss: 315.1478
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 47.6495
                       Mean reward: 669.52
               Mean episode length: 188.95
    Episode_Reward/reaching_object: 0.8126
     Episode_Reward/lifting_object: 127.3258
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 2.09s
                      Time elapsed: 00:35:27
                               ETA: 00:38:31

################################################################################
                     [1m Learning iteration 959/2000 [0m                      

                       Computation: 46908 steps/s (collection: 1.933s, learning 0.163s)
             Mean action noise std: 2.18
          Mean value_function loss: 316.2352
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 47.6532
                       Mean reward: 687.49
               Mean episode length: 192.47
    Episode_Reward/reaching_object: 0.7957
     Episode_Reward/lifting_object: 124.0850
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 2.10s
                      Time elapsed: 00:35:29
                               ETA: 00:38:29

################################################################################
                     [1m Learning iteration 960/2000 [0m                      

                       Computation: 46575 steps/s (collection: 2.017s, learning 0.094s)
             Mean action noise std: 2.18
          Mean value_function loss: 336.9371
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 47.6615
                       Mean reward: 562.30
               Mean episode length: 166.25
    Episode_Reward/reaching_object: 0.8039
     Episode_Reward/lifting_object: 125.7845
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 2.11s
                      Time elapsed: 00:35:31
                               ETA: 00:38:27

################################################################################
                     [1m Learning iteration 961/2000 [0m                      

                       Computation: 41911 steps/s (collection: 2.173s, learning 0.173s)
             Mean action noise std: 2.19
          Mean value_function loss: 360.5924
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 47.6731
                       Mean reward: 605.61
               Mean episode length: 174.79
    Episode_Reward/reaching_object: 0.8112
     Episode_Reward/lifting_object: 127.4946
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 2.35s
                      Time elapsed: 00:35:34
                               ETA: 00:38:25

################################################################################
                     [1m Learning iteration 962/2000 [0m                      

                       Computation: 46489 steps/s (collection: 2.016s, learning 0.099s)
             Mean action noise std: 2.19
          Mean value_function loss: 356.0369
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 47.6835
                       Mean reward: 582.68
               Mean episode length: 170.87
    Episode_Reward/reaching_object: 0.8115
     Episode_Reward/lifting_object: 126.6499
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 2.11s
                      Time elapsed: 00:35:36
                               ETA: 00:38:22

################################################################################
                     [1m Learning iteration 963/2000 [0m                      

                       Computation: 44894 steps/s (collection: 2.055s, learning 0.135s)
             Mean action noise std: 2.19
          Mean value_function loss: 366.5803
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 47.6925
                       Mean reward: 576.62
               Mean episode length: 168.24
    Episode_Reward/reaching_object: 0.7482
     Episode_Reward/lifting_object: 115.6408
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 2.19s
                      Time elapsed: 00:35:38
                               ETA: 00:38:20

################################################################################
                     [1m Learning iteration 964/2000 [0m                      

                       Computation: 46735 steps/s (collection: 1.968s, learning 0.136s)
             Mean action noise std: 2.19
          Mean value_function loss: 335.6296
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 47.7035
                       Mean reward: 613.53
               Mean episode length: 174.47
    Episode_Reward/reaching_object: 0.7500
     Episode_Reward/lifting_object: 116.9358
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.1667
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 2.10s
                      Time elapsed: 00:35:40
                               ETA: 00:38:18

################################################################################
                     [1m Learning iteration 965/2000 [0m                      

                       Computation: 46747 steps/s (collection: 1.975s, learning 0.128s)
             Mean action noise std: 2.19
          Mean value_function loss: 337.1155
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 47.7130
                       Mean reward: 682.54
               Mean episode length: 187.76
    Episode_Reward/reaching_object: 0.8269
     Episode_Reward/lifting_object: 131.2330
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 2.10s
                      Time elapsed: 00:35:42
                               ETA: 00:38:15

################################################################################
                     [1m Learning iteration 966/2000 [0m                      

                       Computation: 46735 steps/s (collection: 2.001s, learning 0.103s)
             Mean action noise std: 2.19
          Mean value_function loss: 355.8607
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 47.7168
                       Mean reward: 630.65
               Mean episode length: 179.36
    Episode_Reward/reaching_object: 0.7986
     Episode_Reward/lifting_object: 125.4551
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 2.10s
                      Time elapsed: 00:35:44
                               ETA: 00:38:13

################################################################################
                     [1m Learning iteration 967/2000 [0m                      

                       Computation: 47380 steps/s (collection: 1.963s, learning 0.112s)
             Mean action noise std: 2.19
          Mean value_function loss: 329.6410
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 47.7212
                       Mean reward: 580.17
               Mean episode length: 168.24
    Episode_Reward/reaching_object: 0.7802
     Episode_Reward/lifting_object: 122.9026
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 2.07s
                      Time elapsed: 00:35:46
                               ETA: 00:38:11

################################################################################
                     [1m Learning iteration 968/2000 [0m                      

                       Computation: 48669 steps/s (collection: 1.928s, learning 0.092s)
             Mean action noise std: 2.19
          Mean value_function loss: 398.2829
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 47.7372
                       Mean reward: 602.22
               Mean episode length: 173.71
    Episode_Reward/reaching_object: 0.8023
     Episode_Reward/lifting_object: 125.4806
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 2.02s
                      Time elapsed: 00:35:49
                               ETA: 00:38:08

################################################################################
                     [1m Learning iteration 969/2000 [0m                      

                       Computation: 47489 steps/s (collection: 1.972s, learning 0.098s)
             Mean action noise std: 2.20
          Mean value_function loss: 367.9674
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 47.7566
                       Mean reward: 645.01
               Mean episode length: 180.96
    Episode_Reward/reaching_object: 0.8244
     Episode_Reward/lifting_object: 130.5396
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 2.07s
                      Time elapsed: 00:35:51
                               ETA: 00:38:06

################################################################################
                     [1m Learning iteration 970/2000 [0m                      

                       Computation: 44279 steps/s (collection: 2.064s, learning 0.156s)
             Mean action noise std: 2.20
          Mean value_function loss: 365.1542
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 47.7631
                       Mean reward: 612.14
               Mean episode length: 174.14
    Episode_Reward/reaching_object: 0.8099
     Episode_Reward/lifting_object: 128.3454
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 2.22s
                      Time elapsed: 00:35:53
                               ETA: 00:38:04

################################################################################
                     [1m Learning iteration 971/2000 [0m                      

                       Computation: 47967 steps/s (collection: 1.953s, learning 0.096s)
             Mean action noise std: 2.20
          Mean value_function loss: 344.6074
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 47.7692
                       Mean reward: 670.27
               Mean episode length: 189.62
    Episode_Reward/reaching_object: 0.8058
     Episode_Reward/lifting_object: 127.3935
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 2.05s
                      Time elapsed: 00:35:55
                               ETA: 00:38:01

################################################################################
                     [1m Learning iteration 972/2000 [0m                      

                       Computation: 47445 steps/s (collection: 1.977s, learning 0.095s)
             Mean action noise std: 2.20
          Mean value_function loss: 332.7568
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 47.7785
                       Mean reward: 544.23
               Mean episode length: 160.96
    Episode_Reward/reaching_object: 0.7736
     Episode_Reward/lifting_object: 121.5817
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 2.07s
                      Time elapsed: 00:35:57
                               ETA: 00:37:59

################################################################################
                     [1m Learning iteration 973/2000 [0m                      

                       Computation: 48429 steps/s (collection: 1.935s, learning 0.095s)
             Mean action noise std: 2.20
          Mean value_function loss: 308.4331
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 47.7911
                       Mean reward: 736.86
               Mean episode length: 202.80
    Episode_Reward/reaching_object: 0.8480
     Episode_Reward/lifting_object: 135.9095
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 2.03s
                      Time elapsed: 00:35:59
                               ETA: 00:37:56

################################################################################
                     [1m Learning iteration 974/2000 [0m                      

                       Computation: 48084 steps/s (collection: 1.944s, learning 0.100s)
             Mean action noise std: 2.20
          Mean value_function loss: 321.8445
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 47.7995
                       Mean reward: 636.09
               Mean episode length: 181.15
    Episode_Reward/reaching_object: 0.8272
     Episode_Reward/lifting_object: 131.6176
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 2.04s
                      Time elapsed: 00:36:01
                               ETA: 00:37:54

################################################################################
                     [1m Learning iteration 975/2000 [0m                      

                       Computation: 47892 steps/s (collection: 1.959s, learning 0.094s)
             Mean action noise std: 2.20
          Mean value_function loss: 322.4403
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 47.8026
                       Mean reward: 577.23
               Mean episode length: 168.69
    Episode_Reward/reaching_object: 0.7942
     Episode_Reward/lifting_object: 125.7333
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 2.05s
                      Time elapsed: 00:36:03
                               ETA: 00:37:52

################################################################################
                     [1m Learning iteration 976/2000 [0m                      

                       Computation: 47551 steps/s (collection: 1.946s, learning 0.122s)
             Mean action noise std: 2.20
          Mean value_function loss: 322.0668
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 47.8071
                       Mean reward: 581.65
               Mean episode length: 167.09
    Episode_Reward/reaching_object: 0.7789
     Episode_Reward/lifting_object: 123.0666
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 2.07s
                      Time elapsed: 00:36:05
                               ETA: 00:37:49

################################################################################
                     [1m Learning iteration 977/2000 [0m                      

                       Computation: 46377 steps/s (collection: 1.974s, learning 0.146s)
             Mean action noise std: 2.21
          Mean value_function loss: 322.6623
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 47.8220
                       Mean reward: 589.44
               Mean episode length: 169.51
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 121.1690
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 2.12s
                      Time elapsed: 00:36:07
                               ETA: 00:37:47

################################################################################
                     [1m Learning iteration 978/2000 [0m                      

                       Computation: 43259 steps/s (collection: 2.105s, learning 0.167s)
             Mean action noise std: 2.21
          Mean value_function loss: 337.9309
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 47.8378
                       Mean reward: 670.72
               Mean episode length: 188.85
    Episode_Reward/reaching_object: 0.8084
     Episode_Reward/lifting_object: 129.0706
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 2.27s
                      Time elapsed: 00:36:10
                               ETA: 00:37:45

################################################################################
                     [1m Learning iteration 979/2000 [0m                      

                       Computation: 43979 steps/s (collection: 2.138s, learning 0.098s)
             Mean action noise std: 2.21
          Mean value_function loss: 389.7076
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 47.8739
                       Mean reward: 660.20
               Mean episode length: 185.28
    Episode_Reward/reaching_object: 0.8200
     Episode_Reward/lifting_object: 131.1105
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 2.24s
                      Time elapsed: 00:36:12
                               ETA: 00:37:43

################################################################################
                     [1m Learning iteration 980/2000 [0m                      

                       Computation: 41904 steps/s (collection: 2.150s, learning 0.196s)
             Mean action noise std: 2.21
          Mean value_function loss: 400.3448
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 47.8994
                       Mean reward: 595.05
               Mean episode length: 169.95
    Episode_Reward/reaching_object: 0.8228
     Episode_Reward/lifting_object: 130.9796
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 2.35s
                      Time elapsed: 00:36:14
                               ETA: 00:37:41

################################################################################
                     [1m Learning iteration 981/2000 [0m                      

                       Computation: 44986 steps/s (collection: 2.026s, learning 0.160s)
             Mean action noise std: 2.22
          Mean value_function loss: 388.6763
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 47.9148
                       Mean reward: 633.46
               Mean episode length: 179.71
    Episode_Reward/reaching_object: 0.8202
     Episode_Reward/lifting_object: 131.1138
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 2.19s
                      Time elapsed: 00:36:16
                               ETA: 00:37:38

################################################################################
                     [1m Learning iteration 982/2000 [0m                      

                       Computation: 46285 steps/s (collection: 2.021s, learning 0.103s)
             Mean action noise std: 2.22
          Mean value_function loss: 364.6545
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 47.9324
                       Mean reward: 557.45
               Mean episode length: 162.83
    Episode_Reward/reaching_object: 0.7819
     Episode_Reward/lifting_object: 123.1659
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 2.12s
                      Time elapsed: 00:36:18
                               ETA: 00:37:36

################################################################################
                     [1m Learning iteration 983/2000 [0m                      

                       Computation: 47131 steps/s (collection: 1.952s, learning 0.134s)
             Mean action noise std: 2.22
          Mean value_function loss: 359.9600
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 47.9576
                       Mean reward: 618.31
               Mean episode length: 177.21
    Episode_Reward/reaching_object: 0.7912
     Episode_Reward/lifting_object: 125.3420
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 2.09s
                      Time elapsed: 00:36:20
                               ETA: 00:37:34

################################################################################
                     [1m Learning iteration 984/2000 [0m                      

                       Computation: 47064 steps/s (collection: 1.988s, learning 0.101s)
             Mean action noise std: 2.22
          Mean value_function loss: 404.8362
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 47.9720
                       Mean reward: 644.93
               Mean episode length: 183.09
    Episode_Reward/reaching_object: 0.7867
     Episode_Reward/lifting_object: 125.1021
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 2.09s
                      Time elapsed: 00:36:23
                               ETA: 00:37:31

################################################################################
                     [1m Learning iteration 985/2000 [0m                      

                       Computation: 45334 steps/s (collection: 2.015s, learning 0.154s)
             Mean action noise std: 2.22
          Mean value_function loss: 394.9275
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 47.9878
                       Mean reward: 669.07
               Mean episode length: 188.51
    Episode_Reward/reaching_object: 0.8262
     Episode_Reward/lifting_object: 132.2594
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 2.17s
                      Time elapsed: 00:36:25
                               ETA: 00:37:29

################################################################################
                     [1m Learning iteration 986/2000 [0m                      

                       Computation: 45849 steps/s (collection: 2.006s, learning 0.138s)
             Mean action noise std: 2.23
          Mean value_function loss: 361.5566
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 48.0055
                       Mean reward: 625.11
               Mean episode length: 176.18
    Episode_Reward/reaching_object: 0.8259
     Episode_Reward/lifting_object: 132.3854
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 2.14s
                      Time elapsed: 00:36:27
                               ETA: 00:37:27

################################################################################
                     [1m Learning iteration 987/2000 [0m                      

                       Computation: 46974 steps/s (collection: 1.967s, learning 0.126s)
             Mean action noise std: 2.23
          Mean value_function loss: 363.4332
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 48.0286
                       Mean reward: 660.11
               Mean episode length: 186.76
    Episode_Reward/reaching_object: 0.8475
     Episode_Reward/lifting_object: 135.2944
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 2.09s
                      Time elapsed: 00:36:29
                               ETA: 00:37:24

################################################################################
                     [1m Learning iteration 988/2000 [0m                      

                       Computation: 46425 steps/s (collection: 1.998s, learning 0.120s)
             Mean action noise std: 2.23
          Mean value_function loss: 288.0632
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 48.0426
                       Mean reward: 664.08
               Mean episode length: 187.56
    Episode_Reward/reaching_object: 0.8618
     Episode_Reward/lifting_object: 139.7008
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 2.12s
                      Time elapsed: 00:36:31
                               ETA: 00:37:22

################################################################################
                     [1m Learning iteration 989/2000 [0m                      

                       Computation: 46807 steps/s (collection: 2.003s, learning 0.098s)
             Mean action noise std: 2.23
          Mean value_function loss: 304.2268
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 48.0592
                       Mean reward: 684.96
               Mean episode length: 191.49
    Episode_Reward/reaching_object: 0.8415
     Episode_Reward/lifting_object: 135.3217
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 2.10s
                      Time elapsed: 00:36:33
                               ETA: 00:37:20

################################################################################
                     [1m Learning iteration 990/2000 [0m                      

                       Computation: 45796 steps/s (collection: 2.049s, learning 0.097s)
             Mean action noise std: 2.23
          Mean value_function loss: 320.8188
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 48.0758
                       Mean reward: 639.02
               Mean episode length: 181.12
    Episode_Reward/reaching_object: 0.8384
     Episode_Reward/lifting_object: 134.4561
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 2.15s
                      Time elapsed: 00:36:35
                               ETA: 00:37:17

################################################################################
                     [1m Learning iteration 991/2000 [0m                      

                       Computation: 47963 steps/s (collection: 1.951s, learning 0.099s)
             Mean action noise std: 2.23
          Mean value_function loss: 313.8450
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 48.0922
                       Mean reward: 773.85
               Mean episode length: 212.31
    Episode_Reward/reaching_object: 0.8793
     Episode_Reward/lifting_object: 142.5069
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 2.05s
                      Time elapsed: 00:36:37
                               ETA: 00:37:15

################################################################################
                     [1m Learning iteration 992/2000 [0m                      

                       Computation: 46768 steps/s (collection: 1.985s, learning 0.117s)
             Mean action noise std: 2.24
          Mean value_function loss: 344.8236
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 48.1126
                       Mean reward: 678.66
               Mean episode length: 190.25
    Episode_Reward/reaching_object: 0.8470
     Episode_Reward/lifting_object: 136.4734
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 2.10s
                      Time elapsed: 00:36:39
                               ETA: 00:37:13

################################################################################
                     [1m Learning iteration 993/2000 [0m                      

                       Computation: 46598 steps/s (collection: 1.943s, learning 0.167s)
             Mean action noise std: 2.24
          Mean value_function loss: 376.4414
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.1272
                       Mean reward: 678.27
               Mean episode length: 188.66
    Episode_Reward/reaching_object: 0.8140
     Episode_Reward/lifting_object: 129.7884
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 2.11s
                      Time elapsed: 00:36:42
                               ETA: 00:37:10

################################################################################
                     [1m Learning iteration 994/2000 [0m                      

                       Computation: 46569 steps/s (collection: 1.980s, learning 0.131s)
             Mean action noise std: 2.24
          Mean value_function loss: 371.0032
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 48.1424
                       Mean reward: 686.89
               Mean episode length: 192.80
    Episode_Reward/reaching_object: 0.8409
     Episode_Reward/lifting_object: 135.0580
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 2.11s
                      Time elapsed: 00:36:44
                               ETA: 00:37:08

################################################################################
                     [1m Learning iteration 995/2000 [0m                      

                       Computation: 47750 steps/s (collection: 1.949s, learning 0.110s)
             Mean action noise std: 2.24
          Mean value_function loss: 316.1111
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 48.1589
                       Mean reward: 712.96
               Mean episode length: 197.87
    Episode_Reward/reaching_object: 0.8576
     Episode_Reward/lifting_object: 137.7343
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 2.06s
                      Time elapsed: 00:36:46
                               ETA: 00:37:06

################################################################################
                     [1m Learning iteration 996/2000 [0m                      

                       Computation: 47580 steps/s (collection: 1.962s, learning 0.104s)
             Mean action noise std: 2.24
          Mean value_function loss: 296.1145
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 48.1733
                       Mean reward: 650.41
               Mean episode length: 182.73
    Episode_Reward/reaching_object: 0.8324
     Episode_Reward/lifting_object: 134.1632
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 2.07s
                      Time elapsed: 00:36:48
                               ETA: 00:37:03

################################################################################
                     [1m Learning iteration 997/2000 [0m                      

                       Computation: 48104 steps/s (collection: 1.943s, learning 0.101s)
             Mean action noise std: 2.25
          Mean value_function loss: 341.7546
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 48.1859
                       Mean reward: 653.34
               Mean episode length: 182.96
    Episode_Reward/reaching_object: 0.8584
     Episode_Reward/lifting_object: 138.0612
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 2.04s
                      Time elapsed: 00:36:50
                               ETA: 00:37:01

################################################################################
                     [1m Learning iteration 998/2000 [0m                      

                       Computation: 47138 steps/s (collection: 1.987s, learning 0.099s)
             Mean action noise std: 2.25
          Mean value_function loss: 261.7783
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 48.1982
                       Mean reward: 694.87
               Mean episode length: 194.22
    Episode_Reward/reaching_object: 0.8526
     Episode_Reward/lifting_object: 136.1578
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 2.09s
                      Time elapsed: 00:36:52
                               ETA: 00:36:59

################################################################################
                     [1m Learning iteration 999/2000 [0m                      

                       Computation: 47744 steps/s (collection: 1.951s, learning 0.108s)
             Mean action noise std: 2.25
          Mean value_function loss: 540.3796
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 48.2074
                       Mean reward: 702.50
               Mean episode length: 196.98
    Episode_Reward/reaching_object: 0.8380
     Episode_Reward/lifting_object: 133.9732
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 2.06s
                      Time elapsed: 00:36:54
                               ETA: 00:36:56

################################################################################
                     [1m Learning iteration 1000/2000 [0m                     

                       Computation: 14461 steps/s (collection: 6.681s, learning 0.117s)
             Mean action noise std: 2.25
          Mean value_function loss: 388.4509
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 48.2178
                       Mean reward: 722.52
               Mean episode length: 199.63
    Episode_Reward/reaching_object: 0.8363
     Episode_Reward/lifting_object: 134.2415
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 6.80s
                      Time elapsed: 00:37:01
                               ETA: 00:36:59

################################################################################
                     [1m Learning iteration 1001/2000 [0m                     

                       Computation: 14118 steps/s (collection: 6.760s, learning 0.203s)
             Mean action noise std: 2.25
          Mean value_function loss: 271.3224
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 48.2380
                       Mean reward: 682.00
               Mean episode length: 189.48
    Episode_Reward/reaching_object: 0.8555
     Episode_Reward/lifting_object: 138.2262
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 6.96s
                      Time elapsed: 00:37:08
                               ETA: 00:37:01

################################################################################
                     [1m Learning iteration 1002/2000 [0m                     

                       Computation: 13968 steps/s (collection: 6.871s, learning 0.167s)
             Mean action noise std: 2.25
          Mean value_function loss: 289.8115
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 48.2532
                       Mean reward: 620.66
               Mean episode length: 178.16
    Episode_Reward/reaching_object: 0.8604
     Episode_Reward/lifting_object: 139.0620
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 7.04s
                      Time elapsed: 00:37:15
                               ETA: 00:37:04

################################################################################
                     [1m Learning iteration 1003/2000 [0m                     

                       Computation: 14027 steps/s (collection: 6.899s, learning 0.109s)
             Mean action noise std: 2.25
          Mean value_function loss: 307.9335
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.2598
                       Mean reward: 637.92
               Mean episode length: 180.76
    Episode_Reward/reaching_object: 0.8345
     Episode_Reward/lifting_object: 133.9666
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 7.01s
                      Time elapsed: 00:37:22
                               ETA: 00:37:06

################################################################################
                     [1m Learning iteration 1004/2000 [0m                     

                       Computation: 14620 steps/s (collection: 6.593s, learning 0.131s)
             Mean action noise std: 2.26
          Mean value_function loss: 291.4377
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 48.2661
                       Mean reward: 715.42
               Mean episode length: 197.73
    Episode_Reward/reaching_object: 0.8578
     Episode_Reward/lifting_object: 138.9278
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 6.72s
                      Time elapsed: 00:37:29
                               ETA: 00:37:08

################################################################################
                     [1m Learning iteration 1005/2000 [0m                     

                       Computation: 14468 steps/s (collection: 6.682s, learning 0.112s)
             Mean action noise std: 2.26
          Mean value_function loss: 296.0689
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 48.2864
                       Mean reward: 755.69
               Mean episode length: 207.02
    Episode_Reward/reaching_object: 0.8484
     Episode_Reward/lifting_object: 136.8141
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 6.79s
                      Time elapsed: 00:37:35
                               ETA: 00:37:11

################################################################################
                     [1m Learning iteration 1006/2000 [0m                     

                       Computation: 13929 steps/s (collection: 6.936s, learning 0.121s)
             Mean action noise std: 2.26
          Mean value_function loss: 293.6714
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 48.3102
                       Mean reward: 677.25
               Mean episode length: 190.80
    Episode_Reward/reaching_object: 0.8509
     Episode_Reward/lifting_object: 137.1907
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 7.06s
                      Time elapsed: 00:37:42
                               ETA: 00:37:13

################################################################################
                     [1m Learning iteration 1007/2000 [0m                     

                       Computation: 14416 steps/s (collection: 6.701s, learning 0.118s)
             Mean action noise std: 2.26
          Mean value_function loss: 312.3284
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 48.3195
                       Mean reward: 621.16
               Mean episode length: 177.67
    Episode_Reward/reaching_object: 0.8570
     Episode_Reward/lifting_object: 138.5224
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 6.82s
                      Time elapsed: 00:37:49
                               ETA: 00:37:15

################################################################################
                     [1m Learning iteration 1008/2000 [0m                     

                       Computation: 16537 steps/s (collection: 5.843s, learning 0.101s)
             Mean action noise std: 2.27
          Mean value_function loss: 489.3451
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 48.3311
                       Mean reward: 682.69
               Mean episode length: 193.26
    Episode_Reward/reaching_object: 0.8525
     Episode_Reward/lifting_object: 136.3705
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 5.94s
                      Time elapsed: 00:37:55
                               ETA: 00:37:17

################################################################################
                     [1m Learning iteration 1009/2000 [0m                     

                       Computation: 50074 steps/s (collection: 1.848s, learning 0.115s)
             Mean action noise std: 2.27
          Mean value_function loss: 318.2966
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 48.3576
                       Mean reward: 676.56
               Mean episode length: 190.42
    Episode_Reward/reaching_object: 0.8424
     Episode_Reward/lifting_object: 135.5866
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 1.96s
                      Time elapsed: 00:37:57
                               ETA: 00:37:14

################################################################################
                     [1m Learning iteration 1010/2000 [0m                     

                       Computation: 49967 steps/s (collection: 1.846s, learning 0.122s)
             Mean action noise std: 2.27
          Mean value_function loss: 278.4598
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.3722
                       Mean reward: 641.26
               Mean episode length: 183.78
    Episode_Reward/reaching_object: 0.8513
     Episode_Reward/lifting_object: 136.6110
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 1.97s
                      Time elapsed: 00:37:59
                               ETA: 00:37:12

################################################################################
                     [1m Learning iteration 1011/2000 [0m                     

                       Computation: 46059 steps/s (collection: 1.992s, learning 0.142s)
             Mean action noise std: 2.27
          Mean value_function loss: 315.2115
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 48.3815
                       Mean reward: 670.29
               Mean episode length: 188.51
    Episode_Reward/reaching_object: 0.8574
     Episode_Reward/lifting_object: 138.3449
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 2.13s
                      Time elapsed: 00:38:01
                               ETA: 00:37:09

################################################################################
                     [1m Learning iteration 1012/2000 [0m                     

                       Computation: 48715 steps/s (collection: 1.927s, learning 0.091s)
             Mean action noise std: 2.27
          Mean value_function loss: 358.5136
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 48.4034
                       Mean reward: 699.03
               Mean episode length: 197.18
    Episode_Reward/reaching_object: 0.8520
     Episode_Reward/lifting_object: 136.4400
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 2.02s
                      Time elapsed: 00:38:03
                               ETA: 00:37:07

################################################################################
                     [1m Learning iteration 1013/2000 [0m                     

                       Computation: 50787 steps/s (collection: 1.810s, learning 0.126s)
             Mean action noise std: 2.28
          Mean value_function loss: 305.2204
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 48.4298
                       Mean reward: 710.22
               Mean episode length: 198.01
    Episode_Reward/reaching_object: 0.8668
     Episode_Reward/lifting_object: 139.8998
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 1.94s
                      Time elapsed: 00:38:05
                               ETA: 00:37:04

################################################################################
                     [1m Learning iteration 1014/2000 [0m                     

                       Computation: 51343 steps/s (collection: 1.822s, learning 0.093s)
             Mean action noise std: 2.28
          Mean value_function loss: 287.0895
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 48.4376
                       Mean reward: 686.97
               Mean episode length: 191.87
    Episode_Reward/reaching_object: 0.8701
     Episode_Reward/lifting_object: 140.2946
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 1.91s
                      Time elapsed: 00:38:07
                               ETA: 00:37:02

################################################################################
                     [1m Learning iteration 1015/2000 [0m                     

                       Computation: 51724 steps/s (collection: 1.802s, learning 0.098s)
             Mean action noise std: 2.28
          Mean value_function loss: 271.6796
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 48.4412
                       Mean reward: 740.85
               Mean episode length: 204.48
    Episode_Reward/reaching_object: 0.8518
     Episode_Reward/lifting_object: 137.5320
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 1.90s
                      Time elapsed: 00:38:09
                               ETA: 00:36:59

################################################################################
                     [1m Learning iteration 1016/2000 [0m                     

                       Computation: 51389 steps/s (collection: 1.820s, learning 0.093s)
             Mean action noise std: 2.28
          Mean value_function loss: 315.7392
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 48.4548
                       Mean reward: 743.38
               Mean episode length: 206.51
    Episode_Reward/reaching_object: 0.8673
     Episode_Reward/lifting_object: 140.4116
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 1.91s
                      Time elapsed: 00:38:11
                               ETA: 00:36:57

################################################################################
                     [1m Learning iteration 1017/2000 [0m                     

                       Computation: 50778 steps/s (collection: 1.850s, learning 0.086s)
             Mean action noise std: 2.28
          Mean value_function loss: 307.4130
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 48.4739
                       Mean reward: 668.38
               Mean episode length: 187.27
    Episode_Reward/reaching_object: 0.8364
     Episode_Reward/lifting_object: 134.7798
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 1.94s
                      Time elapsed: 00:38:13
                               ETA: 00:36:54

################################################################################
                     [1m Learning iteration 1018/2000 [0m                     

                       Computation: 50497 steps/s (collection: 1.841s, learning 0.106s)
             Mean action noise std: 2.28
          Mean value_function loss: 331.7462
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 48.4793
                       Mean reward: 652.02
               Mean episode length: 183.71
    Episode_Reward/reaching_object: 0.8564
     Episode_Reward/lifting_object: 138.6070
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 1.95s
                      Time elapsed: 00:38:15
                               ETA: 00:36:51

################################################################################
                     [1m Learning iteration 1019/2000 [0m                     

                       Computation: 50207 steps/s (collection: 1.864s, learning 0.094s)
             Mean action noise std: 2.29
          Mean value_function loss: 248.3581
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 48.4972
                       Mean reward: 745.38
               Mean episode length: 206.49
    Episode_Reward/reaching_object: 0.8708
     Episode_Reward/lifting_object: 140.9005
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 1.96s
                      Time elapsed: 00:38:17
                               ETA: 00:36:49

################################################################################
                     [1m Learning iteration 1020/2000 [0m                     

                       Computation: 49677 steps/s (collection: 1.854s, learning 0.125s)
             Mean action noise std: 2.29
          Mean value_function loss: 272.5616
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 48.5191
                       Mean reward: 690.15
               Mean episode length: 192.34
    Episode_Reward/reaching_object: 0.8621
     Episode_Reward/lifting_object: 139.6719
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 1.98s
                      Time elapsed: 00:38:19
                               ETA: 00:36:46

################################################################################
                     [1m Learning iteration 1021/2000 [0m                     

                       Computation: 50477 steps/s (collection: 1.856s, learning 0.092s)
             Mean action noise std: 2.29
          Mean value_function loss: 282.9530
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 48.5262
                       Mean reward: 654.34
               Mean episode length: 185.24
    Episode_Reward/reaching_object: 0.8633
     Episode_Reward/lifting_object: 139.5703
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 1.95s
                      Time elapsed: 00:38:21
                               ETA: 00:36:44

################################################################################
                     [1m Learning iteration 1022/2000 [0m                     

                       Computation: 48797 steps/s (collection: 1.909s, learning 0.105s)
             Mean action noise std: 2.29
          Mean value_function loss: 279.7782
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 48.5364
                       Mean reward: 747.56
               Mean episode length: 204.92
    Episode_Reward/reaching_object: 0.8521
     Episode_Reward/lifting_object: 138.1850
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 2.01s
                      Time elapsed: 00:38:23
                               ETA: 00:36:41

################################################################################
                     [1m Learning iteration 1023/2000 [0m                     

                       Computation: 47910 steps/s (collection: 1.929s, learning 0.123s)
             Mean action noise std: 2.29
          Mean value_function loss: 274.0800
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 48.5519
                       Mean reward: 655.98
               Mean episode length: 185.06
    Episode_Reward/reaching_object: 0.8720
     Episode_Reward/lifting_object: 141.9016
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 2.05s
                      Time elapsed: 00:38:25
                               ETA: 00:36:39

################################################################################
                     [1m Learning iteration 1024/2000 [0m                     

                       Computation: 49067 steps/s (collection: 1.896s, learning 0.107s)
             Mean action noise std: 2.29
          Mean value_function loss: 266.8087
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.5679
                       Mean reward: 679.24
               Mean episode length: 190.19
    Episode_Reward/reaching_object: 0.9013
     Episode_Reward/lifting_object: 146.7363
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 2.00s
                      Time elapsed: 00:38:27
                               ETA: 00:36:36

################################################################################
                     [1m Learning iteration 1025/2000 [0m                     

                       Computation: 50240 steps/s (collection: 1.859s, learning 0.098s)
             Mean action noise std: 2.30
          Mean value_function loss: 290.1960
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.5832
                       Mean reward: 679.84
               Mean episode length: 189.24
    Episode_Reward/reaching_object: 0.8819
     Episode_Reward/lifting_object: 143.8680
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 1.96s
                      Time elapsed: 00:38:29
                               ETA: 00:36:34

################################################################################
                     [1m Learning iteration 1026/2000 [0m                     

                       Computation: 49817 steps/s (collection: 1.870s, learning 0.103s)
             Mean action noise std: 2.30
          Mean value_function loss: 313.1854
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.5964
                       Mean reward: 718.99
               Mean episode length: 200.00
    Episode_Reward/reaching_object: 0.8578
     Episode_Reward/lifting_object: 139.0443
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 1.97s
                      Time elapsed: 00:38:31
                               ETA: 00:36:31

################################################################################
                     [1m Learning iteration 1027/2000 [0m                     

                       Computation: 49958 steps/s (collection: 1.873s, learning 0.095s)
             Mean action noise std: 2.30
          Mean value_function loss: 262.7133
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 48.6108
                       Mean reward: 695.03
               Mean episode length: 192.54
    Episode_Reward/reaching_object: 0.8734
     Episode_Reward/lifting_object: 141.9910
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 1.97s
                      Time elapsed: 00:38:33
                               ETA: 00:36:29

################################################################################
                     [1m Learning iteration 1028/2000 [0m                     

                       Computation: 50061 steps/s (collection: 1.878s, learning 0.086s)
             Mean action noise std: 2.30
          Mean value_function loss: 303.9418
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 48.6287
                       Mean reward: 686.11
               Mean episode length: 190.31
    Episode_Reward/reaching_object: 0.8444
     Episode_Reward/lifting_object: 136.2271
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 1.96s
                      Time elapsed: 00:38:35
                               ETA: 00:36:26

################################################################################
                     [1m Learning iteration 1029/2000 [0m                     

                       Computation: 49966 steps/s (collection: 1.855s, learning 0.113s)
             Mean action noise std: 2.30
          Mean value_function loss: 309.1886
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.6405
                       Mean reward: 649.97
               Mean episode length: 182.06
    Episode_Reward/reaching_object: 0.8555
     Episode_Reward/lifting_object: 138.5773
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 1.97s
                      Time elapsed: 00:38:37
                               ETA: 00:36:24

################################################################################
                     [1m Learning iteration 1030/2000 [0m                     

                       Computation: 51358 steps/s (collection: 1.826s, learning 0.089s)
             Mean action noise std: 2.30
          Mean value_function loss: 332.0785
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.6551
                       Mean reward: 674.58
               Mean episode length: 188.27
    Episode_Reward/reaching_object: 0.8830
     Episode_Reward/lifting_object: 144.3210
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 1.91s
                      Time elapsed: 00:38:38
                               ETA: 00:36:21

################################################################################
                     [1m Learning iteration 1031/2000 [0m                     

                       Computation: 50035 steps/s (collection: 1.871s, learning 0.094s)
             Mean action noise std: 2.30
          Mean value_function loss: 278.7913
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 48.6706
                       Mean reward: 796.47
               Mean episode length: 218.45
    Episode_Reward/reaching_object: 0.8949
     Episode_Reward/lifting_object: 146.2662
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 1.96s
                      Time elapsed: 00:38:40
                               ETA: 00:36:19

################################################################################
                     [1m Learning iteration 1032/2000 [0m                     

                       Computation: 47475 steps/s (collection: 1.956s, learning 0.115s)
             Mean action noise std: 2.31
          Mean value_function loss: 277.7609
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 48.6787
                       Mean reward: 769.87
               Mean episode length: 210.79
    Episode_Reward/reaching_object: 0.9014
     Episode_Reward/lifting_object: 147.4079
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 2.07s
                      Time elapsed: 00:38:43
                               ETA: 00:36:16

################################################################################
                     [1m Learning iteration 1033/2000 [0m                     

                       Computation: 48749 steps/s (collection: 1.909s, learning 0.107s)
             Mean action noise std: 2.31
          Mean value_function loss: 282.5976
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 48.6904
                       Mean reward: 730.15
               Mean episode length: 201.49
    Episode_Reward/reaching_object: 0.8868
     Episode_Reward/lifting_object: 144.3853
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 2.02s
                      Time elapsed: 00:38:45
                               ETA: 00:36:14

################################################################################
                     [1m Learning iteration 1034/2000 [0m                     

                       Computation: 46701 steps/s (collection: 2.010s, learning 0.095s)
             Mean action noise std: 2.31
          Mean value_function loss: 256.5600
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 48.7041
                       Mean reward: 810.37
               Mean episode length: 220.81
    Episode_Reward/reaching_object: 0.8722
     Episode_Reward/lifting_object: 141.8181
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 2.10s
                      Time elapsed: 00:38:47
                               ETA: 00:36:12

################################################################################
                     [1m Learning iteration 1035/2000 [0m                     

                       Computation: 47965 steps/s (collection: 1.956s, learning 0.093s)
             Mean action noise std: 2.31
          Mean value_function loss: 278.7470
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 48.7185
                       Mean reward: 698.55
               Mean episode length: 194.68
    Episode_Reward/reaching_object: 0.8701
     Episode_Reward/lifting_object: 141.7689
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 2.05s
                      Time elapsed: 00:38:49
                               ETA: 00:36:09

################################################################################
                     [1m Learning iteration 1036/2000 [0m                     

                       Computation: 51675 steps/s (collection: 1.807s, learning 0.096s)
             Mean action noise std: 2.31
          Mean value_function loss: 295.7564
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.7313
                       Mean reward: 698.25
               Mean episode length: 195.84
    Episode_Reward/reaching_object: 0.8930
     Episode_Reward/lifting_object: 145.1036
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 1.90s
                      Time elapsed: 00:38:51
                               ETA: 00:36:07

################################################################################
                     [1m Learning iteration 1037/2000 [0m                     

                       Computation: 50896 steps/s (collection: 1.832s, learning 0.099s)
             Mean action noise std: 2.31
          Mean value_function loss: 234.4306
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 48.7464
                       Mean reward: 752.91
               Mean episode length: 207.20
    Episode_Reward/reaching_object: 0.9006
     Episode_Reward/lifting_object: 147.5917
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 1.93s
                      Time elapsed: 00:38:53
                               ETA: 00:36:04

################################################################################
                     [1m Learning iteration 1038/2000 [0m                     

                       Computation: 51465 steps/s (collection: 1.799s, learning 0.111s)
             Mean action noise std: 2.32
          Mean value_function loss: 294.6577
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 48.7594
                       Mean reward: 772.27
               Mean episode length: 211.01
    Episode_Reward/reaching_object: 0.9115
     Episode_Reward/lifting_object: 149.3782
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 1.91s
                      Time elapsed: 00:38:54
                               ETA: 00:36:01

################################################################################
                     [1m Learning iteration 1039/2000 [0m                     

                       Computation: 50420 steps/s (collection: 1.842s, learning 0.108s)
             Mean action noise std: 2.32
          Mean value_function loss: 273.2617
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.7682
                       Mean reward: 717.33
               Mean episode length: 199.47
    Episode_Reward/reaching_object: 0.8789
     Episode_Reward/lifting_object: 142.2523
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 1.95s
                      Time elapsed: 00:38:56
                               ETA: 00:35:59

################################################################################
                     [1m Learning iteration 1040/2000 [0m                     

                       Computation: 50501 steps/s (collection: 1.851s, learning 0.096s)
             Mean action noise std: 2.32
          Mean value_function loss: 244.8001
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 48.7870
                       Mean reward: 749.94
               Mean episode length: 206.62
    Episode_Reward/reaching_object: 0.8734
     Episode_Reward/lifting_object: 142.1379
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 1.95s
                      Time elapsed: 00:38:58
                               ETA: 00:35:56

################################################################################
                     [1m Learning iteration 1041/2000 [0m                     

                       Computation: 50230 steps/s (collection: 1.868s, learning 0.090s)
             Mean action noise std: 2.32
          Mean value_function loss: 244.1586
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 48.8189
                       Mean reward: 607.58
               Mean episode length: 173.58
    Episode_Reward/reaching_object: 0.8808
     Episode_Reward/lifting_object: 143.4231
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 1.96s
                      Time elapsed: 00:39:00
                               ETA: 00:35:54

################################################################################
                     [1m Learning iteration 1042/2000 [0m                     

                       Computation: 50102 steps/s (collection: 1.872s, learning 0.090s)
             Mean action noise std: 2.33
          Mean value_function loss: 244.8572
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 48.8472
                       Mean reward: 699.29
               Mean episode length: 193.56
    Episode_Reward/reaching_object: 0.9212
     Episode_Reward/lifting_object: 151.0324
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 1.96s
                      Time elapsed: 00:39:02
                               ETA: 00:35:51

################################################################################
                     [1m Learning iteration 1043/2000 [0m                     

                       Computation: 51357 steps/s (collection: 1.825s, learning 0.089s)
             Mean action noise std: 2.33
          Mean value_function loss: 273.2852
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 48.8634
                       Mean reward: 805.27
               Mean episode length: 219.30
    Episode_Reward/reaching_object: 0.9058
     Episode_Reward/lifting_object: 148.2598
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 1.91s
                      Time elapsed: 00:39:04
                               ETA: 00:35:49

################################################################################
                     [1m Learning iteration 1044/2000 [0m                     

                       Computation: 49973 steps/s (collection: 1.880s, learning 0.088s)
             Mean action noise std: 2.33
          Mean value_function loss: 292.7302
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 48.8944
                       Mean reward: 683.67
               Mean episode length: 192.12
    Episode_Reward/reaching_object: 0.8835
     Episode_Reward/lifting_object: 144.0465
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 1.97s
                      Time elapsed: 00:39:06
                               ETA: 00:35:46

################################################################################
                     [1m Learning iteration 1045/2000 [0m                     

                       Computation: 50777 steps/s (collection: 1.849s, learning 0.087s)
             Mean action noise std: 2.33
          Mean value_function loss: 273.1166
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 48.9237
                       Mean reward: 714.23
               Mean episode length: 198.32
    Episode_Reward/reaching_object: 0.8537
     Episode_Reward/lifting_object: 137.8856
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 1.94s
                      Time elapsed: 00:39:08
                               ETA: 00:35:44

################################################################################
                     [1m Learning iteration 1046/2000 [0m                     

                       Computation: 51624 steps/s (collection: 1.809s, learning 0.096s)
             Mean action noise std: 2.34
          Mean value_function loss: 240.6537
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 48.9472
                       Mean reward: 698.70
               Mean episode length: 194.81
    Episode_Reward/reaching_object: 0.8956
     Episode_Reward/lifting_object: 146.1306
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 1.90s
                      Time elapsed: 00:39:10
                               ETA: 00:35:41

################################################################################
                     [1m Learning iteration 1047/2000 [0m                     

                       Computation: 51077 steps/s (collection: 1.840s, learning 0.085s)
             Mean action noise std: 2.34
          Mean value_function loss: 232.9704
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.9607
                       Mean reward: 755.62
               Mean episode length: 207.19
    Episode_Reward/reaching_object: 0.8819
     Episode_Reward/lifting_object: 143.9031
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 1.92s
                      Time elapsed: 00:39:12
                               ETA: 00:35:39

################################################################################
                     [1m Learning iteration 1048/2000 [0m                     

                       Computation: 50358 steps/s (collection: 1.865s, learning 0.087s)
             Mean action noise std: 2.34
          Mean value_function loss: 232.4327
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.9762
                       Mean reward: 748.92
               Mean episode length: 207.82
    Episode_Reward/reaching_object: 0.9099
     Episode_Reward/lifting_object: 148.8036
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 1.95s
                      Time elapsed: 00:39:14
                               ETA: 00:35:36

################################################################################
                     [1m Learning iteration 1049/2000 [0m                     

                       Computation: 50923 steps/s (collection: 1.823s, learning 0.107s)
             Mean action noise std: 2.34
          Mean value_function loss: 228.8017
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 48.9923
                       Mean reward: 690.59
               Mean episode length: 192.68
    Episode_Reward/reaching_object: 0.9023
     Episode_Reward/lifting_object: 146.9531
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 1.93s
                      Time elapsed: 00:39:16
                               ETA: 00:35:34

################################################################################
                     [1m Learning iteration 1050/2000 [0m                     

                       Computation: 51060 steps/s (collection: 1.823s, learning 0.102s)
             Mean action noise std: 2.34
          Mean value_function loss: 226.0571
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 49.0047
                       Mean reward: 813.39
               Mean episode length: 220.93
    Episode_Reward/reaching_object: 0.8955
     Episode_Reward/lifting_object: 146.5605
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 1.93s
                      Time elapsed: 00:39:18
                               ETA: 00:35:31

################################################################################
                     [1m Learning iteration 1051/2000 [0m                     

                       Computation: 49104 steps/s (collection: 1.881s, learning 0.121s)
             Mean action noise std: 2.35
          Mean value_function loss: 220.2016
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 49.0210
                       Mean reward: 790.74
               Mean episode length: 214.83
    Episode_Reward/reaching_object: 0.9101
     Episode_Reward/lifting_object: 149.1639
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 2.00s
                      Time elapsed: 00:39:20
                               ETA: 00:35:29

################################################################################
                     [1m Learning iteration 1052/2000 [0m                     

                       Computation: 50174 steps/s (collection: 1.839s, learning 0.120s)
             Mean action noise std: 2.35
          Mean value_function loss: 274.1125
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 49.0463
                       Mean reward: 815.16
               Mean episode length: 220.87
    Episode_Reward/reaching_object: 0.9431
     Episode_Reward/lifting_object: 154.7132
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 1.96s
                      Time elapsed: 00:39:22
                               ETA: 00:35:26

################################################################################
                     [1m Learning iteration 1053/2000 [0m                     

                       Computation: 49737 steps/s (collection: 1.842s, learning 0.134s)
             Mean action noise std: 2.35
          Mean value_function loss: 260.8263
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 49.0708
                       Mean reward: 725.57
               Mean episode length: 200.88
    Episode_Reward/reaching_object: 0.8721
     Episode_Reward/lifting_object: 141.9740
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 1.98s
                      Time elapsed: 00:39:24
                               ETA: 00:35:24

################################################################################
                     [1m Learning iteration 1054/2000 [0m                     

                       Computation: 50349 steps/s (collection: 1.859s, learning 0.093s)
             Mean action noise std: 2.36
          Mean value_function loss: 261.6035
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 49.1023
                       Mean reward: 697.68
               Mean episode length: 195.35
    Episode_Reward/reaching_object: 0.9017
     Episode_Reward/lifting_object: 147.0798
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 1.95s
                      Time elapsed: 00:39:26
                               ETA: 00:35:21

################################################################################
                     [1m Learning iteration 1055/2000 [0m                     

                       Computation: 49430 steps/s (collection: 1.877s, learning 0.112s)
             Mean action noise std: 2.36
          Mean value_function loss: 270.0640
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 49.1234
                       Mean reward: 734.93
               Mean episode length: 201.72
    Episode_Reward/reaching_object: 0.8800
     Episode_Reward/lifting_object: 143.5792
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 1.99s
                      Time elapsed: 00:39:28
                               ETA: 00:35:19

################################################################################
                     [1m Learning iteration 1056/2000 [0m                     

                       Computation: 48544 steps/s (collection: 1.896s, learning 0.130s)
             Mean action noise std: 2.36
          Mean value_function loss: 268.2597
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 49.1408
                       Mean reward: 749.10
               Mean episode length: 204.53
    Episode_Reward/reaching_object: 0.8917
     Episode_Reward/lifting_object: 145.6320
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 2.03s
                      Time elapsed: 00:39:30
                               ETA: 00:35:16

################################################################################
                     [1m Learning iteration 1057/2000 [0m                     

                       Computation: 50953 steps/s (collection: 1.835s, learning 0.094s)
             Mean action noise std: 2.36
          Mean value_function loss: 229.2031
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 49.1604
                       Mean reward: 723.07
               Mean episode length: 199.10
    Episode_Reward/reaching_object: 0.9086
     Episode_Reward/lifting_object: 148.4164
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 1.93s
                      Time elapsed: 00:39:32
                               ETA: 00:35:14

################################################################################
                     [1m Learning iteration 1058/2000 [0m                     

                       Computation: 50340 steps/s (collection: 1.843s, learning 0.109s)
             Mean action noise std: 2.36
          Mean value_function loss: 234.1194
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 49.1658
                       Mean reward: 733.26
               Mean episode length: 204.51
    Episode_Reward/reaching_object: 0.9170
     Episode_Reward/lifting_object: 150.1226
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 1.95s
                      Time elapsed: 00:39:33
                               ETA: 00:35:11

################################################################################
                     [1m Learning iteration 1059/2000 [0m                     

                       Computation: 50976 steps/s (collection: 1.841s, learning 0.087s)
             Mean action noise std: 2.36
          Mean value_function loss: 239.8263
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 49.1766
                       Mean reward: 719.99
               Mean episode length: 199.22
    Episode_Reward/reaching_object: 0.9324
     Episode_Reward/lifting_object: 152.4072
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 1.93s
                      Time elapsed: 00:39:35
                               ETA: 00:35:09

################################################################################
                     [1m Learning iteration 1060/2000 [0m                     

                       Computation: 49113 steps/s (collection: 1.882s, learning 0.120s)
             Mean action noise std: 2.37
          Mean value_function loss: 248.9732
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 49.1909
                       Mean reward: 786.36
               Mean episode length: 214.05
    Episode_Reward/reaching_object: 0.9013
     Episode_Reward/lifting_object: 147.1595
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 2.00s
                      Time elapsed: 00:39:37
                               ETA: 00:35:06

################################################################################
                     [1m Learning iteration 1061/2000 [0m                     

                       Computation: 48788 steps/s (collection: 1.929s, learning 0.086s)
             Mean action noise std: 2.37
          Mean value_function loss: 255.8647
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.2036
                       Mean reward: 680.66
               Mean episode length: 190.49
    Episode_Reward/reaching_object: 0.8712
     Episode_Reward/lifting_object: 141.3556
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 2.01s
                      Time elapsed: 00:39:39
                               ETA: 00:35:04

################################################################################
                     [1m Learning iteration 1062/2000 [0m                     

                       Computation: 50427 steps/s (collection: 1.860s, learning 0.089s)
             Mean action noise std: 2.37
          Mean value_function loss: 283.3119
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 49.2222
                       Mean reward: 762.69
               Mean episode length: 208.18
    Episode_Reward/reaching_object: 0.8760
     Episode_Reward/lifting_object: 141.9073
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 1.95s
                      Time elapsed: 00:39:41
                               ETA: 00:35:01

################################################################################
                     [1m Learning iteration 1063/2000 [0m                     

                       Computation: 50061 steps/s (collection: 1.866s, learning 0.097s)
             Mean action noise std: 2.37
          Mean value_function loss: 271.1476
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 49.2434
                       Mean reward: 684.19
               Mean episode length: 188.97
    Episode_Reward/reaching_object: 0.9005
     Episode_Reward/lifting_object: 146.6062
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 1.96s
                      Time elapsed: 00:39:43
                               ETA: 00:34:59

################################################################################
                     [1m Learning iteration 1064/2000 [0m                     

                       Computation: 51373 steps/s (collection: 1.822s, learning 0.092s)
             Mean action noise std: 2.38
          Mean value_function loss: 292.8449
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 49.2671
                       Mean reward: 764.86
               Mean episode length: 209.09
    Episode_Reward/reaching_object: 0.8776
     Episode_Reward/lifting_object: 142.7556
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 1.91s
                      Time elapsed: 00:39:45
                               ETA: 00:34:56

################################################################################
                     [1m Learning iteration 1065/2000 [0m                     

                       Computation: 50990 steps/s (collection: 1.842s, learning 0.086s)
             Mean action noise std: 2.38
          Mean value_function loss: 271.0258
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 49.2959
                       Mean reward: 762.76
               Mean episode length: 210.73
    Episode_Reward/reaching_object: 0.9250
     Episode_Reward/lifting_object: 150.2878
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 1.93s
                      Time elapsed: 00:39:47
                               ETA: 00:34:54

################################################################################
                     [1m Learning iteration 1066/2000 [0m                     

                       Computation: 47838 steps/s (collection: 1.967s, learning 0.088s)
             Mean action noise std: 2.38
          Mean value_function loss: 253.9448
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 49.3139
                       Mean reward: 716.85
               Mean episode length: 196.20
    Episode_Reward/reaching_object: 0.9038
     Episode_Reward/lifting_object: 148.0013
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 2.05s
                      Time elapsed: 00:39:49
                               ETA: 00:34:51

################################################################################
                     [1m Learning iteration 1067/2000 [0m                     

                       Computation: 50347 steps/s (collection: 1.848s, learning 0.104s)
             Mean action noise std: 2.38
          Mean value_function loss: 245.9713
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 49.3286
                       Mean reward: 712.79
               Mean episode length: 197.74
    Episode_Reward/reaching_object: 0.8857
     Episode_Reward/lifting_object: 143.5926
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 1.95s
                      Time elapsed: 00:39:51
                               ETA: 00:34:49

################################################################################
                     [1m Learning iteration 1068/2000 [0m                     

                       Computation: 50712 steps/s (collection: 1.851s, learning 0.087s)
             Mean action noise std: 2.38
          Mean value_function loss: 221.6079
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 49.3525
                       Mean reward: 773.01
               Mean episode length: 210.29
    Episode_Reward/reaching_object: 0.9146
     Episode_Reward/lifting_object: 149.5987
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 1.94s
                      Time elapsed: 00:39:53
                               ETA: 00:34:46

################################################################################
                     [1m Learning iteration 1069/2000 [0m                     

                       Computation: 49064 steps/s (collection: 1.900s, learning 0.104s)
             Mean action noise std: 2.39
          Mean value_function loss: 220.0549
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 49.3793
                       Mean reward: 755.70
               Mean episode length: 205.67
    Episode_Reward/reaching_object: 0.9277
     Episode_Reward/lifting_object: 152.2461
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 2.00s
                      Time elapsed: 00:39:55
                               ETA: 00:34:44

################################################################################
                     [1m Learning iteration 1070/2000 [0m                     

                       Computation: 50386 steps/s (collection: 1.858s, learning 0.093s)
             Mean action noise std: 2.39
          Mean value_function loss: 214.9722
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 49.3947
                       Mean reward: 730.77
               Mean episode length: 200.76
    Episode_Reward/reaching_object: 0.8918
     Episode_Reward/lifting_object: 145.7830
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 1.95s
                      Time elapsed: 00:39:57
                               ETA: 00:34:41

################################################################################
                     [1m Learning iteration 1071/2000 [0m                     

                       Computation: 50374 steps/s (collection: 1.850s, learning 0.102s)
             Mean action noise std: 2.39
          Mean value_function loss: 304.7222
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 49.4158
                       Mean reward: 695.17
               Mean episode length: 190.99
    Episode_Reward/reaching_object: 0.8811
     Episode_Reward/lifting_object: 143.0217
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 1.95s
                      Time elapsed: 00:39:59
                               ETA: 00:34:39

################################################################################
                     [1m Learning iteration 1072/2000 [0m                     

                       Computation: 50694 steps/s (collection: 1.848s, learning 0.091s)
             Mean action noise std: 2.39
          Mean value_function loss: 353.2883
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 49.4391
                       Mean reward: 755.94
               Mean episode length: 207.55
    Episode_Reward/reaching_object: 0.9030
     Episode_Reward/lifting_object: 147.2280
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 1.94s
                      Time elapsed: 00:40:01
                               ETA: 00:34:36

################################################################################
                     [1m Learning iteration 1073/2000 [0m                     

                       Computation: 48616 steps/s (collection: 1.877s, learning 0.145s)
             Mean action noise std: 2.40
          Mean value_function loss: 269.0153
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 49.4572
                       Mean reward: 693.28
               Mean episode length: 192.70
    Episode_Reward/reaching_object: 0.8799
     Episode_Reward/lifting_object: 142.5110
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 2.02s
                      Time elapsed: 00:40:03
                               ETA: 00:34:34

################################################################################
                     [1m Learning iteration 1074/2000 [0m                     

                       Computation: 46323 steps/s (collection: 2.016s, learning 0.107s)
             Mean action noise std: 2.40
          Mean value_function loss: 220.8017
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 49.4725
                       Mean reward: 780.07
               Mean episode length: 213.35
    Episode_Reward/reaching_object: 0.9236
     Episode_Reward/lifting_object: 151.3530
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 2.12s
                      Time elapsed: 00:40:05
                               ETA: 00:34:32

################################################################################
                     [1m Learning iteration 1075/2000 [0m                     

                       Computation: 49314 steps/s (collection: 1.901s, learning 0.092s)
             Mean action noise std: 2.40
          Mean value_function loss: 241.6032
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 49.4880
                       Mean reward: 738.56
               Mean episode length: 203.30
    Episode_Reward/reaching_object: 0.8860
     Episode_Reward/lifting_object: 144.3957
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 1.99s
                      Time elapsed: 00:40:07
                               ETA: 00:34:29

################################################################################
                     [1m Learning iteration 1076/2000 [0m                     

                       Computation: 48654 steps/s (collection: 1.928s, learning 0.092s)
             Mean action noise std: 2.40
          Mean value_function loss: 258.1816
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 49.4978
                       Mean reward: 792.80
               Mean episode length: 217.42
    Episode_Reward/reaching_object: 0.9263
     Episode_Reward/lifting_object: 151.3402
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 2.02s
                      Time elapsed: 00:40:09
                               ETA: 00:34:27

################################################################################
                     [1m Learning iteration 1077/2000 [0m                     

                       Computation: 46178 steps/s (collection: 1.946s, learning 0.183s)
             Mean action noise std: 2.40
          Mean value_function loss: 240.7311
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 49.5171
                       Mean reward: 736.20
               Mean episode length: 203.56
    Episode_Reward/reaching_object: 0.9009
     Episode_Reward/lifting_object: 146.5164
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 2.13s
                      Time elapsed: 00:40:11
                               ETA: 00:34:24

################################################################################
                     [1m Learning iteration 1078/2000 [0m                     

                       Computation: 46837 steps/s (collection: 1.999s, learning 0.100s)
             Mean action noise std: 2.40
          Mean value_function loss: 250.6158
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 49.5353
                       Mean reward: 717.67
               Mean episode length: 197.92
    Episode_Reward/reaching_object: 0.8987
     Episode_Reward/lifting_object: 146.5236
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 2.10s
                      Time elapsed: 00:40:13
                               ETA: 00:34:22

################################################################################
                     [1m Learning iteration 1079/2000 [0m                     

                       Computation: 48590 steps/s (collection: 1.910s, learning 0.113s)
             Mean action noise std: 2.41
          Mean value_function loss: 235.8186
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 49.5551
                       Mean reward: 719.54
               Mean episode length: 198.31
    Episode_Reward/reaching_object: 0.9031
     Episode_Reward/lifting_object: 147.9380
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 2.02s
                      Time elapsed: 00:40:15
                               ETA: 00:34:20

################################################################################
                     [1m Learning iteration 1080/2000 [0m                     

                       Computation: 50806 steps/s (collection: 1.843s, learning 0.092s)
             Mean action noise std: 2.41
          Mean value_function loss: 245.0006
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 49.5683
                       Mean reward: 715.09
               Mean episode length: 196.16
    Episode_Reward/reaching_object: 0.8655
     Episode_Reward/lifting_object: 140.5393
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 1.93s
                      Time elapsed: 00:40:17
                               ETA: 00:34:17

################################################################################
                     [1m Learning iteration 1081/2000 [0m                     

                       Computation: 46318 steps/s (collection: 2.000s, learning 0.123s)
             Mean action noise std: 2.41
          Mean value_function loss: 206.6476
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 49.5780
                       Mean reward: 799.66
               Mean episode length: 218.09
    Episode_Reward/reaching_object: 0.9226
     Episode_Reward/lifting_object: 150.5579
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 2.12s
                      Time elapsed: 00:40:19
                               ETA: 00:34:15

################################################################################
                     [1m Learning iteration 1082/2000 [0m                     

                       Computation: 51479 steps/s (collection: 1.825s, learning 0.085s)
             Mean action noise std: 2.41
          Mean value_function loss: 227.8510
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 49.5898
                       Mean reward: 767.49
               Mean episode length: 209.01
    Episode_Reward/reaching_object: 0.9306
     Episode_Reward/lifting_object: 152.9184
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 1.91s
                      Time elapsed: 00:40:21
                               ETA: 00:34:12

################################################################################
                     [1m Learning iteration 1083/2000 [0m                     

                       Computation: 49320 steps/s (collection: 1.908s, learning 0.085s)
             Mean action noise std: 2.41
          Mean value_function loss: 248.5446
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.6060
                       Mean reward: 752.79
               Mean episode length: 207.89
    Episode_Reward/reaching_object: 0.9133
     Episode_Reward/lifting_object: 148.9443
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 1.99s
                      Time elapsed: 00:40:23
                               ETA: 00:34:10

################################################################################
                     [1m Learning iteration 1084/2000 [0m                     

                       Computation: 50437 steps/s (collection: 1.859s, learning 0.090s)
             Mean action noise std: 2.42
          Mean value_function loss: 285.2181
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 49.6273
                       Mean reward: 737.64
               Mean episode length: 203.05
    Episode_Reward/reaching_object: 0.9140
     Episode_Reward/lifting_object: 148.7490
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 1.95s
                      Time elapsed: 00:40:25
                               ETA: 00:34:07

################################################################################
                     [1m Learning iteration 1085/2000 [0m                     

                       Computation: 48367 steps/s (collection: 1.943s, learning 0.089s)
             Mean action noise std: 2.42
          Mean value_function loss: 218.0514
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 49.6524
                       Mean reward: 826.87
               Mean episode length: 223.73
    Episode_Reward/reaching_object: 0.9667
     Episode_Reward/lifting_object: 158.3836
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 2.03s
                      Time elapsed: 00:40:27
                               ETA: 00:34:05

################################################################################
                     [1m Learning iteration 1086/2000 [0m                     

                       Computation: 47441 steps/s (collection: 1.983s, learning 0.089s)
             Mean action noise std: 2.42
          Mean value_function loss: 202.5277
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 49.6675
                       Mean reward: 681.67
               Mean episode length: 188.80
    Episode_Reward/reaching_object: 0.9011
     Episode_Reward/lifting_object: 147.0777
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 2.07s
                      Time elapsed: 00:40:29
                               ETA: 00:34:03

################################################################################
                     [1m Learning iteration 1087/2000 [0m                     

                       Computation: 49723 steps/s (collection: 1.880s, learning 0.097s)
             Mean action noise std: 2.42
          Mean value_function loss: 224.9231
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 49.6752
                       Mean reward: 765.92
               Mean episode length: 208.51
    Episode_Reward/reaching_object: 0.9195
     Episode_Reward/lifting_object: 150.2828
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 1.98s
                      Time elapsed: 00:40:31
                               ETA: 00:34:00

################################################################################
                     [1m Learning iteration 1088/2000 [0m                     

                       Computation: 46530 steps/s (collection: 1.958s, learning 0.155s)
             Mean action noise std: 2.42
          Mean value_function loss: 214.9311
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 49.6865
                       Mean reward: 762.98
               Mean episode length: 209.92
    Episode_Reward/reaching_object: 0.9244
     Episode_Reward/lifting_object: 151.2339
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 2.11s
                      Time elapsed: 00:40:34
                               ETA: 00:33:58

################################################################################
                     [1m Learning iteration 1089/2000 [0m                     

                       Computation: 44063 steps/s (collection: 2.121s, learning 0.110s)
             Mean action noise std: 2.42
          Mean value_function loss: 227.4545
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 49.6960
                       Mean reward: 726.87
               Mean episode length: 199.17
    Episode_Reward/reaching_object: 0.8968
     Episode_Reward/lifting_object: 146.6672
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 2.23s
                      Time elapsed: 00:40:36
                               ETA: 00:33:56

################################################################################
                     [1m Learning iteration 1090/2000 [0m                     

                       Computation: 48410 steps/s (collection: 1.938s, learning 0.093s)
             Mean action noise std: 2.43
          Mean value_function loss: 206.1127
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 49.7061
                       Mean reward: 753.55
               Mean episode length: 205.96
    Episode_Reward/reaching_object: 0.9219
     Episode_Reward/lifting_object: 150.5350
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 2.03s
                      Time elapsed: 00:40:38
                               ETA: 00:33:53

################################################################################
                     [1m Learning iteration 1091/2000 [0m                     

                       Computation: 48841 steps/s (collection: 1.906s, learning 0.107s)
             Mean action noise std: 2.43
          Mean value_function loss: 199.4176
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 49.7250
                       Mean reward: 754.20
               Mean episode length: 206.55
    Episode_Reward/reaching_object: 0.9205
     Episode_Reward/lifting_object: 150.0578
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 2.01s
                      Time elapsed: 00:40:40
                               ETA: 00:33:51

################################################################################
                     [1m Learning iteration 1092/2000 [0m                     

                       Computation: 49581 steps/s (collection: 1.888s, learning 0.095s)
             Mean action noise std: 2.43
          Mean value_function loss: 194.5044
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 49.7396
                       Mean reward: 765.36
               Mean episode length: 207.96
    Episode_Reward/reaching_object: 0.9167
     Episode_Reward/lifting_object: 150.0014
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 1.98s
                      Time elapsed: 00:40:42
                               ETA: 00:33:48

################################################################################
                     [1m Learning iteration 1093/2000 [0m                     

                       Computation: 50250 steps/s (collection: 1.863s, learning 0.094s)
             Mean action noise std: 2.43
          Mean value_function loss: 208.9030
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 49.7571
                       Mean reward: 763.55
               Mean episode length: 208.14
    Episode_Reward/reaching_object: 0.9291
     Episode_Reward/lifting_object: 151.9157
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 1.96s
                      Time elapsed: 00:40:44
                               ETA: 00:33:46

################################################################################
                     [1m Learning iteration 1094/2000 [0m                     

                       Computation: 49772 steps/s (collection: 1.866s, learning 0.109s)
             Mean action noise std: 2.43
          Mean value_function loss: 220.5672
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 49.7690
                       Mean reward: 700.11
               Mean episode length: 194.97
    Episode_Reward/reaching_object: 0.9224
     Episode_Reward/lifting_object: 150.6430
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 1.98s
                      Time elapsed: 00:40:46
                               ETA: 00:33:43

################################################################################
                     [1m Learning iteration 1095/2000 [0m                     

                       Computation: 49743 steps/s (collection: 1.874s, learning 0.102s)
             Mean action noise std: 2.44
          Mean value_function loss: 272.9569
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 49.7909
                       Mean reward: 793.25
               Mean episode length: 215.51
    Episode_Reward/reaching_object: 0.9522
     Episode_Reward/lifting_object: 156.5508
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 1.98s
                      Time elapsed: 00:40:48
                               ETA: 00:33:41

################################################################################
                     [1m Learning iteration 1096/2000 [0m                     

                       Computation: 50870 steps/s (collection: 1.838s, learning 0.094s)
             Mean action noise std: 2.44
          Mean value_function loss: 237.5010
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.8095
                       Mean reward: 779.64
               Mean episode length: 212.22
    Episode_Reward/reaching_object: 0.9306
     Episode_Reward/lifting_object: 152.5024
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 1.93s
                      Time elapsed: 00:40:50
                               ETA: 00:33:39

################################################################################
                     [1m Learning iteration 1097/2000 [0m                     

                       Computation: 50004 steps/s (collection: 1.864s, learning 0.102s)
             Mean action noise std: 2.44
          Mean value_function loss: 232.0691
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 49.8252
                       Mean reward: 861.20
               Mean episode length: 231.20
    Episode_Reward/reaching_object: 0.9409
     Episode_Reward/lifting_object: 154.9908
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 1.97s
                      Time elapsed: 00:40:52
                               ETA: 00:33:36

################################################################################
                     [1m Learning iteration 1098/2000 [0m                     

                       Computation: 48970 steps/s (collection: 1.886s, learning 0.121s)
             Mean action noise std: 2.44
          Mean value_function loss: 248.1674
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 49.8496
                       Mean reward: 746.12
               Mean episode length: 205.33
    Episode_Reward/reaching_object: 0.9055
     Episode_Reward/lifting_object: 147.7331
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 2.01s
                      Time elapsed: 00:40:54
                               ETA: 00:33:34

################################################################################
                     [1m Learning iteration 1099/2000 [0m                     

                       Computation: 49720 steps/s (collection: 1.869s, learning 0.108s)
             Mean action noise std: 2.44
          Mean value_function loss: 212.6422
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 49.8668
                       Mean reward: 816.95
               Mean episode length: 222.34
    Episode_Reward/reaching_object: 0.9304
     Episode_Reward/lifting_object: 151.9992
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 1.98s
                      Time elapsed: 00:40:56
                               ETA: 00:33:31

################################################################################
                     [1m Learning iteration 1100/2000 [0m                     

                       Computation: 49158 steps/s (collection: 1.866s, learning 0.134s)
             Mean action noise std: 2.45
          Mean value_function loss: 255.6859
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 49.8765
                       Mean reward: 746.17
               Mean episode length: 204.14
    Episode_Reward/reaching_object: 0.8840
     Episode_Reward/lifting_object: 143.7953
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 2.00s
                      Time elapsed: 00:40:58
                               ETA: 00:33:29

################################################################################
                     [1m Learning iteration 1101/2000 [0m                     

                       Computation: 49801 steps/s (collection: 1.888s, learning 0.085s)
             Mean action noise std: 2.45
          Mean value_function loss: 220.0385
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 49.8935
                       Mean reward: 800.05
               Mean episode length: 217.72
    Episode_Reward/reaching_object: 0.9272
     Episode_Reward/lifting_object: 151.6517
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 1.97s
                      Time elapsed: 00:41:00
                               ETA: 00:33:26

################################################################################
                     [1m Learning iteration 1102/2000 [0m                     

                       Computation: 49501 steps/s (collection: 1.882s, learning 0.104s)
             Mean action noise std: 2.45
          Mean value_function loss: 226.0383
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 49.9173
                       Mean reward: 796.98
               Mean episode length: 218.44
    Episode_Reward/reaching_object: 0.9405
     Episode_Reward/lifting_object: 154.3164
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 1.99s
                      Time elapsed: 00:41:02
                               ETA: 00:33:24

################################################################################
                     [1m Learning iteration 1103/2000 [0m                     

                       Computation: 49869 steps/s (collection: 1.858s, learning 0.114s)
             Mean action noise std: 2.45
          Mean value_function loss: 232.3715
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 49.9387
                       Mean reward: 729.86
               Mean episode length: 200.54
    Episode_Reward/reaching_object: 0.9095
     Episode_Reward/lifting_object: 148.2729
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 1.97s
                      Time elapsed: 00:41:03
                               ETA: 00:33:21

################################################################################
                     [1m Learning iteration 1104/2000 [0m                     

                       Computation: 47385 steps/s (collection: 1.975s, learning 0.099s)
             Mean action noise std: 2.46
          Mean value_function loss: 206.8637
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 49.9587
                       Mean reward: 762.33
               Mean episode length: 207.03
    Episode_Reward/reaching_object: 0.9309
     Episode_Reward/lifting_object: 152.6626
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 2.07s
                      Time elapsed: 00:41:06
                               ETA: 00:33:19

################################################################################
                     [1m Learning iteration 1105/2000 [0m                     

                       Computation: 47415 steps/s (collection: 1.977s, learning 0.096s)
             Mean action noise std: 2.46
          Mean value_function loss: 192.2023
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 49.9766
                       Mean reward: 812.81
               Mean episode length: 219.84
    Episode_Reward/reaching_object: 0.9559
     Episode_Reward/lifting_object: 157.1523
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 2.07s
                      Time elapsed: 00:41:08
                               ETA: 00:33:17

################################################################################
                     [1m Learning iteration 1106/2000 [0m                     

                       Computation: 50218 steps/s (collection: 1.846s, learning 0.111s)
             Mean action noise std: 2.46
          Mean value_function loss: 188.7381
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 49.9900
                       Mean reward: 776.23
               Mean episode length: 212.07
    Episode_Reward/reaching_object: 0.9252
     Episode_Reward/lifting_object: 152.2781
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 1.96s
                      Time elapsed: 00:41:10
                               ETA: 00:33:14

################################################################################
                     [1m Learning iteration 1107/2000 [0m                     

                       Computation: 50653 steps/s (collection: 1.851s, learning 0.090s)
             Mean action noise std: 2.46
          Mean value_function loss: 208.1610
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 50.0057
                       Mean reward: 763.03
               Mean episode length: 210.17
    Episode_Reward/reaching_object: 0.9050
     Episode_Reward/lifting_object: 147.5542
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 1.94s
                      Time elapsed: 00:41:12
                               ETA: 00:33:12

################################################################################
                     [1m Learning iteration 1108/2000 [0m                     

                       Computation: 50620 steps/s (collection: 1.854s, learning 0.088s)
             Mean action noise std: 2.46
          Mean value_function loss: 223.9548
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 50.0294
                       Mean reward: 748.70
               Mean episode length: 205.87
    Episode_Reward/reaching_object: 0.9626
     Episode_Reward/lifting_object: 158.5572
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 1.94s
                      Time elapsed: 00:41:13
                               ETA: 00:33:09

################################################################################
                     [1m Learning iteration 1109/2000 [0m                     

                       Computation: 49915 steps/s (collection: 1.868s, learning 0.102s)
             Mean action noise std: 2.47
          Mean value_function loss: 213.9821
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 50.0542
                       Mean reward: 763.89
               Mean episode length: 209.05
    Episode_Reward/reaching_object: 0.9211
     Episode_Reward/lifting_object: 150.9515
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 1.97s
                      Time elapsed: 00:41:15
                               ETA: 00:33:07

################################################################################
                     [1m Learning iteration 1110/2000 [0m                     

                       Computation: 49856 steps/s (collection: 1.857s, learning 0.115s)
             Mean action noise std: 2.47
          Mean value_function loss: 199.9384
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 50.0764
                       Mean reward: 755.59
               Mean episode length: 208.50
    Episode_Reward/reaching_object: 0.9255
     Episode_Reward/lifting_object: 152.3119
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 1.97s
                      Time elapsed: 00:41:17
                               ETA: 00:33:05

################################################################################
                     [1m Learning iteration 1111/2000 [0m                     

                       Computation: 49799 steps/s (collection: 1.875s, learning 0.099s)
             Mean action noise std: 2.47
          Mean value_function loss: 188.0873
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 50.0936
                       Mean reward: 749.13
               Mean episode length: 205.74
    Episode_Reward/reaching_object: 0.9132
     Episode_Reward/lifting_object: 149.5908
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 1.97s
                      Time elapsed: 00:41:19
                               ETA: 00:33:02

################################################################################
                     [1m Learning iteration 1112/2000 [0m                     

                       Computation: 50123 steps/s (collection: 1.862s, learning 0.100s)
             Mean action noise std: 2.47
          Mean value_function loss: 218.1433
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 50.1069
                       Mean reward: 759.50
               Mean episode length: 207.64
    Episode_Reward/reaching_object: 0.8991
     Episode_Reward/lifting_object: 147.1579
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 1.96s
                      Time elapsed: 00:41:21
                               ETA: 00:33:00

################################################################################
                     [1m Learning iteration 1113/2000 [0m                     

                       Computation: 48984 steps/s (collection: 1.898s, learning 0.109s)
             Mean action noise std: 2.47
          Mean value_function loss: 192.3186
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 50.1196
                       Mean reward: 777.13
               Mean episode length: 210.94
    Episode_Reward/reaching_object: 0.9009
     Episode_Reward/lifting_object: 147.0488
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 2.01s
                      Time elapsed: 00:41:23
                               ETA: 00:32:57

################################################################################
                     [1m Learning iteration 1114/2000 [0m                     

                       Computation: 50036 steps/s (collection: 1.866s, learning 0.099s)
             Mean action noise std: 2.48
          Mean value_function loss: 175.5818
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.1419
                       Mean reward: 790.74
               Mean episode length: 215.19
    Episode_Reward/reaching_object: 0.9460
     Episode_Reward/lifting_object: 155.3095
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 1.96s
                      Time elapsed: 00:41:25
                               ETA: 00:32:55

################################################################################
                     [1m Learning iteration 1115/2000 [0m                     

                       Computation: 48890 steps/s (collection: 1.916s, learning 0.095s)
             Mean action noise std: 2.48
          Mean value_function loss: 210.4993
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 50.1641
                       Mean reward: 690.46
               Mean episode length: 191.76
    Episode_Reward/reaching_object: 0.9318
     Episode_Reward/lifting_object: 152.3284
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 2.01s
                      Time elapsed: 00:41:27
                               ETA: 00:32:52

################################################################################
                     [1m Learning iteration 1116/2000 [0m                     

                       Computation: 49811 steps/s (collection: 1.884s, learning 0.090s)
             Mean action noise std: 2.48
          Mean value_function loss: 183.4125
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.1829
                       Mean reward: 763.31
               Mean episode length: 210.38
    Episode_Reward/reaching_object: 0.9511
     Episode_Reward/lifting_object: 155.8810
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 1.97s
                      Time elapsed: 00:41:29
                               ETA: 00:32:50

################################################################################
                     [1m Learning iteration 1117/2000 [0m                     

                       Computation: 49127 steps/s (collection: 1.911s, learning 0.090s)
             Mean action noise std: 2.48
          Mean value_function loss: 227.1584
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 50.2052
                       Mean reward: 777.58
               Mean episode length: 213.77
    Episode_Reward/reaching_object: 0.9221
     Episode_Reward/lifting_object: 150.2152
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 2.00s
                      Time elapsed: 00:41:31
                               ETA: 00:32:48

################################################################################
                     [1m Learning iteration 1118/2000 [0m                     

                       Computation: 49663 steps/s (collection: 1.889s, learning 0.090s)
             Mean action noise std: 2.48
          Mean value_function loss: 209.2953
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 50.2197
                       Mean reward: 790.20
               Mean episode length: 214.06
    Episode_Reward/reaching_object: 0.9202
     Episode_Reward/lifting_object: 149.7567
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 1.98s
                      Time elapsed: 00:41:33
                               ETA: 00:32:45

################################################################################
                     [1m Learning iteration 1119/2000 [0m                     

                       Computation: 49579 steps/s (collection: 1.882s, learning 0.101s)
             Mean action noise std: 2.49
          Mean value_function loss: 212.4958
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.2408
                       Mean reward: 771.88
               Mean episode length: 210.21
    Episode_Reward/reaching_object: 0.9498
     Episode_Reward/lifting_object: 155.8128
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 1.98s
                      Time elapsed: 00:41:35
                               ETA: 00:32:43

################################################################################
                     [1m Learning iteration 1120/2000 [0m                     

                       Computation: 49003 steps/s (collection: 1.865s, learning 0.141s)
             Mean action noise std: 2.49
          Mean value_function loss: 207.0121
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.2678
                       Mean reward: 769.42
               Mean episode length: 210.60
    Episode_Reward/reaching_object: 0.9451
     Episode_Reward/lifting_object: 155.1438
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 2.01s
                      Time elapsed: 00:41:37
                               ETA: 00:32:40

################################################################################
                     [1m Learning iteration 1121/2000 [0m                     

                       Computation: 47409 steps/s (collection: 1.897s, learning 0.177s)
             Mean action noise std: 2.49
          Mean value_function loss: 196.4038
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.2836
                       Mean reward: 753.83
               Mean episode length: 206.60
    Episode_Reward/reaching_object: 0.9113
     Episode_Reward/lifting_object: 148.9782
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 2.07s
                      Time elapsed: 00:41:39
                               ETA: 00:32:38

################################################################################
                     [1m Learning iteration 1122/2000 [0m                     

                       Computation: 48822 steps/s (collection: 1.916s, learning 0.097s)
             Mean action noise std: 2.49
          Mean value_function loss: 223.0708
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.3025
                       Mean reward: 727.16
               Mean episode length: 201.88
    Episode_Reward/reaching_object: 0.9249
     Episode_Reward/lifting_object: 151.0289
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 2.01s
                      Time elapsed: 00:41:41
                               ETA: 00:32:36

################################################################################
                     [1m Learning iteration 1123/2000 [0m                     

                       Computation: 49229 steps/s (collection: 1.906s, learning 0.091s)
             Mean action noise std: 2.50
          Mean value_function loss: 244.4442
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 50.3182
                       Mean reward: 813.50
               Mean episode length: 222.13
    Episode_Reward/reaching_object: 0.9293
     Episode_Reward/lifting_object: 151.7204
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 2.00s
                      Time elapsed: 00:41:43
                               ETA: 00:32:33

################################################################################
                     [1m Learning iteration 1124/2000 [0m                     

                       Computation: 49234 steps/s (collection: 1.868s, learning 0.129s)
             Mean action noise std: 2.50
          Mean value_function loss: 246.4452
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 50.3326
                       Mean reward: 745.53
               Mean episode length: 204.18
    Episode_Reward/reaching_object: 0.9063
     Episode_Reward/lifting_object: 147.0704
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 2.00s
                      Time elapsed: 00:41:45
                               ETA: 00:32:31

################################################################################
                     [1m Learning iteration 1125/2000 [0m                     

                       Computation: 50293 steps/s (collection: 1.858s, learning 0.097s)
             Mean action noise std: 2.50
          Mean value_function loss: 212.8166
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 50.3562
                       Mean reward: 780.82
               Mean episode length: 213.69
    Episode_Reward/reaching_object: 0.9085
     Episode_Reward/lifting_object: 148.0987
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 1.95s
                      Time elapsed: 00:41:47
                               ETA: 00:32:28

################################################################################
                     [1m Learning iteration 1126/2000 [0m                     

                       Computation: 50395 steps/s (collection: 1.854s, learning 0.097s)
             Mean action noise std: 2.50
          Mean value_function loss: 188.8696
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 50.3753
                       Mean reward: 731.89
               Mean episode length: 202.52
    Episode_Reward/reaching_object: 0.9006
     Episode_Reward/lifting_object: 145.8685
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 1.95s
                      Time elapsed: 00:41:49
                               ETA: 00:32:26

################################################################################
                     [1m Learning iteration 1127/2000 [0m                     

                       Computation: 49258 steps/s (collection: 1.864s, learning 0.132s)
             Mean action noise std: 2.50
          Mean value_function loss: 210.1686
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 50.3886
                       Mean reward: 788.65
               Mean episode length: 214.24
    Episode_Reward/reaching_object: 0.9152
     Episode_Reward/lifting_object: 149.9272
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 2.00s
                      Time elapsed: 00:41:51
                               ETA: 00:32:23

################################################################################
                     [1m Learning iteration 1128/2000 [0m                     

                       Computation: 49840 steps/s (collection: 1.874s, learning 0.098s)
             Mean action noise std: 2.51
          Mean value_function loss: 195.2749
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 50.4107
                       Mean reward: 781.11
               Mean episode length: 212.00
    Episode_Reward/reaching_object: 0.9111
     Episode_Reward/lifting_object: 148.6081
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 1.97s
                      Time elapsed: 00:41:53
                               ETA: 00:32:21

################################################################################
                     [1m Learning iteration 1129/2000 [0m                     

                       Computation: 49940 steps/s (collection: 1.873s, learning 0.095s)
             Mean action noise std: 2.51
          Mean value_function loss: 214.7482
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 50.4322
                       Mean reward: 683.41
               Mean episode length: 189.81
    Episode_Reward/reaching_object: 0.9031
     Episode_Reward/lifting_object: 147.6737
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 1.97s
                      Time elapsed: 00:41:55
                               ETA: 00:32:19

################################################################################
                     [1m Learning iteration 1130/2000 [0m                     

                       Computation: 50725 steps/s (collection: 1.849s, learning 0.089s)
             Mean action noise std: 2.51
          Mean value_function loss: 206.5548
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.4478
                       Mean reward: 762.17
               Mean episode length: 209.21
    Episode_Reward/reaching_object: 0.9205
     Episode_Reward/lifting_object: 150.1499
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 1.94s
                      Time elapsed: 00:41:57
                               ETA: 00:32:16

################################################################################
                     [1m Learning iteration 1131/2000 [0m                     

                       Computation: 50422 steps/s (collection: 1.845s, learning 0.105s)
             Mean action noise std: 2.51
          Mean value_function loss: 193.4409
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 50.4661
                       Mean reward: 764.43
               Mean episode length: 208.67
    Episode_Reward/reaching_object: 0.9193
     Episode_Reward/lifting_object: 149.8921
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 1.95s
                      Time elapsed: 00:41:59
                               ETA: 00:32:14

################################################################################
                     [1m Learning iteration 1132/2000 [0m                     

                       Computation: 49765 steps/s (collection: 1.888s, learning 0.088s)
             Mean action noise std: 2.51
          Mean value_function loss: 215.6356
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 50.4901
                       Mean reward: 722.93
               Mean episode length: 198.56
    Episode_Reward/reaching_object: 0.9001
     Episode_Reward/lifting_object: 146.8059
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 1.98s
                      Time elapsed: 00:42:01
                               ETA: 00:32:11

################################################################################
                     [1m Learning iteration 1133/2000 [0m                     

                       Computation: 50355 steps/s (collection: 1.859s, learning 0.094s)
             Mean action noise std: 2.52
          Mean value_function loss: 202.8756
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 50.5082
                       Mean reward: 729.34
               Mean episode length: 203.78
    Episode_Reward/reaching_object: 0.9064
     Episode_Reward/lifting_object: 146.8882
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 1.95s
                      Time elapsed: 00:42:03
                               ETA: 00:32:09

################################################################################
                     [1m Learning iteration 1134/2000 [0m                     

                       Computation: 50356 steps/s (collection: 1.857s, learning 0.096s)
             Mean action noise std: 2.52
          Mean value_function loss: 224.9262
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.5328
                       Mean reward: 779.06
               Mean episode length: 213.17
    Episode_Reward/reaching_object: 0.9067
     Episode_Reward/lifting_object: 148.0018
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 1.95s
                      Time elapsed: 00:42:05
                               ETA: 00:32:06

################################################################################
                     [1m Learning iteration 1135/2000 [0m                     

                       Computation: 49783 steps/s (collection: 1.882s, learning 0.093s)
             Mean action noise std: 2.52
          Mean value_function loss: 179.6664
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 50.5624
                       Mean reward: 777.51
               Mean episode length: 211.84
    Episode_Reward/reaching_object: 0.9699
     Episode_Reward/lifting_object: 159.3303
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 1.97s
                      Time elapsed: 00:42:07
                               ETA: 00:32:04

################################################################################
                     [1m Learning iteration 1136/2000 [0m                     

                       Computation: 50482 steps/s (collection: 1.837s, learning 0.110s)
             Mean action noise std: 2.53
          Mean value_function loss: 212.9541
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 50.5847
                       Mean reward: 695.73
               Mean episode length: 192.89
    Episode_Reward/reaching_object: 0.9063
     Episode_Reward/lifting_object: 147.6659
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 1.95s
                      Time elapsed: 00:42:09
                               ETA: 00:32:02

################################################################################
                     [1m Learning iteration 1137/2000 [0m                     

                       Computation: 47409 steps/s (collection: 1.921s, learning 0.153s)
             Mean action noise std: 2.53
          Mean value_function loss: 203.5564
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 50.5990
                       Mean reward: 764.30
               Mean episode length: 209.50
    Episode_Reward/reaching_object: 0.9269
     Episode_Reward/lifting_object: 150.9789
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 2.07s
                      Time elapsed: 00:42:11
                               ETA: 00:31:59

################################################################################
                     [1m Learning iteration 1138/2000 [0m                     

                       Computation: 50987 steps/s (collection: 1.839s, learning 0.089s)
             Mean action noise std: 2.53
          Mean value_function loss: 200.3221
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 50.6097
                       Mean reward: 684.81
               Mean episode length: 189.00
    Episode_Reward/reaching_object: 0.9072
     Episode_Reward/lifting_object: 147.8449
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 1.93s
                      Time elapsed: 00:42:13
                               ETA: 00:31:57

################################################################################
                     [1m Learning iteration 1139/2000 [0m                     

                       Computation: 51106 steps/s (collection: 1.834s, learning 0.089s)
             Mean action noise std: 2.53
          Mean value_function loss: 180.9850
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 50.6218
                       Mean reward: 813.27
               Mean episode length: 218.87
    Episode_Reward/reaching_object: 0.9335
     Episode_Reward/lifting_object: 152.9019
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 1.92s
                      Time elapsed: 00:42:15
                               ETA: 00:31:54

################################################################################
                     [1m Learning iteration 1140/2000 [0m                     

                       Computation: 50845 steps/s (collection: 1.841s, learning 0.092s)
             Mean action noise std: 2.53
          Mean value_function loss: 191.6851
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 50.6447
                       Mean reward: 812.27
               Mean episode length: 219.95
    Episode_Reward/reaching_object: 0.9434
     Episode_Reward/lifting_object: 154.4322
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 1.93s
                      Time elapsed: 00:42:17
                               ETA: 00:31:52

################################################################################
                     [1m Learning iteration 1141/2000 [0m                     

                       Computation: 49652 steps/s (collection: 1.895s, learning 0.085s)
             Mean action noise std: 2.53
          Mean value_function loss: 199.3169
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 50.6673
                       Mean reward: 692.66
               Mean episode length: 190.88
    Episode_Reward/reaching_object: 0.9103
     Episode_Reward/lifting_object: 148.8552
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 1.98s
                      Time elapsed: 00:42:19
                               ETA: 00:31:49

################################################################################
                     [1m Learning iteration 1142/2000 [0m                     

                       Computation: 49363 steps/s (collection: 1.892s, learning 0.100s)
             Mean action noise std: 2.54
          Mean value_function loss: 173.2996
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 50.6775
                       Mean reward: 829.48
               Mean episode length: 223.50
    Episode_Reward/reaching_object: 0.9172
     Episode_Reward/lifting_object: 149.6279
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 1.99s
                      Time elapsed: 00:42:21
                               ETA: 00:31:47

################################################################################
                     [1m Learning iteration 1143/2000 [0m                     

                       Computation: 48387 steps/s (collection: 1.944s, learning 0.088s)
             Mean action noise std: 2.54
          Mean value_function loss: 173.1221
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 50.6870
                       Mean reward: 858.47
               Mean episode length: 230.22
    Episode_Reward/reaching_object: 0.9260
     Episode_Reward/lifting_object: 151.3037
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 2.03s
                      Time elapsed: 00:42:23
                               ETA: 00:31:45

################################################################################
                     [1m Learning iteration 1144/2000 [0m                     

                       Computation: 48519 steps/s (collection: 1.920s, learning 0.106s)
             Mean action noise std: 2.54
          Mean value_function loss: 163.8469
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.6997
                       Mean reward: 838.52
               Mean episode length: 223.95
    Episode_Reward/reaching_object: 0.9585
     Episode_Reward/lifting_object: 157.7283
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 2.03s
                      Time elapsed: 00:42:25
                               ETA: 00:31:42

################################################################################
                     [1m Learning iteration 1145/2000 [0m                     

                       Computation: 49605 steps/s (collection: 1.894s, learning 0.088s)
             Mean action noise std: 2.54
          Mean value_function loss: 231.3816
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 50.7182
                       Mean reward: 816.83
               Mean episode length: 220.05
    Episode_Reward/reaching_object: 0.9129
     Episode_Reward/lifting_object: 149.3017
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 1.98s
                      Time elapsed: 00:42:27
                               ETA: 00:31:40

################################################################################
                     [1m Learning iteration 1146/2000 [0m                     

                       Computation: 48309 steps/s (collection: 1.940s, learning 0.095s)
             Mean action noise std: 2.54
          Mean value_function loss: 190.7352
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.7326
                       Mean reward: 814.82
               Mean episode length: 219.60
    Episode_Reward/reaching_object: 0.9322
     Episode_Reward/lifting_object: 152.8130
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 2.03s
                      Time elapsed: 00:42:29
                               ETA: 00:31:38

################################################################################
                     [1m Learning iteration 1147/2000 [0m                     

                       Computation: 49057 steps/s (collection: 1.912s, learning 0.092s)
             Mean action noise std: 2.54
          Mean value_function loss: 211.1389
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 50.7470
                       Mean reward: 780.02
               Mean episode length: 213.22
    Episode_Reward/reaching_object: 0.9357
     Episode_Reward/lifting_object: 153.9284
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 2.00s
                      Time elapsed: 00:42:31
                               ETA: 00:31:35

################################################################################
                     [1m Learning iteration 1148/2000 [0m                     

                       Computation: 49287 steps/s (collection: 1.905s, learning 0.089s)
             Mean action noise std: 2.55
          Mean value_function loss: 233.2605
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 50.7649
                       Mean reward: 768.22
               Mean episode length: 209.36
    Episode_Reward/reaching_object: 0.9219
     Episode_Reward/lifting_object: 150.9175
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 1.99s
                      Time elapsed: 00:42:33
                               ETA: 00:31:33

################################################################################
                     [1m Learning iteration 1149/2000 [0m                     

                       Computation: 50234 steps/s (collection: 1.871s, learning 0.086s)
             Mean action noise std: 2.55
          Mean value_function loss: 221.0966
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 50.7819
                       Mean reward: 778.54
               Mean episode length: 210.81
    Episode_Reward/reaching_object: 0.8952
     Episode_Reward/lifting_object: 145.7943
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 1.96s
                      Time elapsed: 00:42:35
                               ETA: 00:31:30

################################################################################
                     [1m Learning iteration 1150/2000 [0m                     

                       Computation: 49774 steps/s (collection: 1.879s, learning 0.096s)
             Mean action noise std: 2.55
          Mean value_function loss: 243.7843
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 50.7972
                       Mean reward: 692.62
               Mean episode length: 192.68
    Episode_Reward/reaching_object: 0.9088
     Episode_Reward/lifting_object: 148.7887
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 1.97s
                      Time elapsed: 00:42:37
                               ETA: 00:31:28

################################################################################
                     [1m Learning iteration 1151/2000 [0m                     

                       Computation: 48902 steps/s (collection: 1.899s, learning 0.111s)
             Mean action noise std: 2.55
          Mean value_function loss: 191.1607
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 50.8108
                       Mean reward: 776.62
               Mean episode length: 211.26
    Episode_Reward/reaching_object: 0.9140
     Episode_Reward/lifting_object: 149.6844
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 2.01s
                      Time elapsed: 00:42:39
                               ETA: 00:31:26

################################################################################
                     [1m Learning iteration 1152/2000 [0m                     

                       Computation: 49312 steps/s (collection: 1.891s, learning 0.103s)
             Mean action noise std: 2.55
          Mean value_function loss: 231.6764
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 50.8231
                       Mean reward: 765.39
               Mean episode length: 208.61
    Episode_Reward/reaching_object: 0.9163
     Episode_Reward/lifting_object: 149.9711
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 1.99s
                      Time elapsed: 00:42:41
                               ETA: 00:31:23

################################################################################
                     [1m Learning iteration 1153/2000 [0m                     

                       Computation: 48403 steps/s (collection: 1.920s, learning 0.111s)
             Mean action noise std: 2.55
          Mean value_function loss: 181.2044
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.8410
                       Mean reward: 708.12
               Mean episode length: 197.07
    Episode_Reward/reaching_object: 0.9238
     Episode_Reward/lifting_object: 150.2484
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 2.03s
                      Time elapsed: 00:42:43
                               ETA: 00:31:21

################################################################################
                     [1m Learning iteration 1154/2000 [0m                     

                       Computation: 50019 steps/s (collection: 1.877s, learning 0.089s)
             Mean action noise std: 2.56
          Mean value_function loss: 184.3973
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 50.8581
                       Mean reward: 755.10
               Mean episode length: 206.42
    Episode_Reward/reaching_object: 0.9370
     Episode_Reward/lifting_object: 153.0718
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 1.97s
                      Time elapsed: 00:42:45
                               ETA: 00:31:18

################################################################################
                     [1m Learning iteration 1155/2000 [0m                     

                       Computation: 50015 steps/s (collection: 1.870s, learning 0.096s)
             Mean action noise std: 2.56
          Mean value_function loss: 174.5217
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 50.8776
                       Mean reward: 729.97
               Mean episode length: 200.61
    Episode_Reward/reaching_object: 0.9395
     Episode_Reward/lifting_object: 154.1259
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 1.97s
                      Time elapsed: 00:42:47
                               ETA: 00:31:16

################################################################################
                     [1m Learning iteration 1156/2000 [0m                     

                       Computation: 50137 steps/s (collection: 1.853s, learning 0.108s)
             Mean action noise std: 2.56
          Mean value_function loss: 187.1867
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.8956
                       Mean reward: 814.26
               Mean episode length: 220.74
    Episode_Reward/reaching_object: 0.9678
     Episode_Reward/lifting_object: 158.9433
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 1.96s
                      Time elapsed: 00:42:49
                               ETA: 00:31:14

################################################################################
                     [1m Learning iteration 1157/2000 [0m                     

                       Computation: 50048 steps/s (collection: 1.867s, learning 0.098s)
             Mean action noise std: 2.56
          Mean value_function loss: 204.3806
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 50.9150
                       Mean reward: 822.11
               Mean episode length: 222.82
    Episode_Reward/reaching_object: 0.9312
     Episode_Reward/lifting_object: 151.5761
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 1.96s
                      Time elapsed: 00:42:51
                               ETA: 00:31:11

################################################################################
                     [1m Learning iteration 1158/2000 [0m                     

                       Computation: 49040 steps/s (collection: 1.887s, learning 0.118s)
             Mean action noise std: 2.57
          Mean value_function loss: 212.9961
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 50.9325
                       Mean reward: 717.63
               Mean episode length: 198.35
    Episode_Reward/reaching_object: 0.9261
     Episode_Reward/lifting_object: 150.7394
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 2.00s
                      Time elapsed: 00:42:53
                               ETA: 00:31:09

################################################################################
                     [1m Learning iteration 1159/2000 [0m                     

                       Computation: 48867 steps/s (collection: 1.902s, learning 0.110s)
             Mean action noise std: 2.57
          Mean value_function loss: 191.2236
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 50.9454
                       Mean reward: 789.74
               Mean episode length: 215.24
    Episode_Reward/reaching_object: 0.9440
     Episode_Reward/lifting_object: 154.2511
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 2.01s
                      Time elapsed: 00:42:55
                               ETA: 00:31:06

################################################################################
                     [1m Learning iteration 1160/2000 [0m                     

                       Computation: 49092 steps/s (collection: 1.891s, learning 0.111s)
             Mean action noise std: 2.57
          Mean value_function loss: 210.8537
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 50.9570
                       Mean reward: 730.82
               Mean episode length: 201.11
    Episode_Reward/reaching_object: 0.9258
     Episode_Reward/lifting_object: 150.7665
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 2.00s
                      Time elapsed: 00:42:57
                               ETA: 00:31:04

################################################################################
                     [1m Learning iteration 1161/2000 [0m                     

                       Computation: 48992 steps/s (collection: 1.885s, learning 0.121s)
             Mean action noise std: 2.57
          Mean value_function loss: 183.6353
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 50.9739
                       Mean reward: 791.51
               Mean episode length: 215.30
    Episode_Reward/reaching_object: 0.9186
     Episode_Reward/lifting_object: 149.3209
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 2.01s
                      Time elapsed: 00:42:59
                               ETA: 00:31:02

################################################################################
                     [1m Learning iteration 1162/2000 [0m                     

                       Computation: 49722 steps/s (collection: 1.867s, learning 0.111s)
             Mean action noise std: 2.57
          Mean value_function loss: 207.4979
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 50.9948
                       Mean reward: 731.60
               Mean episode length: 199.54
    Episode_Reward/reaching_object: 0.9240
     Episode_Reward/lifting_object: 150.4609
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 1.98s
                      Time elapsed: 00:43:01
                               ETA: 00:30:59

################################################################################
                     [1m Learning iteration 1163/2000 [0m                     

                       Computation: 46320 steps/s (collection: 2.001s, learning 0.121s)
             Mean action noise std: 2.58
          Mean value_function loss: 215.7164
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 51.0138
                       Mean reward: 783.66
               Mean episode length: 212.88
    Episode_Reward/reaching_object: 0.9259
     Episode_Reward/lifting_object: 151.1329
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 2.12s
                      Time elapsed: 00:43:03
                               ETA: 00:30:57

################################################################################
                     [1m Learning iteration 1164/2000 [0m                     

                       Computation: 42719 steps/s (collection: 2.188s, learning 0.113s)
             Mean action noise std: 2.58
          Mean value_function loss: 182.9161
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 51.0273
                       Mean reward: 769.37
               Mean episode length: 209.01
    Episode_Reward/reaching_object: 0.9232
     Episode_Reward/lifting_object: 150.4015
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 2.30s
                      Time elapsed: 00:43:05
                               ETA: 00:30:55

################################################################################
                     [1m Learning iteration 1165/2000 [0m                     

                       Computation: 47031 steps/s (collection: 2.000s, learning 0.090s)
             Mean action noise std: 2.58
          Mean value_function loss: 191.9711
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.0399
                       Mean reward: 793.48
               Mean episode length: 217.94
    Episode_Reward/reaching_object: 0.9697
     Episode_Reward/lifting_object: 158.4751
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 2.09s
                      Time elapsed: 00:43:07
                               ETA: 00:30:53

################################################################################
                     [1m Learning iteration 1166/2000 [0m                     

                       Computation: 50652 steps/s (collection: 1.851s, learning 0.090s)
             Mean action noise std: 2.58
          Mean value_function loss: 202.9149
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 51.0609
                       Mean reward: 738.12
               Mean episode length: 202.00
    Episode_Reward/reaching_object: 0.8861
     Episode_Reward/lifting_object: 143.2621
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 1.94s
                      Time elapsed: 00:43:09
                               ETA: 00:30:50

################################################################################
                     [1m Learning iteration 1167/2000 [0m                     

                       Computation: 49376 steps/s (collection: 1.901s, learning 0.090s)
             Mean action noise std: 2.58
          Mean value_function loss: 159.8033
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 51.0744
                       Mean reward: 820.92
               Mean episode length: 220.63
    Episode_Reward/reaching_object: 0.9596
     Episode_Reward/lifting_object: 157.5568
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 1.99s
                      Time elapsed: 00:43:11
                               ETA: 00:30:48

################################################################################
                     [1m Learning iteration 1168/2000 [0m                     

                       Computation: 49393 steps/s (collection: 1.882s, learning 0.108s)
             Mean action noise std: 2.58
          Mean value_function loss: 192.7658
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.0878
                       Mean reward: 757.87
               Mean episode length: 206.20
    Episode_Reward/reaching_object: 0.9234
     Episode_Reward/lifting_object: 150.5231
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 1.99s
                      Time elapsed: 00:43:13
                               ETA: 00:30:45

################################################################################
                     [1m Learning iteration 1169/2000 [0m                     

                       Computation: 48750 steps/s (collection: 1.898s, learning 0.118s)
             Mean action noise std: 2.59
          Mean value_function loss: 177.0328
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 51.1004
                       Mean reward: 756.14
               Mean episode length: 205.29
    Episode_Reward/reaching_object: 0.9210
     Episode_Reward/lifting_object: 150.5143
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 2.02s
                      Time elapsed: 00:43:15
                               ETA: 00:30:43

################################################################################
                     [1m Learning iteration 1170/2000 [0m                     

                       Computation: 47078 steps/s (collection: 1.984s, learning 0.104s)
             Mean action noise std: 2.59
          Mean value_function loss: 182.4527
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 51.1125
                       Mean reward: 760.67
               Mean episode length: 208.14
    Episode_Reward/reaching_object: 0.9440
     Episode_Reward/lifting_object: 153.9335
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 2.09s
                      Time elapsed: 00:43:17
                               ETA: 00:30:41

################################################################################
                     [1m Learning iteration 1171/2000 [0m                     

                       Computation: 50202 steps/s (collection: 1.871s, learning 0.088s)
             Mean action noise std: 2.59
          Mean value_function loss: 193.9549
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 51.1266
                       Mean reward: 797.56
               Mean episode length: 217.01
    Episode_Reward/reaching_object: 0.9187
     Episode_Reward/lifting_object: 149.7264
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 1.96s
                      Time elapsed: 00:43:19
                               ETA: 00:30:38

################################################################################
                     [1m Learning iteration 1172/2000 [0m                     

                       Computation: 50553 steps/s (collection: 1.856s, learning 0.089s)
             Mean action noise std: 2.59
          Mean value_function loss: 237.1554
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 51.1477
                       Mean reward: 791.56
               Mean episode length: 215.68
    Episode_Reward/reaching_object: 0.9601
     Episode_Reward/lifting_object: 157.2639
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 1.94s
                      Time elapsed: 00:43:21
                               ETA: 00:30:36

################################################################################
                     [1m Learning iteration 1173/2000 [0m                     

                       Computation: 49858 steps/s (collection: 1.877s, learning 0.095s)
             Mean action noise std: 2.59
          Mean value_function loss: 213.6778
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 51.1684
                       Mean reward: 700.79
               Mean episode length: 193.44
    Episode_Reward/reaching_object: 0.9184
     Episode_Reward/lifting_object: 149.6721
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 1.97s
                      Time elapsed: 00:43:23
                               ETA: 00:30:33

################################################################################
                     [1m Learning iteration 1174/2000 [0m                     

                       Computation: 50765 steps/s (collection: 1.848s, learning 0.088s)
             Mean action noise std: 2.60
          Mean value_function loss: 177.3338
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 51.1907
                       Mean reward: 787.24
               Mean episode length: 213.57
    Episode_Reward/reaching_object: 0.9081
     Episode_Reward/lifting_object: 147.5818
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 1.94s
                      Time elapsed: 00:43:25
                               ETA: 00:30:31

################################################################################
                     [1m Learning iteration 1175/2000 [0m                     

                       Computation: 49254 steps/s (collection: 1.905s, learning 0.091s)
             Mean action noise std: 2.60
          Mean value_function loss: 193.5539
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 51.2111
                       Mean reward: 688.18
               Mean episode length: 189.72
    Episode_Reward/reaching_object: 0.8723
     Episode_Reward/lifting_object: 141.5378
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 2.00s
                      Time elapsed: 00:43:27
                               ETA: 00:30:29

################################################################################
                     [1m Learning iteration 1176/2000 [0m                     

                       Computation: 47471 steps/s (collection: 1.959s, learning 0.112s)
             Mean action noise std: 2.60
          Mean value_function loss: 173.6102
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.2301
                       Mean reward: 707.90
               Mean episode length: 193.31
    Episode_Reward/reaching_object: 0.9307
     Episode_Reward/lifting_object: 152.5391
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 2.07s
                      Time elapsed: 00:43:29
                               ETA: 00:30:26

################################################################################
                     [1m Learning iteration 1177/2000 [0m                     

                       Computation: 49584 steps/s (collection: 1.897s, learning 0.085s)
             Mean action noise std: 2.60
          Mean value_function loss: 230.7717
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 51.2477
                       Mean reward: 774.64
               Mean episode length: 210.04
    Episode_Reward/reaching_object: 0.9281
     Episode_Reward/lifting_object: 151.5260
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 1.98s
                      Time elapsed: 00:43:31
                               ETA: 00:30:24

################################################################################
                     [1m Learning iteration 1178/2000 [0m                     

                       Computation: 49979 steps/s (collection: 1.878s, learning 0.089s)
             Mean action noise std: 2.60
          Mean value_function loss: 211.2561
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 51.2590
                       Mean reward: 755.59
               Mean episode length: 204.41
    Episode_Reward/reaching_object: 0.9154
     Episode_Reward/lifting_object: 148.8443
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 1.97s
                      Time elapsed: 00:43:33
                               ETA: 00:30:22

################################################################################
                     [1m Learning iteration 1179/2000 [0m                     

                       Computation: 50064 steps/s (collection: 1.861s, learning 0.103s)
             Mean action noise std: 2.60
          Mean value_function loss: 202.6375
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 51.2656
                       Mean reward: 727.18
               Mean episode length: 198.68
    Episode_Reward/reaching_object: 0.9344
     Episode_Reward/lifting_object: 152.6525
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 1.96s
                      Time elapsed: 00:43:35
                               ETA: 00:30:19

################################################################################
                     [1m Learning iteration 1180/2000 [0m                     

                       Computation: 50017 steps/s (collection: 1.864s, learning 0.102s)
             Mean action noise std: 2.61
          Mean value_function loss: 184.9669
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 51.2827
                       Mean reward: 769.37
               Mean episode length: 208.62
    Episode_Reward/reaching_object: 0.9359
     Episode_Reward/lifting_object: 153.1765
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 1.97s
                      Time elapsed: 00:43:37
                               ETA: 00:30:17

################################################################################
                     [1m Learning iteration 1181/2000 [0m                     

                       Computation: 48860 steps/s (collection: 1.885s, learning 0.127s)
             Mean action noise std: 2.61
          Mean value_function loss: 201.5547
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 51.2954
                       Mean reward: 729.21
               Mean episode length: 198.33
    Episode_Reward/reaching_object: 0.9156
     Episode_Reward/lifting_object: 149.1806
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 2.01s
                      Time elapsed: 00:43:39
                               ETA: 00:30:14

################################################################################
                     [1m Learning iteration 1182/2000 [0m                     

                       Computation: 49559 steps/s (collection: 1.895s, learning 0.089s)
             Mean action noise std: 2.61
          Mean value_function loss: 171.2649
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 51.3056
                       Mean reward: 764.53
               Mean episode length: 208.36
    Episode_Reward/reaching_object: 0.9452
     Episode_Reward/lifting_object: 154.3050
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 1.98s
                      Time elapsed: 00:43:41
                               ETA: 00:30:12

################################################################################
                     [1m Learning iteration 1183/2000 [0m                     

                       Computation: 49359 steps/s (collection: 1.870s, learning 0.122s)
             Mean action noise std: 2.61
          Mean value_function loss: 180.6118
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 51.3166
                       Mean reward: 735.59
               Mean episode length: 201.03
    Episode_Reward/reaching_object: 0.9426
     Episode_Reward/lifting_object: 154.1891
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 1.99s
                      Time elapsed: 00:43:43
                               ETA: 00:30:10

################################################################################
                     [1m Learning iteration 1184/2000 [0m                     

                       Computation: 49891 steps/s (collection: 1.851s, learning 0.119s)
             Mean action noise std: 2.61
          Mean value_function loss: 171.1581
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 51.3296
                       Mean reward: 716.71
               Mean episode length: 197.09
    Episode_Reward/reaching_object: 0.9455
     Episode_Reward/lifting_object: 154.8406
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 1.97s
                      Time elapsed: 00:43:45
                               ETA: 00:30:07

################################################################################
                     [1m Learning iteration 1185/2000 [0m                     

                       Computation: 49709 steps/s (collection: 1.885s, learning 0.093s)
             Mean action noise std: 2.61
          Mean value_function loss: 174.3171
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 51.3384
                       Mean reward: 733.05
               Mean episode length: 200.52
    Episode_Reward/reaching_object: 0.8918
     Episode_Reward/lifting_object: 144.8967
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 1.98s
                      Time elapsed: 00:43:47
                               ETA: 00:30:05

################################################################################
                     [1m Learning iteration 1186/2000 [0m                     

                       Computation: 48531 steps/s (collection: 1.846s, learning 0.179s)
             Mean action noise std: 2.62
          Mean value_function loss: 199.0914
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 51.3575
                       Mean reward: 817.37
               Mean episode length: 221.32
    Episode_Reward/reaching_object: 0.9542
     Episode_Reward/lifting_object: 156.5253
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 2.03s
                      Time elapsed: 00:43:49
                               ETA: 00:30:03

################################################################################
                     [1m Learning iteration 1187/2000 [0m                     

                       Computation: 46858 steps/s (collection: 1.925s, learning 0.173s)
             Mean action noise std: 2.62
          Mean value_function loss: 193.0451
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 51.3774
                       Mean reward: 730.88
               Mean episode length: 199.92
    Episode_Reward/reaching_object: 0.9163
     Episode_Reward/lifting_object: 149.1604
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 2.10s
                      Time elapsed: 00:43:51
                               ETA: 00:30:00

################################################################################
                     [1m Learning iteration 1188/2000 [0m                     

                       Computation: 49914 steps/s (collection: 1.861s, learning 0.108s)
             Mean action noise std: 2.62
          Mean value_function loss: 169.5403
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 51.3897
                       Mean reward: 812.02
               Mean episode length: 217.90
    Episode_Reward/reaching_object: 0.9409
     Episode_Reward/lifting_object: 154.4482
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 1.97s
                      Time elapsed: 00:43:53
                               ETA: 00:29:58

################################################################################
                     [1m Learning iteration 1189/2000 [0m                     

                       Computation: 48929 steps/s (collection: 1.909s, learning 0.101s)
             Mean action noise std: 2.62
          Mean value_function loss: 179.9425
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 51.4024
                       Mean reward: 786.40
               Mean episode length: 211.81
    Episode_Reward/reaching_object: 0.9458
     Episode_Reward/lifting_object: 154.9062
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 2.01s
                      Time elapsed: 00:43:55
                               ETA: 00:29:56

################################################################################
                     [1m Learning iteration 1190/2000 [0m                     

                       Computation: 48517 steps/s (collection: 1.935s, learning 0.091s)
             Mean action noise std: 2.62
          Mean value_function loss: 212.0071
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 51.4148
                       Mean reward: 734.32
               Mean episode length: 199.91
    Episode_Reward/reaching_object: 0.9147
     Episode_Reward/lifting_object: 149.2538
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 2.03s
                      Time elapsed: 00:43:57
                               ETA: 00:29:53

################################################################################
                     [1m Learning iteration 1191/2000 [0m                     

                       Computation: 49240 steps/s (collection: 1.873s, learning 0.124s)
             Mean action noise std: 2.62
          Mean value_function loss: 191.6363
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 51.4313
                       Mean reward: 675.52
               Mean episode length: 186.00
    Episode_Reward/reaching_object: 0.8881
     Episode_Reward/lifting_object: 143.9437
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 2.00s
                      Time elapsed: 00:43:59
                               ETA: 00:29:51

################################################################################
                     [1m Learning iteration 1192/2000 [0m                     

                       Computation: 49487 steps/s (collection: 1.887s, learning 0.100s)
             Mean action noise std: 2.63
          Mean value_function loss: 188.0415
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 51.4474
                       Mean reward: 760.99
               Mean episode length: 206.94
    Episode_Reward/reaching_object: 0.9470
     Episode_Reward/lifting_object: 155.6257
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 1.99s
                      Time elapsed: 00:44:01
                               ETA: 00:29:49

################################################################################
                     [1m Learning iteration 1193/2000 [0m                     

                       Computation: 49068 steps/s (collection: 1.911s, learning 0.093s)
             Mean action noise std: 2.63
          Mean value_function loss: 170.4542
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.4546
                       Mean reward: 740.93
               Mean episode length: 203.30
    Episode_Reward/reaching_object: 0.9476
     Episode_Reward/lifting_object: 155.1544
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 2.00s
                      Time elapsed: 00:44:03
                               ETA: 00:29:46

################################################################################
                     [1m Learning iteration 1194/2000 [0m                     

                       Computation: 48127 steps/s (collection: 1.926s, learning 0.116s)
             Mean action noise std: 2.63
          Mean value_function loss: 207.0535
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.4635
                       Mean reward: 762.59
               Mean episode length: 206.11
    Episode_Reward/reaching_object: 0.9310
     Episode_Reward/lifting_object: 152.6598
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 2.04s
                      Time elapsed: 00:44:05
                               ETA: 00:29:44

################################################################################
                     [1m Learning iteration 1195/2000 [0m                     

                       Computation: 49065 steps/s (collection: 1.916s, learning 0.088s)
             Mean action noise std: 2.63
          Mean value_function loss: 182.2191
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 51.4771
                       Mean reward: 736.37
               Mean episode length: 200.05
    Episode_Reward/reaching_object: 0.9129
     Episode_Reward/lifting_object: 148.7126
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 2.00s
                      Time elapsed: 00:44:07
                               ETA: 00:29:41

################################################################################
                     [1m Learning iteration 1196/2000 [0m                     

                       Computation: 46682 steps/s (collection: 2.005s, learning 0.101s)
             Mean action noise std: 2.63
          Mean value_function loss: 175.6392
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.4933
                       Mean reward: 758.37
               Mean episode length: 207.47
    Episode_Reward/reaching_object: 0.9138
     Episode_Reward/lifting_object: 148.7924
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 2.11s
                      Time elapsed: 00:44:09
                               ETA: 00:29:39

################################################################################
                     [1m Learning iteration 1197/2000 [0m                     

                       Computation: 50113 steps/s (collection: 1.867s, learning 0.095s)
             Mean action noise std: 2.63
          Mean value_function loss: 167.5474
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 51.5072
                       Mean reward: 749.28
               Mean episode length: 203.51
    Episode_Reward/reaching_object: 0.9498
     Episode_Reward/lifting_object: 155.8925
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 1.96s
                      Time elapsed: 00:44:11
                               ETA: 00:29:37

################################################################################
                     [1m Learning iteration 1198/2000 [0m                     

                       Computation: 49885 steps/s (collection: 1.878s, learning 0.092s)
             Mean action noise std: 2.64
          Mean value_function loss: 210.3835
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.5234
                       Mean reward: 756.78
               Mean episode length: 206.31
    Episode_Reward/reaching_object: 0.9436
     Episode_Reward/lifting_object: 154.0585
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 1.97s
                      Time elapsed: 00:44:13
                               ETA: 00:29:34

################################################################################
                     [1m Learning iteration 1199/2000 [0m                     

                       Computation: 48126 steps/s (collection: 1.874s, learning 0.169s)
             Mean action noise std: 2.64
          Mean value_function loss: 184.2577
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 51.5402
                       Mean reward: 750.53
               Mean episode length: 206.38
    Episode_Reward/reaching_object: 0.9232
     Episode_Reward/lifting_object: 150.5854
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 2.04s
                      Time elapsed: 00:44:15
                               ETA: 00:29:32

################################################################################
                     [1m Learning iteration 1200/2000 [0m                     

                       Computation: 48287 steps/s (collection: 1.908s, learning 0.128s)
             Mean action noise std: 2.64
          Mean value_function loss: 198.1716
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 51.5526
                       Mean reward: 773.06
               Mean episode length: 210.24
    Episode_Reward/reaching_object: 0.9298
     Episode_Reward/lifting_object: 152.2128
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 2.04s
                      Time elapsed: 00:44:17
                               ETA: 00:29:30

################################################################################
                     [1m Learning iteration 1201/2000 [0m                     

                       Computation: 48623 steps/s (collection: 1.927s, learning 0.095s)
             Mean action noise std: 2.64
          Mean value_function loss: 187.7198
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 51.5683
                       Mean reward: 809.35
               Mean episode length: 219.09
    Episode_Reward/reaching_object: 0.9412
     Episode_Reward/lifting_object: 153.4250
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 2.02s
                      Time elapsed: 00:44:19
                               ETA: 00:29:27

################################################################################
                     [1m Learning iteration 1202/2000 [0m                     

                       Computation: 49788 steps/s (collection: 1.879s, learning 0.096s)
             Mean action noise std: 2.64
          Mean value_function loss: 182.9527
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.5821
                       Mean reward: 789.22
               Mean episode length: 212.93
    Episode_Reward/reaching_object: 0.9232
     Episode_Reward/lifting_object: 151.0285
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 1.97s
                      Time elapsed: 00:44:21
                               ETA: 00:29:25

################################################################################
                     [1m Learning iteration 1203/2000 [0m                     

                       Computation: 48627 steps/s (collection: 1.870s, learning 0.151s)
             Mean action noise std: 2.64
          Mean value_function loss: 162.2135
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 51.5954
                       Mean reward: 762.66
               Mean episode length: 208.43
    Episode_Reward/reaching_object: 0.9398
     Episode_Reward/lifting_object: 153.8346
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 2.02s
                      Time elapsed: 00:44:23
                               ETA: 00:29:23

################################################################################
                     [1m Learning iteration 1204/2000 [0m                     

                       Computation: 49031 steps/s (collection: 1.907s, learning 0.098s)
             Mean action noise std: 2.65
          Mean value_function loss: 211.5889
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.6116
                       Mean reward: 800.56
               Mean episode length: 218.17
    Episode_Reward/reaching_object: 0.9640
     Episode_Reward/lifting_object: 157.9445
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 2.00s
                      Time elapsed: 00:44:25
                               ETA: 00:29:20

################################################################################
                     [1m Learning iteration 1205/2000 [0m                     

                       Computation: 46920 steps/s (collection: 1.948s, learning 0.148s)
             Mean action noise std: 2.65
          Mean value_function loss: 187.2529
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 51.6275
                       Mean reward: 787.50
               Mean episode length: 213.55
    Episode_Reward/reaching_object: 0.9441
     Episode_Reward/lifting_object: 154.7115
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 2.10s
                      Time elapsed: 00:44:27
                               ETA: 00:29:18

################################################################################
                     [1m Learning iteration 1206/2000 [0m                     

                       Computation: 47803 steps/s (collection: 1.940s, learning 0.116s)
             Mean action noise std: 2.65
          Mean value_function loss: 167.5231
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.6355
                       Mean reward: 800.95
               Mean episode length: 217.22
    Episode_Reward/reaching_object: 0.9245
     Episode_Reward/lifting_object: 150.3415
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 2.06s
                      Time elapsed: 00:44:29
                               ETA: 00:29:16

################################################################################
                     [1m Learning iteration 1207/2000 [0m                     

                       Computation: 49952 steps/s (collection: 1.878s, learning 0.090s)
             Mean action noise std: 2.65
          Mean value_function loss: 158.8875
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.6446
                       Mean reward: 808.86
               Mean episode length: 219.83
    Episode_Reward/reaching_object: 0.9709
     Episode_Reward/lifting_object: 159.2128
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 1.97s
                      Time elapsed: 00:44:31
                               ETA: 00:29:13

################################################################################
                     [1m Learning iteration 1208/2000 [0m                     

                       Computation: 49337 steps/s (collection: 1.888s, learning 0.104s)
             Mean action noise std: 2.65
          Mean value_function loss: 160.2024
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 51.6552
                       Mean reward: 771.31
               Mean episode length: 208.35
    Episode_Reward/reaching_object: 0.9549
     Episode_Reward/lifting_object: 156.3266
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 1.99s
                      Time elapsed: 00:44:33
                               ETA: 00:29:11

################################################################################
                     [1m Learning iteration 1209/2000 [0m                     

                       Computation: 49856 steps/s (collection: 1.888s, learning 0.084s)
             Mean action noise std: 2.65
          Mean value_function loss: 184.5464
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 51.6626
                       Mean reward: 796.53
               Mean episode length: 214.14
    Episode_Reward/reaching_object: 0.9668
     Episode_Reward/lifting_object: 159.0608
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 1.97s
                      Time elapsed: 00:44:35
                               ETA: 00:29:09

################################################################################
                     [1m Learning iteration 1210/2000 [0m                     

                       Computation: 49584 steps/s (collection: 1.877s, learning 0.105s)
             Mean action noise std: 2.66
          Mean value_function loss: 168.5181
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 51.6792
                       Mean reward: 847.05
               Mean episode length: 226.34
    Episode_Reward/reaching_object: 0.9545
     Episode_Reward/lifting_object: 156.5734
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 1.98s
                      Time elapsed: 00:44:37
                               ETA: 00:29:06

################################################################################
                     [1m Learning iteration 1211/2000 [0m                     

                       Computation: 49947 steps/s (collection: 1.868s, learning 0.100s)
             Mean action noise std: 2.66
          Mean value_function loss: 184.5033
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 51.6946
                       Mean reward: 785.30
               Mean episode length: 211.86
    Episode_Reward/reaching_object: 0.9617
     Episode_Reward/lifting_object: 157.6439
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 1.97s
                      Time elapsed: 00:44:39
                               ETA: 00:29:04

################################################################################
                     [1m Learning iteration 1212/2000 [0m                     

                       Computation: 48546 steps/s (collection: 1.889s, learning 0.136s)
             Mean action noise std: 2.66
          Mean value_function loss: 181.8542
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 51.7109
                       Mean reward: 803.82
               Mean episode length: 216.67
    Episode_Reward/reaching_object: 0.9537
     Episode_Reward/lifting_object: 156.3565
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 2.02s
                      Time elapsed: 00:44:41
                               ETA: 00:29:02

################################################################################
                     [1m Learning iteration 1213/2000 [0m                     

                       Computation: 49387 steps/s (collection: 1.877s, learning 0.114s)
             Mean action noise std: 2.66
          Mean value_function loss: 151.7792
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 51.7194
                       Mean reward: 749.93
               Mean episode length: 203.04
    Episode_Reward/reaching_object: 0.9492
     Episode_Reward/lifting_object: 156.0795
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 1.99s
                      Time elapsed: 00:44:43
                               ETA: 00:28:59

################################################################################
                     [1m Learning iteration 1214/2000 [0m                     

                       Computation: 49222 steps/s (collection: 1.895s, learning 0.103s)
             Mean action noise std: 2.66
          Mean value_function loss: 165.6110
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 51.7278
                       Mean reward: 781.40
               Mean episode length: 212.45
    Episode_Reward/reaching_object: 0.9469
     Episode_Reward/lifting_object: 154.6765
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 2.00s
                      Time elapsed: 00:44:45
                               ETA: 00:28:57

################################################################################
                     [1m Learning iteration 1215/2000 [0m                     

                       Computation: 49744 steps/s (collection: 1.882s, learning 0.094s)
             Mean action noise std: 2.66
          Mean value_function loss: 182.7442
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.7376
                       Mean reward: 813.71
               Mean episode length: 219.03
    Episode_Reward/reaching_object: 0.9376
     Episode_Reward/lifting_object: 153.2038
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 1.98s
                      Time elapsed: 00:44:47
                               ETA: 00:28:55

################################################################################
                     [1m Learning iteration 1216/2000 [0m                     

                       Computation: 50178 steps/s (collection: 1.871s, learning 0.088s)
             Mean action noise std: 2.66
          Mean value_function loss: 182.7349
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 51.7529
                       Mean reward: 735.35
               Mean episode length: 201.34
    Episode_Reward/reaching_object: 0.9464
     Episode_Reward/lifting_object: 155.3229
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 1.96s
                      Time elapsed: 00:44:49
                               ETA: 00:28:52

################################################################################
                     [1m Learning iteration 1217/2000 [0m                     

                       Computation: 48486 steps/s (collection: 1.939s, learning 0.089s)
             Mean action noise std: 2.67
          Mean value_function loss: 183.8620
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 51.7742
                       Mean reward: 790.03
               Mean episode length: 213.91
    Episode_Reward/reaching_object: 0.9414
     Episode_Reward/lifting_object: 154.0323
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 2.03s
                      Time elapsed: 00:44:51
                               ETA: 00:28:50

################################################################################
                     [1m Learning iteration 1218/2000 [0m                     

                       Computation: 49506 steps/s (collection: 1.896s, learning 0.090s)
             Mean action noise std: 2.67
          Mean value_function loss: 151.0926
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.7927
                       Mean reward: 818.89
               Mean episode length: 221.30
    Episode_Reward/reaching_object: 0.9524
     Episode_Reward/lifting_object: 155.6774
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 1.99s
                      Time elapsed: 00:44:53
                               ETA: 00:28:47

################################################################################
                     [1m Learning iteration 1219/2000 [0m                     

                       Computation: 48602 steps/s (collection: 1.924s, learning 0.099s)
             Mean action noise std: 2.67
          Mean value_function loss: 203.2876
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.8088
                       Mean reward: 833.62
               Mean episode length: 223.79
    Episode_Reward/reaching_object: 0.9781
     Episode_Reward/lifting_object: 160.7706
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 2.02s
                      Time elapsed: 00:44:55
                               ETA: 00:28:45

################################################################################
                     [1m Learning iteration 1220/2000 [0m                     

                       Computation: 48631 steps/s (collection: 1.900s, learning 0.121s)
             Mean action noise std: 2.67
          Mean value_function loss: 217.2686
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.8276
                       Mean reward: 789.13
               Mean episode length: 215.04
    Episode_Reward/reaching_object: 0.9281
     Episode_Reward/lifting_object: 151.2421
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 2.02s
                      Time elapsed: 00:44:57
                               ETA: 00:28:43

################################################################################
                     [1m Learning iteration 1221/2000 [0m                     

                       Computation: 47701 steps/s (collection: 1.944s, learning 0.117s)
             Mean action noise std: 2.67
          Mean value_function loss: 180.0682
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 51.8406
                       Mean reward: 779.78
               Mean episode length: 211.33
    Episode_Reward/reaching_object: 0.9548
     Episode_Reward/lifting_object: 156.4918
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 2.06s
                      Time elapsed: 00:44:59
                               ETA: 00:28:41

################################################################################
                     [1m Learning iteration 1222/2000 [0m                     

                       Computation: 47172 steps/s (collection: 1.962s, learning 0.122s)
             Mean action noise std: 2.68
          Mean value_function loss: 192.0595
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 51.8541
                       Mean reward: 838.27
               Mean episode length: 225.63
    Episode_Reward/reaching_object: 0.9696
     Episode_Reward/lifting_object: 159.3850
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 2.08s
                      Time elapsed: 00:45:01
                               ETA: 00:28:38

################################################################################
                     [1m Learning iteration 1223/2000 [0m                     

                       Computation: 48367 steps/s (collection: 1.943s, learning 0.089s)
             Mean action noise std: 2.68
          Mean value_function loss: 207.3858
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 51.8673
                       Mean reward: 781.95
               Mean episode length: 211.89
    Episode_Reward/reaching_object: 0.9482
     Episode_Reward/lifting_object: 155.1083
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 2.03s
                      Time elapsed: 00:45:03
                               ETA: 00:28:36

################################################################################
                     [1m Learning iteration 1224/2000 [0m                     

                       Computation: 46597 steps/s (collection: 2.008s, learning 0.102s)
             Mean action noise std: 2.68
          Mean value_function loss: 139.2293
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 51.8763
                       Mean reward: 829.33
               Mean episode length: 222.12
    Episode_Reward/reaching_object: 0.9745
     Episode_Reward/lifting_object: 160.0719
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 2.11s
                      Time elapsed: 00:45:05
                               ETA: 00:28:34

################################################################################
                     [1m Learning iteration 1225/2000 [0m                     

                       Computation: 48630 steps/s (collection: 1.918s, learning 0.103s)
             Mean action noise std: 2.68
          Mean value_function loss: 162.3928
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 51.8826
                       Mean reward: 810.19
               Mean episode length: 220.53
    Episode_Reward/reaching_object: 0.9826
     Episode_Reward/lifting_object: 161.9861
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 2.02s
                      Time elapsed: 00:45:07
                               ETA: 00:28:31

################################################################################
                     [1m Learning iteration 1226/2000 [0m                     

                       Computation: 48926 steps/s (collection: 1.916s, learning 0.093s)
             Mean action noise std: 2.68
          Mean value_function loss: 206.9473
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 51.8917
                       Mean reward: 805.36
               Mean episode length: 216.54
    Episode_Reward/reaching_object: 0.9271
     Episode_Reward/lifting_object: 152.2379
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 2.01s
                      Time elapsed: 00:45:09
                               ETA: 00:28:29

################################################################################
                     [1m Learning iteration 1227/2000 [0m                     

                       Computation: 50164 steps/s (collection: 1.868s, learning 0.092s)
             Mean action noise std: 2.68
          Mean value_function loss: 187.8219
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 51.9062
                       Mean reward: 779.24
               Mean episode length: 211.55
    Episode_Reward/reaching_object: 0.9444
     Episode_Reward/lifting_object: 155.0778
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 1.96s
                      Time elapsed: 00:45:11
                               ETA: 00:28:27

################################################################################
                     [1m Learning iteration 1228/2000 [0m                     

                       Computation: 49174 steps/s (collection: 1.910s, learning 0.089s)
             Mean action noise std: 2.68
          Mean value_function loss: 180.5294
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 51.9164
                       Mean reward: 737.89
               Mean episode length: 202.42
    Episode_Reward/reaching_object: 0.9364
     Episode_Reward/lifting_object: 153.6248
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 2.00s
                      Time elapsed: 00:45:13
                               ETA: 00:28:24

################################################################################
                     [1m Learning iteration 1229/2000 [0m                     

                       Computation: 49570 steps/s (collection: 1.891s, learning 0.093s)
             Mean action noise std: 2.68
          Mean value_function loss: 194.0857
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.9264
                       Mean reward: 786.84
               Mean episode length: 212.98
    Episode_Reward/reaching_object: 0.9559
     Episode_Reward/lifting_object: 156.6700
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 1.98s
                      Time elapsed: 00:45:15
                               ETA: 00:28:22

################################################################################
                     [1m Learning iteration 1230/2000 [0m                     

                       Computation: 49101 steps/s (collection: 1.908s, learning 0.094s)
             Mean action noise std: 2.69
          Mean value_function loss: 198.3614
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 51.9471
                       Mean reward: 742.68
               Mean episode length: 203.19
    Episode_Reward/reaching_object: 0.8993
     Episode_Reward/lifting_object: 146.5678
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 2.00s
                      Time elapsed: 00:45:17
                               ETA: 00:28:20

################################################################################
                     [1m Learning iteration 1231/2000 [0m                     

                       Computation: 48663 steps/s (collection: 1.929s, learning 0.092s)
             Mean action noise std: 2.69
          Mean value_function loss: 147.4947
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.9639
                       Mean reward: 789.79
               Mean episode length: 213.26
    Episode_Reward/reaching_object: 0.9319
     Episode_Reward/lifting_object: 152.2747
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 2.02s
                      Time elapsed: 00:45:19
                               ETA: 00:28:17

################################################################################
                     [1m Learning iteration 1232/2000 [0m                     

                       Computation: 48625 steps/s (collection: 1.926s, learning 0.096s)
             Mean action noise std: 2.69
          Mean value_function loss: 138.2968
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 51.9780
                       Mean reward: 802.78
               Mean episode length: 216.28
    Episode_Reward/reaching_object: 0.9742
     Episode_Reward/lifting_object: 160.4154
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 2.02s
                      Time elapsed: 00:45:21
                               ETA: 00:28:15

################################################################################
                     [1m Learning iteration 1233/2000 [0m                     

                       Computation: 49281 steps/s (collection: 1.905s, learning 0.090s)
             Mean action noise std: 2.69
          Mean value_function loss: 131.4731
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 51.9872
                       Mean reward: 826.87
               Mean episode length: 221.48
    Episode_Reward/reaching_object: 0.9940
     Episode_Reward/lifting_object: 164.9312
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 1.99s
                      Time elapsed: 00:45:23
                               ETA: 00:28:13

################################################################################
                     [1m Learning iteration 1234/2000 [0m                     

                       Computation: 47262 steps/s (collection: 1.969s, learning 0.111s)
             Mean action noise std: 2.69
          Mean value_function loss: 160.8335
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 52.0006
                       Mean reward: 775.23
               Mean episode length: 209.85
    Episode_Reward/reaching_object: 0.9585
     Episode_Reward/lifting_object: 157.6228
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 2.08s
                      Time elapsed: 00:45:26
                               ETA: 00:28:10

################################################################################
                     [1m Learning iteration 1235/2000 [0m                     

                       Computation: 46389 steps/s (collection: 1.950s, learning 0.169s)
             Mean action noise std: 2.69
          Mean value_function loss: 159.7996
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 52.0125
                       Mean reward: 827.26
               Mean episode length: 223.39
    Episode_Reward/reaching_object: 0.9618
     Episode_Reward/lifting_object: 158.3849
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 2.12s
                      Time elapsed: 00:45:28
                               ETA: 00:28:08

################################################################################
                     [1m Learning iteration 1236/2000 [0m                     

                       Computation: 44031 steps/s (collection: 2.067s, learning 0.165s)
             Mean action noise std: 2.70
          Mean value_function loss: 165.0157
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.0269
                       Mean reward: 766.05
               Mean episode length: 208.25
    Episode_Reward/reaching_object: 0.9477
     Episode_Reward/lifting_object: 156.2687
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 2.23s
                      Time elapsed: 00:45:30
                               ETA: 00:28:06

################################################################################
                     [1m Learning iteration 1237/2000 [0m                     

                       Computation: 48162 steps/s (collection: 1.923s, learning 0.118s)
             Mean action noise std: 2.70
          Mean value_function loss: 169.7124
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 52.0459
                       Mean reward: 751.79
               Mean episode length: 204.91
    Episode_Reward/reaching_object: 0.9283
     Episode_Reward/lifting_object: 152.8641
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 2.04s
                      Time elapsed: 00:45:32
                               ETA: 00:28:04

################################################################################
                     [1m Learning iteration 1238/2000 [0m                     

                       Computation: 48017 steps/s (collection: 1.939s, learning 0.108s)
             Mean action noise std: 2.70
          Mean value_function loss: 161.7719
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 52.0669
                       Mean reward: 775.71
               Mean episode length: 209.28
    Episode_Reward/reaching_object: 0.9664
     Episode_Reward/lifting_object: 160.0593
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 2.05s
                      Time elapsed: 00:45:34
                               ETA: 00:28:01

################################################################################
                     [1m Learning iteration 1239/2000 [0m                     

                       Computation: 48199 steps/s (collection: 1.921s, learning 0.118s)
             Mean action noise std: 2.70
          Mean value_function loss: 142.0050
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 52.0801
                       Mean reward: 809.10
               Mean episode length: 217.74
    Episode_Reward/reaching_object: 0.9514
     Episode_Reward/lifting_object: 156.9194
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 2.04s
                      Time elapsed: 00:45:36
                               ETA: 00:27:59

################################################################################
                     [1m Learning iteration 1240/2000 [0m                     

                       Computation: 47909 steps/s (collection: 1.949s, learning 0.103s)
             Mean action noise std: 2.70
          Mean value_function loss: 191.6344
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 52.0976
                       Mean reward: 771.20
               Mean episode length: 209.36
    Episode_Reward/reaching_object: 0.9463
     Episode_Reward/lifting_object: 155.5214
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 2.05s
                      Time elapsed: 00:45:38
                               ETA: 00:27:57

################################################################################
                     [1m Learning iteration 1241/2000 [0m                     

                       Computation: 47306 steps/s (collection: 1.954s, learning 0.124s)
             Mean action noise std: 2.71
          Mean value_function loss: 179.3592
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 52.1050
                       Mean reward: 798.19
               Mean episode length: 214.37
    Episode_Reward/reaching_object: 0.9557
     Episode_Reward/lifting_object: 157.5623
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 2.08s
                      Time elapsed: 00:45:40
                               ETA: 00:27:54

################################################################################
                     [1m Learning iteration 1242/2000 [0m                     

                       Computation: 48011 steps/s (collection: 1.944s, learning 0.104s)
             Mean action noise std: 2.71
          Mean value_function loss: 199.8158
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 52.1157
                       Mean reward: 770.39
               Mean episode length: 207.79
    Episode_Reward/reaching_object: 0.9370
     Episode_Reward/lifting_object: 154.2289
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 2.05s
                      Time elapsed: 00:45:42
                               ETA: 00:27:52

################################################################################
                     [1m Learning iteration 1243/2000 [0m                     

                       Computation: 47219 steps/s (collection: 1.940s, learning 0.142s)
             Mean action noise std: 2.71
          Mean value_function loss: 156.3418
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.1304
                       Mean reward: 741.81
               Mean episode length: 202.59
    Episode_Reward/reaching_object: 0.9506
     Episode_Reward/lifting_object: 156.3896
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 2.08s
                      Time elapsed: 00:45:44
                               ETA: 00:27:50

################################################################################
                     [1m Learning iteration 1244/2000 [0m                     

                       Computation: 45853 steps/s (collection: 1.989s, learning 0.155s)
             Mean action noise std: 2.71
          Mean value_function loss: 167.2866
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 52.1484
                       Mean reward: 811.40
               Mean episode length: 219.38
    Episode_Reward/reaching_object: 0.9773
     Episode_Reward/lifting_object: 161.4624
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 2.14s
                      Time elapsed: 00:45:46
                               ETA: 00:27:48

################################################################################
                     [1m Learning iteration 1245/2000 [0m                     

                       Computation: 47073 steps/s (collection: 1.987s, learning 0.101s)
             Mean action noise std: 2.71
          Mean value_function loss: 182.2912
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.1622
                       Mean reward: 675.28
               Mean episode length: 187.75
    Episode_Reward/reaching_object: 0.9287
     Episode_Reward/lifting_object: 152.7287
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 2.09s
                      Time elapsed: 00:45:49
                               ETA: 00:27:45

################################################################################
                     [1m Learning iteration 1246/2000 [0m                     

                       Computation: 43598 steps/s (collection: 2.089s, learning 0.166s)
             Mean action noise std: 2.71
          Mean value_function loss: 175.0740
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.1757
                       Mean reward: 752.23
               Mean episode length: 204.06
    Episode_Reward/reaching_object: 0.9623
     Episode_Reward/lifting_object: 158.0517
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 2.25s
                      Time elapsed: 00:45:51
                               ETA: 00:27:43

################################################################################
                     [1m Learning iteration 1247/2000 [0m                     

                       Computation: 47246 steps/s (collection: 1.985s, learning 0.096s)
             Mean action noise std: 2.72
          Mean value_function loss: 170.4278
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 52.1886
                       Mean reward: 793.06
               Mean episode length: 213.95
    Episode_Reward/reaching_object: 0.9430
     Episode_Reward/lifting_object: 155.1279
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 2.08s
                      Time elapsed: 00:45:53
                               ETA: 00:27:41

################################################################################
                     [1m Learning iteration 1248/2000 [0m                     

                       Computation: 47469 steps/s (collection: 1.972s, learning 0.099s)
             Mean action noise std: 2.72
          Mean value_function loss: 181.4586
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 52.1982
                       Mean reward: 810.83
               Mean episode length: 218.38
    Episode_Reward/reaching_object: 0.9508
     Episode_Reward/lifting_object: 156.4374
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 2.07s
                      Time elapsed: 00:45:55
                               ETA: 00:27:38

################################################################################
                     [1m Learning iteration 1249/2000 [0m                     

                       Computation: 47639 steps/s (collection: 1.967s, learning 0.096s)
             Mean action noise std: 2.72
          Mean value_function loss: 185.9253
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 52.2068
                       Mean reward: 749.92
               Mean episode length: 203.37
    Episode_Reward/reaching_object: 0.9594
     Episode_Reward/lifting_object: 158.4068
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 2.06s
                      Time elapsed: 00:45:57
                               ETA: 00:27:36

################################################################################
                     [1m Learning iteration 1250/2000 [0m                     

                       Computation: 45325 steps/s (collection: 2.039s, learning 0.130s)
             Mean action noise std: 2.72
          Mean value_function loss: 146.1291
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.2207
                       Mean reward: 864.91
               Mean episode length: 230.43
    Episode_Reward/reaching_object: 0.9924
     Episode_Reward/lifting_object: 163.9891
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 2.17s
                      Time elapsed: 00:45:59
                               ETA: 00:27:34

################################################################################
                     [1m Learning iteration 1251/2000 [0m                     

                       Computation: 45651 steps/s (collection: 2.013s, learning 0.140s)
             Mean action noise std: 2.72
          Mean value_function loss: 166.5222
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.2357
                       Mean reward: 868.82
               Mean episode length: 231.44
    Episode_Reward/reaching_object: 0.9605
     Episode_Reward/lifting_object: 158.6675
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 2.15s
                      Time elapsed: 00:46:01
                               ETA: 00:27:32

################################################################################
                     [1m Learning iteration 1252/2000 [0m                     

                       Computation: 45895 steps/s (collection: 2.020s, learning 0.122s)
             Mean action noise std: 2.72
          Mean value_function loss: 155.5211
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.2488
                       Mean reward: 817.46
               Mean episode length: 219.34
    Episode_Reward/reaching_object: 0.9591
     Episode_Reward/lifting_object: 157.6925
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 2.14s
                      Time elapsed: 00:46:03
                               ETA: 00:27:29

################################################################################
                     [1m Learning iteration 1253/2000 [0m                     

                       Computation: 44075 steps/s (collection: 2.124s, learning 0.107s)
             Mean action noise std: 2.73
          Mean value_function loss: 179.7206
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 52.2626
                       Mean reward: 828.82
               Mean episode length: 223.69
    Episode_Reward/reaching_object: 0.9436
     Episode_Reward/lifting_object: 155.3854
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 2.23s
                      Time elapsed: 00:46:06
                               ETA: 00:27:27

################################################################################
                     [1m Learning iteration 1254/2000 [0m                     

                       Computation: 46960 steps/s (collection: 1.989s, learning 0.104s)
             Mean action noise std: 2.73
          Mean value_function loss: 189.5043
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 52.2732
                       Mean reward: 788.66
               Mean episode length: 212.63
    Episode_Reward/reaching_object: 0.9371
     Episode_Reward/lifting_object: 154.0015
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 2.09s
                      Time elapsed: 00:46:08
                               ETA: 00:27:25

################################################################################
                     [1m Learning iteration 1255/2000 [0m                     

                       Computation: 45192 steps/s (collection: 2.058s, learning 0.117s)
             Mean action noise std: 2.73
          Mean value_function loss: 167.6519
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.2781
                       Mean reward: 756.18
               Mean episode length: 205.31
    Episode_Reward/reaching_object: 0.9316
     Episode_Reward/lifting_object: 152.9436
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 2.18s
                      Time elapsed: 00:46:10
                               ETA: 00:27:23

################################################################################
                     [1m Learning iteration 1256/2000 [0m                     

                       Computation: 47683 steps/s (collection: 1.968s, learning 0.094s)
             Mean action noise std: 2.73
          Mean value_function loss: 210.1876
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 52.2891
                       Mean reward: 758.51
               Mean episode length: 206.93
    Episode_Reward/reaching_object: 0.9662
     Episode_Reward/lifting_object: 159.2681
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 2.06s
                      Time elapsed: 00:46:12
                               ETA: 00:27:21

################################################################################
                     [1m Learning iteration 1257/2000 [0m                     

                       Computation: 48138 steps/s (collection: 1.941s, learning 0.102s)
             Mean action noise std: 2.73
          Mean value_function loss: 168.7098
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 52.3030
                       Mean reward: 782.71
               Mean episode length: 211.01
    Episode_Reward/reaching_object: 0.9715
     Episode_Reward/lifting_object: 160.7179
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 2.04s
                      Time elapsed: 00:46:14
                               ETA: 00:27:18

################################################################################
                     [1m Learning iteration 1258/2000 [0m                     

                       Computation: 47780 steps/s (collection: 1.958s, learning 0.099s)
             Mean action noise std: 2.73
          Mean value_function loss: 186.6093
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 52.3084
                       Mean reward: 764.87
               Mean episode length: 207.93
    Episode_Reward/reaching_object: 0.9524
     Episode_Reward/lifting_object: 156.8215
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 2.06s
                      Time elapsed: 00:46:16
                               ETA: 00:27:16

################################################################################
                     [1m Learning iteration 1259/2000 [0m                     

                       Computation: 48060 steps/s (collection: 1.948s, learning 0.097s)
             Mean action noise std: 2.73
          Mean value_function loss: 196.3439
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 52.3138
                       Mean reward: 813.41
               Mean episode length: 218.83
    Episode_Reward/reaching_object: 0.9635
     Episode_Reward/lifting_object: 158.7103
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 2.05s
                      Time elapsed: 00:46:18
                               ETA: 00:27:14

################################################################################
                     [1m Learning iteration 1260/2000 [0m                     

                       Computation: 48115 steps/s (collection: 1.939s, learning 0.104s)
             Mean action noise std: 2.73
          Mean value_function loss: 156.4229
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.3211
                       Mean reward: 758.45
               Mean episode length: 205.96
    Episode_Reward/reaching_object: 0.9622
     Episode_Reward/lifting_object: 158.1642
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 2.04s
                      Time elapsed: 00:46:20
                               ETA: 00:27:11

################################################################################
                     [1m Learning iteration 1261/2000 [0m                     

                       Computation: 48198 steps/s (collection: 1.940s, learning 0.100s)
             Mean action noise std: 2.73
          Mean value_function loss: 139.1785
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 52.3263
                       Mean reward: 827.81
               Mean episode length: 222.27
    Episode_Reward/reaching_object: 1.0098
     Episode_Reward/lifting_object: 167.2681
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 2.04s
                      Time elapsed: 00:46:22
                               ETA: 00:27:09

################################################################################
                     [1m Learning iteration 1262/2000 [0m                     

                       Computation: 48035 steps/s (collection: 1.934s, learning 0.113s)
             Mean action noise std: 2.74
          Mean value_function loss: 164.1572
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.3347
                       Mean reward: 842.84
               Mean episode length: 225.15
    Episode_Reward/reaching_object: 0.9974
     Episode_Reward/lifting_object: 164.7961
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 2.05s
                      Time elapsed: 00:46:24
                               ETA: 00:27:07

################################################################################
                     [1m Learning iteration 1263/2000 [0m                     

                       Computation: 48535 steps/s (collection: 1.925s, learning 0.101s)
             Mean action noise std: 2.74
          Mean value_function loss: 155.4582
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 52.3468
                       Mean reward: 872.25
               Mean episode length: 232.61
    Episode_Reward/reaching_object: 0.9883
     Episode_Reward/lifting_object: 162.6237
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 2.03s
                      Time elapsed: 00:46:26
                               ETA: 00:27:04

################################################################################
                     [1m Learning iteration 1264/2000 [0m                     

                       Computation: 45947 steps/s (collection: 1.986s, learning 0.154s)
             Mean action noise std: 2.74
          Mean value_function loss: 167.7309
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 52.3609
                       Mean reward: 776.90
               Mean episode length: 208.89
    Episode_Reward/reaching_object: 0.9531
     Episode_Reward/lifting_object: 156.7509
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 2.14s
                      Time elapsed: 00:46:28
                               ETA: 00:27:02

################################################################################
                     [1m Learning iteration 1265/2000 [0m                     

                       Computation: 46102 steps/s (collection: 2.001s, learning 0.132s)
             Mean action noise std: 2.74
          Mean value_function loss: 173.4281
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 52.3741
                       Mean reward: 814.89
               Mean episode length: 218.52
    Episode_Reward/reaching_object: 0.9666
     Episode_Reward/lifting_object: 159.1124
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 2.13s
                      Time elapsed: 00:46:31
                               ETA: 00:27:00

################################################################################
                     [1m Learning iteration 1266/2000 [0m                     

                       Computation: 48536 steps/s (collection: 1.926s, learning 0.099s)
             Mean action noise std: 2.74
          Mean value_function loss: 159.6776
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 52.3935
                       Mean reward: 806.01
               Mean episode length: 218.76
    Episode_Reward/reaching_object: 0.9986
     Episode_Reward/lifting_object: 165.2151
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 2.03s
                      Time elapsed: 00:46:33
                               ETA: 00:26:58

################################################################################
                     [1m Learning iteration 1267/2000 [0m                     

                       Computation: 45338 steps/s (collection: 2.037s, learning 0.132s)
             Mean action noise std: 2.74
          Mean value_function loss: 142.1040
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 52.4049
                       Mean reward: 848.68
               Mean episode length: 227.07
    Episode_Reward/reaching_object: 0.9783
     Episode_Reward/lifting_object: 160.9535
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 2.17s
                      Time elapsed: 00:46:35
                               ETA: 00:26:55

################################################################################
                     [1m Learning iteration 1268/2000 [0m                     

                       Computation: 47569 steps/s (collection: 1.969s, learning 0.098s)
             Mean action noise std: 2.74
          Mean value_function loss: 154.4970
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 52.4158
                       Mean reward: 831.71
               Mean episode length: 222.75
    Episode_Reward/reaching_object: 1.0007
     Episode_Reward/lifting_object: 165.1179
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 2.07s
                      Time elapsed: 00:46:37
                               ETA: 00:26:53

################################################################################
                     [1m Learning iteration 1269/2000 [0m                     

                       Computation: 48066 steps/s (collection: 1.954s, learning 0.091s)
             Mean action noise std: 2.75
          Mean value_function loss: 151.9907
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 52.4251
                       Mean reward: 813.37
               Mean episode length: 218.93
    Episode_Reward/reaching_object: 0.9951
     Episode_Reward/lifting_object: 164.3515
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 2.05s
                      Time elapsed: 00:46:39
                               ETA: 00:26:51

################################################################################
                     [1m Learning iteration 1270/2000 [0m                     

                       Computation: 47395 steps/s (collection: 1.951s, learning 0.123s)
             Mean action noise std: 2.75
          Mean value_function loss: 144.5928
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.4288
                       Mean reward: 801.65
               Mean episode length: 215.80
    Episode_Reward/reaching_object: 0.9361
     Episode_Reward/lifting_object: 153.6463
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 2.07s
                      Time elapsed: 00:46:41
                               ETA: 00:26:49

################################################################################
                     [1m Learning iteration 1271/2000 [0m                     

                       Computation: 46908 steps/s (collection: 1.966s, learning 0.130s)
             Mean action noise std: 2.75
          Mean value_function loss: 146.0524
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.4339
                       Mean reward: 844.70
               Mean episode length: 226.34
    Episode_Reward/reaching_object: 0.9865
     Episode_Reward/lifting_object: 162.3312
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 2.10s
                      Time elapsed: 00:46:43
                               ETA: 00:26:46

################################################################################
                     [1m Learning iteration 1272/2000 [0m                     

                       Computation: 47766 steps/s (collection: 1.953s, learning 0.105s)
             Mean action noise std: 2.75
          Mean value_function loss: 150.7121
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.4480
                       Mean reward: 758.55
               Mean episode length: 205.53
    Episode_Reward/reaching_object: 0.9783
     Episode_Reward/lifting_object: 161.2392
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 2.06s
                      Time elapsed: 00:46:45
                               ETA: 00:26:44

################################################################################
                     [1m Learning iteration 1273/2000 [0m                     

                       Computation: 48173 steps/s (collection: 1.947s, learning 0.094s)
             Mean action noise std: 2.75
          Mean value_function loss: 124.5581
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 52.4670
                       Mean reward: 787.20
               Mean episode length: 212.93
    Episode_Reward/reaching_object: 1.0112
     Episode_Reward/lifting_object: 167.5964
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 2.04s
                      Time elapsed: 00:46:47
                               ETA: 00:26:42

################################################################################
                     [1m Learning iteration 1274/2000 [0m                     

                       Computation: 46945 steps/s (collection: 1.988s, learning 0.106s)
             Mean action noise std: 2.75
          Mean value_function loss: 146.0297
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 52.4823
                       Mean reward: 811.90
               Mean episode length: 219.18
    Episode_Reward/reaching_object: 0.9836
     Episode_Reward/lifting_object: 161.7654
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 2.09s
                      Time elapsed: 00:46:49
                               ETA: 00:26:39

################################################################################
                     [1m Learning iteration 1275/2000 [0m                     

                       Computation: 48341 steps/s (collection: 1.933s, learning 0.100s)
             Mean action noise std: 2.75
          Mean value_function loss: 150.3490
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 52.4947
                       Mean reward: 805.54
               Mean episode length: 218.29
    Episode_Reward/reaching_object: 0.9718
     Episode_Reward/lifting_object: 158.9268
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 2.03s
                      Time elapsed: 00:46:51
                               ETA: 00:26:37

################################################################################
                     [1m Learning iteration 1276/2000 [0m                     

                       Computation: 48532 steps/s (collection: 1.926s, learning 0.100s)
             Mean action noise std: 2.76
          Mean value_function loss: 168.9255
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 52.5058
                       Mean reward: 791.55
               Mean episode length: 212.57
    Episode_Reward/reaching_object: 0.9810
     Episode_Reward/lifting_object: 161.8469
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 2.03s
                      Time elapsed: 00:46:53
                               ETA: 00:26:35

################################################################################
                     [1m Learning iteration 1277/2000 [0m                     

                       Computation: 46379 steps/s (collection: 1.989s, learning 0.130s)
             Mean action noise std: 2.76
          Mean value_function loss: 141.5466
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.5186
                       Mean reward: 830.95
               Mean episode length: 222.13
    Episode_Reward/reaching_object: 0.9771
     Episode_Reward/lifting_object: 161.2542
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 2.12s
                      Time elapsed: 00:46:55
                               ETA: 00:26:33

################################################################################
                     [1m Learning iteration 1278/2000 [0m                     

                       Computation: 48904 steps/s (collection: 1.914s, learning 0.097s)
             Mean action noise std: 2.76
          Mean value_function loss: 137.7505
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 52.5349
                       Mean reward: 841.27
               Mean episode length: 225.32
    Episode_Reward/reaching_object: 0.9907
     Episode_Reward/lifting_object: 163.2676
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 2.01s
                      Time elapsed: 00:46:57
                               ETA: 00:26:30

################################################################################
                     [1m Learning iteration 1279/2000 [0m                     

                       Computation: 46057 steps/s (collection: 1.981s, learning 0.154s)
             Mean action noise std: 2.76
          Mean value_function loss: 153.4615
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 52.5452
                       Mean reward: 779.13
               Mean episode length: 210.92
    Episode_Reward/reaching_object: 0.9984
     Episode_Reward/lifting_object: 164.8235
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 2.13s
                      Time elapsed: 00:47:00
                               ETA: 00:26:28

################################################################################
                     [1m Learning iteration 1280/2000 [0m                     

                       Computation: 47603 steps/s (collection: 1.950s, learning 0.115s)
             Mean action noise std: 2.76
          Mean value_function loss: 139.5892
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.5548
                       Mean reward: 846.14
               Mean episode length: 227.11
    Episode_Reward/reaching_object: 0.9863
     Episode_Reward/lifting_object: 162.8179
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 2.07s
                      Time elapsed: 00:47:02
                               ETA: 00:26:26

################################################################################
                     [1m Learning iteration 1281/2000 [0m                     

                       Computation: 47715 steps/s (collection: 1.935s, learning 0.125s)
             Mean action noise std: 2.76
          Mean value_function loss: 145.0925
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 52.5634
                       Mean reward: 806.31
               Mean episode length: 216.87
    Episode_Reward/reaching_object: 0.9701
     Episode_Reward/lifting_object: 160.2632
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 2.06s
                      Time elapsed: 00:47:04
                               ETA: 00:26:23

################################################################################
                     [1m Learning iteration 1282/2000 [0m                     

                       Computation: 48744 steps/s (collection: 1.923s, learning 0.094s)
             Mean action noise std: 2.76
          Mean value_function loss: 145.0873
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 52.5724
                       Mean reward: 803.44
               Mean episode length: 216.23
    Episode_Reward/reaching_object: 1.0030
     Episode_Reward/lifting_object: 165.9868
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 2.02s
                      Time elapsed: 00:47:06
                               ETA: 00:26:21

################################################################################
                     [1m Learning iteration 1283/2000 [0m                     

                       Computation: 47591 steps/s (collection: 1.967s, learning 0.099s)
             Mean action noise std: 2.77
          Mean value_function loss: 138.6785
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 52.5851
                       Mean reward: 841.22
               Mean episode length: 225.73
    Episode_Reward/reaching_object: 0.9743
     Episode_Reward/lifting_object: 160.5178
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 2.07s
                      Time elapsed: 00:47:08
                               ETA: 00:26:19

################################################################################
                     [1m Learning iteration 1284/2000 [0m                     

                       Computation: 48501 steps/s (collection: 1.929s, learning 0.098s)
             Mean action noise std: 2.77
          Mean value_function loss: 134.4315
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.5987
                       Mean reward: 843.23
               Mean episode length: 226.21
    Episode_Reward/reaching_object: 0.9931
     Episode_Reward/lifting_object: 163.4922
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 2.03s
                      Time elapsed: 00:47:10
                               ETA: 00:26:17

################################################################################
                     [1m Learning iteration 1285/2000 [0m                     

                       Computation: 47297 steps/s (collection: 1.959s, learning 0.119s)
             Mean action noise std: 2.77
          Mean value_function loss: 135.7157
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 52.6124
                       Mean reward: 820.19
               Mean episode length: 220.38
    Episode_Reward/reaching_object: 1.0022
     Episode_Reward/lifting_object: 164.8005
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 2.08s
                      Time elapsed: 00:47:12
                               ETA: 00:26:14

################################################################################
                     [1m Learning iteration 1286/2000 [0m                     

                       Computation: 47880 steps/s (collection: 1.958s, learning 0.096s)
             Mean action noise std: 2.77
          Mean value_function loss: 125.6868
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 52.6238
                       Mean reward: 882.48
               Mean episode length: 236.94
    Episode_Reward/reaching_object: 1.0062
     Episode_Reward/lifting_object: 165.9354
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 2.05s
                      Time elapsed: 00:47:14
                               ETA: 00:26:12

################################################################################
                     [1m Learning iteration 1287/2000 [0m                     

                       Computation: 48684 steps/s (collection: 1.920s, learning 0.099s)
             Mean action noise std: 2.77
          Mean value_function loss: 139.1088
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 52.6379
                       Mean reward: 842.33
               Mean episode length: 225.44
    Episode_Reward/reaching_object: 1.0132
     Episode_Reward/lifting_object: 168.1117
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 2.02s
                      Time elapsed: 00:47:16
                               ETA: 00:26:10

################################################################################
                     [1m Learning iteration 1288/2000 [0m                     

                       Computation: 47381 steps/s (collection: 1.971s, learning 0.104s)
             Mean action noise std: 2.78
          Mean value_function loss: 143.5907
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 52.6577
                       Mean reward: 848.50
               Mean episode length: 226.14
    Episode_Reward/reaching_object: 0.9724
     Episode_Reward/lifting_object: 160.8308
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 2.07s
                      Time elapsed: 00:47:18
                               ETA: 00:26:07

################################################################################
                     [1m Learning iteration 1289/2000 [0m                     

                       Computation: 47845 steps/s (collection: 1.964s, learning 0.091s)
             Mean action noise std: 2.78
          Mean value_function loss: 155.0250
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 52.6840
                       Mean reward: 866.58
               Mean episode length: 231.12
    Episode_Reward/reaching_object: 0.9813
     Episode_Reward/lifting_object: 162.2773
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 2.05s
                      Time elapsed: 00:47:20
                               ETA: 00:26:05

################################################################################
                     [1m Learning iteration 1290/2000 [0m                     

                       Computation: 48352 steps/s (collection: 1.931s, learning 0.103s)
             Mean action noise std: 2.78
          Mean value_function loss: 156.4886
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.6990
                       Mean reward: 814.16
               Mean episode length: 219.45
    Episode_Reward/reaching_object: 0.9733
     Episode_Reward/lifting_object: 160.6151
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 2.03s
                      Time elapsed: 00:47:22
                               ETA: 00:26:03

################################################################################
                     [1m Learning iteration 1291/2000 [0m                     

                       Computation: 47963 steps/s (collection: 1.945s, learning 0.105s)
             Mean action noise std: 2.78
          Mean value_function loss: 190.9284
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.7118
                       Mean reward: 791.77
               Mean episode length: 213.41
    Episode_Reward/reaching_object: 0.9620
     Episode_Reward/lifting_object: 158.9812
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 2.05s
                      Time elapsed: 00:47:24
                               ETA: 00:26:01

################################################################################
                     [1m Learning iteration 1292/2000 [0m                     

                       Computation: 47065 steps/s (collection: 1.948s, learning 0.141s)
             Mean action noise std: 2.78
          Mean value_function loss: 190.8398
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 52.7250
                       Mean reward: 819.19
               Mean episode length: 221.11
    Episode_Reward/reaching_object: 0.9659
     Episode_Reward/lifting_object: 159.0138
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 2.09s
                      Time elapsed: 00:47:26
                               ETA: 00:25:58

################################################################################
                     [1m Learning iteration 1293/2000 [0m                     

                       Computation: 45714 steps/s (collection: 2.059s, learning 0.091s)
             Mean action noise std: 2.79
          Mean value_function loss: 176.3671
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.7379
                       Mean reward: 769.33
               Mean episode length: 210.65
    Episode_Reward/reaching_object: 0.9647
     Episode_Reward/lifting_object: 158.8566
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 2.15s
                      Time elapsed: 00:47:28
                               ETA: 00:25:56

################################################################################
                     [1m Learning iteration 1294/2000 [0m                     

                       Computation: 46845 steps/s (collection: 1.989s, learning 0.110s)
             Mean action noise std: 2.79
          Mean value_function loss: 210.7482
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.7564
                       Mean reward: 826.72
               Mean episode length: 221.98
    Episode_Reward/reaching_object: 0.9716
     Episode_Reward/lifting_object: 161.1081
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 2.10s
                      Time elapsed: 00:47:31
                               ETA: 00:25:54

################################################################################
                     [1m Learning iteration 1295/2000 [0m                     

                       Computation: 45152 steps/s (collection: 2.065s, learning 0.113s)
             Mean action noise std: 2.79
          Mean value_function loss: 202.0639
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 52.7726
                       Mean reward: 803.19
               Mean episode length: 217.80
    Episode_Reward/reaching_object: 0.9826
     Episode_Reward/lifting_object: 162.2720
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 2.18s
                      Time elapsed: 00:47:33
                               ETA: 00:25:52

################################################################################
                     [1m Learning iteration 1296/2000 [0m                     

                       Computation: 47545 steps/s (collection: 1.973s, learning 0.094s)
             Mean action noise std: 2.79
          Mean value_function loss: 140.1690
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 52.7915
                       Mean reward: 745.34
               Mean episode length: 203.45
    Episode_Reward/reaching_object: 0.9607
     Episode_Reward/lifting_object: 158.4226
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 2.07s
                      Time elapsed: 00:47:35
                               ETA: 00:25:49

################################################################################
                     [1m Learning iteration 1297/2000 [0m                     

                       Computation: 47304 steps/s (collection: 1.947s, learning 0.131s)
             Mean action noise std: 2.79
          Mean value_function loss: 136.1444
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.8056
                       Mean reward: 859.78
               Mean episode length: 229.52
    Episode_Reward/reaching_object: 0.9892
     Episode_Reward/lifting_object: 164.0320
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 2.08s
                      Time elapsed: 00:47:37
                               ETA: 00:25:47

################################################################################
                     [1m Learning iteration 1298/2000 [0m                     

                       Computation: 47083 steps/s (collection: 1.993s, learning 0.095s)
             Mean action noise std: 2.79
          Mean value_function loss: 122.1549
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.8165
                       Mean reward: 834.74
               Mean episode length: 224.91
    Episode_Reward/reaching_object: 0.9955
     Episode_Reward/lifting_object: 165.1990
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 2.09s
                      Time elapsed: 00:47:39
                               ETA: 00:25:45

################################################################################
                     [1m Learning iteration 1299/2000 [0m                     

                       Computation: 47747 steps/s (collection: 1.953s, learning 0.106s)
             Mean action noise std: 2.80
          Mean value_function loss: 109.9733
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.8240
                       Mean reward: 863.84
               Mean episode length: 230.51
    Episode_Reward/reaching_object: 1.0052
     Episode_Reward/lifting_object: 166.7926
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 2.06s
                      Time elapsed: 00:47:41
                               ETA: 00:25:43

################################################################################
                     [1m Learning iteration 1300/2000 [0m                     

                       Computation: 48768 steps/s (collection: 1.917s, learning 0.099s)
             Mean action noise std: 2.80
          Mean value_function loss: 190.9866
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 52.8321
                       Mean reward: 830.54
               Mean episode length: 224.71
    Episode_Reward/reaching_object: 0.9927
     Episode_Reward/lifting_object: 163.7842
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 2.02s
                      Time elapsed: 00:47:43
                               ETA: 00:25:40

################################################################################
                     [1m Learning iteration 1301/2000 [0m                     

                       Computation: 46792 steps/s (collection: 1.995s, learning 0.106s)
             Mean action noise std: 2.80
          Mean value_function loss: 134.4842
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 52.8432
                       Mean reward: 821.14
               Mean episode length: 220.82
    Episode_Reward/reaching_object: 0.9867
     Episode_Reward/lifting_object: 163.6156
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 2.10s
                      Time elapsed: 00:47:45
                               ETA: 00:25:38

################################################################################
                     [1m Learning iteration 1302/2000 [0m                     

                       Computation: 47939 steps/s (collection: 1.927s, learning 0.124s)
             Mean action noise std: 2.80
          Mean value_function loss: 137.3773
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 52.8518
                       Mean reward: 853.71
               Mean episode length: 228.17
    Episode_Reward/reaching_object: 1.0123
     Episode_Reward/lifting_object: 168.3497
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 2.05s
                      Time elapsed: 00:47:47
                               ETA: 00:25:36

################################################################################
                     [1m Learning iteration 1303/2000 [0m                     

                       Computation: 48453 steps/s (collection: 1.938s, learning 0.091s)
             Mean action noise std: 2.80
          Mean value_function loss: 186.8944
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.8654
                       Mean reward: 787.08
               Mean episode length: 213.36
    Episode_Reward/reaching_object: 0.9866
     Episode_Reward/lifting_object: 163.3561
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 2.03s
                      Time elapsed: 00:47:49
                               ETA: 00:25:33

################################################################################
                     [1m Learning iteration 1304/2000 [0m                     

                       Computation: 47734 steps/s (collection: 1.966s, learning 0.093s)
             Mean action noise std: 2.80
          Mean value_function loss: 180.4387
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.8781
                       Mean reward: 805.24
               Mean episode length: 216.31
    Episode_Reward/reaching_object: 0.9697
     Episode_Reward/lifting_object: 160.3669
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 2.06s
                      Time elapsed: 00:47:51
                               ETA: 00:25:31

################################################################################
                     [1m Learning iteration 1305/2000 [0m                     

                       Computation: 48460 steps/s (collection: 1.932s, learning 0.096s)
             Mean action noise std: 2.80
          Mean value_function loss: 163.1966
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 52.8847
                       Mean reward: 776.60
               Mean episode length: 210.62
    Episode_Reward/reaching_object: 0.9514
     Episode_Reward/lifting_object: 157.3522
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 2.03s
                      Time elapsed: 00:47:53
                               ETA: 00:25:29

################################################################################
                     [1m Learning iteration 1306/2000 [0m                     

                       Computation: 46919 steps/s (collection: 1.999s, learning 0.096s)
             Mean action noise std: 2.81
          Mean value_function loss: 156.7777
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 52.8953
                       Mean reward: 761.97
               Mean episode length: 208.86
    Episode_Reward/reaching_object: 0.9610
     Episode_Reward/lifting_object: 158.5115
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 2.10s
                      Time elapsed: 00:47:55
                               ETA: 00:25:27

################################################################################
                     [1m Learning iteration 1307/2000 [0m                     

                       Computation: 43016 steps/s (collection: 2.167s, learning 0.119s)
             Mean action noise std: 2.81
          Mean value_function loss: 162.8146
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.9088
                       Mean reward: 789.01
               Mean episode length: 213.74
    Episode_Reward/reaching_object: 0.9831
     Episode_Reward/lifting_object: 162.9269
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 2.29s
                      Time elapsed: 00:47:58
                               ETA: 00:25:24

################################################################################
                     [1m Learning iteration 1308/2000 [0m                     

                       Computation: 47146 steps/s (collection: 1.949s, learning 0.136s)
             Mean action noise std: 2.81
          Mean value_function loss: 148.4266
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 52.9265
                       Mean reward: 779.95
               Mean episode length: 212.89
    Episode_Reward/reaching_object: 0.9763
     Episode_Reward/lifting_object: 161.5520
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 2.09s
                      Time elapsed: 00:48:00
                               ETA: 00:25:22

################################################################################
                     [1m Learning iteration 1309/2000 [0m                     

                       Computation: 47375 steps/s (collection: 1.959s, learning 0.116s)
             Mean action noise std: 2.81
          Mean value_function loss: 180.1170
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 52.9470
                       Mean reward: 827.23
               Mean episode length: 223.40
    Episode_Reward/reaching_object: 0.9760
     Episode_Reward/lifting_object: 161.9195
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 2.07s
                      Time elapsed: 00:48:02
                               ETA: 00:25:20

################################################################################
                     [1m Learning iteration 1310/2000 [0m                     

                       Computation: 48272 steps/s (collection: 1.939s, learning 0.097s)
             Mean action noise std: 2.81
          Mean value_function loss: 159.5323
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 52.9633
                       Mean reward: 830.66
               Mean episode length: 223.04
    Episode_Reward/reaching_object: 0.9863
     Episode_Reward/lifting_object: 163.4359
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 2.04s
                      Time elapsed: 00:48:04
                               ETA: 00:25:18

################################################################################
                     [1m Learning iteration 1311/2000 [0m                     

                       Computation: 47044 steps/s (collection: 1.973s, learning 0.117s)
             Mean action noise std: 2.82
          Mean value_function loss: 157.7993
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 52.9761
                       Mean reward: 761.65
               Mean episode length: 206.45
    Episode_Reward/reaching_object: 0.9550
     Episode_Reward/lifting_object: 157.8103
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 2.09s
                      Time elapsed: 00:48:06
                               ETA: 00:25:15

################################################################################
                     [1m Learning iteration 1312/2000 [0m                     

                       Computation: 42922 steps/s (collection: 2.132s, learning 0.159s)
             Mean action noise std: 2.82
          Mean value_function loss: 169.1413
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 52.9854
                       Mean reward: 803.37
               Mean episode length: 216.44
    Episode_Reward/reaching_object: 0.9510
     Episode_Reward/lifting_object: 157.0462
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 2.29s
                      Time elapsed: 00:48:08
                               ETA: 00:25:13

################################################################################
                     [1m Learning iteration 1313/2000 [0m                     

                       Computation: 47982 steps/s (collection: 1.956s, learning 0.093s)
             Mean action noise std: 2.82
          Mean value_function loss: 171.5408
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.0064
                       Mean reward: 772.44
               Mean episode length: 211.14
    Episode_Reward/reaching_object: 0.9650
     Episode_Reward/lifting_object: 159.0995
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 2.05s
                      Time elapsed: 00:48:10
                               ETA: 00:25:11

################################################################################
                     [1m Learning iteration 1314/2000 [0m                     

                       Computation: 46964 steps/s (collection: 1.998s, learning 0.096s)
             Mean action noise std: 2.82
          Mean value_function loss: 160.7426
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 53.0248
                       Mean reward: 801.90
               Mean episode length: 216.94
    Episode_Reward/reaching_object: 0.9801
     Episode_Reward/lifting_object: 162.1846
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 2.09s
                      Time elapsed: 00:48:12
                               ETA: 00:25:09

################################################################################
                     [1m Learning iteration 1315/2000 [0m                     

                       Computation: 47950 steps/s (collection: 1.939s, learning 0.111s)
             Mean action noise std: 2.82
          Mean value_function loss: 134.4989
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 53.0331
                       Mean reward: 874.94
               Mean episode length: 234.23
    Episode_Reward/reaching_object: 0.9842
     Episode_Reward/lifting_object: 162.9915
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 2.05s
                      Time elapsed: 00:48:14
                               ETA: 00:25:06

################################################################################
                     [1m Learning iteration 1316/2000 [0m                     

                       Computation: 47364 steps/s (collection: 1.975s, learning 0.100s)
             Mean action noise std: 2.83
          Mean value_function loss: 155.8860
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.0484
                       Mean reward: 802.36
               Mean episode length: 217.35
    Episode_Reward/reaching_object: 1.0015
     Episode_Reward/lifting_object: 166.0012
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 2.08s
                      Time elapsed: 00:48:16
                               ETA: 00:25:04

################################################################################
                     [1m Learning iteration 1317/2000 [0m                     

                       Computation: 42686 steps/s (collection: 2.100s, learning 0.203s)
             Mean action noise std: 2.83
          Mean value_function loss: 136.1984
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.0657
                       Mean reward: 844.32
               Mean episode length: 226.58
    Episode_Reward/reaching_object: 0.9906
     Episode_Reward/lifting_object: 164.0171
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 2.30s
                      Time elapsed: 00:48:19
                               ETA: 00:25:02

################################################################################
                     [1m Learning iteration 1318/2000 [0m                     

                       Computation: 47429 steps/s (collection: 1.958s, learning 0.115s)
             Mean action noise std: 2.83
          Mean value_function loss: 156.4364
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 53.0775
                       Mean reward: 864.46
               Mean episode length: 231.52
    Episode_Reward/reaching_object: 0.9937
     Episode_Reward/lifting_object: 164.3783
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 2.07s
                      Time elapsed: 00:48:21
                               ETA: 00:25:00

################################################################################
                     [1m Learning iteration 1319/2000 [0m                     

                       Computation: 45584 steps/s (collection: 2.023s, learning 0.134s)
             Mean action noise std: 2.83
          Mean value_function loss: 129.8499
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 53.0876
                       Mean reward: 844.26
               Mean episode length: 225.35
    Episode_Reward/reaching_object: 0.9771
     Episode_Reward/lifting_object: 162.4181
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 2.16s
                      Time elapsed: 00:48:23
                               ETA: 00:24:57

################################################################################
                     [1m Learning iteration 1320/2000 [0m                     

                       Computation: 45952 steps/s (collection: 2.026s, learning 0.113s)
             Mean action noise std: 2.83
          Mean value_function loss: 136.8794
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.0984
                       Mean reward: 825.35
               Mean episode length: 221.26
    Episode_Reward/reaching_object: 0.9773
     Episode_Reward/lifting_object: 162.6395
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 2.14s
                      Time elapsed: 00:48:25
                               ETA: 00:24:55

################################################################################
                     [1m Learning iteration 1321/2000 [0m                     

                       Computation: 45211 steps/s (collection: 2.075s, learning 0.099s)
             Mean action noise std: 2.83
          Mean value_function loss: 151.4273
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 53.1102
                       Mean reward: 819.70
               Mean episode length: 219.65
    Episode_Reward/reaching_object: 0.9700
     Episode_Reward/lifting_object: 161.3371
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 2.17s
                      Time elapsed: 00:48:27
                               ETA: 00:24:53

################################################################################
                     [1m Learning iteration 1322/2000 [0m                     

                       Computation: 46686 steps/s (collection: 2.001s, learning 0.105s)
             Mean action noise std: 2.84
          Mean value_function loss: 140.4320
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.1286
                       Mean reward: 857.76
               Mean episode length: 228.70
    Episode_Reward/reaching_object: 0.9885
     Episode_Reward/lifting_object: 164.2149
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 2.11s
                      Time elapsed: 00:48:29
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 1323/2000 [0m                     

                       Computation: 47880 steps/s (collection: 1.944s, learning 0.110s)
             Mean action noise std: 2.84
          Mean value_function loss: 169.2025
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.1420
                       Mean reward: 773.67
               Mean episode length: 210.07
    Episode_Reward/reaching_object: 0.9725
     Episode_Reward/lifting_object: 161.4038
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 2.05s
                      Time elapsed: 00:48:31
                               ETA: 00:24:48

################################################################################
                     [1m Learning iteration 1324/2000 [0m                     

                       Computation: 44692 steps/s (collection: 2.057s, learning 0.143s)
             Mean action noise std: 2.84
          Mean value_function loss: 163.5857
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 53.1553
                       Mean reward: 852.89
               Mean episode length: 228.07
    Episode_Reward/reaching_object: 0.9764
     Episode_Reward/lifting_object: 162.5909
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 2.20s
                      Time elapsed: 00:48:34
                               ETA: 00:24:46

################################################################################
                     [1m Learning iteration 1325/2000 [0m                     

                       Computation: 45602 steps/s (collection: 2.035s, learning 0.121s)
             Mean action noise std: 2.84
          Mean value_function loss: 143.5002
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.1653
                       Mean reward: 836.76
               Mean episode length: 225.88
    Episode_Reward/reaching_object: 1.0060
     Episode_Reward/lifting_object: 167.4341
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 2.16s
                      Time elapsed: 00:48:36
                               ETA: 00:24:44

################################################################################
                     [1m Learning iteration 1326/2000 [0m                     

                       Computation: 44731 steps/s (collection: 1.999s, learning 0.199s)
             Mean action noise std: 2.84
          Mean value_function loss: 112.2116
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 53.1768
                       Mean reward: 836.86
               Mean episode length: 225.69
    Episode_Reward/reaching_object: 0.9768
     Episode_Reward/lifting_object: 161.6211
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 2.20s
                      Time elapsed: 00:48:38
                               ETA: 00:24:42

################################################################################
                     [1m Learning iteration 1327/2000 [0m                     

                       Computation: 45357 steps/s (collection: 2.046s, learning 0.122s)
             Mean action noise std: 2.84
          Mean value_function loss: 139.3604
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.1832
                       Mean reward: 847.19
               Mean episode length: 227.13
    Episode_Reward/reaching_object: 0.9899
     Episode_Reward/lifting_object: 164.1918
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 2.17s
                      Time elapsed: 00:48:40
                               ETA: 00:24:40

################################################################################
                     [1m Learning iteration 1328/2000 [0m                     

                       Computation: 47090 steps/s (collection: 1.982s, learning 0.105s)
             Mean action noise std: 2.84
          Mean value_function loss: 194.7301
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 53.1944
                       Mean reward: 879.29
               Mean episode length: 233.12
    Episode_Reward/reaching_object: 0.9967
     Episode_Reward/lifting_object: 165.9774
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 2.09s
                      Time elapsed: 00:48:42
                               ETA: 00:24:37

################################################################################
                     [1m Learning iteration 1329/2000 [0m                     

                       Computation: 47811 steps/s (collection: 1.950s, learning 0.106s)
             Mean action noise std: 2.85
          Mean value_function loss: 154.6326
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 53.2114
                       Mean reward: 766.77
               Mean episode length: 206.54
    Episode_Reward/reaching_object: 0.9670
     Episode_Reward/lifting_object: 160.6622
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 2.06s
                      Time elapsed: 00:48:44
                               ETA: 00:24:35

################################################################################
                     [1m Learning iteration 1330/2000 [0m                     

                       Computation: 46866 steps/s (collection: 1.988s, learning 0.110s)
             Mean action noise std: 2.85
          Mean value_function loss: 146.0673
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.2245
                       Mean reward: 859.15
               Mean episode length: 230.74
    Episode_Reward/reaching_object: 0.9776
     Episode_Reward/lifting_object: 162.7977
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 2.10s
                      Time elapsed: 00:48:46
                               ETA: 00:24:33

################################################################################
                     [1m Learning iteration 1331/2000 [0m                     

                       Computation: 47488 steps/s (collection: 1.963s, learning 0.107s)
             Mean action noise std: 2.85
          Mean value_function loss: 149.5055
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.2396
                       Mean reward: 823.17
               Mean episode length: 220.85
    Episode_Reward/reaching_object: 0.9899
     Episode_Reward/lifting_object: 164.6406
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 2.07s
                      Time elapsed: 00:48:49
                               ETA: 00:24:31

################################################################################
                     [1m Learning iteration 1332/2000 [0m                     

                       Computation: 46632 steps/s (collection: 2.001s, learning 0.108s)
             Mean action noise std: 2.85
          Mean value_function loss: 142.9259
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.2499
                       Mean reward: 837.31
               Mean episode length: 225.80
    Episode_Reward/reaching_object: 0.9896
     Episode_Reward/lifting_object: 163.7503
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 2.11s
                      Time elapsed: 00:48:51
                               ETA: 00:24:28

################################################################################
                     [1m Learning iteration 1333/2000 [0m                     

                       Computation: 18996 steps/s (collection: 5.054s, learning 0.121s)
             Mean action noise std: 2.85
          Mean value_function loss: 147.5501
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.2647
                       Mean reward: 793.74
               Mean episode length: 215.85
    Episode_Reward/reaching_object: 1.0088
     Episode_Reward/lifting_object: 167.7517
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 5.17s
                      Time elapsed: 00:48:56
                               ETA: 00:24:28

################################################################################
                     [1m Learning iteration 1334/2000 [0m                     

                       Computation: 14351 steps/s (collection: 6.736s, learning 0.114s)
             Mean action noise std: 2.85
          Mean value_function loss: 170.6660
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.2736
                       Mean reward: 812.21
               Mean episode length: 219.36
    Episode_Reward/reaching_object: 0.9588
     Episode_Reward/lifting_object: 158.2074
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 6.85s
                      Time elapsed: 00:49:03
                               ETA: 00:24:28

################################################################################
                     [1m Learning iteration 1335/2000 [0m                     

                       Computation: 14349 steps/s (collection: 6.739s, learning 0.111s)
             Mean action noise std: 2.85
          Mean value_function loss: 155.6289
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.2775
                       Mean reward: 858.56
               Mean episode length: 229.79
    Episode_Reward/reaching_object: 0.9913
     Episode_Reward/lifting_object: 165.1302
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 6.85s
                      Time elapsed: 00:49:10
                               ETA: 00:24:28

################################################################################
                     [1m Learning iteration 1336/2000 [0m                     

                       Computation: 14255 steps/s (collection: 6.784s, learning 0.112s)
             Mean action noise std: 2.86
          Mean value_function loss: 172.4809
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 53.2836
                       Mean reward: 762.08
               Mean episode length: 205.99
    Episode_Reward/reaching_object: 0.9622
     Episode_Reward/lifting_object: 159.5649
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 6.90s
                      Time elapsed: 00:49:16
                               ETA: 00:24:28

################################################################################
                     [1m Learning iteration 1337/2000 [0m                     

                       Computation: 14481 steps/s (collection: 6.666s, learning 0.123s)
             Mean action noise std: 2.86
          Mean value_function loss: 140.9320
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 53.2949
                       Mean reward: 760.37
               Mean episode length: 207.36
    Episode_Reward/reaching_object: 0.9400
     Episode_Reward/lifting_object: 155.2808
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 6.79s
                      Time elapsed: 00:49:23
                               ETA: 00:24:28

################################################################################
                     [1m Learning iteration 1338/2000 [0m                     

                       Computation: 14440 steps/s (collection: 6.692s, learning 0.115s)
             Mean action noise std: 2.86
          Mean value_function loss: 111.5821
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.3101
                       Mean reward: 803.49
               Mean episode length: 216.01
    Episode_Reward/reaching_object: 0.9862
     Episode_Reward/lifting_object: 164.3429
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 6.81s
                      Time elapsed: 00:49:30
                               ETA: 00:24:28

################################################################################
                     [1m Learning iteration 1339/2000 [0m                     

                       Computation: 14370 steps/s (collection: 6.719s, learning 0.122s)
             Mean action noise std: 2.86
          Mean value_function loss: 145.5799
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.3199
                       Mean reward: 826.15
               Mean episode length: 222.32
    Episode_Reward/reaching_object: 0.9812
     Episode_Reward/lifting_object: 162.4049
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 6.84s
                      Time elapsed: 00:49:37
                               ETA: 00:24:28

################################################################################
                     [1m Learning iteration 1340/2000 [0m                     

                       Computation: 14119 steps/s (collection: 6.829s, learning 0.133s)
             Mean action noise std: 2.86
          Mean value_function loss: 154.7106
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 53.3293
                       Mean reward: 807.25
               Mean episode length: 218.20
    Episode_Reward/reaching_object: 0.9660
     Episode_Reward/lifting_object: 159.8859
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 6.96s
                      Time elapsed: 00:49:44
                               ETA: 00:24:28

################################################################################
                     [1m Learning iteration 1341/2000 [0m                     

                       Computation: 12867 steps/s (collection: 7.523s, learning 0.117s)
             Mean action noise std: 2.86
          Mean value_function loss: 147.4242
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.3440
                       Mean reward: 845.85
               Mean episode length: 226.83
    Episode_Reward/reaching_object: 1.0022
     Episode_Reward/lifting_object: 166.7809
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 7.64s
                      Time elapsed: 00:49:51
                               ETA: 00:24:29

################################################################################
                     [1m Learning iteration 1342/2000 [0m                     

                       Computation: 49848 steps/s (collection: 1.885s, learning 0.088s)
             Mean action noise std: 2.87
          Mean value_function loss: 173.5514
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.3571
                       Mean reward: 767.84
               Mean episode length: 209.05
    Episode_Reward/reaching_object: 0.9407
     Episode_Reward/lifting_object: 155.8094
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 1.97s
                      Time elapsed: 00:49:53
                               ETA: 00:24:26

################################################################################
                     [1m Learning iteration 1343/2000 [0m                     

                       Computation: 51098 steps/s (collection: 1.822s, learning 0.102s)
             Mean action noise std: 2.87
          Mean value_function loss: 162.5293
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 53.3702
                       Mean reward: 792.33
               Mean episode length: 214.78
    Episode_Reward/reaching_object: 0.9863
     Episode_Reward/lifting_object: 164.1716
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 1.92s
                      Time elapsed: 00:49:55
                               ETA: 00:24:24

################################################################################
                     [1m Learning iteration 1344/2000 [0m                     

                       Computation: 47603 steps/s (collection: 1.934s, learning 0.131s)
             Mean action noise std: 2.87
          Mean value_function loss: 172.7143
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 53.3839
                       Mean reward: 819.55
               Mean episode length: 220.48
    Episode_Reward/reaching_object: 0.9791
     Episode_Reward/lifting_object: 162.7865
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 2.07s
                      Time elapsed: 00:49:57
                               ETA: 00:24:22

################################################################################
                     [1m Learning iteration 1345/2000 [0m                     

                       Computation: 42167 steps/s (collection: 2.171s, learning 0.160s)
             Mean action noise std: 2.87
          Mean value_function loss: 152.3226
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.3932
                       Mean reward: 809.37
               Mean episode length: 218.54
    Episode_Reward/reaching_object: 0.9983
     Episode_Reward/lifting_object: 166.2772
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 2.33s
                      Time elapsed: 00:50:00
                               ETA: 00:24:19

################################################################################
                     [1m Learning iteration 1346/2000 [0m                     

                       Computation: 48448 steps/s (collection: 1.912s, learning 0.117s)
             Mean action noise std: 2.87
          Mean value_function loss: 202.0226
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 53.4013
                       Mean reward: 819.79
               Mean episode length: 220.49
    Episode_Reward/reaching_object: 0.9789
     Episode_Reward/lifting_object: 162.4050
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 2.03s
                      Time elapsed: 00:50:02
                               ETA: 00:24:17

################################################################################
                     [1m Learning iteration 1347/2000 [0m                     

                       Computation: 49487 steps/s (collection: 1.898s, learning 0.088s)
             Mean action noise std: 2.87
          Mean value_function loss: 265.6618
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 53.4152
                       Mean reward: 803.22
               Mean episode length: 216.13
    Episode_Reward/reaching_object: 0.9713
     Episode_Reward/lifting_object: 161.2230
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 1.99s
                      Time elapsed: 00:50:04
                               ETA: 00:24:15

################################################################################
                     [1m Learning iteration 1348/2000 [0m                     

                       Computation: 49772 steps/s (collection: 1.885s, learning 0.090s)
             Mean action noise std: 2.88
          Mean value_function loss: 135.6116
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.4278
                       Mean reward: 820.87
               Mean episode length: 220.49
    Episode_Reward/reaching_object: 0.9779
     Episode_Reward/lifting_object: 162.1324
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 1.98s
                      Time elapsed: 00:50:06
                               ETA: 00:24:12

################################################################################
                     [1m Learning iteration 1349/2000 [0m                     

                       Computation: 48962 steps/s (collection: 1.890s, learning 0.118s)
             Mean action noise std: 2.88
          Mean value_function loss: 106.8325
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 53.4469
                       Mean reward: 834.33
               Mean episode length: 224.46
    Episode_Reward/reaching_object: 1.0144
     Episode_Reward/lifting_object: 168.3136
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 2.01s
                      Time elapsed: 00:50:08
                               ETA: 00:24:10

################################################################################
                     [1m Learning iteration 1350/2000 [0m                     

                       Computation: 50272 steps/s (collection: 1.867s, learning 0.088s)
             Mean action noise std: 2.88
          Mean value_function loss: 113.6616
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.4573
                       Mean reward: 854.94
               Mean episode length: 228.64
    Episode_Reward/reaching_object: 1.0158
     Episode_Reward/lifting_object: 169.1730
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 1.96s
                      Time elapsed: 00:50:10
                               ETA: 00:24:08

################################################################################
                     [1m Learning iteration 1351/2000 [0m                     

                       Computation: 51105 steps/s (collection: 1.838s, learning 0.085s)
             Mean action noise std: 2.88
          Mean value_function loss: 143.7757
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.4677
                       Mean reward: 780.54
               Mean episode length: 210.88
    Episode_Reward/reaching_object: 0.9893
     Episode_Reward/lifting_object: 163.9881
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 1.92s
                      Time elapsed: 00:50:12
                               ETA: 00:24:05

################################################################################
                     [1m Learning iteration 1352/2000 [0m                     

                       Computation: 50920 steps/s (collection: 1.840s, learning 0.090s)
             Mean action noise std: 2.88
          Mean value_function loss: 162.1176
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 53.4809
                       Mean reward: 825.27
               Mean episode length: 222.37
    Episode_Reward/reaching_object: 0.9976
     Episode_Reward/lifting_object: 165.8058
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 1.93s
                      Time elapsed: 00:50:14
                               ETA: 00:24:03

################################################################################
                     [1m Learning iteration 1353/2000 [0m                     

                       Computation: 49782 steps/s (collection: 1.860s, learning 0.115s)
             Mean action noise std: 2.88
          Mean value_function loss: 129.3239
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 53.4937
                       Mean reward: 855.68
               Mean episode length: 227.80
    Episode_Reward/reaching_object: 1.0156
     Episode_Reward/lifting_object: 168.7379
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 1.97s
                      Time elapsed: 00:50:16
                               ETA: 00:24:01

################################################################################
                     [1m Learning iteration 1354/2000 [0m                     

                       Computation: 48830 steps/s (collection: 1.850s, learning 0.164s)
             Mean action noise std: 2.88
          Mean value_function loss: 148.6798
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 53.5027
                       Mean reward: 836.48
               Mean episode length: 224.69
    Episode_Reward/reaching_object: 0.9832
     Episode_Reward/lifting_object: 162.9209
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 2.01s
                      Time elapsed: 00:50:18
                               ETA: 00:23:58

################################################################################
                     [1m Learning iteration 1355/2000 [0m                     

                       Computation: 49556 steps/s (collection: 1.837s, learning 0.147s)
             Mean action noise std: 2.89
          Mean value_function loss: 147.0340
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 53.5119
                       Mean reward: 825.22
               Mean episode length: 222.21
    Episode_Reward/reaching_object: 0.9713
     Episode_Reward/lifting_object: 160.8331
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 1.98s
                      Time elapsed: 00:50:20
                               ETA: 00:23:56

################################################################################
                     [1m Learning iteration 1356/2000 [0m                     

                       Computation: 50864 steps/s (collection: 1.803s, learning 0.130s)
             Mean action noise std: 2.89
          Mean value_function loss: 105.5860
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 53.5296
                       Mean reward: 876.92
               Mean episode length: 232.81
    Episode_Reward/reaching_object: 1.0342
     Episode_Reward/lifting_object: 172.6349
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 1.93s
                      Time elapsed: 00:50:21
                               ETA: 00:23:54

################################################################################
                     [1m Learning iteration 1357/2000 [0m                     

                       Computation: 48659 steps/s (collection: 1.877s, learning 0.143s)
             Mean action noise std: 2.89
          Mean value_function loss: 146.5723
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 53.5449
                       Mean reward: 853.99
               Mean episode length: 229.84
    Episode_Reward/reaching_object: 0.9537
     Episode_Reward/lifting_object: 157.4548
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 2.02s
                      Time elapsed: 00:50:23
                               ETA: 00:23:51

################################################################################
                     [1m Learning iteration 1358/2000 [0m                     

                       Computation: 46541 steps/s (collection: 1.945s, learning 0.168s)
             Mean action noise std: 2.89
          Mean value_function loss: 137.3624
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 53.5562
                       Mean reward: 799.12
               Mean episode length: 215.68
    Episode_Reward/reaching_object: 0.9582
     Episode_Reward/lifting_object: 158.3243
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 2.11s
                      Time elapsed: 00:50:26
                               ETA: 00:23:49

################################################################################
                     [1m Learning iteration 1359/2000 [0m                     

                       Computation: 48236 steps/s (collection: 1.902s, learning 0.136s)
             Mean action noise std: 2.89
          Mean value_function loss: 120.7271
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.5705
                       Mean reward: 801.29
               Mean episode length: 216.10
    Episode_Reward/reaching_object: 0.9764
     Episode_Reward/lifting_object: 161.9789
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 2.04s
                      Time elapsed: 00:50:28
                               ETA: 00:23:47

################################################################################
                     [1m Learning iteration 1360/2000 [0m                     

                       Computation: 47566 steps/s (collection: 1.954s, learning 0.113s)
             Mean action noise std: 2.90
          Mean value_function loss: 128.1566
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 53.5837
                       Mean reward: 861.06
               Mean episode length: 230.85
    Episode_Reward/reaching_object: 1.0091
     Episode_Reward/lifting_object: 167.4229
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 2.07s
                      Time elapsed: 00:50:30
                               ETA: 00:23:44

################################################################################
                     [1m Learning iteration 1361/2000 [0m                     

                       Computation: 49584 steps/s (collection: 1.885s, learning 0.098s)
             Mean action noise std: 2.90
          Mean value_function loss: 117.6780
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 53.5929
                       Mean reward: 778.17
               Mean episode length: 210.25
    Episode_Reward/reaching_object: 0.9887
     Episode_Reward/lifting_object: 164.0862
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 1.98s
                      Time elapsed: 00:50:32
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 1362/2000 [0m                     

                       Computation: 48441 steps/s (collection: 1.920s, learning 0.109s)
             Mean action noise std: 2.90
          Mean value_function loss: 110.3321
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.6052
                       Mean reward: 863.73
               Mean episode length: 229.65
    Episode_Reward/reaching_object: 1.0002
     Episode_Reward/lifting_object: 166.0659
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 2.03s
                      Time elapsed: 00:50:34
                               ETA: 00:23:40

################################################################################
                     [1m Learning iteration 1363/2000 [0m                     

                       Computation: 50009 steps/s (collection: 1.875s, learning 0.091s)
             Mean action noise std: 2.90
          Mean value_function loss: 157.5665
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.6267
                       Mean reward: 841.52
               Mean episode length: 226.16
    Episode_Reward/reaching_object: 1.0139
     Episode_Reward/lifting_object: 168.4155
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 1.97s
                      Time elapsed: 00:50:36
                               ETA: 00:23:37

################################################################################
                     [1m Learning iteration 1364/2000 [0m                     

                       Computation: 49886 steps/s (collection: 1.862s, learning 0.109s)
             Mean action noise std: 2.90
          Mean value_function loss: 116.2544
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.6449
                       Mean reward: 834.98
               Mean episode length: 223.30
    Episode_Reward/reaching_object: 0.9931
     Episode_Reward/lifting_object: 165.0077
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 1.97s
                      Time elapsed: 00:50:38
                               ETA: 00:23:35

################################################################################
                     [1m Learning iteration 1365/2000 [0m                     

                       Computation: 48678 steps/s (collection: 1.928s, learning 0.092s)
             Mean action noise std: 2.91
          Mean value_function loss: 147.5189
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.6605
                       Mean reward: 833.13
               Mean episode length: 224.70
    Episode_Reward/reaching_object: 1.0286
     Episode_Reward/lifting_object: 171.2449
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 2.02s
                      Time elapsed: 00:50:40
                               ETA: 00:23:33

################################################################################
                     [1m Learning iteration 1366/2000 [0m                     

                       Computation: 50047 steps/s (collection: 1.842s, learning 0.123s)
             Mean action noise std: 2.91
          Mean value_function loss: 116.8883
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 53.6712
                       Mean reward: 886.41
               Mean episode length: 235.19
    Episode_Reward/reaching_object: 0.9848
     Episode_Reward/lifting_object: 163.7812
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 1.96s
                      Time elapsed: 00:50:42
                               ETA: 00:23:30

################################################################################
                     [1m Learning iteration 1367/2000 [0m                     

                       Computation: 48435 steps/s (collection: 1.912s, learning 0.118s)
             Mean action noise std: 2.91
          Mean value_function loss: 140.3833
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 53.6798
                       Mean reward: 792.94
               Mean episode length: 214.55
    Episode_Reward/reaching_object: 0.9687
     Episode_Reward/lifting_object: 160.9335
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 2.03s
                      Time elapsed: 00:50:44
                               ETA: 00:23:28

################################################################################
                     [1m Learning iteration 1368/2000 [0m                     

                       Computation: 48767 steps/s (collection: 1.859s, learning 0.157s)
             Mean action noise std: 2.91
          Mean value_function loss: 134.6501
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 53.6862
                       Mean reward: 861.95
               Mean episode length: 229.72
    Episode_Reward/reaching_object: 0.9848
     Episode_Reward/lifting_object: 163.8853
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 2.02s
                      Time elapsed: 00:50:46
                               ETA: 00:23:26

################################################################################
                     [1m Learning iteration 1369/2000 [0m                     

                       Computation: 49728 steps/s (collection: 1.865s, learning 0.112s)
             Mean action noise std: 2.91
          Mean value_function loss: 127.8479
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 53.6965
                       Mean reward: 841.37
               Mean episode length: 224.64
    Episode_Reward/reaching_object: 1.0001
     Episode_Reward/lifting_object: 166.6628
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 1.98s
                      Time elapsed: 00:50:48
                               ETA: 00:23:23

################################################################################
                     [1m Learning iteration 1370/2000 [0m                     

                       Computation: 48850 steps/s (collection: 1.884s, learning 0.128s)
             Mean action noise std: 2.91
          Mean value_function loss: 122.2855
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.7114
                       Mean reward: 843.96
               Mean episode length: 225.77
    Episode_Reward/reaching_object: 0.9791
     Episode_Reward/lifting_object: 162.8868
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 2.01s
                      Time elapsed: 00:50:50
                               ETA: 00:23:21

################################################################################
                     [1m Learning iteration 1371/2000 [0m                     

                       Computation: 49734 steps/s (collection: 1.841s, learning 0.135s)
             Mean action noise std: 2.91
          Mean value_function loss: 148.2454
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.7243
                       Mean reward: 813.85
               Mean episode length: 218.10
    Episode_Reward/reaching_object: 0.9931
     Episode_Reward/lifting_object: 165.7316
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 1.98s
                      Time elapsed: 00:50:52
                               ETA: 00:23:19

################################################################################
                     [1m Learning iteration 1372/2000 [0m                     

                       Computation: 48470 steps/s (collection: 1.882s, learning 0.146s)
             Mean action noise std: 2.91
          Mean value_function loss: 99.2760
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.7307
                       Mean reward: 857.69
               Mean episode length: 228.53
    Episode_Reward/reaching_object: 1.0023
     Episode_Reward/lifting_object: 167.1076
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 2.03s
                      Time elapsed: 00:50:54
                               ETA: 00:23:16

################################################################################
                     [1m Learning iteration 1373/2000 [0m                     

                       Computation: 50486 steps/s (collection: 1.853s, learning 0.094s)
             Mean action noise std: 2.92
          Mean value_function loss: 125.4061
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 53.7376
                       Mean reward: 786.59
               Mean episode length: 212.45
    Episode_Reward/reaching_object: 1.0009
     Episode_Reward/lifting_object: 167.3246
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 1.95s
                      Time elapsed: 00:50:56
                               ETA: 00:23:14

################################################################################
                     [1m Learning iteration 1374/2000 [0m                     

                       Computation: 50164 steps/s (collection: 1.853s, learning 0.107s)
             Mean action noise std: 2.92
          Mean value_function loss: 147.9185
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 53.7542
                       Mean reward: 810.45
               Mean episode length: 217.08
    Episode_Reward/reaching_object: 0.9790
     Episode_Reward/lifting_object: 163.3395
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 1.96s
                      Time elapsed: 00:50:58
                               ETA: 00:23:12

################################################################################
                     [1m Learning iteration 1375/2000 [0m                     

                       Computation: 49322 steps/s (collection: 1.841s, learning 0.153s)
             Mean action noise std: 2.92
          Mean value_function loss: 116.0930
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.7636
                       Mean reward: 864.18
               Mean episode length: 229.84
    Episode_Reward/reaching_object: 1.0039
     Episode_Reward/lifting_object: 168.4464
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 1.99s
                      Time elapsed: 00:51:00
                               ETA: 00:23:09

################################################################################
                     [1m Learning iteration 1376/2000 [0m                     

                       Computation: 48057 steps/s (collection: 1.955s, learning 0.091s)
             Mean action noise std: 2.92
          Mean value_function loss: 144.6182
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 53.7720
                       Mean reward: 869.38
               Mean episode length: 231.64
    Episode_Reward/reaching_object: 1.0024
     Episode_Reward/lifting_object: 167.6375
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 2.05s
                      Time elapsed: 00:51:02
                               ETA: 00:23:07

################################################################################
                     [1m Learning iteration 1377/2000 [0m                     

                       Computation: 49783 steps/s (collection: 1.852s, learning 0.123s)
             Mean action noise std: 2.92
          Mean value_function loss: 108.2282
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.7821
                       Mean reward: 827.01
               Mean episode length: 221.46
    Episode_Reward/reaching_object: 1.0049
     Episode_Reward/lifting_object: 167.7920
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 1.97s
                      Time elapsed: 00:51:04
                               ETA: 00:23:05

################################################################################
                     [1m Learning iteration 1378/2000 [0m                     

                       Computation: 49643 steps/s (collection: 1.862s, learning 0.118s)
             Mean action noise std: 2.92
          Mean value_function loss: 115.1042
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.7948
                       Mean reward: 804.15
               Mean episode length: 216.65
    Episode_Reward/reaching_object: 1.0168
     Episode_Reward/lifting_object: 170.2599
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 1.98s
                      Time elapsed: 00:51:06
                               ETA: 00:23:02

################################################################################
                     [1m Learning iteration 1379/2000 [0m                     

                       Computation: 46552 steps/s (collection: 1.937s, learning 0.175s)
             Mean action noise std: 2.92
          Mean value_function loss: 98.8527
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.8075
                       Mean reward: 828.64
               Mean episode length: 222.98
    Episode_Reward/reaching_object: 0.9994
     Episode_Reward/lifting_object: 167.2502
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 2.11s
                      Time elapsed: 00:51:08
                               ETA: 00:23:00

################################################################################
                     [1m Learning iteration 1380/2000 [0m                     

                       Computation: 49371 steps/s (collection: 1.836s, learning 0.155s)
             Mean action noise std: 2.93
          Mean value_function loss: 127.4037
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 53.8152
                       Mean reward: 820.77
               Mean episode length: 221.04
    Episode_Reward/reaching_object: 0.9982
     Episode_Reward/lifting_object: 166.3346
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 1.99s
                      Time elapsed: 00:51:10
                               ETA: 00:22:58

################################################################################
                     [1m Learning iteration 1381/2000 [0m                     

                       Computation: 50040 steps/s (collection: 1.834s, learning 0.130s)
             Mean action noise std: 2.93
          Mean value_function loss: 141.2900
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.8242
                       Mean reward: 860.37
               Mean episode length: 228.99
    Episode_Reward/reaching_object: 0.9899
     Episode_Reward/lifting_object: 165.0525
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 1.96s
                      Time elapsed: 00:51:12
                               ETA: 00:22:56

################################################################################
                     [1m Learning iteration 1382/2000 [0m                     

                       Computation: 48890 steps/s (collection: 1.876s, learning 0.135s)
             Mean action noise std: 2.93
          Mean value_function loss: 185.7261
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 53.8362
                       Mean reward: 769.22
               Mean episode length: 208.90
    Episode_Reward/reaching_object: 0.9642
     Episode_Reward/lifting_object: 159.9115
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 2.01s
                      Time elapsed: 00:51:14
                               ETA: 00:22:53

################################################################################
                     [1m Learning iteration 1383/2000 [0m                     

                       Computation: 49573 steps/s (collection: 1.849s, learning 0.134s)
             Mean action noise std: 2.93
          Mean value_function loss: 169.8850
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 53.8491
                       Mean reward: 812.39
               Mean episode length: 219.80
    Episode_Reward/reaching_object: 0.9958
     Episode_Reward/lifting_object: 165.5576
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 1.98s
                      Time elapsed: 00:51:16
                               ETA: 00:22:51

################################################################################
                     [1m Learning iteration 1384/2000 [0m                     

                       Computation: 49862 steps/s (collection: 1.872s, learning 0.099s)
             Mean action noise std: 2.93
          Mean value_function loss: 177.2536
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 53.8527
                       Mean reward: 797.68
               Mean episode length: 216.70
    Episode_Reward/reaching_object: 0.9806
     Episode_Reward/lifting_object: 162.2233
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 1.97s
                      Time elapsed: 00:51:18
                               ETA: 00:22:49

################################################################################
                     [1m Learning iteration 1385/2000 [0m                     

                       Computation: 50862 steps/s (collection: 1.841s, learning 0.092s)
             Mean action noise std: 2.93
          Mean value_function loss: 118.3647
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 53.8596
                       Mean reward: 844.06
               Mean episode length: 225.51
    Episode_Reward/reaching_object: 0.9935
     Episode_Reward/lifting_object: 165.5259
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 1.93s
                      Time elapsed: 00:51:20
                               ETA: 00:22:46

################################################################################
                     [1m Learning iteration 1386/2000 [0m                     

                       Computation: 48401 steps/s (collection: 1.884s, learning 0.147s)
             Mean action noise std: 2.93
          Mean value_function loss: 114.8683
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 53.8670
                       Mean reward: 828.95
               Mean episode length: 222.60
    Episode_Reward/reaching_object: 1.0013
     Episode_Reward/lifting_object: 166.1387
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 2.03s
                      Time elapsed: 00:51:22
                               ETA: 00:22:44

################################################################################
                     [1m Learning iteration 1387/2000 [0m                     

                       Computation: 48685 steps/s (collection: 1.898s, learning 0.121s)
             Mean action noise std: 2.93
          Mean value_function loss: 147.4241
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 53.8743
                       Mean reward: 741.67
               Mean episode length: 200.49
    Episode_Reward/reaching_object: 0.9515
     Episode_Reward/lifting_object: 157.5978
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 2.02s
                      Time elapsed: 00:51:24
                               ETA: 00:22:42

################################################################################
                     [1m Learning iteration 1388/2000 [0m                     

                       Computation: 48121 steps/s (collection: 1.893s, learning 0.150s)
             Mean action noise std: 2.93
          Mean value_function loss: 116.7788
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.8809
                       Mean reward: 778.90
               Mean episode length: 209.62
    Episode_Reward/reaching_object: 0.9820
     Episode_Reward/lifting_object: 162.9147
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 2.04s
                      Time elapsed: 00:51:26
                               ETA: 00:22:39

################################################################################
                     [1m Learning iteration 1389/2000 [0m                     

                       Computation: 50502 steps/s (collection: 1.856s, learning 0.091s)
             Mean action noise std: 2.94
          Mean value_function loss: 100.0063
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 53.8914
                       Mean reward: 881.47
               Mean episode length: 234.71
    Episode_Reward/reaching_object: 1.0104
     Episode_Reward/lifting_object: 168.4377
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 1.95s
                      Time elapsed: 00:51:28
                               ETA: 00:22:37

################################################################################
                     [1m Learning iteration 1390/2000 [0m                     

                       Computation: 50564 steps/s (collection: 1.856s, learning 0.088s)
             Mean action noise std: 2.94
          Mean value_function loss: 147.9576
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.9078
                       Mean reward: 847.21
               Mean episode length: 227.01
    Episode_Reward/reaching_object: 1.0197
     Episode_Reward/lifting_object: 169.7356
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 1.94s
                      Time elapsed: 00:51:30
                               ETA: 00:22:35

################################################################################
                     [1m Learning iteration 1391/2000 [0m                     

                       Computation: 47789 steps/s (collection: 1.913s, learning 0.144s)
             Mean action noise std: 2.94
          Mean value_function loss: 169.7285
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 53.9217
                       Mean reward: 801.36
               Mean episode length: 215.95
    Episode_Reward/reaching_object: 0.9691
     Episode_Reward/lifting_object: 160.8222
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 2.06s
                      Time elapsed: 00:51:32
                               ETA: 00:22:32

################################################################################
                     [1m Learning iteration 1392/2000 [0m                     

                       Computation: 50517 steps/s (collection: 1.855s, learning 0.091s)
             Mean action noise std: 2.94
          Mean value_function loss: 130.8417
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 53.9303
                       Mean reward: 827.43
               Mean episode length: 223.13
    Episode_Reward/reaching_object: 0.9846
     Episode_Reward/lifting_object: 163.6975
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 1.95s
                      Time elapsed: 00:51:34
                               ETA: 00:22:30

################################################################################
                     [1m Learning iteration 1393/2000 [0m                     

                       Computation: 50536 steps/s (collection: 1.857s, learning 0.089s)
             Mean action noise std: 2.94
          Mean value_function loss: 142.7567
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.9362
                       Mean reward: 827.99
               Mean episode length: 222.48
    Episode_Reward/reaching_object: 1.0048
     Episode_Reward/lifting_object: 167.2931
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 1.95s
                      Time elapsed: 00:51:35
                               ETA: 00:22:28

################################################################################
                     [1m Learning iteration 1394/2000 [0m                     

                       Computation: 49246 steps/s (collection: 1.903s, learning 0.093s)
             Mean action noise std: 2.94
          Mean value_function loss: 146.9836
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.9451
                       Mean reward: 784.44
               Mean episode length: 211.08
    Episode_Reward/reaching_object: 0.9622
     Episode_Reward/lifting_object: 160.0325
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 2.00s
                      Time elapsed: 00:51:37
                               ETA: 00:22:25

################################################################################
                     [1m Learning iteration 1395/2000 [0m                     

                       Computation: 50811 steps/s (collection: 1.843s, learning 0.092s)
             Mean action noise std: 2.94
          Mean value_function loss: 121.0215
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 53.9552
                       Mean reward: 829.87
               Mean episode length: 222.65
    Episode_Reward/reaching_object: 1.0015
     Episode_Reward/lifting_object: 167.4089
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 1.93s
                      Time elapsed: 00:51:39
                               ETA: 00:22:23

################################################################################
                     [1m Learning iteration 1396/2000 [0m                     

                       Computation: 49486 steps/s (collection: 1.841s, learning 0.145s)
             Mean action noise std: 2.95
          Mean value_function loss: 137.3484
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.9688
                       Mean reward: 798.04
               Mean episode length: 215.01
    Episode_Reward/reaching_object: 0.9697
     Episode_Reward/lifting_object: 161.8236
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 1.99s
                      Time elapsed: 00:51:41
                               ETA: 00:22:21

################################################################################
                     [1m Learning iteration 1397/2000 [0m                     

                       Computation: 48769 steps/s (collection: 1.915s, learning 0.101s)
             Mean action noise std: 2.95
          Mean value_function loss: 136.1772
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 53.9784
                       Mean reward: 794.35
               Mean episode length: 214.25
    Episode_Reward/reaching_object: 0.9932
     Episode_Reward/lifting_object: 165.6710
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 2.02s
                      Time elapsed: 00:51:43
                               ETA: 00:22:18

################################################################################
                     [1m Learning iteration 1398/2000 [0m                     

                       Computation: 51179 steps/s (collection: 1.828s, learning 0.093s)
             Mean action noise std: 2.95
          Mean value_function loss: 118.6375
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.9924
                       Mean reward: 827.74
               Mean episode length: 221.44
    Episode_Reward/reaching_object: 0.9731
     Episode_Reward/lifting_object: 162.6842
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 1.92s
                      Time elapsed: 00:51:45
                               ETA: 00:22:16

################################################################################
                     [1m Learning iteration 1399/2000 [0m                     

                       Computation: 49014 steps/s (collection: 1.898s, learning 0.108s)
             Mean action noise std: 2.95
          Mean value_function loss: 136.0362
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.0062
                       Mean reward: 790.03
               Mean episode length: 212.23
    Episode_Reward/reaching_object: 0.9684
     Episode_Reward/lifting_object: 161.3976
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 2.01s
                      Time elapsed: 00:51:47
                               ETA: 00:22:14

################################################################################
                     [1m Learning iteration 1400/2000 [0m                     

                       Computation: 51095 steps/s (collection: 1.833s, learning 0.091s)
             Mean action noise std: 2.95
          Mean value_function loss: 203.9833
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.0153
                       Mean reward: 852.32
               Mean episode length: 227.17
    Episode_Reward/reaching_object: 0.9927
     Episode_Reward/lifting_object: 165.7288
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 1.92s
                      Time elapsed: 00:51:49
                               ETA: 00:22:11

################################################################################
                     [1m Learning iteration 1401/2000 [0m                     

                       Computation: 48770 steps/s (collection: 1.904s, learning 0.112s)
             Mean action noise std: 2.95
          Mean value_function loss: 126.7194
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 54.0240
                       Mean reward: 852.25
               Mean episode length: 228.07
    Episode_Reward/reaching_object: 1.0036
     Episode_Reward/lifting_object: 167.5185
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 2.02s
                      Time elapsed: 00:51:51
                               ETA: 00:22:09

################################################################################
                     [1m Learning iteration 1402/2000 [0m                     

                       Computation: 50553 steps/s (collection: 1.855s, learning 0.090s)
             Mean action noise std: 2.96
          Mean value_function loss: 130.4770
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.0393
                       Mean reward: 813.76
               Mean episode length: 218.26
    Episode_Reward/reaching_object: 0.9671
     Episode_Reward/lifting_object: 161.0498
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 1.94s
                      Time elapsed: 00:51:53
                               ETA: 00:22:07

################################################################################
                     [1m Learning iteration 1403/2000 [0m                     

                       Computation: 50241 steps/s (collection: 1.864s, learning 0.093s)
             Mean action noise std: 2.96
          Mean value_function loss: 119.7920
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.0549
                       Mean reward: 810.50
               Mean episode length: 217.51
    Episode_Reward/reaching_object: 0.9928
     Episode_Reward/lifting_object: 165.9940
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 1.96s
                      Time elapsed: 00:51:55
                               ETA: 00:22:04

################################################################################
                     [1m Learning iteration 1404/2000 [0m                     

                       Computation: 50803 steps/s (collection: 1.839s, learning 0.096s)
             Mean action noise std: 2.96
          Mean value_function loss: 152.2176
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.0627
                       Mean reward: 827.68
               Mean episode length: 222.39
    Episode_Reward/reaching_object: 0.9863
     Episode_Reward/lifting_object: 164.6100
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 1.93s
                      Time elapsed: 00:51:57
                               ETA: 00:22:02

################################################################################
                     [1m Learning iteration 1405/2000 [0m                     

                       Computation: 50305 steps/s (collection: 1.863s, learning 0.091s)
             Mean action noise std: 2.96
          Mean value_function loss: 130.6602
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.0683
                       Mean reward: 828.07
               Mean episode length: 222.68
    Episode_Reward/reaching_object: 0.9891
     Episode_Reward/lifting_object: 165.6306
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 1.95s
                      Time elapsed: 00:51:59
                               ETA: 00:22:00

################################################################################
                     [1m Learning iteration 1406/2000 [0m                     

                       Computation: 50590 steps/s (collection: 1.851s, learning 0.092s)
             Mean action noise std: 2.96
          Mean value_function loss: 120.2353
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.0728
                       Mean reward: 835.85
               Mean episode length: 223.03
    Episode_Reward/reaching_object: 1.0126
     Episode_Reward/lifting_object: 169.8737
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 1.94s
                      Time elapsed: 00:52:01
                               ETA: 00:21:57

################################################################################
                     [1m Learning iteration 1407/2000 [0m                     

                       Computation: 48130 steps/s (collection: 1.943s, learning 0.100s)
             Mean action noise std: 2.96
          Mean value_function loss: 103.5802
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.0779
                       Mean reward: 845.25
               Mean episode length: 225.95
    Episode_Reward/reaching_object: 1.0143
     Episode_Reward/lifting_object: 170.3486
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 2.04s
                      Time elapsed: 00:52:03
                               ETA: 00:21:55

################################################################################
                     [1m Learning iteration 1408/2000 [0m                     

                       Computation: 50537 steps/s (collection: 1.851s, learning 0.094s)
             Mean action noise std: 2.96
          Mean value_function loss: 109.3559
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 54.0857
                       Mean reward: 829.78
               Mean episode length: 222.40
    Episode_Reward/reaching_object: 1.0080
     Episode_Reward/lifting_object: 169.6196
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 1.95s
                      Time elapsed: 00:52:05
                               ETA: 00:21:53

################################################################################
                     [1m Learning iteration 1409/2000 [0m                     

                       Computation: 47294 steps/s (collection: 1.971s, learning 0.108s)
             Mean action noise std: 2.96
          Mean value_function loss: 136.5126
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.0981
                       Mean reward: 842.00
               Mean episode length: 227.43
    Episode_Reward/reaching_object: 0.9998
     Episode_Reward/lifting_object: 167.7461
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 2.08s
                      Time elapsed: 00:52:07
                               ETA: 00:21:50

################################################################################
                     [1m Learning iteration 1410/2000 [0m                     

                       Computation: 46770 steps/s (collection: 1.981s, learning 0.121s)
             Mean action noise std: 2.97
          Mean value_function loss: 157.3080
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.1089
                       Mean reward: 853.03
               Mean episode length: 229.00
    Episode_Reward/reaching_object: 0.9753
     Episode_Reward/lifting_object: 163.2530
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 2.10s
                      Time elapsed: 00:52:09
                               ETA: 00:21:48

################################################################################
                     [1m Learning iteration 1411/2000 [0m                     

                       Computation: 46822 steps/s (collection: 1.998s, learning 0.101s)
             Mean action noise std: 2.97
          Mean value_function loss: 160.6625
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.1191
                       Mean reward: 836.73
               Mean episode length: 224.39
    Episode_Reward/reaching_object: 0.9722
     Episode_Reward/lifting_object: 162.4471
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 2.10s
                      Time elapsed: 00:52:11
                               ETA: 00:21:46

################################################################################
                     [1m Learning iteration 1412/2000 [0m                     

                       Computation: 50788 steps/s (collection: 1.835s, learning 0.100s)
             Mean action noise std: 2.97
          Mean value_function loss: 146.1159
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.1269
                       Mean reward: 744.52
               Mean episode length: 202.26
    Episode_Reward/reaching_object: 0.9624
     Episode_Reward/lifting_object: 160.9194
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 1.94s
                      Time elapsed: 00:52:13
                               ETA: 00:21:44

################################################################################
                     [1m Learning iteration 1413/2000 [0m                     

                       Computation: 47949 steps/s (collection: 1.943s, learning 0.108s)
             Mean action noise std: 2.97
          Mean value_function loss: 126.0410
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 54.1316
                       Mean reward: 850.50
               Mean episode length: 227.91
    Episode_Reward/reaching_object: 0.9846
     Episode_Reward/lifting_object: 164.9019
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 2.05s
                      Time elapsed: 00:52:15
                               ETA: 00:21:41

################################################################################
                     [1m Learning iteration 1414/2000 [0m                     

                       Computation: 50288 steps/s (collection: 1.859s, learning 0.096s)
             Mean action noise std: 2.97
          Mean value_function loss: 155.5331
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.1370
                       Mean reward: 770.84
               Mean episode length: 209.03
    Episode_Reward/reaching_object: 0.9726
     Episode_Reward/lifting_object: 162.9279
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 1.95s
                      Time elapsed: 00:52:17
                               ETA: 00:21:39

################################################################################
                     [1m Learning iteration 1415/2000 [0m                     

                       Computation: 50659 steps/s (collection: 1.833s, learning 0.108s)
             Mean action noise std: 2.97
          Mean value_function loss: 122.0885
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.1461
                       Mean reward: 859.45
               Mean episode length: 231.08
    Episode_Reward/reaching_object: 0.9888
     Episode_Reward/lifting_object: 165.3196
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 1.94s
                      Time elapsed: 00:52:19
                               ETA: 00:21:37

################################################################################
                     [1m Learning iteration 1416/2000 [0m                     

                       Computation: 50632 steps/s (collection: 1.842s, learning 0.099s)
             Mean action noise std: 2.97
          Mean value_function loss: 97.3232
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.1577
                       Mean reward: 846.47
               Mean episode length: 226.17
    Episode_Reward/reaching_object: 0.9884
     Episode_Reward/lifting_object: 165.6033
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 1.94s
                      Time elapsed: 00:52:21
                               ETA: 00:21:34

################################################################################
                     [1m Learning iteration 1417/2000 [0m                     

                       Computation: 50256 steps/s (collection: 1.869s, learning 0.087s)
             Mean action noise std: 2.97
          Mean value_function loss: 115.6244
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.1673
                       Mean reward: 842.29
               Mean episode length: 224.63
    Episode_Reward/reaching_object: 1.0224
     Episode_Reward/lifting_object: 171.7622
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 1.96s
                      Time elapsed: 00:52:23
                               ETA: 00:21:32

################################################################################
                     [1m Learning iteration 1418/2000 [0m                     

                       Computation: 49790 steps/s (collection: 1.888s, learning 0.086s)
             Mean action noise std: 2.97
          Mean value_function loss: 132.8436
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 54.1751
                       Mean reward: 842.99
               Mean episode length: 224.57
    Episode_Reward/reaching_object: 0.9843
     Episode_Reward/lifting_object: 165.2691
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 1.97s
                      Time elapsed: 00:52:25
                               ETA: 00:21:30

################################################################################
                     [1m Learning iteration 1419/2000 [0m                     

                       Computation: 50699 steps/s (collection: 1.851s, learning 0.088s)
             Mean action noise std: 2.98
          Mean value_function loss: 136.3287
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.1887
                       Mean reward: 794.67
               Mean episode length: 213.48
    Episode_Reward/reaching_object: 0.9703
     Episode_Reward/lifting_object: 162.1452
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 1.94s
                      Time elapsed: 00:52:27
                               ETA: 00:21:27

################################################################################
                     [1m Learning iteration 1420/2000 [0m                     

                       Computation: 50075 steps/s (collection: 1.857s, learning 0.106s)
             Mean action noise std: 2.98
          Mean value_function loss: 119.0765
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.1998
                       Mean reward: 840.24
               Mean episode length: 224.54
    Episode_Reward/reaching_object: 0.9813
     Episode_Reward/lifting_object: 164.3916
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 1.96s
                      Time elapsed: 00:52:29
                               ETA: 00:21:25

################################################################################
                     [1m Learning iteration 1421/2000 [0m                     

                       Computation: 50925 steps/s (collection: 1.822s, learning 0.109s)
             Mean action noise std: 2.98
          Mean value_function loss: 123.1243
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 54.2105
                       Mean reward: 815.98
               Mean episode length: 218.81
    Episode_Reward/reaching_object: 0.9771
     Episode_Reward/lifting_object: 163.9942
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 1.93s
                      Time elapsed: 00:52:31
                               ETA: 00:21:23

################################################################################
                     [1m Learning iteration 1422/2000 [0m                     

                       Computation: 49852 steps/s (collection: 1.867s, learning 0.105s)
             Mean action noise std: 2.98
          Mean value_function loss: 116.4754
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.2203
                       Mean reward: 821.80
               Mean episode length: 221.24
    Episode_Reward/reaching_object: 1.0078
     Episode_Reward/lifting_object: 169.1068
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 1.97s
                      Time elapsed: 00:52:33
                               ETA: 00:21:20

################################################################################
                     [1m Learning iteration 1423/2000 [0m                     

                       Computation: 49599 steps/s (collection: 1.886s, learning 0.096s)
             Mean action noise std: 2.98
          Mean value_function loss: 120.2343
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.2298
                       Mean reward: 831.11
               Mean episode length: 222.37
    Episode_Reward/reaching_object: 1.0022
     Episode_Reward/lifting_object: 168.0731
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 1.98s
                      Time elapsed: 00:52:35
                               ETA: 00:21:18

################################################################################
                     [1m Learning iteration 1424/2000 [0m                     

                       Computation: 49924 steps/s (collection: 1.855s, learning 0.114s)
             Mean action noise std: 2.98
          Mean value_function loss: 112.0903
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.2387
                       Mean reward: 819.20
               Mean episode length: 220.31
    Episode_Reward/reaching_object: 0.9996
     Episode_Reward/lifting_object: 167.7476
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 1.97s
                      Time elapsed: 00:52:37
                               ETA: 00:21:16

################################################################################
                     [1m Learning iteration 1425/2000 [0m                     

                       Computation: 50077 steps/s (collection: 1.852s, learning 0.111s)
             Mean action noise std: 2.98
          Mean value_function loss: 154.7642
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.2466
                       Mean reward: 814.06
               Mean episode length: 218.01
    Episode_Reward/reaching_object: 1.0027
     Episode_Reward/lifting_object: 168.3305
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 1.96s
                      Time elapsed: 00:52:39
                               ETA: 00:21:13

################################################################################
                     [1m Learning iteration 1426/2000 [0m                     

                       Computation: 50719 steps/s (collection: 1.835s, learning 0.104s)
             Mean action noise std: 2.99
          Mean value_function loss: 165.7123
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.2549
                       Mean reward: 818.09
               Mean episode length: 219.54
    Episode_Reward/reaching_object: 0.9922
     Episode_Reward/lifting_object: 165.9554
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 1.94s
                      Time elapsed: 00:52:41
                               ETA: 00:21:11

################################################################################
                     [1m Learning iteration 1427/2000 [0m                     

                       Computation: 50360 steps/s (collection: 1.849s, learning 0.103s)
             Mean action noise std: 2.99
          Mean value_function loss: 140.6082
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.2695
                       Mean reward: 844.04
               Mean episode length: 225.87
    Episode_Reward/reaching_object: 0.9893
     Episode_Reward/lifting_object: 165.7390
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 1.95s
                      Time elapsed: 00:52:43
                               ETA: 00:21:09

################################################################################
                     [1m Learning iteration 1428/2000 [0m                     

                       Computation: 50711 steps/s (collection: 1.841s, learning 0.097s)
             Mean action noise std: 2.99
          Mean value_function loss: 149.3387
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 54.2841
                       Mean reward: 821.91
               Mean episode length: 220.40
    Episode_Reward/reaching_object: 0.9920
     Episode_Reward/lifting_object: 166.0193
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 1.94s
                      Time elapsed: 00:52:45
                               ETA: 00:21:06

################################################################################
                     [1m Learning iteration 1429/2000 [0m                     

                       Computation: 51039 steps/s (collection: 1.833s, learning 0.093s)
             Mean action noise std: 2.99
          Mean value_function loss: 121.3914
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.2955
                       Mean reward: 752.18
               Mean episode length: 204.00
    Episode_Reward/reaching_object: 0.9805
     Episode_Reward/lifting_object: 164.5493
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 1.93s
                      Time elapsed: 00:52:46
                               ETA: 00:21:04

################################################################################
                     [1m Learning iteration 1430/2000 [0m                     

                       Computation: 51054 steps/s (collection: 1.840s, learning 0.085s)
             Mean action noise std: 2.99
          Mean value_function loss: 152.8387
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 54.3068
                       Mean reward: 836.66
               Mean episode length: 224.22
    Episode_Reward/reaching_object: 1.0034
     Episode_Reward/lifting_object: 168.7559
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 1.93s
                      Time elapsed: 00:52:48
                               ETA: 00:21:02

################################################################################
                     [1m Learning iteration 1431/2000 [0m                     

                       Computation: 50514 steps/s (collection: 1.856s, learning 0.090s)
             Mean action noise std: 2.99
          Mean value_function loss: 155.3657
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 54.3148
                       Mean reward: 808.75
               Mean episode length: 217.81
    Episode_Reward/reaching_object: 0.9503
     Episode_Reward/lifting_object: 158.9793
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 1.95s
                      Time elapsed: 00:52:50
                               ETA: 00:20:59

################################################################################
                     [1m Learning iteration 1432/2000 [0m                     

                       Computation: 50532 steps/s (collection: 1.854s, learning 0.091s)
             Mean action noise std: 2.99
          Mean value_function loss: 118.9919
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.3225
                       Mean reward: 807.86
               Mean episode length: 217.44
    Episode_Reward/reaching_object: 1.0062
     Episode_Reward/lifting_object: 169.2881
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 1.95s
                      Time elapsed: 00:52:52
                               ETA: 00:20:57

################################################################################
                     [1m Learning iteration 1433/2000 [0m                     

                       Computation: 49461 steps/s (collection: 1.899s, learning 0.089s)
             Mean action noise std: 3.00
          Mean value_function loss: 177.9586
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 54.3287
                       Mean reward: 785.33
               Mean episode length: 212.00
    Episode_Reward/reaching_object: 1.0052
     Episode_Reward/lifting_object: 169.3801
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 1.99s
                      Time elapsed: 00:52:54
                               ETA: 00:20:55

################################################################################
                     [1m Learning iteration 1434/2000 [0m                     

                       Computation: 51044 steps/s (collection: 1.833s, learning 0.093s)
             Mean action noise std: 3.00
          Mean value_function loss: 134.5333
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.3404
                       Mean reward: 832.01
               Mean episode length: 222.19
    Episode_Reward/reaching_object: 0.9938
     Episode_Reward/lifting_object: 166.8232
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 1.93s
                      Time elapsed: 00:52:56
                               ETA: 00:20:52

################################################################################
                     [1m Learning iteration 1435/2000 [0m                     

                       Computation: 51387 steps/s (collection: 1.824s, learning 0.089s)
             Mean action noise std: 3.00
          Mean value_function loss: 136.6273
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 54.3504
                       Mean reward: 806.45
               Mean episode length: 217.28
    Episode_Reward/reaching_object: 0.9888
     Episode_Reward/lifting_object: 166.0964
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 1.91s
                      Time elapsed: 00:52:58
                               ETA: 00:20:50

################################################################################
                     [1m Learning iteration 1436/2000 [0m                     

                       Computation: 50789 steps/s (collection: 1.830s, learning 0.105s)
             Mean action noise std: 3.00
          Mean value_function loss: 126.2664
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.3588
                       Mean reward: 857.22
               Mean episode length: 228.74
    Episode_Reward/reaching_object: 0.9979
     Episode_Reward/lifting_object: 167.6826
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 1.94s
                      Time elapsed: 00:53:00
                               ETA: 00:20:48

################################################################################
                     [1m Learning iteration 1437/2000 [0m                     

                       Computation: 50488 steps/s (collection: 1.838s, learning 0.109s)
             Mean action noise std: 3.00
          Mean value_function loss: 116.2495
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 54.3678
                       Mean reward: 835.77
               Mean episode length: 224.14
    Episode_Reward/reaching_object: 0.9920
     Episode_Reward/lifting_object: 166.9311
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 1.95s
                      Time elapsed: 00:53:02
                               ETA: 00:20:46

################################################################################
                     [1m Learning iteration 1438/2000 [0m                     

                       Computation: 50478 steps/s (collection: 1.855s, learning 0.092s)
             Mean action noise std: 3.00
          Mean value_function loss: 124.6441
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.3771
                       Mean reward: 834.02
               Mean episode length: 222.56
    Episode_Reward/reaching_object: 0.9896
     Episode_Reward/lifting_object: 166.4012
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 1.95s
                      Time elapsed: 00:53:04
                               ETA: 00:20:43

################################################################################
                     [1m Learning iteration 1439/2000 [0m                     

                       Computation: 50318 steps/s (collection: 1.860s, learning 0.094s)
             Mean action noise std: 3.00
          Mean value_function loss: 130.8954
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.3866
                       Mean reward: 808.66
               Mean episode length: 217.70
    Episode_Reward/reaching_object: 0.9725
     Episode_Reward/lifting_object: 162.8116
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 1.95s
                      Time elapsed: 00:53:06
                               ETA: 00:20:41

################################################################################
                     [1m Learning iteration 1440/2000 [0m                     

                       Computation: 51038 steps/s (collection: 1.838s, learning 0.088s)
             Mean action noise std: 3.01
          Mean value_function loss: 120.9330
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.3980
                       Mean reward: 844.16
               Mean episode length: 225.83
    Episode_Reward/reaching_object: 1.0181
     Episode_Reward/lifting_object: 171.2572
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 1.93s
                      Time elapsed: 00:53:08
                               ETA: 00:20:39

################################################################################
                     [1m Learning iteration 1441/2000 [0m                     

                       Computation: 50513 steps/s (collection: 1.854s, learning 0.093s)
             Mean action noise std: 3.01
          Mean value_function loss: 138.1792
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.4140
                       Mean reward: 826.24
               Mean episode length: 221.99
    Episode_Reward/reaching_object: 0.9886
     Episode_Reward/lifting_object: 165.5339
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 1.95s
                      Time elapsed: 00:53:10
                               ETA: 00:20:36

################################################################################
                     [1m Learning iteration 1442/2000 [0m                     

                       Computation: 50465 steps/s (collection: 1.858s, learning 0.090s)
             Mean action noise std: 3.01
          Mean value_function loss: 127.7497
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 54.4291
                       Mean reward: 812.68
               Mean episode length: 218.63
    Episode_Reward/reaching_object: 0.9969
     Episode_Reward/lifting_object: 167.6064
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 1.95s
                      Time elapsed: 00:53:12
                               ETA: 00:20:34

################################################################################
                     [1m Learning iteration 1443/2000 [0m                     

                       Computation: 50491 steps/s (collection: 1.852s, learning 0.095s)
             Mean action noise std: 3.01
          Mean value_function loss: 118.8439
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.4316
                       Mean reward: 816.89
               Mean episode length: 219.37
    Episode_Reward/reaching_object: 0.9854
     Episode_Reward/lifting_object: 165.4039
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 1.95s
                      Time elapsed: 00:53:14
                               ETA: 00:20:32

################################################################################
                     [1m Learning iteration 1444/2000 [0m                     

                       Computation: 49167 steps/s (collection: 1.889s, learning 0.110s)
             Mean action noise std: 3.01
          Mean value_function loss: 119.1799
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.4359
                       Mean reward: 848.26
               Mean episode length: 226.41
    Episode_Reward/reaching_object: 1.0142
     Episode_Reward/lifting_object: 170.7681
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 2.00s
                      Time elapsed: 00:53:16
                               ETA: 00:20:29

################################################################################
                     [1m Learning iteration 1445/2000 [0m                     

                       Computation: 50527 steps/s (collection: 1.855s, learning 0.091s)
             Mean action noise std: 3.01
          Mean value_function loss: 122.2063
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.4453
                       Mean reward: 838.93
               Mean episode length: 223.84
    Episode_Reward/reaching_object: 0.9900
     Episode_Reward/lifting_object: 166.8664
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 1.95s
                      Time elapsed: 00:53:18
                               ETA: 00:20:27

################################################################################
                     [1m Learning iteration 1446/2000 [0m                     

                       Computation: 50878 steps/s (collection: 1.837s, learning 0.096s)
             Mean action noise std: 3.01
          Mean value_function loss: 131.1534
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.4562
                       Mean reward: 788.64
               Mean episode length: 212.83
    Episode_Reward/reaching_object: 0.9577
     Episode_Reward/lifting_object: 160.3117
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 1.93s
                      Time elapsed: 00:53:20
                               ETA: 00:20:25

################################################################################
                     [1m Learning iteration 1447/2000 [0m                     

                       Computation: 51001 steps/s (collection: 1.839s, learning 0.088s)
             Mean action noise std: 3.01
          Mean value_function loss: 132.2313
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.4664
                       Mean reward: 860.45
               Mean episode length: 229.25
    Episode_Reward/reaching_object: 1.0053
     Episode_Reward/lifting_object: 169.2728
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 1.93s
                      Time elapsed: 00:53:21
                               ETA: 00:20:22

################################################################################
                     [1m Learning iteration 1448/2000 [0m                     

                       Computation: 50950 steps/s (collection: 1.838s, learning 0.092s)
             Mean action noise std: 3.02
          Mean value_function loss: 107.8108
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.4754
                       Mean reward: 873.80
               Mean episode length: 232.62
    Episode_Reward/reaching_object: 0.9877
     Episode_Reward/lifting_object: 165.9940
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 1.93s
                      Time elapsed: 00:53:23
                               ETA: 00:20:20

################################################################################
                     [1m Learning iteration 1449/2000 [0m                     

                       Computation: 50788 steps/s (collection: 1.845s, learning 0.091s)
             Mean action noise std: 3.02
          Mean value_function loss: 119.0002
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 54.4866
                       Mean reward: 855.89
               Mean episode length: 227.72
    Episode_Reward/reaching_object: 0.9954
     Episode_Reward/lifting_object: 167.3729
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 1.94s
                      Time elapsed: 00:53:25
                               ETA: 00:20:18

################################################################################
                     [1m Learning iteration 1450/2000 [0m                     

                       Computation: 51311 steps/s (collection: 1.827s, learning 0.089s)
             Mean action noise std: 3.02
          Mean value_function loss: 177.9230
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 54.5020
                       Mean reward: 838.17
               Mean episode length: 224.95
    Episode_Reward/reaching_object: 0.9969
     Episode_Reward/lifting_object: 167.4557
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 1.92s
                      Time elapsed: 00:53:27
                               ETA: 00:20:15

################################################################################
                     [1m Learning iteration 1451/2000 [0m                     

                       Computation: 49261 steps/s (collection: 1.893s, learning 0.103s)
             Mean action noise std: 3.02
          Mean value_function loss: 109.9395
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 54.5117
                       Mean reward: 855.87
               Mean episode length: 228.85
    Episode_Reward/reaching_object: 1.0203
     Episode_Reward/lifting_object: 171.5900
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 2.00s
                      Time elapsed: 00:53:29
                               ETA: 00:20:13

################################################################################
                     [1m Learning iteration 1452/2000 [0m                     

                       Computation: 49689 steps/s (collection: 1.864s, learning 0.115s)
             Mean action noise std: 3.02
          Mean value_function loss: 134.4444
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.5233
                       Mean reward: 875.92
               Mean episode length: 232.63
    Episode_Reward/reaching_object: 1.0094
     Episode_Reward/lifting_object: 169.5989
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 1.98s
                      Time elapsed: 00:53:31
                               ETA: 00:20:11

################################################################################
                     [1m Learning iteration 1453/2000 [0m                     

                       Computation: 49050 steps/s (collection: 1.888s, learning 0.117s)
             Mean action noise std: 3.02
          Mean value_function loss: 144.5777
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.5342
                       Mean reward: 832.38
               Mean episode length: 222.69
    Episode_Reward/reaching_object: 0.9811
     Episode_Reward/lifting_object: 164.2032
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 2.00s
                      Time elapsed: 00:53:33
                               ETA: 00:20:09

################################################################################
                     [1m Learning iteration 1454/2000 [0m                     

                       Computation: 49384 steps/s (collection: 1.902s, learning 0.089s)
             Mean action noise std: 3.03
          Mean value_function loss: 154.0535
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 54.5448
                       Mean reward: 773.36
               Mean episode length: 208.74
    Episode_Reward/reaching_object: 0.9718
     Episode_Reward/lifting_object: 162.6700
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 1.99s
                      Time elapsed: 00:53:35
                               ETA: 00:20:06

################################################################################
                     [1m Learning iteration 1455/2000 [0m                     

                       Computation: 50336 steps/s (collection: 1.857s, learning 0.096s)
             Mean action noise std: 3.03
          Mean value_function loss: 139.0614
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 54.5542
                       Mean reward: 858.60
               Mean episode length: 228.68
    Episode_Reward/reaching_object: 0.9838
     Episode_Reward/lifting_object: 164.6101
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 1.95s
                      Time elapsed: 00:53:37
                               ETA: 00:20:04

################################################################################
                     [1m Learning iteration 1456/2000 [0m                     

                       Computation: 48251 steps/s (collection: 1.942s, learning 0.095s)
             Mean action noise std: 3.03
          Mean value_function loss: 161.3882
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.5616
                       Mean reward: 830.05
               Mean episode length: 222.57
    Episode_Reward/reaching_object: 0.9934
     Episode_Reward/lifting_object: 166.7416
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 2.04s
                      Time elapsed: 00:53:39
                               ETA: 00:20:02

################################################################################
                     [1m Learning iteration 1457/2000 [0m                     

                       Computation: 51141 steps/s (collection: 1.831s, learning 0.091s)
             Mean action noise std: 3.03
          Mean value_function loss: 115.7388
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.5735
                       Mean reward: 826.08
               Mean episode length: 221.74
    Episode_Reward/reaching_object: 0.9962
     Episode_Reward/lifting_object: 167.3827
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 1.92s
                      Time elapsed: 00:53:41
                               ETA: 00:19:59

################################################################################
                     [1m Learning iteration 1458/2000 [0m                     

                       Computation: 48751 steps/s (collection: 1.903s, learning 0.114s)
             Mean action noise std: 3.03
          Mean value_function loss: 111.1763
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.5835
                       Mean reward: 837.48
               Mean episode length: 223.26
    Episode_Reward/reaching_object: 0.9800
     Episode_Reward/lifting_object: 164.4626
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 2.02s
                      Time elapsed: 00:53:43
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 1459/2000 [0m                     

                       Computation: 49984 steps/s (collection: 1.870s, learning 0.097s)
             Mean action noise std: 3.03
          Mean value_function loss: 116.7707
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 54.5922
                       Mean reward: 815.04
               Mean episode length: 217.79
    Episode_Reward/reaching_object: 1.0053
     Episode_Reward/lifting_object: 168.9687
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 1.97s
                      Time elapsed: 00:53:45
                               ETA: 00:19:55

################################################################################
                     [1m Learning iteration 1460/2000 [0m                     

                       Computation: 51576 steps/s (collection: 1.817s, learning 0.089s)
             Mean action noise std: 3.03
          Mean value_function loss: 101.0486
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 54.6009
                       Mean reward: 806.00
               Mean episode length: 216.36
    Episode_Reward/reaching_object: 0.9878
     Episode_Reward/lifting_object: 165.5986
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 1.91s
                      Time elapsed: 00:53:47
                               ETA: 00:19:52

################################################################################
                     [1m Learning iteration 1461/2000 [0m                     

                       Computation: 49987 steps/s (collection: 1.875s, learning 0.092s)
             Mean action noise std: 3.03
          Mean value_function loss: 113.9415
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.6079
                       Mean reward: 860.26
               Mean episode length: 229.47
    Episode_Reward/reaching_object: 1.0085
     Episode_Reward/lifting_object: 169.2857
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 1.97s
                      Time elapsed: 00:53:49
                               ETA: 00:19:50

################################################################################
                     [1m Learning iteration 1462/2000 [0m                     

                       Computation: 50230 steps/s (collection: 1.871s, learning 0.087s)
             Mean action noise std: 3.04
          Mean value_function loss: 102.8300
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.6139
                       Mean reward: 871.03
               Mean episode length: 231.50
    Episode_Reward/reaching_object: 1.0082
     Episode_Reward/lifting_object: 169.3758
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 1.96s
                      Time elapsed: 00:53:51
                               ETA: 00:19:48

################################################################################
                     [1m Learning iteration 1463/2000 [0m                     

                       Computation: 50348 steps/s (collection: 1.865s, learning 0.088s)
             Mean action noise std: 3.04
          Mean value_function loss: 109.0455
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.6254
                       Mean reward: 887.60
               Mean episode length: 235.51
    Episode_Reward/reaching_object: 1.0315
     Episode_Reward/lifting_object: 173.1911
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 1.95s
                      Time elapsed: 00:53:53
                               ETA: 00:19:46

################################################################################
                     [1m Learning iteration 1464/2000 [0m                     

                       Computation: 51172 steps/s (collection: 1.830s, learning 0.091s)
             Mean action noise std: 3.04
          Mean value_function loss: 97.5519
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.6432
                       Mean reward: 867.95
               Mean episode length: 231.41
    Episode_Reward/reaching_object: 1.0138
     Episode_Reward/lifting_object: 170.4581
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 1.92s
                      Time elapsed: 00:53:55
                               ETA: 00:19:43

################################################################################
                     [1m Learning iteration 1465/2000 [0m                     

                       Computation: 50156 steps/s (collection: 1.858s, learning 0.102s)
             Mean action noise std: 3.04
          Mean value_function loss: 135.7436
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 54.6509
                       Mean reward: 822.78
               Mean episode length: 220.95
    Episode_Reward/reaching_object: 1.0081
     Episode_Reward/lifting_object: 168.7237
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 1.96s
                      Time elapsed: 00:53:57
                               ETA: 00:19:41

################################################################################
                     [1m Learning iteration 1466/2000 [0m                     

                       Computation: 50595 steps/s (collection: 1.837s, learning 0.106s)
             Mean action noise std: 3.04
          Mean value_function loss: 143.0178
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 54.6557
                       Mean reward: 872.16
               Mean episode length: 231.69
    Episode_Reward/reaching_object: 1.0174
     Episode_Reward/lifting_object: 170.3683
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 1.94s
                      Time elapsed: 00:53:59
                               ETA: 00:19:39

################################################################################
                     [1m Learning iteration 1467/2000 [0m                     

                       Computation: 51392 steps/s (collection: 1.821s, learning 0.092s)
             Mean action noise std: 3.04
          Mean value_function loss: 130.9007
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.6690
                       Mean reward: 858.28
               Mean episode length: 228.31
    Episode_Reward/reaching_object: 1.0000
     Episode_Reward/lifting_object: 167.5092
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 1.91s
                      Time elapsed: 00:54:01
                               ETA: 00:19:36

################################################################################
                     [1m Learning iteration 1468/2000 [0m                     

                       Computation: 49823 steps/s (collection: 1.867s, learning 0.106s)
             Mean action noise std: 3.05
          Mean value_function loss: 129.5339
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.6843
                       Mean reward: 857.10
               Mean episode length: 229.08
    Episode_Reward/reaching_object: 0.9779
     Episode_Reward/lifting_object: 163.4926
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 1.97s
                      Time elapsed: 00:54:03
                               ETA: 00:19:34

################################################################################
                     [1m Learning iteration 1469/2000 [0m                     

                       Computation: 51452 steps/s (collection: 1.821s, learning 0.090s)
             Mean action noise std: 3.05
          Mean value_function loss: 115.5616
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.6994
                       Mean reward: 896.11
               Mean episode length: 237.50
    Episode_Reward/reaching_object: 0.9864
     Episode_Reward/lifting_object: 165.5911
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 1.91s
                      Time elapsed: 00:54:05
                               ETA: 00:19:32

################################################################################
                     [1m Learning iteration 1470/2000 [0m                     

                       Computation: 50753 steps/s (collection: 1.834s, learning 0.103s)
             Mean action noise std: 3.05
          Mean value_function loss: 134.8787
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.7157
                       Mean reward: 839.19
               Mean episode length: 223.30
    Episode_Reward/reaching_object: 0.9898
     Episode_Reward/lifting_object: 166.6422
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 1.94s
                      Time elapsed: 00:54:06
                               ETA: 00:19:29

################################################################################
                     [1m Learning iteration 1471/2000 [0m                     

                       Computation: 51033 steps/s (collection: 1.829s, learning 0.097s)
             Mean action noise std: 3.05
          Mean value_function loss: 145.2870
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.7281
                       Mean reward: 829.01
               Mean episode length: 222.48
    Episode_Reward/reaching_object: 0.9987
     Episode_Reward/lifting_object: 167.9516
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 1.93s
                      Time elapsed: 00:54:08
                               ETA: 00:19:27

################################################################################
                     [1m Learning iteration 1472/2000 [0m                     

                       Computation: 50593 steps/s (collection: 1.854s, learning 0.089s)
             Mean action noise std: 3.05
          Mean value_function loss: 130.0208
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 54.7351
                       Mean reward: 852.18
               Mean episode length: 226.66
    Episode_Reward/reaching_object: 0.9993
     Episode_Reward/lifting_object: 167.8445
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 1.94s
                      Time elapsed: 00:54:10
                               ETA: 00:19:25

################################################################################
                     [1m Learning iteration 1473/2000 [0m                     

                       Computation: 50449 steps/s (collection: 1.858s, learning 0.091s)
             Mean action noise std: 3.05
          Mean value_function loss: 122.4478
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.7454
                       Mean reward: 859.63
               Mean episode length: 228.81
    Episode_Reward/reaching_object: 0.9928
     Episode_Reward/lifting_object: 167.0082
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 1.95s
                      Time elapsed: 00:54:12
                               ETA: 00:19:22

################################################################################
                     [1m Learning iteration 1474/2000 [0m                     

                       Computation: 51457 steps/s (collection: 1.820s, learning 0.090s)
             Mean action noise std: 3.05
          Mean value_function loss: 143.6992
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.7518
                       Mean reward: 832.35
               Mean episode length: 222.20
    Episode_Reward/reaching_object: 0.9722
     Episode_Reward/lifting_object: 163.3313
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 1.91s
                      Time elapsed: 00:54:14
                               ETA: 00:19:20

################################################################################
                     [1m Learning iteration 1475/2000 [0m                     

                       Computation: 50889 steps/s (collection: 1.830s, learning 0.102s)
             Mean action noise std: 3.06
          Mean value_function loss: 135.9108
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.7583
                       Mean reward: 833.10
               Mean episode length: 223.84
    Episode_Reward/reaching_object: 0.9800
     Episode_Reward/lifting_object: 164.5416
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 1.93s
                      Time elapsed: 00:54:16
                               ETA: 00:19:18

################################################################################
                     [1m Learning iteration 1476/2000 [0m                     

                       Computation: 51122 steps/s (collection: 1.833s, learning 0.090s)
             Mean action noise std: 3.06
          Mean value_function loss: 124.7650
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 54.7731
                       Mean reward: 871.30
               Mean episode length: 232.36
    Episode_Reward/reaching_object: 0.9924
     Episode_Reward/lifting_object: 166.6671
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 1.92s
                      Time elapsed: 00:54:18
                               ETA: 00:19:16

################################################################################
                     [1m Learning iteration 1477/2000 [0m                     

                       Computation: 51138 steps/s (collection: 1.828s, learning 0.095s)
             Mean action noise std: 3.06
          Mean value_function loss: 128.0819
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.7870
                       Mean reward: 804.90
               Mean episode length: 215.71
    Episode_Reward/reaching_object: 0.9990
     Episode_Reward/lifting_object: 168.1514
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 1.92s
                      Time elapsed: 00:54:20
                               ETA: 00:19:13

################################################################################
                     [1m Learning iteration 1478/2000 [0m                     

                       Computation: 50133 steps/s (collection: 1.864s, learning 0.097s)
             Mean action noise std: 3.06
          Mean value_function loss: 117.0696
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 54.8046
                       Mean reward: 864.98
               Mean episode length: 229.89
    Episode_Reward/reaching_object: 1.0118
     Episode_Reward/lifting_object: 170.1396
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 1.96s
                      Time elapsed: 00:54:22
                               ETA: 00:19:11

################################################################################
                     [1m Learning iteration 1479/2000 [0m                     

                       Computation: 51079 steps/s (collection: 1.833s, learning 0.092s)
             Mean action noise std: 3.07
          Mean value_function loss: 109.9381
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 54.8219
                       Mean reward: 877.23
               Mean episode length: 232.28
    Episode_Reward/reaching_object: 1.0111
     Episode_Reward/lifting_object: 169.9996
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 1.92s
                      Time elapsed: 00:54:24
                               ETA: 00:19:09

################################################################################
                     [1m Learning iteration 1480/2000 [0m                     

                       Computation: 50361 steps/s (collection: 1.861s, learning 0.091s)
             Mean action noise std: 3.07
          Mean value_function loss: 115.7635
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.8364
                       Mean reward: 841.29
               Mean episode length: 224.16
    Episode_Reward/reaching_object: 0.9770
     Episode_Reward/lifting_object: 164.0038
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 1.95s
                      Time elapsed: 00:54:26
                               ETA: 00:19:06

################################################################################
                     [1m Learning iteration 1481/2000 [0m                     

                       Computation: 50813 steps/s (collection: 1.842s, learning 0.093s)
             Mean action noise std: 3.07
          Mean value_function loss: 124.0753
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 54.8490
                       Mean reward: 817.59
               Mean episode length: 217.96
    Episode_Reward/reaching_object: 0.9892
     Episode_Reward/lifting_object: 166.4391
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 1.93s
                      Time elapsed: 00:54:28
                               ETA: 00:19:04

################################################################################
                     [1m Learning iteration 1482/2000 [0m                     

                       Computation: 50894 steps/s (collection: 1.824s, learning 0.108s)
             Mean action noise std: 3.07
          Mean value_function loss: 96.3907
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.8519
                       Mean reward: 831.07
               Mean episode length: 221.57
    Episode_Reward/reaching_object: 1.0136
     Episode_Reward/lifting_object: 170.6642
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 1.93s
                      Time elapsed: 00:54:30
                               ETA: 00:19:02

################################################################################
                     [1m Learning iteration 1483/2000 [0m                     

                       Computation: 50231 steps/s (collection: 1.857s, learning 0.101s)
             Mean action noise std: 3.07
          Mean value_function loss: 108.3667
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.8583
                       Mean reward: 919.54
               Mean episode length: 242.59
    Episode_Reward/reaching_object: 1.0379
     Episode_Reward/lifting_object: 175.4086
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 1.96s
                      Time elapsed: 00:54:32
                               ETA: 00:18:59

################################################################################
                     [1m Learning iteration 1484/2000 [0m                     

                       Computation: 49632 steps/s (collection: 1.863s, learning 0.117s)
             Mean action noise std: 3.07
          Mean value_function loss: 98.4935
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.8695
                       Mean reward: 861.12
               Mean episode length: 229.08
    Episode_Reward/reaching_object: 1.0247
     Episode_Reward/lifting_object: 172.8427
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 1.98s
                      Time elapsed: 00:54:34
                               ETA: 00:18:57

################################################################################
                     [1m Learning iteration 1485/2000 [0m                     

                       Computation: 51133 steps/s (collection: 1.829s, learning 0.093s)
             Mean action noise std: 3.07
          Mean value_function loss: 106.7753
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 54.8812
                       Mean reward: 890.40
               Mean episode length: 236.11
    Episode_Reward/reaching_object: 0.9966
     Episode_Reward/lifting_object: 167.5169
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 1.92s
                      Time elapsed: 00:54:36
                               ETA: 00:18:55

################################################################################
                     [1m Learning iteration 1486/2000 [0m                     

                       Computation: 51304 steps/s (collection: 1.827s, learning 0.090s)
             Mean action noise std: 3.07
          Mean value_function loss: 103.5527
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 54.8939
                       Mean reward: 858.46
               Mean episode length: 228.15
    Episode_Reward/reaching_object: 1.0103
     Episode_Reward/lifting_object: 170.0704
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 1.92s
                      Time elapsed: 00:54:37
                               ETA: 00:18:53

################################################################################
                     [1m Learning iteration 1487/2000 [0m                     

                       Computation: 51767 steps/s (collection: 1.809s, learning 0.090s)
             Mean action noise std: 3.08
          Mean value_function loss: 124.6033
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 54.9021
                       Mean reward: 848.27
               Mean episode length: 225.19
    Episode_Reward/reaching_object: 0.9941
     Episode_Reward/lifting_object: 167.0940
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 1.90s
                      Time elapsed: 00:54:39
                               ETA: 00:18:50

################################################################################
                     [1m Learning iteration 1488/2000 [0m                     

                       Computation: 51402 steps/s (collection: 1.815s, learning 0.098s)
             Mean action noise std: 3.08
          Mean value_function loss: 130.4271
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 54.9155
                       Mean reward: 815.63
               Mean episode length: 217.74
    Episode_Reward/reaching_object: 0.9820
     Episode_Reward/lifting_object: 164.9320
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 1.91s
                      Time elapsed: 00:54:41
                               ETA: 00:18:48

################################################################################
                     [1m Learning iteration 1489/2000 [0m                     

                       Computation: 50582 steps/s (collection: 1.843s, learning 0.101s)
             Mean action noise std: 3.08
          Mean value_function loss: 107.4817
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.9309
                       Mean reward: 906.60
               Mean episode length: 239.71
    Episode_Reward/reaching_object: 0.9998
     Episode_Reward/lifting_object: 168.2059
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 1.94s
                      Time elapsed: 00:54:43
                               ETA: 00:18:46

################################################################################
                     [1m Learning iteration 1490/2000 [0m                     

                       Computation: 50965 steps/s (collection: 1.833s, learning 0.096s)
             Mean action noise std: 3.08
          Mean value_function loss: 121.2622
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.9435
                       Mean reward: 824.80
               Mean episode length: 220.81
    Episode_Reward/reaching_object: 0.9904
     Episode_Reward/lifting_object: 166.7814
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 1.93s
                      Time elapsed: 00:54:45
                               ETA: 00:18:43

################################################################################
                     [1m Learning iteration 1491/2000 [0m                     

                       Computation: 50624 steps/s (collection: 1.852s, learning 0.090s)
             Mean action noise std: 3.08
          Mean value_function loss: 158.6426
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.9560
                       Mean reward: 782.85
               Mean episode length: 212.03
    Episode_Reward/reaching_object: 0.9710
     Episode_Reward/lifting_object: 162.8856
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 1.94s
                      Time elapsed: 00:54:47
                               ETA: 00:18:41

################################################################################
                     [1m Learning iteration 1492/2000 [0m                     

                       Computation: 50841 steps/s (collection: 1.847s, learning 0.087s)
             Mean action noise std: 3.09
          Mean value_function loss: 129.5444
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 54.9688
                       Mean reward: 824.31
               Mean episode length: 222.59
    Episode_Reward/reaching_object: 0.9664
     Episode_Reward/lifting_object: 160.8094
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 1.93s
                      Time elapsed: 00:54:49
                               ETA: 00:18:39

################################################################################
                     [1m Learning iteration 1493/2000 [0m                     

                       Computation: 50669 steps/s (collection: 1.850s, learning 0.090s)
             Mean action noise std: 3.09
          Mean value_function loss: 120.5957
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.9818
                       Mean reward: 879.51
               Mean episode length: 234.03
    Episode_Reward/reaching_object: 1.0322
     Episode_Reward/lifting_object: 173.9613
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 1.94s
                      Time elapsed: 00:54:51
                               ETA: 00:18:36

################################################################################
                     [1m Learning iteration 1494/2000 [0m                     

                       Computation: 51335 steps/s (collection: 1.825s, learning 0.090s)
             Mean action noise std: 3.09
          Mean value_function loss: 129.5878
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.9976
                       Mean reward: 856.02
               Mean episode length: 228.31
    Episode_Reward/reaching_object: 0.9911
     Episode_Reward/lifting_object: 166.2653
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 1.91s
                      Time elapsed: 00:54:53
                               ETA: 00:18:34

################################################################################
                     [1m Learning iteration 1495/2000 [0m                     

                       Computation: 50759 steps/s (collection: 1.848s, learning 0.089s)
             Mean action noise std: 3.09
          Mean value_function loss: 130.4285
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.0139
                       Mean reward: 813.69
               Mean episode length: 218.58
    Episode_Reward/reaching_object: 0.9909
     Episode_Reward/lifting_object: 166.4396
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 1.94s
                      Time elapsed: 00:54:55
                               ETA: 00:18:32

################################################################################
                     [1m Learning iteration 1496/2000 [0m                     

                       Computation: 50885 steps/s (collection: 1.842s, learning 0.090s)
             Mean action noise std: 3.09
          Mean value_function loss: 151.6236
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 55.0250
                       Mean reward: 861.84
               Mean episode length: 228.87
    Episode_Reward/reaching_object: 1.0037
     Episode_Reward/lifting_object: 168.6641
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 1.93s
                      Time elapsed: 00:54:57
                               ETA: 00:18:30

################################################################################
                     [1m Learning iteration 1497/2000 [0m                     

                       Computation: 50623 steps/s (collection: 1.844s, learning 0.098s)
             Mean action noise std: 3.09
          Mean value_function loss: 116.8558
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 55.0337
                       Mean reward: 851.43
               Mean episode length: 226.67
    Episode_Reward/reaching_object: 0.9786
     Episode_Reward/lifting_object: 163.9209
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 1.94s
                      Time elapsed: 00:54:59
                               ETA: 00:18:27

################################################################################
                     [1m Learning iteration 1498/2000 [0m                     

                       Computation: 49015 steps/s (collection: 1.890s, learning 0.115s)
             Mean action noise std: 3.10
          Mean value_function loss: 100.1431
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 55.0469
                       Mean reward: 840.14
               Mean episode length: 224.49
    Episode_Reward/reaching_object: 1.0032
     Episode_Reward/lifting_object: 168.2702
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 2.01s
                      Time elapsed: 00:55:01
                               ETA: 00:18:25

################################################################################
                     [1m Learning iteration 1499/2000 [0m                     

                       Computation: 49832 steps/s (collection: 1.855s, learning 0.118s)
             Mean action noise std: 3.10
          Mean value_function loss: 121.8698
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 55.0571
                       Mean reward: 877.87
               Mean episode length: 232.42
    Episode_Reward/reaching_object: 1.0102
     Episode_Reward/lifting_object: 170.1647
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 1.97s
                      Time elapsed: 00:55:03
                               ETA: 00:18:23

################################################################################
                     [1m Learning iteration 1500/2000 [0m                     

                       Computation: 50894 steps/s (collection: 1.822s, learning 0.109s)
             Mean action noise std: 3.10
          Mean value_function loss: 136.4202
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.0662
                       Mean reward: 836.65
               Mean episode length: 222.39
    Episode_Reward/reaching_object: 1.0094
     Episode_Reward/lifting_object: 170.1117
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 1.93s
                      Time elapsed: 00:55:05
                               ETA: 00:18:20

################################################################################
                     [1m Learning iteration 1501/2000 [0m                     

                       Computation: 50015 steps/s (collection: 1.877s, learning 0.089s)
             Mean action noise std: 3.10
          Mean value_function loss: 96.1060
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.0775
                       Mean reward: 834.87
               Mean episode length: 222.95
    Episode_Reward/reaching_object: 0.9956
     Episode_Reward/lifting_object: 167.1560
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 1.97s
                      Time elapsed: 00:55:07
                               ETA: 00:18:18

################################################################################
                     [1m Learning iteration 1502/2000 [0m                     

                       Computation: 50639 steps/s (collection: 1.847s, learning 0.094s)
             Mean action noise std: 3.10
          Mean value_function loss: 116.7160
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 55.0904
                       Mean reward: 866.07
               Mean episode length: 230.48
    Episode_Reward/reaching_object: 1.0103
     Episode_Reward/lifting_object: 170.2117
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 1.94s
                      Time elapsed: 00:55:08
                               ETA: 00:18:16

################################################################################
                     [1m Learning iteration 1503/2000 [0m                     

                       Computation: 48858 steps/s (collection: 1.884s, learning 0.128s)
             Mean action noise std: 3.10
          Mean value_function loss: 124.0548
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.1000
                       Mean reward: 828.66
               Mean episode length: 222.72
    Episode_Reward/reaching_object: 0.9703
     Episode_Reward/lifting_object: 162.5294
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 2.01s
                      Time elapsed: 00:55:11
                               ETA: 00:18:14

################################################################################
                     [1m Learning iteration 1504/2000 [0m                     

                       Computation: 48955 steps/s (collection: 1.900s, learning 0.108s)
             Mean action noise std: 3.11
          Mean value_function loss: 137.3837
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.1117
                       Mean reward: 785.49
               Mean episode length: 211.58
    Episode_Reward/reaching_object: 0.9878
     Episode_Reward/lifting_object: 166.0268
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0523
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 2.01s
                      Time elapsed: 00:55:13
                               ETA: 00:18:11

################################################################################
                     [1m Learning iteration 1505/2000 [0m                     

                       Computation: 49274 steps/s (collection: 1.884s, learning 0.111s)
             Mean action noise std: 3.11
          Mean value_function loss: 120.9694
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 55.1188
                       Mean reward: 866.02
               Mean episode length: 229.40
    Episode_Reward/reaching_object: 0.9772
     Episode_Reward/lifting_object: 164.6610
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 2.00s
                      Time elapsed: 00:55:15
                               ETA: 00:18:09

################################################################################
                     [1m Learning iteration 1506/2000 [0m                     

                       Computation: 50885 steps/s (collection: 1.843s, learning 0.089s)
             Mean action noise std: 3.11
          Mean value_function loss: 111.9013
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.1282
                       Mean reward: 891.96
               Mean episode length: 236.49
    Episode_Reward/reaching_object: 0.9954
     Episode_Reward/lifting_object: 167.6760
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 1.93s
                      Time elapsed: 00:55:16
                               ETA: 00:18:07

################################################################################
                     [1m Learning iteration 1507/2000 [0m                     

                       Computation: 50686 steps/s (collection: 1.845s, learning 0.095s)
             Mean action noise std: 3.11
          Mean value_function loss: 140.8413
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.1401
                       Mean reward: 836.60
               Mean episode length: 223.78
    Episode_Reward/reaching_object: 0.9917
     Episode_Reward/lifting_object: 166.4944
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 1.94s
                      Time elapsed: 00:55:18
                               ETA: 00:18:05

################################################################################
                     [1m Learning iteration 1508/2000 [0m                     

                       Computation: 50395 steps/s (collection: 1.859s, learning 0.092s)
             Mean action noise std: 3.11
          Mean value_function loss: 132.3803
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 55.1483
                       Mean reward: 841.62
               Mean episode length: 224.17
    Episode_Reward/reaching_object: 1.0196
     Episode_Reward/lifting_object: 172.3250
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 1.95s
                      Time elapsed: 00:55:20
                               ETA: 00:18:02

################################################################################
                     [1m Learning iteration 1509/2000 [0m                     

                       Computation: 50624 steps/s (collection: 1.852s, learning 0.090s)
             Mean action noise std: 3.11
          Mean value_function loss: 142.8370
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 55.1524
                       Mean reward: 850.22
               Mean episode length: 225.63
    Episode_Reward/reaching_object: 1.0102
     Episode_Reward/lifting_object: 170.1062
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 1.94s
                      Time elapsed: 00:55:22
                               ETA: 00:18:00

################################################################################
                     [1m Learning iteration 1510/2000 [0m                     

                       Computation: 51121 steps/s (collection: 1.829s, learning 0.094s)
             Mean action noise std: 3.11
          Mean value_function loss: 113.2218
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 55.1632
                       Mean reward: 838.93
               Mean episode length: 225.51
    Episode_Reward/reaching_object: 1.0130
     Episode_Reward/lifting_object: 170.0885
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 1.92s
                      Time elapsed: 00:55:24
                               ETA: 00:17:58

################################################################################
                     [1m Learning iteration 1511/2000 [0m                     

                       Computation: 50534 steps/s (collection: 1.859s, learning 0.086s)
             Mean action noise std: 3.11
          Mean value_function loss: 113.3573
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.1755
                       Mean reward: 882.76
               Mean episode length: 233.75
    Episode_Reward/reaching_object: 1.0108
     Episode_Reward/lifting_object: 170.0024
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 1.95s
                      Time elapsed: 00:55:26
                               ETA: 00:17:55

################################################################################
                     [1m Learning iteration 1512/2000 [0m                     

                       Computation: 50780 steps/s (collection: 1.845s, learning 0.091s)
             Mean action noise std: 3.12
          Mean value_function loss: 107.7274
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 55.1890
                       Mean reward: 830.29
               Mean episode length: 222.71
    Episode_Reward/reaching_object: 0.9991
     Episode_Reward/lifting_object: 168.0872
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 1.94s
                      Time elapsed: 00:55:28
                               ETA: 00:17:53

################################################################################
                     [1m Learning iteration 1513/2000 [0m                     

                       Computation: 50483 steps/s (collection: 1.843s, learning 0.105s)
             Mean action noise std: 3.12
          Mean value_function loss: 132.0329
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.2018
                       Mean reward: 896.69
               Mean episode length: 237.06
    Episode_Reward/reaching_object: 1.0018
     Episode_Reward/lifting_object: 168.9266
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 1.95s
                      Time elapsed: 00:55:30
                               ETA: 00:17:51

################################################################################
                     [1m Learning iteration 1514/2000 [0m                     

                       Computation: 49373 steps/s (collection: 1.895s, learning 0.096s)
             Mean action noise std: 3.12
          Mean value_function loss: 137.0517
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 55.2126
                       Mean reward: 858.28
               Mean episode length: 229.16
    Episode_Reward/reaching_object: 1.0055
     Episode_Reward/lifting_object: 169.1513
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 1.99s
                      Time elapsed: 00:55:32
                               ETA: 00:17:49

################################################################################
                     [1m Learning iteration 1515/2000 [0m                     

                       Computation: 50194 steps/s (collection: 1.867s, learning 0.091s)
             Mean action noise std: 3.12
          Mean value_function loss: 100.0508
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.2267
                       Mean reward: 899.88
               Mean episode length: 238.29
    Episode_Reward/reaching_object: 1.0427
     Episode_Reward/lifting_object: 176.3228
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 1.96s
                      Time elapsed: 00:55:34
                               ETA: 00:17:46

################################################################################
                     [1m Learning iteration 1516/2000 [0m                     

                       Computation: 50140 steps/s (collection: 1.864s, learning 0.096s)
             Mean action noise std: 3.12
          Mean value_function loss: 112.3187
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.2357
                       Mean reward: 876.11
               Mean episode length: 232.83
    Episode_Reward/reaching_object: 1.0049
     Episode_Reward/lifting_object: 168.7681
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 1.96s
                      Time elapsed: 00:55:36
                               ETA: 00:17:44

################################################################################
                     [1m Learning iteration 1517/2000 [0m                     

                       Computation: 50423 steps/s (collection: 1.851s, learning 0.099s)
             Mean action noise std: 3.12
          Mean value_function loss: 104.9194
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.2453
                       Mean reward: 867.03
               Mean episode length: 231.16
    Episode_Reward/reaching_object: 0.9994
     Episode_Reward/lifting_object: 168.0272
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 1.95s
                      Time elapsed: 00:55:38
                               ETA: 00:17:42

################################################################################
                     [1m Learning iteration 1518/2000 [0m                     

                       Computation: 50079 steps/s (collection: 1.866s, learning 0.097s)
             Mean action noise std: 3.13
          Mean value_function loss: 165.3560
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.2563
                       Mean reward: 814.03
               Mean episode length: 219.17
    Episode_Reward/reaching_object: 0.9805
     Episode_Reward/lifting_object: 164.7727
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 1.96s
                      Time elapsed: 00:55:40
                               ETA: 00:17:39

################################################################################
                     [1m Learning iteration 1519/2000 [0m                     

                       Computation: 50230 steps/s (collection: 1.854s, learning 0.103s)
             Mean action noise std: 3.13
          Mean value_function loss: 126.0745
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 55.2663
                       Mean reward: 897.26
               Mean episode length: 237.90
    Episode_Reward/reaching_object: 1.0080
     Episode_Reward/lifting_object: 170.0953
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 1.96s
                      Time elapsed: 00:55:42
                               ETA: 00:17:37

################################################################################
                     [1m Learning iteration 1520/2000 [0m                     

                       Computation: 50195 steps/s (collection: 1.871s, learning 0.087s)
             Mean action noise std: 3.13
          Mean value_function loss: 127.1651
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.2746
                       Mean reward: 839.52
               Mean episode length: 224.81
    Episode_Reward/reaching_object: 1.0034
     Episode_Reward/lifting_object: 169.4826
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 1.96s
                      Time elapsed: 00:55:44
                               ETA: 00:17:35

################################################################################
                     [1m Learning iteration 1521/2000 [0m                     

                       Computation: 50206 steps/s (collection: 1.868s, learning 0.090s)
             Mean action noise std: 3.13
          Mean value_function loss: 139.6201
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 55.2874
                       Mean reward: 864.18
               Mean episode length: 229.38
    Episode_Reward/reaching_object: 0.9908
     Episode_Reward/lifting_object: 167.0086
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0533
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 1.96s
                      Time elapsed: 00:55:46
                               ETA: 00:17:33

################################################################################
                     [1m Learning iteration 1522/2000 [0m                     

                       Computation: 49626 steps/s (collection: 1.895s, learning 0.086s)
             Mean action noise std: 3.13
          Mean value_function loss: 133.3286
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 55.2986
                       Mean reward: 857.59
               Mean episode length: 227.66
    Episode_Reward/reaching_object: 0.9927
     Episode_Reward/lifting_object: 167.6122
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 1.98s
                      Time elapsed: 00:55:48
                               ETA: 00:17:30

################################################################################
                     [1m Learning iteration 1523/2000 [0m                     

                       Computation: 50818 steps/s (collection: 1.845s, learning 0.090s)
             Mean action noise std: 3.13
          Mean value_function loss: 129.4896
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.3040
                       Mean reward: 818.38
               Mean episode length: 218.83
    Episode_Reward/reaching_object: 0.9945
     Episode_Reward/lifting_object: 167.9841
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 1.93s
                      Time elapsed: 00:55:50
                               ETA: 00:17:28

################################################################################
                     [1m Learning iteration 1524/2000 [0m                     

                       Computation: 51004 steps/s (collection: 1.840s, learning 0.088s)
             Mean action noise std: 3.13
          Mean value_function loss: 164.6790
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.3093
                       Mean reward: 868.95
               Mean episode length: 231.69
    Episode_Reward/reaching_object: 1.0032
     Episode_Reward/lifting_object: 169.2878
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 1.93s
                      Time elapsed: 00:55:52
                               ETA: 00:17:26

################################################################################
                     [1m Learning iteration 1525/2000 [0m                     

                       Computation: 51121 steps/s (collection: 1.833s, learning 0.090s)
             Mean action noise std: 3.13
          Mean value_function loss: 126.2158
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.3187
                       Mean reward: 887.91
               Mean episode length: 235.10
    Episode_Reward/reaching_object: 1.0005
     Episode_Reward/lifting_object: 168.8590
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 1.92s
                      Time elapsed: 00:55:53
                               ETA: 00:17:23

################################################################################
                     [1m Learning iteration 1526/2000 [0m                     

                       Computation: 50402 steps/s (collection: 1.861s, learning 0.090s)
             Mean action noise std: 3.14
          Mean value_function loss: 121.9140
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.3289
                       Mean reward: 851.03
               Mean episode length: 226.63
    Episode_Reward/reaching_object: 1.0013
     Episode_Reward/lifting_object: 168.8215
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 1.95s
                      Time elapsed: 00:55:55
                               ETA: 00:17:21

################################################################################
                     [1m Learning iteration 1527/2000 [0m                     

                       Computation: 49670 steps/s (collection: 1.891s, learning 0.089s)
             Mean action noise std: 3.14
          Mean value_function loss: 120.2306
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 55.3408
                       Mean reward: 839.58
               Mean episode length: 223.92
    Episode_Reward/reaching_object: 1.0062
     Episode_Reward/lifting_object: 169.7433
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 1.98s
                      Time elapsed: 00:55:57
                               ETA: 00:17:19

################################################################################
                     [1m Learning iteration 1528/2000 [0m                     

                       Computation: 50221 steps/s (collection: 1.849s, learning 0.109s)
             Mean action noise std: 3.14
          Mean value_function loss: 121.8137
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.3545
                       Mean reward: 862.35
               Mean episode length: 229.52
    Episode_Reward/reaching_object: 0.9962
     Episode_Reward/lifting_object: 167.7562
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 1.96s
                      Time elapsed: 00:55:59
                               ETA: 00:17:17

################################################################################
                     [1m Learning iteration 1529/2000 [0m                     

                       Computation: 50516 steps/s (collection: 1.844s, learning 0.102s)
             Mean action noise std: 3.14
          Mean value_function loss: 113.6958
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.3730
                       Mean reward: 837.42
               Mean episode length: 223.93
    Episode_Reward/reaching_object: 1.0050
     Episode_Reward/lifting_object: 169.9667
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 1.95s
                      Time elapsed: 00:56:01
                               ETA: 00:17:14

################################################################################
                     [1m Learning iteration 1530/2000 [0m                     

                       Computation: 48727 steps/s (collection: 1.916s, learning 0.101s)
             Mean action noise std: 3.14
          Mean value_function loss: 111.9083
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.3884
                       Mean reward: 848.20
               Mean episode length: 226.34
    Episode_Reward/reaching_object: 1.0370
     Episode_Reward/lifting_object: 175.1613
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 2.02s
                      Time elapsed: 00:56:03
                               ETA: 00:17:12

################################################################################
                     [1m Learning iteration 1531/2000 [0m                     

                       Computation: 49666 steps/s (collection: 1.877s, learning 0.102s)
             Mean action noise std: 3.15
          Mean value_function loss: 128.1453
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.4021
                       Mean reward: 897.99
               Mean episode length: 238.26
    Episode_Reward/reaching_object: 1.0147
     Episode_Reward/lifting_object: 171.1348
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 1.98s
                      Time elapsed: 00:56:05
                               ETA: 00:17:10

################################################################################
                     [1m Learning iteration 1532/2000 [0m                     

                       Computation: 50170 steps/s (collection: 1.871s, learning 0.089s)
             Mean action noise std: 3.15
          Mean value_function loss: 150.2163
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 55.4207
                       Mean reward: 877.35
               Mean episode length: 233.50
    Episode_Reward/reaching_object: 0.9921
     Episode_Reward/lifting_object: 167.1349
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 1.96s
                      Time elapsed: 00:56:07
                               ETA: 00:17:08

################################################################################
                     [1m Learning iteration 1533/2000 [0m                     

                       Computation: 51044 steps/s (collection: 1.840s, learning 0.086s)
             Mean action noise std: 3.15
          Mean value_function loss: 136.2366
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 55.4301
                       Mean reward: 855.03
               Mean episode length: 227.70
    Episode_Reward/reaching_object: 0.9990
     Episode_Reward/lifting_object: 168.3625
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 1.93s
                      Time elapsed: 00:56:09
                               ETA: 00:17:05

################################################################################
                     [1m Learning iteration 1534/2000 [0m                     

                       Computation: 50727 steps/s (collection: 1.852s, learning 0.086s)
             Mean action noise std: 3.15
          Mean value_function loss: 120.7573
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 55.4410
                       Mean reward: 841.66
               Mean episode length: 224.37
    Episode_Reward/reaching_object: 0.9958
     Episode_Reward/lifting_object: 168.1286
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 1.94s
                      Time elapsed: 00:56:11
                               ETA: 00:17:03

################################################################################
                     [1m Learning iteration 1535/2000 [0m                     

                       Computation: 48057 steps/s (collection: 1.924s, learning 0.122s)
             Mean action noise std: 3.15
          Mean value_function loss: 135.2733
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 55.4481
                       Mean reward: 858.82
               Mean episode length: 227.74
    Episode_Reward/reaching_object: 0.9982
     Episode_Reward/lifting_object: 168.8244
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 2.05s
                      Time elapsed: 00:56:13
                               ETA: 00:17:01

################################################################################
                     [1m Learning iteration 1536/2000 [0m                     

                       Computation: 48459 steps/s (collection: 1.899s, learning 0.130s)
             Mean action noise std: 3.15
          Mean value_function loss: 161.4387
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.4564
                       Mean reward: 813.24
               Mean episode length: 218.01
    Episode_Reward/reaching_object: 0.9919
     Episode_Reward/lifting_object: 167.7034
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 2.03s
                      Time elapsed: 00:56:15
                               ETA: 00:16:59

################################################################################
                     [1m Learning iteration 1537/2000 [0m                     

                       Computation: 46612 steps/s (collection: 1.997s, learning 0.112s)
             Mean action noise std: 3.16
          Mean value_function loss: 163.0848
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.4726
                       Mean reward: 783.93
               Mean episode length: 211.76
    Episode_Reward/reaching_object: 0.9571
     Episode_Reward/lifting_object: 160.7867
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 2.11s
                      Time elapsed: 00:56:17
                               ETA: 00:16:56

################################################################################
                     [1m Learning iteration 1538/2000 [0m                     

                       Computation: 46207 steps/s (collection: 2.017s, learning 0.110s)
             Mean action noise std: 3.16
          Mean value_function loss: 176.6260
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 55.4864
                       Mean reward: 799.80
               Mean episode length: 214.14
    Episode_Reward/reaching_object: 0.9616
     Episode_Reward/lifting_object: 162.4996
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 2.13s
                      Time elapsed: 00:56:19
                               ETA: 00:16:54

################################################################################
                     [1m Learning iteration 1539/2000 [0m                     

                       Computation: 47976 steps/s (collection: 1.936s, learning 0.113s)
             Mean action noise std: 3.16
          Mean value_function loss: 130.3683
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.4977
                       Mean reward: 838.92
               Mean episode length: 223.56
    Episode_Reward/reaching_object: 1.0117
     Episode_Reward/lifting_object: 171.5692
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 2.05s
                      Time elapsed: 00:56:21
                               ETA: 00:16:52

################################################################################
                     [1m Learning iteration 1540/2000 [0m                     

                       Computation: 44985 steps/s (collection: 2.012s, learning 0.174s)
             Mean action noise std: 3.16
          Mean value_function loss: 144.7930
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 55.5085
                       Mean reward: 779.18
               Mean episode length: 209.96
    Episode_Reward/reaching_object: 0.9662
     Episode_Reward/lifting_object: 162.9892
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 2.19s
                      Time elapsed: 00:56:24
                               ETA: 00:16:50

################################################################################
                     [1m Learning iteration 1541/2000 [0m                     

                       Computation: 47335 steps/s (collection: 1.933s, learning 0.144s)
             Mean action noise std: 3.16
          Mean value_function loss: 157.1125
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.5199
                       Mean reward: 841.06
               Mean episode length: 225.34
    Episode_Reward/reaching_object: 0.9896
     Episode_Reward/lifting_object: 167.2513
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 2.08s
                      Time elapsed: 00:56:26
                               ETA: 00:16:47

################################################################################
                     [1m Learning iteration 1542/2000 [0m                     

                       Computation: 50010 steps/s (collection: 1.862s, learning 0.104s)
             Mean action noise std: 3.17
          Mean value_function loss: 134.2250
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 55.5349
                       Mean reward: 866.94
               Mean episode length: 229.69
    Episode_Reward/reaching_object: 0.9832
     Episode_Reward/lifting_object: 165.9167
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 1.97s
                      Time elapsed: 00:56:28
                               ETA: 00:16:45

################################################################################
                     [1m Learning iteration 1543/2000 [0m                     

                       Computation: 47949 steps/s (collection: 1.891s, learning 0.159s)
             Mean action noise std: 3.17
          Mean value_function loss: 153.5925
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.5448
                       Mean reward: 834.74
               Mean episode length: 223.15
    Episode_Reward/reaching_object: 0.9775
     Episode_Reward/lifting_object: 165.1664
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 2.05s
                      Time elapsed: 00:56:30
                               ETA: 00:16:43

################################################################################
                     [1m Learning iteration 1544/2000 [0m                     

                       Computation: 48389 steps/s (collection: 1.898s, learning 0.133s)
             Mean action noise std: 3.17
          Mean value_function loss: 113.1718
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 55.5579
                       Mean reward: 865.66
               Mean episode length: 230.04
    Episode_Reward/reaching_object: 0.9997
     Episode_Reward/lifting_object: 169.4744
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 2.03s
                      Time elapsed: 00:56:32
                               ETA: 00:16:41

################################################################################
                     [1m Learning iteration 1545/2000 [0m                     

                       Computation: 47778 steps/s (collection: 1.967s, learning 0.090s)
             Mean action noise std: 3.17
          Mean value_function loss: 120.7433
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.5653
                       Mean reward: 864.57
               Mean episode length: 229.94
    Episode_Reward/reaching_object: 0.9996
     Episode_Reward/lifting_object: 169.0898
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0549
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 2.06s
                      Time elapsed: 00:56:34
                               ETA: 00:16:38

################################################################################
                     [1m Learning iteration 1546/2000 [0m                     

                       Computation: 49032 steps/s (collection: 1.900s, learning 0.105s)
             Mean action noise std: 3.17
          Mean value_function loss: 107.8136
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 55.5699
                       Mean reward: 866.02
               Mean episode length: 230.65
    Episode_Reward/reaching_object: 0.9964
     Episode_Reward/lifting_object: 168.8008
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 2.00s
                      Time elapsed: 00:56:36
                               ETA: 00:16:36

################################################################################
                     [1m Learning iteration 1547/2000 [0m                     

                       Computation: 46147 steps/s (collection: 2.028s, learning 0.102s)
             Mean action noise std: 3.17
          Mean value_function loss: 118.8640
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 55.5755
                       Mean reward: 833.89
               Mean episode length: 222.74
    Episode_Reward/reaching_object: 0.9861
     Episode_Reward/lifting_object: 166.7213
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 2.13s
                      Time elapsed: 00:56:38
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 1548/2000 [0m                     

                       Computation: 50683 steps/s (collection: 1.845s, learning 0.094s)
             Mean action noise std: 3.17
          Mean value_function loss: 130.3663
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.5868
                       Mean reward: 829.54
               Mean episode length: 221.08
    Episode_Reward/reaching_object: 0.9958
     Episode_Reward/lifting_object: 168.8095
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 1.94s
                      Time elapsed: 00:56:40
                               ETA: 00:16:32

################################################################################
                     [1m Learning iteration 1549/2000 [0m                     

                       Computation: 49257 steps/s (collection: 1.883s, learning 0.113s)
             Mean action noise std: 3.17
          Mean value_function loss: 117.8312
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.5951
                       Mean reward: 841.33
               Mean episode length: 225.09
    Episode_Reward/reaching_object: 1.0016
     Episode_Reward/lifting_object: 169.8619
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 2.00s
                      Time elapsed: 00:56:42
                               ETA: 00:16:29

################################################################################
                     [1m Learning iteration 1550/2000 [0m                     

                       Computation: 49381 steps/s (collection: 1.902s, learning 0.089s)
             Mean action noise std: 3.18
          Mean value_function loss: 125.9533
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.6040
                       Mean reward: 858.70
               Mean episode length: 228.68
    Episode_Reward/reaching_object: 0.9931
     Episode_Reward/lifting_object: 167.9979
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 1.99s
                      Time elapsed: 00:56:44
                               ETA: 00:16:27

################################################################################
                     [1m Learning iteration 1551/2000 [0m                     

                       Computation: 48785 steps/s (collection: 1.883s, learning 0.132s)
             Mean action noise std: 3.18
          Mean value_function loss: 140.5179
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 55.6150
                       Mean reward: 832.27
               Mean episode length: 223.55
    Episode_Reward/reaching_object: 0.9873
     Episode_Reward/lifting_object: 166.9642
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 2.02s
                      Time elapsed: 00:56:46
                               ETA: 00:16:25

################################################################################
                     [1m Learning iteration 1552/2000 [0m                     

                       Computation: 49386 steps/s (collection: 1.853s, learning 0.138s)
             Mean action noise std: 3.18
          Mean value_function loss: 99.3340
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 55.6222
                       Mean reward: 916.03
               Mean episode length: 241.48
    Episode_Reward/reaching_object: 1.0160
     Episode_Reward/lifting_object: 172.1942
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 1.99s
                      Time elapsed: 00:56:48
                               ETA: 00:16:23

################################################################################
                     [1m Learning iteration 1553/2000 [0m                     

                       Computation: 50179 steps/s (collection: 1.848s, learning 0.111s)
             Mean action noise std: 3.18
          Mean value_function loss: 149.5123
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.6358
                       Mean reward: 855.41
               Mean episode length: 227.49
    Episode_Reward/reaching_object: 0.9934
     Episode_Reward/lifting_object: 167.9434
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 1.96s
                      Time elapsed: 00:56:50
                               ETA: 00:16:20

################################################################################
                     [1m Learning iteration 1554/2000 [0m                     

                       Computation: 49528 steps/s (collection: 1.844s, learning 0.141s)
             Mean action noise std: 3.18
          Mean value_function loss: 109.0347
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 55.6484
                       Mean reward: 835.94
               Mean episode length: 222.61
    Episode_Reward/reaching_object: 1.0201
     Episode_Reward/lifting_object: 172.7390
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 1.98s
                      Time elapsed: 00:56:52
                               ETA: 00:16:18

################################################################################
                     [1m Learning iteration 1555/2000 [0m                     

                       Computation: 49703 steps/s (collection: 1.856s, learning 0.122s)
             Mean action noise std: 3.18
          Mean value_function loss: 109.6849
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 55.6601
                       Mean reward: 807.48
               Mean episode length: 216.45
    Episode_Reward/reaching_object: 1.0016
     Episode_Reward/lifting_object: 169.4459
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 1.98s
                      Time elapsed: 00:56:54
                               ETA: 00:16:16

################################################################################
                     [1m Learning iteration 1556/2000 [0m                     

                       Computation: 48036 steps/s (collection: 1.912s, learning 0.135s)
             Mean action noise std: 3.19
          Mean value_function loss: 110.2736
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 55.6650
                       Mean reward: 881.47
               Mean episode length: 233.93
    Episode_Reward/reaching_object: 1.0229
     Episode_Reward/lifting_object: 173.3224
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 2.05s
                      Time elapsed: 00:56:56
                               ETA: 00:16:14

################################################################################
                     [1m Learning iteration 1557/2000 [0m                     

                       Computation: 49723 steps/s (collection: 1.878s, learning 0.099s)
             Mean action noise std: 3.19
          Mean value_function loss: 138.9273
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.6689
                       Mean reward: 836.61
               Mean episode length: 223.87
    Episode_Reward/reaching_object: 0.9987
     Episode_Reward/lifting_object: 168.8760
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 1.98s
                      Time elapsed: 00:56:58
                               ETA: 00:16:11

################################################################################
                     [1m Learning iteration 1558/2000 [0m                     

                       Computation: 49458 steps/s (collection: 1.892s, learning 0.096s)
             Mean action noise std: 3.19
          Mean value_function loss: 129.7763
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.6807
                       Mean reward: 821.52
               Mean episode length: 219.65
    Episode_Reward/reaching_object: 1.0030
     Episode_Reward/lifting_object: 169.4447
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 1.99s
                      Time elapsed: 00:57:00
                               ETA: 00:16:09

################################################################################
                     [1m Learning iteration 1559/2000 [0m                     

                       Computation: 48812 steps/s (collection: 1.918s, learning 0.096s)
             Mean action noise std: 3.19
          Mean value_function loss: 135.0783
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.6992
                       Mean reward: 827.95
               Mean episode length: 221.43
    Episode_Reward/reaching_object: 1.0200
     Episode_Reward/lifting_object: 172.1696
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 2.01s
                      Time elapsed: 00:57:02
                               ETA: 00:16:07

################################################################################
                     [1m Learning iteration 1560/2000 [0m                     

                       Computation: 48201 steps/s (collection: 1.904s, learning 0.135s)
             Mean action noise std: 3.19
          Mean value_function loss: 139.9623
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 55.7139
                       Mean reward: 825.32
               Mean episode length: 220.85
    Episode_Reward/reaching_object: 1.0053
     Episode_Reward/lifting_object: 169.3439
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 2.04s
                      Time elapsed: 00:57:04
                               ETA: 00:16:05

################################################################################
                     [1m Learning iteration 1561/2000 [0m                     

                       Computation: 49176 steps/s (collection: 1.905s, learning 0.094s)
             Mean action noise std: 3.19
          Mean value_function loss: 137.6675
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.7259
                       Mean reward: 894.36
               Mean episode length: 237.59
    Episode_Reward/reaching_object: 1.0138
     Episode_Reward/lifting_object: 171.0753
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 2.00s
                      Time elapsed: 00:57:06
                               ETA: 00:16:02

################################################################################
                     [1m Learning iteration 1562/2000 [0m                     

                       Computation: 46200 steps/s (collection: 1.951s, learning 0.177s)
             Mean action noise std: 3.20
          Mean value_function loss: 142.5579
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 55.7395
                       Mean reward: 855.71
               Mean episode length: 227.89
    Episode_Reward/reaching_object: 0.9759
     Episode_Reward/lifting_object: 164.2316
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 2.13s
                      Time elapsed: 00:57:08
                               ETA: 00:16:00

################################################################################
                     [1m Learning iteration 1563/2000 [0m                     

                       Computation: 46446 steps/s (collection: 2.001s, learning 0.115s)
             Mean action noise std: 3.20
          Mean value_function loss: 111.2646
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 55.7495
                       Mean reward: 864.91
               Mean episode length: 229.58
    Episode_Reward/reaching_object: 0.9966
     Episode_Reward/lifting_object: 167.6145
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 2.12s
                      Time elapsed: 00:57:10
                               ETA: 00:15:58

################################################################################
                     [1m Learning iteration 1564/2000 [0m                     

                       Computation: 47824 steps/s (collection: 1.967s, learning 0.089s)
             Mean action noise std: 3.20
          Mean value_function loss: 128.0148
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.7577
                       Mean reward: 868.34
               Mean episode length: 231.23
    Episode_Reward/reaching_object: 0.9838
     Episode_Reward/lifting_object: 165.6605
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 2.06s
                      Time elapsed: 00:57:12
                               ETA: 00:15:56

################################################################################
                     [1m Learning iteration 1565/2000 [0m                     

                       Computation: 48413 steps/s (collection: 1.938s, learning 0.093s)
             Mean action noise std: 3.20
          Mean value_function loss: 122.8083
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 55.7684
                       Mean reward: 894.12
               Mean episode length: 237.28
    Episode_Reward/reaching_object: 1.0108
     Episode_Reward/lifting_object: 170.2664
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 2.03s
                      Time elapsed: 00:57:14
                               ETA: 00:15:54

################################################################################
                     [1m Learning iteration 1566/2000 [0m                     

                       Computation: 48266 steps/s (collection: 1.924s, learning 0.112s)
             Mean action noise std: 3.20
          Mean value_function loss: 97.7474
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.7805
                       Mean reward: 859.20
               Mean episode length: 227.68
    Episode_Reward/reaching_object: 0.9970
     Episode_Reward/lifting_object: 167.8525
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 2.04s
                      Time elapsed: 00:57:16
                               ETA: 00:15:51

################################################################################
                     [1m Learning iteration 1567/2000 [0m                     

                       Computation: 46040 steps/s (collection: 2.045s, learning 0.091s)
             Mean action noise std: 3.20
          Mean value_function loss: 117.1094
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.7916
                       Mean reward: 860.02
               Mean episode length: 228.64
    Episode_Reward/reaching_object: 1.0217
     Episode_Reward/lifting_object: 171.9597
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 2.14s
                      Time elapsed: 00:57:18
                               ETA: 00:15:49

################################################################################
                     [1m Learning iteration 1568/2000 [0m                     

                       Computation: 48343 steps/s (collection: 1.937s, learning 0.097s)
             Mean action noise std: 3.21
          Mean value_function loss: 118.4835
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 55.8008
                       Mean reward: 855.92
               Mean episode length: 227.77
    Episode_Reward/reaching_object: 0.9939
     Episode_Reward/lifting_object: 167.2012
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 2.03s
                      Time elapsed: 00:57:20
                               ETA: 00:15:47

################################################################################
                     [1m Learning iteration 1569/2000 [0m                     

                       Computation: 46567 steps/s (collection: 2.016s, learning 0.095s)
             Mean action noise std: 3.21
          Mean value_function loss: 125.7505
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.8127
                       Mean reward: 841.03
               Mean episode length: 223.71
    Episode_Reward/reaching_object: 1.0024
     Episode_Reward/lifting_object: 168.5868
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 2.11s
                      Time elapsed: 00:57:23
                               ETA: 00:15:45

################################################################################
                     [1m Learning iteration 1570/2000 [0m                     

                       Computation: 45650 steps/s (collection: 2.016s, learning 0.137s)
             Mean action noise std: 3.21
          Mean value_function loss: 110.3248
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.8220
                       Mean reward: 877.34
               Mean episode length: 232.50
    Episode_Reward/reaching_object: 1.0189
     Episode_Reward/lifting_object: 171.8752
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 2.15s
                      Time elapsed: 00:57:25
                               ETA: 00:15:42

################################################################################
                     [1m Learning iteration 1571/2000 [0m                     

                       Computation: 49227 steps/s (collection: 1.907s, learning 0.090s)
             Mean action noise std: 3.21
          Mean value_function loss: 116.9677
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.8299
                       Mean reward: 840.71
               Mean episode length: 224.68
    Episode_Reward/reaching_object: 1.0262
     Episode_Reward/lifting_object: 173.4100
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 2.00s
                      Time elapsed: 00:57:27
                               ETA: 00:15:40

################################################################################
                     [1m Learning iteration 1572/2000 [0m                     

                       Computation: 44234 steps/s (collection: 2.079s, learning 0.144s)
             Mean action noise std: 3.21
          Mean value_function loss: 134.8263
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.8440
                       Mean reward: 859.92
               Mean episode length: 228.76
    Episode_Reward/reaching_object: 0.9984
     Episode_Reward/lifting_object: 168.3058
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 2.22s
                      Time elapsed: 00:57:29
                               ETA: 00:15:38

################################################################################
                     [1m Learning iteration 1573/2000 [0m                     

                       Computation: 49420 steps/s (collection: 1.888s, learning 0.101s)
             Mean action noise std: 3.21
          Mean value_function loss: 90.7881
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 55.8552
                       Mean reward: 882.03
               Mean episode length: 233.52
    Episode_Reward/reaching_object: 1.0237
     Episode_Reward/lifting_object: 173.1800
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 1.99s
                      Time elapsed: 00:57:31
                               ETA: 00:15:36

################################################################################
                     [1m Learning iteration 1574/2000 [0m                     

                       Computation: 46656 steps/s (collection: 1.998s, learning 0.109s)
             Mean action noise std: 3.22
          Mean value_function loss: 101.6212
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 55.8661
                       Mean reward: 847.21
               Mean episode length: 225.49
    Episode_Reward/reaching_object: 1.0161
     Episode_Reward/lifting_object: 171.8107
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 2.11s
                      Time elapsed: 00:57:33
                               ETA: 00:15:34

################################################################################
                     [1m Learning iteration 1575/2000 [0m                     

                       Computation: 47809 steps/s (collection: 1.957s, learning 0.099s)
             Mean action noise std: 3.22
          Mean value_function loss: 129.0992
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.8796
                       Mean reward: 860.46
               Mean episode length: 229.68
    Episode_Reward/reaching_object: 1.0189
     Episode_Reward/lifting_object: 172.0148
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 2.06s
                      Time elapsed: 00:57:35
                               ETA: 00:15:31

################################################################################
                     [1m Learning iteration 1576/2000 [0m                     

                       Computation: 48266 steps/s (collection: 1.944s, learning 0.093s)
             Mean action noise std: 3.22
          Mean value_function loss: 113.3813
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.8866
                       Mean reward: 883.88
               Mean episode length: 234.26
    Episode_Reward/reaching_object: 1.0073
     Episode_Reward/lifting_object: 170.2086
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0565
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 2.04s
                      Time elapsed: 00:57:37
                               ETA: 00:15:29

################################################################################
                     [1m Learning iteration 1577/2000 [0m                     

                       Computation: 48428 steps/s (collection: 1.936s, learning 0.094s)
             Mean action noise std: 3.22
          Mean value_function loss: 101.0038
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 55.8887
                       Mean reward: 876.13
               Mean episode length: 232.89
    Episode_Reward/reaching_object: 0.9962
     Episode_Reward/lifting_object: 168.0470
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 2.03s
                      Time elapsed: 00:57:39
                               ETA: 00:15:27

################################################################################
                     [1m Learning iteration 1578/2000 [0m                     

                       Computation: 49559 steps/s (collection: 1.891s, learning 0.092s)
             Mean action noise std: 3.22
          Mean value_function loss: 109.2901
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.8939
                       Mean reward: 863.04
               Mean episode length: 229.06
    Episode_Reward/reaching_object: 1.0258
     Episode_Reward/lifting_object: 173.7773
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 1.98s
                      Time elapsed: 00:57:41
                               ETA: 00:15:25

################################################################################
                     [1m Learning iteration 1579/2000 [0m                     

                       Computation: 49060 steps/s (collection: 1.915s, learning 0.089s)
             Mean action noise std: 3.22
          Mean value_function loss: 85.6360
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 55.9023
                       Mean reward: 890.11
               Mean episode length: 235.58
    Episode_Reward/reaching_object: 1.0441
     Episode_Reward/lifting_object: 177.0037
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0584
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 2.00s
                      Time elapsed: 00:57:43
                               ETA: 00:15:22

################################################################################
                     [1m Learning iteration 1580/2000 [0m                     

                       Computation: 47651 steps/s (collection: 1.954s, learning 0.109s)
             Mean action noise std: 3.22
          Mean value_function loss: 105.5672
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 55.9107
                       Mean reward: 850.59
               Mean episode length: 226.93
    Episode_Reward/reaching_object: 1.0019
     Episode_Reward/lifting_object: 168.7617
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 2.06s
                      Time elapsed: 00:57:45
                               ETA: 00:15:20

################################################################################
                     [1m Learning iteration 1581/2000 [0m                     

                       Computation: 43808 steps/s (collection: 2.138s, learning 0.106s)
             Mean action noise std: 3.22
          Mean value_function loss: 134.6229
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.9158
                       Mean reward: 858.55
               Mean episode length: 227.82
    Episode_Reward/reaching_object: 1.0362
     Episode_Reward/lifting_object: 174.8783
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 2.24s
                      Time elapsed: 00:57:47
                               ETA: 00:15:18

################################################################################
                     [1m Learning iteration 1582/2000 [0m                     

                       Computation: 46513 steps/s (collection: 1.964s, learning 0.149s)
             Mean action noise std: 3.22
          Mean value_function loss: 108.1784
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.9224
                       Mean reward: 899.80
               Mean episode length: 237.90
    Episode_Reward/reaching_object: 1.0033
     Episode_Reward/lifting_object: 169.4819
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 2.11s
                      Time elapsed: 00:57:50
                               ETA: 00:15:16

################################################################################
                     [1m Learning iteration 1583/2000 [0m                     

                       Computation: 44937 steps/s (collection: 2.087s, learning 0.101s)
             Mean action noise std: 3.23
          Mean value_function loss: 105.9553
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 55.9322
                       Mean reward: 832.14
               Mean episode length: 222.18
    Episode_Reward/reaching_object: 0.9899
     Episode_Reward/lifting_object: 166.5988
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 2.19s
                      Time elapsed: 00:57:52
                               ETA: 00:15:14

################################################################################
                     [1m Learning iteration 1584/2000 [0m                     

                       Computation: 49286 steps/s (collection: 1.896s, learning 0.099s)
             Mean action noise std: 3.23
          Mean value_function loss: 132.1593
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.9418
                       Mean reward: 867.07
               Mean episode length: 229.80
    Episode_Reward/reaching_object: 1.0145
     Episode_Reward/lifting_object: 171.4044
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 1.99s
                      Time elapsed: 00:57:54
                               ETA: 00:15:11

################################################################################
                     [1m Learning iteration 1585/2000 [0m                     

                       Computation: 48950 steps/s (collection: 1.913s, learning 0.096s)
             Mean action noise std: 3.23
          Mean value_function loss: 136.0947
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 55.9560
                       Mean reward: 841.11
               Mean episode length: 224.42
    Episode_Reward/reaching_object: 0.9876
     Episode_Reward/lifting_object: 166.1094
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 2.01s
                      Time elapsed: 00:57:56
                               ETA: 00:15:09

################################################################################
                     [1m Learning iteration 1586/2000 [0m                     

                       Computation: 49194 steps/s (collection: 1.905s, learning 0.093s)
             Mean action noise std: 3.23
          Mean value_function loss: 159.8880
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 55.9674
                       Mean reward: 839.91
               Mean episode length: 224.06
    Episode_Reward/reaching_object: 0.9908
     Episode_Reward/lifting_object: 166.6547
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 2.00s
                      Time elapsed: 00:57:58
                               ETA: 00:15:07

################################################################################
                     [1m Learning iteration 1587/2000 [0m                     

                       Computation: 48902 steps/s (collection: 1.915s, learning 0.096s)
             Mean action noise std: 3.23
          Mean value_function loss: 116.6299
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 55.9736
                       Mean reward: 834.55
               Mean episode length: 222.54
    Episode_Reward/reaching_object: 0.9790
     Episode_Reward/lifting_object: 164.4346
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0549
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 2.01s
                      Time elapsed: 00:58:00
                               ETA: 00:15:05

################################################################################
                     [1m Learning iteration 1588/2000 [0m                     

                       Computation: 48877 steps/s (collection: 1.917s, learning 0.094s)
             Mean action noise std: 3.23
          Mean value_function loss: 106.2537
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.9799
                       Mean reward: 826.47
               Mean episode length: 220.65
    Episode_Reward/reaching_object: 0.9929
     Episode_Reward/lifting_object: 167.0184
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 2.01s
                      Time elapsed: 00:58:02
                               ETA: 00:15:02

################################################################################
                     [1m Learning iteration 1589/2000 [0m                     

                       Computation: 48822 steps/s (collection: 1.918s, learning 0.095s)
             Mean action noise std: 3.23
          Mean value_function loss: 122.8172
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.9851
                       Mean reward: 880.35
               Mean episode length: 232.85
    Episode_Reward/reaching_object: 1.0316
     Episode_Reward/lifting_object: 173.9238
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 2.01s
                      Time elapsed: 00:58:04
                               ETA: 00:15:00

################################################################################
                     [1m Learning iteration 1590/2000 [0m                     

                       Computation: 48687 steps/s (collection: 1.923s, learning 0.096s)
             Mean action noise std: 3.24
          Mean value_function loss: 77.8441
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 56.0001
                       Mean reward: 885.15
               Mean episode length: 234.73
    Episode_Reward/reaching_object: 1.0290
     Episode_Reward/lifting_object: 173.5290
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 2.02s
                      Time elapsed: 00:58:06
                               ETA: 00:14:58

################################################################################
                     [1m Learning iteration 1591/2000 [0m                     

                       Computation: 48788 steps/s (collection: 1.906s, learning 0.109s)
             Mean action noise std: 3.24
          Mean value_function loss: 91.0388
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 56.0142
                       Mean reward: 914.45
               Mean episode length: 241.18
    Episode_Reward/reaching_object: 1.0446
     Episode_Reward/lifting_object: 176.5339
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 2.01s
                      Time elapsed: 00:58:08
                               ETA: 00:14:56

################################################################################
                     [1m Learning iteration 1592/2000 [0m                     

                       Computation: 49185 steps/s (collection: 1.903s, learning 0.096s)
             Mean action noise std: 3.24
          Mean value_function loss: 104.2451
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 56.0194
                       Mean reward: 881.69
               Mean episode length: 233.88
    Episode_Reward/reaching_object: 1.0061
     Episode_Reward/lifting_object: 169.5292
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 2.00s
                      Time elapsed: 00:58:10
                               ETA: 00:14:53

################################################################################
                     [1m Learning iteration 1593/2000 [0m                     

                       Computation: 47833 steps/s (collection: 1.964s, learning 0.092s)
             Mean action noise std: 3.24
          Mean value_function loss: 104.2684
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.0249
                       Mean reward: 874.57
               Mean episode length: 231.84
    Episode_Reward/reaching_object: 1.0172
     Episode_Reward/lifting_object: 171.3533
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 2.06s
                      Time elapsed: 00:58:12
                               ETA: 00:14:51

################################################################################
                     [1m Learning iteration 1594/2000 [0m                     

                       Computation: 47382 steps/s (collection: 1.956s, learning 0.119s)
             Mean action noise std: 3.24
          Mean value_function loss: 128.7260
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 56.0330
                       Mean reward: 811.76
               Mean episode length: 217.33
    Episode_Reward/reaching_object: 0.9934
     Episode_Reward/lifting_object: 166.8500
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 2.07s
                      Time elapsed: 00:58:14
                               ETA: 00:14:49

################################################################################
                     [1m Learning iteration 1595/2000 [0m                     

                       Computation: 48259 steps/s (collection: 1.943s, learning 0.094s)
             Mean action noise std: 3.24
          Mean value_function loss: 95.3832
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 56.0394
                       Mean reward: 873.57
               Mean episode length: 231.66
    Episode_Reward/reaching_object: 1.0351
     Episode_Reward/lifting_object: 174.6584
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 2.04s
                      Time elapsed: 00:58:16
                               ETA: 00:14:47

################################################################################
                     [1m Learning iteration 1596/2000 [0m                     

                       Computation: 47751 steps/s (collection: 1.935s, learning 0.124s)
             Mean action noise std: 3.24
          Mean value_function loss: 129.5252
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.0462
                       Mean reward: 875.10
               Mean episode length: 232.38
    Episode_Reward/reaching_object: 1.0247
     Episode_Reward/lifting_object: 172.8764
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 2.06s
                      Time elapsed: 00:58:18
                               ETA: 00:14:45

################################################################################
                     [1m Learning iteration 1597/2000 [0m                     

                       Computation: 47032 steps/s (collection: 1.958s, learning 0.132s)
             Mean action noise std: 3.24
          Mean value_function loss: 125.1306
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.0552
                       Mean reward: 863.15
               Mean episode length: 229.73
    Episode_Reward/reaching_object: 1.0170
     Episode_Reward/lifting_object: 171.4459
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 2.09s
                      Time elapsed: 00:58:20
                               ETA: 00:14:42

################################################################################
                     [1m Learning iteration 1598/2000 [0m                     

                       Computation: 47963 steps/s (collection: 1.939s, learning 0.111s)
             Mean action noise std: 3.25
          Mean value_function loss: 95.8164
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.0674
                       Mean reward: 875.77
               Mean episode length: 232.11
    Episode_Reward/reaching_object: 1.0210
     Episode_Reward/lifting_object: 172.3453
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 2.05s
                      Time elapsed: 00:58:22
                               ETA: 00:14:40

################################################################################
                     [1m Learning iteration 1599/2000 [0m                     

                       Computation: 47560 steps/s (collection: 1.949s, learning 0.118s)
             Mean action noise std: 3.25
          Mean value_function loss: 111.8813
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 56.0732
                       Mean reward: 840.88
               Mean episode length: 223.75
    Episode_Reward/reaching_object: 0.9871
     Episode_Reward/lifting_object: 166.0681
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 2.07s
                      Time elapsed: 00:58:24
                               ETA: 00:14:38

################################################################################
                     [1m Learning iteration 1600/2000 [0m                     

                       Computation: 45675 steps/s (collection: 2.054s, learning 0.098s)
             Mean action noise std: 3.25
          Mean value_function loss: 107.0412
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 56.0759
                       Mean reward: 908.02
               Mean episode length: 239.57
    Episode_Reward/reaching_object: 1.0156
     Episode_Reward/lifting_object: 171.0312
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 2.15s
                      Time elapsed: 00:58:26
                               ETA: 00:14:36

################################################################################
                     [1m Learning iteration 1601/2000 [0m                     

                       Computation: 47463 steps/s (collection: 1.980s, learning 0.091s)
             Mean action noise std: 3.25
          Mean value_function loss: 113.6875
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.0822
                       Mean reward: 866.39
               Mean episode length: 229.85
    Episode_Reward/reaching_object: 1.0110
     Episode_Reward/lifting_object: 170.1951
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 2.07s
                      Time elapsed: 00:58:28
                               ETA: 00:14:33

################################################################################
                     [1m Learning iteration 1602/2000 [0m                     

                       Computation: 47366 steps/s (collection: 1.983s, learning 0.092s)
             Mean action noise std: 3.25
          Mean value_function loss: 105.3158
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.0913
                       Mean reward: 845.34
               Mean episode length: 225.50
    Episode_Reward/reaching_object: 1.0130
     Episode_Reward/lifting_object: 170.6415
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 2.08s
                      Time elapsed: 00:58:31
                               ETA: 00:14:31

################################################################################
                     [1m Learning iteration 1603/2000 [0m                     

                       Computation: 48761 steps/s (collection: 1.924s, learning 0.092s)
             Mean action noise std: 3.25
          Mean value_function loss: 119.5099
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 56.1039
                       Mean reward: 833.63
               Mean episode length: 223.78
    Episode_Reward/reaching_object: 1.0084
     Episode_Reward/lifting_object: 169.2817
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 2.02s
                      Time elapsed: 00:58:33
                               ETA: 00:14:29

################################################################################
                     [1m Learning iteration 1604/2000 [0m                     

                       Computation: 48811 steps/s (collection: 1.922s, learning 0.092s)
             Mean action noise std: 3.25
          Mean value_function loss: 82.6686
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 56.1169
                       Mean reward: 877.28
               Mean episode length: 232.93
    Episode_Reward/reaching_object: 1.0132
     Episode_Reward/lifting_object: 170.3066
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 2.01s
                      Time elapsed: 00:58:35
                               ETA: 00:14:27

################################################################################
                     [1m Learning iteration 1605/2000 [0m                     

                       Computation: 47964 steps/s (collection: 1.933s, learning 0.116s)
             Mean action noise std: 3.25
          Mean value_function loss: 102.4276
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.1252
                       Mean reward: 839.56
               Mean episode length: 223.16
    Episode_Reward/reaching_object: 1.0203
     Episode_Reward/lifting_object: 171.6971
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0576
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 2.05s
                      Time elapsed: 00:58:37
                               ETA: 00:14:25

################################################################################
                     [1m Learning iteration 1606/2000 [0m                     

                       Computation: 47193 steps/s (collection: 1.976s, learning 0.107s)
             Mean action noise std: 3.26
          Mean value_function loss: 82.2216
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 56.1360
                       Mean reward: 878.16
               Mean episode length: 233.42
    Episode_Reward/reaching_object: 1.0276
     Episode_Reward/lifting_object: 173.2129
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 2.08s
                      Time elapsed: 00:58:39
                               ETA: 00:14:22

################################################################################
                     [1m Learning iteration 1607/2000 [0m                     

                       Computation: 48142 steps/s (collection: 1.947s, learning 0.095s)
             Mean action noise std: 3.26
          Mean value_function loss: 107.9838
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 56.1411
                       Mean reward: 857.74
               Mean episode length: 228.53
    Episode_Reward/reaching_object: 1.0117
     Episode_Reward/lifting_object: 170.1127
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 2.04s
                      Time elapsed: 00:58:41
                               ETA: 00:14:20

################################################################################
                     [1m Learning iteration 1608/2000 [0m                     

                       Computation: 47903 steps/s (collection: 1.949s, learning 0.103s)
             Mean action noise std: 3.26
          Mean value_function loss: 132.3916
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 56.1484
                       Mean reward: 824.01
               Mean episode length: 220.97
    Episode_Reward/reaching_object: 1.0002
     Episode_Reward/lifting_object: 167.8885
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 2.05s
                      Time elapsed: 00:58:43
                               ETA: 00:14:18

################################################################################
                     [1m Learning iteration 1609/2000 [0m                     

                       Computation: 48652 steps/s (collection: 1.920s, learning 0.101s)
             Mean action noise std: 3.26
          Mean value_function loss: 130.5227
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.1653
                       Mean reward: 809.67
               Mean episode length: 218.80
    Episode_Reward/reaching_object: 1.0193
     Episode_Reward/lifting_object: 171.2320
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 2.02s
                      Time elapsed: 00:58:45
                               ETA: 00:14:16

################################################################################
                     [1m Learning iteration 1610/2000 [0m                     

                       Computation: 48120 steps/s (collection: 1.950s, learning 0.093s)
             Mean action noise std: 3.26
          Mean value_function loss: 84.1985
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.1754
                       Mean reward: 860.02
               Mean episode length: 228.86
    Episode_Reward/reaching_object: 1.0190
     Episode_Reward/lifting_object: 171.3404
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 2.04s
                      Time elapsed: 00:58:47
                               ETA: 00:14:13

################################################################################
                     [1m Learning iteration 1611/2000 [0m                     

                       Computation: 48338 steps/s (collection: 1.939s, learning 0.095s)
             Mean action noise std: 3.26
          Mean value_function loss: 110.8750
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 56.1862
                       Mean reward: 828.90
               Mean episode length: 221.50
    Episode_Reward/reaching_object: 1.0055
     Episode_Reward/lifting_object: 169.2862
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 2.03s
                      Time elapsed: 00:58:49
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 1612/2000 [0m                     

                       Computation: 46049 steps/s (collection: 2.038s, learning 0.097s)
             Mean action noise std: 3.27
          Mean value_function loss: 117.0999
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 56.2041
                       Mean reward: 876.29
               Mean episode length: 232.38
    Episode_Reward/reaching_object: 1.0080
     Episode_Reward/lifting_object: 169.5273
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 2.13s
                      Time elapsed: 00:58:51
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 1613/2000 [0m                     

                       Computation: 48399 steps/s (collection: 1.926s, learning 0.106s)
             Mean action noise std: 3.27
          Mean value_function loss: 107.9973
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.2147
                       Mean reward: 889.81
               Mean episode length: 235.74
    Episode_Reward/reaching_object: 1.0358
     Episode_Reward/lifting_object: 174.6465
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 2.03s
                      Time elapsed: 00:58:53
                               ETA: 00:14:07

################################################################################
                     [1m Learning iteration 1614/2000 [0m                     

                       Computation: 45954 steps/s (collection: 1.997s, learning 0.142s)
             Mean action noise std: 3.27
          Mean value_function loss: 119.8932
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.2287
                       Mean reward: 844.01
               Mean episode length: 226.35
    Episode_Reward/reaching_object: 1.0016
     Episode_Reward/lifting_object: 168.2341
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 2.14s
                      Time elapsed: 00:58:55
                               ETA: 00:14:05

################################################################################
                     [1m Learning iteration 1615/2000 [0m                     

                       Computation: 47914 steps/s (collection: 1.948s, learning 0.104s)
             Mean action noise std: 3.27
          Mean value_function loss: 96.9284
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.2452
                       Mean reward: 869.34
               Mean episode length: 230.89
    Episode_Reward/reaching_object: 0.9993
     Episode_Reward/lifting_object: 168.0249
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 2.05s
                      Time elapsed: 00:58:57
                               ETA: 00:14:02

################################################################################
                     [1m Learning iteration 1616/2000 [0m                     

                       Computation: 47512 steps/s (collection: 1.951s, learning 0.118s)
             Mean action noise std: 3.27
          Mean value_function loss: 82.8052
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 56.2557
                       Mean reward: 864.73
               Mean episode length: 230.45
    Episode_Reward/reaching_object: 1.0399
     Episode_Reward/lifting_object: 175.1268
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 2.07s
                      Time elapsed: 00:58:59
                               ETA: 00:14:00

################################################################################
                     [1m Learning iteration 1617/2000 [0m                     

                       Computation: 45041 steps/s (collection: 2.076s, learning 0.107s)
             Mean action noise std: 3.27
          Mean value_function loss: 88.8681
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 56.2611
                       Mean reward: 870.19
               Mean episode length: 231.71
    Episode_Reward/reaching_object: 1.0507
     Episode_Reward/lifting_object: 177.3026
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 2.18s
                      Time elapsed: 00:59:02
                               ETA: 00:13:58

################################################################################
                     [1m Learning iteration 1618/2000 [0m                     

                       Computation: 47055 steps/s (collection: 1.985s, learning 0.105s)
             Mean action noise std: 3.28
          Mean value_function loss: 105.4438
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.2709
                       Mean reward: 902.93
               Mean episode length: 239.15
    Episode_Reward/reaching_object: 1.0301
     Episode_Reward/lifting_object: 173.3842
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 2.09s
                      Time elapsed: 00:59:04
                               ETA: 00:13:56

################################################################################
                     [1m Learning iteration 1619/2000 [0m                     

                       Computation: 48210 steps/s (collection: 1.945s, learning 0.094s)
             Mean action noise std: 3.28
          Mean value_function loss: 98.0428
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.2877
                       Mean reward: 882.58
               Mean episode length: 234.90
    Episode_Reward/reaching_object: 1.0157
     Episode_Reward/lifting_object: 170.5371
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 2.04s
                      Time elapsed: 00:59:06
                               ETA: 00:13:53

################################################################################
                     [1m Learning iteration 1620/2000 [0m                     

                       Computation: 47801 steps/s (collection: 1.963s, learning 0.093s)
             Mean action noise std: 3.28
          Mean value_function loss: 95.3292
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.3025
                       Mean reward: 834.25
               Mean episode length: 223.72
    Episode_Reward/reaching_object: 1.0186
     Episode_Reward/lifting_object: 171.1005
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 2.06s
                      Time elapsed: 00:59:08
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 1621/2000 [0m                     

                       Computation: 44472 steps/s (collection: 2.005s, learning 0.206s)
             Mean action noise std: 3.28
          Mean value_function loss: 99.9464
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.3143
                       Mean reward: 820.04
               Mean episode length: 219.40
    Episode_Reward/reaching_object: 1.0168
     Episode_Reward/lifting_object: 171.0258
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0584
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 2.21s
                      Time elapsed: 00:59:10
                               ETA: 00:13:49

################################################################################
                     [1m Learning iteration 1622/2000 [0m                     

                       Computation: 44809 steps/s (collection: 2.095s, learning 0.099s)
             Mean action noise std: 3.28
          Mean value_function loss: 98.5066
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 56.3278
                       Mean reward: 872.88
               Mean episode length: 231.57
    Episode_Reward/reaching_object: 1.0377
     Episode_Reward/lifting_object: 174.9726
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 2.19s
                      Time elapsed: 00:59:12
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 1623/2000 [0m                     

                       Computation: 47999 steps/s (collection: 1.920s, learning 0.128s)
             Mean action noise std: 3.29
          Mean value_function loss: 106.5122
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.3354
                       Mean reward: 849.36
               Mean episode length: 226.58
    Episode_Reward/reaching_object: 0.9889
     Episode_Reward/lifting_object: 165.9569
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 2.05s
                      Time elapsed: 00:59:14
                               ETA: 00:13:45

################################################################################
                     [1m Learning iteration 1624/2000 [0m                     

                       Computation: 46864 steps/s (collection: 1.993s, learning 0.105s)
             Mean action noise std: 3.29
          Mean value_function loss: 101.3788
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 56.3426
                       Mean reward: 827.81
               Mean episode length: 222.07
    Episode_Reward/reaching_object: 1.0389
     Episode_Reward/lifting_object: 174.9635
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 2.10s
                      Time elapsed: 00:59:16
                               ETA: 00:13:42

################################################################################
                     [1m Learning iteration 1625/2000 [0m                     

                       Computation: 47915 steps/s (collection: 1.942s, learning 0.110s)
             Mean action noise std: 3.29
          Mean value_function loss: 99.9254
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.3468
                       Mean reward: 868.97
               Mean episode length: 231.48
    Episode_Reward/reaching_object: 1.0115
     Episode_Reward/lifting_object: 169.9701
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 2.05s
                      Time elapsed: 00:59:18
                               ETA: 00:13:40

################################################################################
                     [1m Learning iteration 1626/2000 [0m                     

                       Computation: 46545 steps/s (collection: 1.981s, learning 0.131s)
             Mean action noise std: 3.29
          Mean value_function loss: 101.9961
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 56.3521
                       Mean reward: 840.79
               Mean episode length: 225.44
    Episode_Reward/reaching_object: 1.0083
     Episode_Reward/lifting_object: 169.5346
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0584
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 2.11s
                      Time elapsed: 00:59:20
                               ETA: 00:13:38

################################################################################
                     [1m Learning iteration 1627/2000 [0m                     

                       Computation: 47786 steps/s (collection: 1.962s, learning 0.096s)
             Mean action noise std: 3.29
          Mean value_function loss: 140.4270
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.3599
                       Mean reward: 833.75
               Mean episode length: 223.09
    Episode_Reward/reaching_object: 1.0254
     Episode_Reward/lifting_object: 172.3793
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 2.06s
                      Time elapsed: 00:59:22
                               ETA: 00:13:36

################################################################################
                     [1m Learning iteration 1628/2000 [0m                     

                       Computation: 45573 steps/s (collection: 2.033s, learning 0.124s)
             Mean action noise std: 3.29
          Mean value_function loss: 110.0785
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 56.3714
                       Mean reward: 890.34
               Mean episode length: 237.76
    Episode_Reward/reaching_object: 1.0362
     Episode_Reward/lifting_object: 174.0802
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 2.16s
                      Time elapsed: 00:59:25
                               ETA: 00:13:34

################################################################################
                     [1m Learning iteration 1629/2000 [0m                     

                       Computation: 47475 steps/s (collection: 1.976s, learning 0.095s)
             Mean action noise std: 3.29
          Mean value_function loss: 88.5068
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.3755
                       Mean reward: 869.41
               Mean episode length: 231.99
    Episode_Reward/reaching_object: 1.0041
     Episode_Reward/lifting_object: 168.7974
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 2.07s
                      Time elapsed: 00:59:27
                               ETA: 00:13:31

################################################################################
                     [1m Learning iteration 1630/2000 [0m                     

                       Computation: 48591 steps/s (collection: 1.915s, learning 0.109s)
             Mean action noise std: 3.29
          Mean value_function loss: 108.7978
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 56.3859
                       Mean reward: 878.90
               Mean episode length: 234.38
    Episode_Reward/reaching_object: 1.0380
     Episode_Reward/lifting_object: 175.2668
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 2.02s
                      Time elapsed: 00:59:29
                               ETA: 00:13:29

################################################################################
                     [1m Learning iteration 1631/2000 [0m                     

                       Computation: 48213 steps/s (collection: 1.947s, learning 0.092s)
             Mean action noise std: 3.29
          Mean value_function loss: 121.9202
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 56.3892
                       Mean reward: 840.62
               Mean episode length: 225.68
    Episode_Reward/reaching_object: 1.0366
     Episode_Reward/lifting_object: 175.0756
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 2.04s
                      Time elapsed: 00:59:31
                               ETA: 00:13:27

################################################################################
                     [1m Learning iteration 1632/2000 [0m                     

                       Computation: 48892 steps/s (collection: 1.919s, learning 0.092s)
             Mean action noise std: 3.29
          Mean value_function loss: 119.0894
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 56.3912
                       Mean reward: 836.57
               Mean episode length: 225.00
    Episode_Reward/reaching_object: 0.9919
     Episode_Reward/lifting_object: 165.9771
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 2.01s
                      Time elapsed: 00:59:33
                               ETA: 00:13:25

################################################################################
                     [1m Learning iteration 1633/2000 [0m                     

                       Computation: 48035 steps/s (collection: 1.954s, learning 0.093s)
             Mean action noise std: 3.29
          Mean value_function loss: 128.7101
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.3952
                       Mean reward: 841.18
               Mean episode length: 225.32
    Episode_Reward/reaching_object: 1.0117
     Episode_Reward/lifting_object: 169.7607
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 2.05s
                      Time elapsed: 00:59:35
                               ETA: 00:13:23

################################################################################
                     [1m Learning iteration 1634/2000 [0m                     

                       Computation: 47834 steps/s (collection: 1.954s, learning 0.102s)
             Mean action noise std: 3.30
          Mean value_function loss: 131.0302
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 56.4024
                       Mean reward: 856.27
               Mean episode length: 229.50
    Episode_Reward/reaching_object: 1.0173
     Episode_Reward/lifting_object: 170.3995
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 2.06s
                      Time elapsed: 00:59:37
                               ETA: 00:13:20

################################################################################
                     [1m Learning iteration 1635/2000 [0m                     

                       Computation: 48332 steps/s (collection: 1.938s, learning 0.096s)
             Mean action noise std: 3.30
          Mean value_function loss: 110.2933
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.4100
                       Mean reward: 840.06
               Mean episode length: 224.73
    Episode_Reward/reaching_object: 1.0081
     Episode_Reward/lifting_object: 169.6509
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 2.03s
                      Time elapsed: 00:59:39
                               ETA: 00:13:18

################################################################################
                     [1m Learning iteration 1636/2000 [0m                     

                       Computation: 44376 steps/s (collection: 2.000s, learning 0.215s)
             Mean action noise std: 3.30
          Mean value_function loss: 128.7782
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.4175
                       Mean reward: 879.45
               Mean episode length: 233.98
    Episode_Reward/reaching_object: 1.0212
     Episode_Reward/lifting_object: 171.6670
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 2.22s
                      Time elapsed: 00:59:41
                               ETA: 00:13:16

################################################################################
                     [1m Learning iteration 1637/2000 [0m                     

                       Computation: 46589 steps/s (collection: 1.977s, learning 0.133s)
             Mean action noise std: 3.30
          Mean value_function loss: 96.2684
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 56.4231
                       Mean reward: 846.19
               Mean episode length: 225.15
    Episode_Reward/reaching_object: 1.0206
     Episode_Reward/lifting_object: 171.6336
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 2.11s
                      Time elapsed: 00:59:43
                               ETA: 00:13:14

################################################################################
                     [1m Learning iteration 1638/2000 [0m                     

                       Computation: 47260 steps/s (collection: 1.959s, learning 0.121s)
             Mean action noise std: 3.30
          Mean value_function loss: 91.3359
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.4359
                       Mean reward: 847.97
               Mean episode length: 226.26
    Episode_Reward/reaching_object: 1.0275
     Episode_Reward/lifting_object: 172.8400
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 2.08s
                      Time elapsed: 00:59:45
                               ETA: 00:13:11

################################################################################
                     [1m Learning iteration 1639/2000 [0m                     

                       Computation: 45931 steps/s (collection: 2.012s, learning 0.129s)
             Mean action noise std: 3.30
          Mean value_function loss: 84.6317
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 56.4511
                       Mean reward: 921.73
               Mean episode length: 243.38
    Episode_Reward/reaching_object: 1.0430
     Episode_Reward/lifting_object: 175.6716
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 2.14s
                      Time elapsed: 00:59:47
                               ETA: 00:13:09

################################################################################
                     [1m Learning iteration 1640/2000 [0m                     

                       Computation: 47308 steps/s (collection: 1.957s, learning 0.121s)
             Mean action noise std: 3.30
          Mean value_function loss: 82.5189
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 56.4615
                       Mean reward: 886.49
               Mean episode length: 234.54
    Episode_Reward/reaching_object: 1.0358
     Episode_Reward/lifting_object: 174.4885
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 2.08s
                      Time elapsed: 00:59:50
                               ETA: 00:13:07

################################################################################
                     [1m Learning iteration 1641/2000 [0m                     

                       Computation: 47361 steps/s (collection: 1.970s, learning 0.106s)
             Mean action noise std: 3.31
          Mean value_function loss: 103.0494
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 56.4644
                       Mean reward: 832.75
               Mean episode length: 224.66
    Episode_Reward/reaching_object: 1.0175
     Episode_Reward/lifting_object: 170.5611
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 2.08s
                      Time elapsed: 00:59:52
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 1642/2000 [0m                     

                       Computation: 48211 steps/s (collection: 1.935s, learning 0.104s)
             Mean action noise std: 3.31
          Mean value_function loss: 112.2801
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.4664
                       Mean reward: 875.20
               Mean episode length: 232.38
    Episode_Reward/reaching_object: 1.0367
     Episode_Reward/lifting_object: 174.6885
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0613
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 2.04s
                      Time elapsed: 00:59:54
                               ETA: 00:13:03

################################################################################
                     [1m Learning iteration 1643/2000 [0m                     

                       Computation: 44630 steps/s (collection: 1.965s, learning 0.237s)
             Mean action noise std: 3.31
          Mean value_function loss: 95.2096
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.4745
                       Mean reward: 874.54
               Mean episode length: 232.50
    Episode_Reward/reaching_object: 1.0261
     Episode_Reward/lifting_object: 172.4159
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 2.20s
                      Time elapsed: 00:59:56
                               ETA: 00:13:00

################################################################################
                     [1m Learning iteration 1644/2000 [0m                     

                       Computation: 45076 steps/s (collection: 2.082s, learning 0.099s)
             Mean action noise std: 3.31
          Mean value_function loss: 120.0469
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.4831
                       Mean reward: 864.25
               Mean episode length: 230.55
    Episode_Reward/reaching_object: 1.0302
     Episode_Reward/lifting_object: 173.4561
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 2.18s
                      Time elapsed: 00:59:58
                               ETA: 00:12:58

################################################################################
                     [1m Learning iteration 1645/2000 [0m                     

                       Computation: 48892 steps/s (collection: 1.914s, learning 0.097s)
             Mean action noise std: 3.31
          Mean value_function loss: 105.5472
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.4915
                       Mean reward: 886.34
               Mean episode length: 234.39
    Episode_Reward/reaching_object: 1.0098
     Episode_Reward/lifting_object: 170.0963
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 2.01s
                      Time elapsed: 01:00:00
                               ETA: 00:12:56

################################################################################
                     [1m Learning iteration 1646/2000 [0m                     

                       Computation: 48443 steps/s (collection: 1.937s, learning 0.092s)
             Mean action noise std: 3.31
          Mean value_function loss: 125.7426
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.4959
                       Mean reward: 845.44
               Mean episode length: 224.71
    Episode_Reward/reaching_object: 1.0166
     Episode_Reward/lifting_object: 171.4709
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 2.03s
                      Time elapsed: 01:00:02
                               ETA: 00:12:54

################################################################################
                     [1m Learning iteration 1647/2000 [0m                     

                       Computation: 47200 steps/s (collection: 1.985s, learning 0.098s)
             Mean action noise std: 3.31
          Mean value_function loss: 136.0833
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 56.4986
                       Mean reward: 877.94
               Mean episode length: 232.76
    Episode_Reward/reaching_object: 1.0013
     Episode_Reward/lifting_object: 168.3578
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 2.08s
                      Time elapsed: 01:00:04
                               ETA: 00:12:52

################################################################################
                     [1m Learning iteration 1648/2000 [0m                     

                       Computation: 48303 steps/s (collection: 1.939s, learning 0.096s)
             Mean action noise std: 3.31
          Mean value_function loss: 89.0217
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 56.5042
                       Mean reward: 847.66
               Mean episode length: 225.71
    Episode_Reward/reaching_object: 1.0243
     Episode_Reward/lifting_object: 172.7129
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0608
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 2.04s
                      Time elapsed: 01:00:06
                               ETA: 00:12:49

################################################################################
                     [1m Learning iteration 1649/2000 [0m                     

                       Computation: 47650 steps/s (collection: 1.959s, learning 0.104s)
             Mean action noise std: 3.31
          Mean value_function loss: 149.4626
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 56.5151
                       Mean reward: 842.49
               Mean episode length: 225.29
    Episode_Reward/reaching_object: 1.0311
     Episode_Reward/lifting_object: 173.4778
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0613
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 2.06s
                      Time elapsed: 01:00:08
                               ETA: 00:12:47

################################################################################
                     [1m Learning iteration 1650/2000 [0m                     

                       Computation: 48426 steps/s (collection: 1.938s, learning 0.092s)
             Mean action noise std: 3.32
          Mean value_function loss: 77.1391
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.5270
                       Mean reward: 848.78
               Mean episode length: 225.31
    Episode_Reward/reaching_object: 1.0355
     Episode_Reward/lifting_object: 175.0554
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 2.03s
                      Time elapsed: 01:00:10
                               ETA: 00:12:45

################################################################################
                     [1m Learning iteration 1651/2000 [0m                     

                       Computation: 45888 steps/s (collection: 2.039s, learning 0.103s)
             Mean action noise std: 3.32
          Mean value_function loss: 81.9030
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 56.5377
                       Mean reward: 911.23
               Mean episode length: 240.57
    Episode_Reward/reaching_object: 1.0130
     Episode_Reward/lifting_object: 170.3883
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 2.14s
                      Time elapsed: 01:00:12
                               ETA: 00:12:43

################################################################################
                     [1m Learning iteration 1652/2000 [0m                     

                       Computation: 47068 steps/s (collection: 1.945s, learning 0.144s)
             Mean action noise std: 3.32
          Mean value_function loss: 81.8966
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 56.5507
                       Mean reward: 884.62
               Mean episode length: 234.35
    Episode_Reward/reaching_object: 1.0322
     Episode_Reward/lifting_object: 174.2473
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 2.09s
                      Time elapsed: 01:00:15
                               ETA: 00:12:41

################################################################################
                     [1m Learning iteration 1653/2000 [0m                     

                       Computation: 48053 steps/s (collection: 1.913s, learning 0.133s)
             Mean action noise std: 3.32
          Mean value_function loss: 99.0167
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 56.5585
                       Mean reward: 870.24
               Mean episode length: 231.29
    Episode_Reward/reaching_object: 1.0286
     Episode_Reward/lifting_object: 173.1942
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0612
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 2.05s
                      Time elapsed: 01:00:17
                               ETA: 00:12:38

################################################################################
                     [1m Learning iteration 1654/2000 [0m                     

                       Computation: 48144 steps/s (collection: 1.929s, learning 0.113s)
             Mean action noise std: 3.32
          Mean value_function loss: 137.2794
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 56.5676
                       Mean reward: 874.96
               Mean episode length: 233.43
    Episode_Reward/reaching_object: 1.0368
     Episode_Reward/lifting_object: 174.3581
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0617
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 2.04s
                      Time elapsed: 01:00:19
                               ETA: 00:12:36

################################################################################
                     [1m Learning iteration 1655/2000 [0m                     

                       Computation: 49012 steps/s (collection: 1.913s, learning 0.093s)
             Mean action noise std: 3.32
          Mean value_function loss: 113.8865
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 56.5751
                       Mean reward: 863.36
               Mean episode length: 228.90
    Episode_Reward/reaching_object: 1.0137
     Episode_Reward/lifting_object: 170.4022
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 2.01s
                      Time elapsed: 01:00:21
                               ETA: 00:12:34

################################################################################
                     [1m Learning iteration 1656/2000 [0m                     

                       Computation: 47269 steps/s (collection: 1.936s, learning 0.144s)
             Mean action noise std: 3.32
          Mean value_function loss: 93.2633
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 56.5798
                       Mean reward: 829.25
               Mean episode length: 220.91
    Episode_Reward/reaching_object: 1.0239
     Episode_Reward/lifting_object: 171.9812
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 2.08s
                      Time elapsed: 01:00:23
                               ETA: 00:12:32

################################################################################
                     [1m Learning iteration 1657/2000 [0m                     

                       Computation: 46954 steps/s (collection: 1.979s, learning 0.115s)
             Mean action noise std: 3.33
          Mean value_function loss: 95.3046
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.5915
                       Mean reward: 866.82
               Mean episode length: 230.39
    Episode_Reward/reaching_object: 1.0269
     Episode_Reward/lifting_object: 172.6153
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 2.09s
                      Time elapsed: 01:00:25
                               ETA: 00:12:29

################################################################################
                     [1m Learning iteration 1658/2000 [0m                     

                       Computation: 46183 steps/s (collection: 2.036s, learning 0.092s)
             Mean action noise std: 3.33
          Mean value_function loss: 102.9236
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 56.6077
                       Mean reward: 854.96
               Mean episode length: 227.58
    Episode_Reward/reaching_object: 1.0123
     Episode_Reward/lifting_object: 169.8520
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 2.13s
                      Time elapsed: 01:00:27
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 1659/2000 [0m                     

                       Computation: 48989 steps/s (collection: 1.916s, learning 0.091s)
             Mean action noise std: 3.33
          Mean value_function loss: 82.5287
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 56.6208
                       Mean reward: 885.32
               Mean episode length: 234.30
    Episode_Reward/reaching_object: 1.0356
     Episode_Reward/lifting_object: 173.8605
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0616
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 2.01s
                      Time elapsed: 01:00:29
                               ETA: 00:12:25

################################################################################
                     [1m Learning iteration 1660/2000 [0m                     

                       Computation: 46750 steps/s (collection: 2.010s, learning 0.093s)
             Mean action noise std: 3.33
          Mean value_function loss: 99.1835
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 56.6251
                       Mean reward: 829.82
               Mean episode length: 222.06
    Episode_Reward/reaching_object: 1.0015
     Episode_Reward/lifting_object: 168.0259
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 2.10s
                      Time elapsed: 01:00:31
                               ETA: 00:12:23

################################################################################
                     [1m Learning iteration 1661/2000 [0m                     

                       Computation: 48455 steps/s (collection: 1.937s, learning 0.092s)
             Mean action noise std: 3.33
          Mean value_function loss: 109.7727
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 56.6345
                       Mean reward: 842.43
               Mean episode length: 223.70
    Episode_Reward/reaching_object: 1.0052
     Episode_Reward/lifting_object: 168.7333
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 2.03s
                      Time elapsed: 01:00:33
                               ETA: 00:12:21

################################################################################
                     [1m Learning iteration 1662/2000 [0m                     

                       Computation: 49309 steps/s (collection: 1.903s, learning 0.091s)
             Mean action noise std: 3.33
          Mean value_function loss: 114.0946
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 56.6470
                       Mean reward: 871.84
               Mean episode length: 230.63
    Episode_Reward/reaching_object: 0.9985
     Episode_Reward/lifting_object: 167.7684
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 1.99s
                      Time elapsed: 01:00:35
                               ETA: 00:12:18

################################################################################
                     [1m Learning iteration 1663/2000 [0m                     

                       Computation: 45896 steps/s (collection: 2.052s, learning 0.090s)
             Mean action noise std: 3.34
          Mean value_function loss: 134.4545
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.6583
                       Mean reward: 848.99
               Mean episode length: 225.39
    Episode_Reward/reaching_object: 1.0184
     Episode_Reward/lifting_object: 170.9843
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 2.14s
                      Time elapsed: 01:00:37
                               ETA: 00:12:16

################################################################################
                     [1m Learning iteration 1664/2000 [0m                     

                       Computation: 47811 steps/s (collection: 1.964s, learning 0.092s)
             Mean action noise std: 3.34
          Mean value_function loss: 118.9600
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 56.6681
                       Mean reward: 880.56
               Mean episode length: 233.33
    Episode_Reward/reaching_object: 1.0275
     Episode_Reward/lifting_object: 172.6849
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 2.06s
                      Time elapsed: 01:00:39
                               ETA: 00:12:14

################################################################################
                     [1m Learning iteration 1665/2000 [0m                     

                       Computation: 47386 steps/s (collection: 1.944s, learning 0.131s)
             Mean action noise std: 3.34
          Mean value_function loss: 130.8971
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.6709
                       Mean reward: 864.92
               Mean episode length: 229.54
    Episode_Reward/reaching_object: 1.0145
     Episode_Reward/lifting_object: 170.4554
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0608
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 2.07s
                      Time elapsed: 01:00:41
                               ETA: 00:12:12

################################################################################
                     [1m Learning iteration 1666/2000 [0m                     

                       Computation: 27398 steps/s (collection: 3.466s, learning 0.122s)
             Mean action noise std: 3.34
          Mean value_function loss: 117.8032
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 56.6806
                       Mean reward: 848.99
               Mean episode length: 225.92
    Episode_Reward/reaching_object: 1.0199
     Episode_Reward/lifting_object: 171.3820
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 3.59s
                      Time elapsed: 01:00:45
                               ETA: 00:12:10

################################################################################
                     [1m Learning iteration 1667/2000 [0m                     

                       Computation: 14602 steps/s (collection: 6.610s, learning 0.122s)
             Mean action noise std: 3.34
          Mean value_function loss: 148.0587
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.6925
                       Mean reward: 843.02
               Mean episode length: 225.35
    Episode_Reward/reaching_object: 1.0089
     Episode_Reward/lifting_object: 169.1505
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 6.73s
                      Time elapsed: 01:00:52
                               ETA: 00:12:09

################################################################################
                     [1m Learning iteration 1668/2000 [0m                     

                       Computation: 14115 steps/s (collection: 6.830s, learning 0.134s)
             Mean action noise std: 3.34
          Mean value_function loss: 122.1447
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 56.6998
                       Mean reward: 854.75
               Mean episode length: 227.73
    Episode_Reward/reaching_object: 0.9979
     Episode_Reward/lifting_object: 167.7346
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 6.96s
                      Time elapsed: 01:00:59
                               ETA: 00:12:07

################################################################################
                     [1m Learning iteration 1669/2000 [0m                     

                       Computation: 14371 steps/s (collection: 6.720s, learning 0.120s)
             Mean action noise std: 3.34
          Mean value_function loss: 96.7831
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 56.7059
                       Mean reward: 857.43
               Mean episode length: 227.69
    Episode_Reward/reaching_object: 1.0206
     Episode_Reward/lifting_object: 171.9557
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 6.84s
                      Time elapsed: 01:01:05
                               ETA: 00:12:06

################################################################################
                     [1m Learning iteration 1670/2000 [0m                     

                       Computation: 14339 steps/s (collection: 6.729s, learning 0.126s)
             Mean action noise std: 3.34
          Mean value_function loss: 107.7091
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.7172
                       Mean reward: 850.28
               Mean episode length: 225.91
    Episode_Reward/reaching_object: 1.0238
     Episode_Reward/lifting_object: 172.6306
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 6.86s
                      Time elapsed: 01:01:12
                               ETA: 00:12:05

################################################################################
                     [1m Learning iteration 1671/2000 [0m                     

                       Computation: 14437 steps/s (collection: 6.698s, learning 0.111s)
             Mean action noise std: 3.35
          Mean value_function loss: 99.5169
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 56.7319
                       Mean reward: 872.67
               Mean episode length: 231.04
    Episode_Reward/reaching_object: 1.0240
     Episode_Reward/lifting_object: 172.6182
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0615
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 6.81s
                      Time elapsed: 01:01:19
                               ETA: 00:12:04

################################################################################
                     [1m Learning iteration 1672/2000 [0m                     

                       Computation: 14596 steps/s (collection: 6.614s, learning 0.121s)
             Mean action noise std: 3.35
          Mean value_function loss: 94.1494
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.7400
                       Mean reward: 899.87
               Mean episode length: 238.10
    Episode_Reward/reaching_object: 1.0297
     Episode_Reward/lifting_object: 173.5251
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 6.73s
                      Time elapsed: 01:01:26
                               ETA: 00:12:02

################################################################################
                     [1m Learning iteration 1673/2000 [0m                     

                       Computation: 14373 steps/s (collection: 6.734s, learning 0.106s)
             Mean action noise std: 3.35
          Mean value_function loss: 114.3196
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.7564
                       Mean reward: 858.08
               Mean episode length: 228.04
    Episode_Reward/reaching_object: 1.0098
     Episode_Reward/lifting_object: 169.7645
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 6.84s
                      Time elapsed: 01:01:33
                               ETA: 00:12:01

################################################################################
                     [1m Learning iteration 1674/2000 [0m                     

                       Computation: 14319 steps/s (collection: 6.725s, learning 0.140s)
             Mean action noise std: 3.35
          Mean value_function loss: 93.9124
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.7692
                       Mean reward: 877.54
               Mean episode length: 232.47
    Episode_Reward/reaching_object: 1.0427
     Episode_Reward/lifting_object: 175.4724
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 6.87s
                      Time elapsed: 01:01:40
                               ETA: 00:12:00

################################################################################
                     [1m Learning iteration 1675/2000 [0m                     

                       Computation: 20233 steps/s (collection: 4.756s, learning 0.103s)
             Mean action noise std: 3.35
          Mean value_function loss: 131.1715
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.7801
                       Mean reward: 814.30
               Mean episode length: 218.15
    Episode_Reward/reaching_object: 1.0113
     Episode_Reward/lifting_object: 169.7386
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0612
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 4.86s
                      Time elapsed: 01:01:44
                               ETA: 00:11:58

################################################################################
                     [1m Learning iteration 1676/2000 [0m                     

                       Computation: 49137 steps/s (collection: 1.901s, learning 0.100s)
             Mean action noise std: 3.35
          Mean value_function loss: 118.5889
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 56.7889
                       Mean reward: 880.01
               Mean episode length: 232.86
    Episode_Reward/reaching_object: 1.0312
     Episode_Reward/lifting_object: 173.1772
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 2.00s
                      Time elapsed: 01:01:46
                               ETA: 00:11:56

################################################################################
                     [1m Learning iteration 1677/2000 [0m                     

                       Computation: 45185 steps/s (collection: 2.034s, learning 0.142s)
             Mean action noise std: 3.35
          Mean value_function loss: 99.3994
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 56.7948
                       Mean reward: 851.58
               Mean episode length: 227.34
    Episode_Reward/reaching_object: 1.0007
     Episode_Reward/lifting_object: 167.5328
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 2.18s
                      Time elapsed: 01:01:49
                               ETA: 00:11:53

################################################################################
                     [1m Learning iteration 1678/2000 [0m                     

                       Computation: 42121 steps/s (collection: 2.156s, learning 0.178s)
             Mean action noise std: 3.36
          Mean value_function loss: 91.2172
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.8044
                       Mean reward: 826.24
               Mean episode length: 220.72
    Episode_Reward/reaching_object: 1.0270
     Episode_Reward/lifting_object: 172.3748
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 2.33s
                      Time elapsed: 01:01:51
                               ETA: 00:11:51

################################################################################
                     [1m Learning iteration 1679/2000 [0m                     

                       Computation: 48312 steps/s (collection: 1.895s, learning 0.140s)
             Mean action noise std: 3.36
          Mean value_function loss: 84.5543
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.8130
                       Mean reward: 887.08
               Mean episode length: 234.91
    Episode_Reward/reaching_object: 1.0464
     Episode_Reward/lifting_object: 175.6474
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 2.03s
                      Time elapsed: 01:01:53
                               ETA: 00:11:49

################################################################################
                     [1m Learning iteration 1680/2000 [0m                     

                       Computation: 51436 steps/s (collection: 1.813s, learning 0.098s)
             Mean action noise std: 3.36
          Mean value_function loss: 107.6422
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 56.8219
                       Mean reward: 856.04
               Mean episode length: 228.07
    Episode_Reward/reaching_object: 1.0153
     Episode_Reward/lifting_object: 170.2193
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 1.91s
                      Time elapsed: 01:01:55
                               ETA: 00:11:47

################################################################################
                     [1m Learning iteration 1681/2000 [0m                     

                       Computation: 51594 steps/s (collection: 1.807s, learning 0.099s)
             Mean action noise std: 3.36
          Mean value_function loss: 106.1706
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.8337
                       Mean reward: 909.88
               Mean episode length: 240.07
    Episode_Reward/reaching_object: 1.0296
     Episode_Reward/lifting_object: 172.4695
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 1.91s
                      Time elapsed: 01:01:57
                               ETA: 00:11:44

################################################################################
                     [1m Learning iteration 1682/2000 [0m                     

                       Computation: 50306 steps/s (collection: 1.869s, learning 0.085s)
             Mean action noise std: 3.36
          Mean value_function loss: 81.4060
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.8493
                       Mean reward: 825.16
               Mean episode length: 220.98
    Episode_Reward/reaching_object: 1.0381
     Episode_Reward/lifting_object: 173.7375
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 1.95s
                      Time elapsed: 01:01:59
                               ETA: 00:11:42

################################################################################
                     [1m Learning iteration 1683/2000 [0m                     

                       Computation: 48677 steps/s (collection: 1.883s, learning 0.136s)
             Mean action noise std: 3.36
          Mean value_function loss: 119.1605
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.8617
                       Mean reward: 856.32
               Mean episode length: 227.96
    Episode_Reward/reaching_object: 1.0294
     Episode_Reward/lifting_object: 172.6839
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 2.02s
                      Time elapsed: 01:02:01
                               ETA: 00:11:40

################################################################################
                     [1m Learning iteration 1684/2000 [0m                     

                       Computation: 49325 steps/s (collection: 1.885s, learning 0.108s)
             Mean action noise std: 3.37
          Mean value_function loss: 127.8921
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 56.8659
                       Mean reward: 866.27
               Mean episode length: 229.46
    Episode_Reward/reaching_object: 1.0013
     Episode_Reward/lifting_object: 168.0050
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 1.99s
                      Time elapsed: 01:02:03
                               ETA: 00:11:38

################################################################################
                     [1m Learning iteration 1685/2000 [0m                     

                       Computation: 51079 steps/s (collection: 1.826s, learning 0.098s)
             Mean action noise std: 3.37
          Mean value_function loss: 141.3480
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 56.8742
                       Mean reward: 841.42
               Mean episode length: 224.05
    Episode_Reward/reaching_object: 1.0086
     Episode_Reward/lifting_object: 168.7005
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 1.92s
                      Time elapsed: 01:02:05
                               ETA: 00:11:35

################################################################################
                     [1m Learning iteration 1686/2000 [0m                     

                       Computation: 50763 steps/s (collection: 1.848s, learning 0.089s)
             Mean action noise std: 3.37
          Mean value_function loss: 124.1927
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 56.8888
                       Mean reward: 883.78
               Mean episode length: 234.68
    Episode_Reward/reaching_object: 1.0432
     Episode_Reward/lifting_object: 174.9568
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 1.94s
                      Time elapsed: 01:02:07
                               ETA: 00:11:33

################################################################################
                     [1m Learning iteration 1687/2000 [0m                     

                       Computation: 49937 steps/s (collection: 1.822s, learning 0.147s)
             Mean action noise std: 3.37
          Mean value_function loss: 114.2221
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.9004
                       Mean reward: 838.27
               Mean episode length: 224.84
    Episode_Reward/reaching_object: 1.0202
     Episode_Reward/lifting_object: 170.8359
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 1.97s
                      Time elapsed: 01:02:09
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 1688/2000 [0m                     

                       Computation: 49158 steps/s (collection: 1.906s, learning 0.094s)
             Mean action noise std: 3.37
          Mean value_function loss: 118.8551
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 56.9098
                       Mean reward: 874.16
               Mean episode length: 232.22
    Episode_Reward/reaching_object: 1.0142
     Episode_Reward/lifting_object: 169.7729
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 2.00s
                      Time elapsed: 01:02:11
                               ETA: 00:11:29

################################################################################
                     [1m Learning iteration 1689/2000 [0m                     

                       Computation: 52008 steps/s (collection: 1.799s, learning 0.091s)
             Mean action noise std: 3.37
          Mean value_function loss: 123.5477
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.9138
                       Mean reward: 841.52
               Mean episode length: 224.55
    Episode_Reward/reaching_object: 1.0114
     Episode_Reward/lifting_object: 169.4298
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 1.89s
                      Time elapsed: 01:02:12
                               ETA: 00:11:26

################################################################################
                     [1m Learning iteration 1690/2000 [0m                     

                       Computation: 51358 steps/s (collection: 1.799s, learning 0.115s)
             Mean action noise std: 3.37
          Mean value_function loss: 135.1294
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 56.9216
                       Mean reward: 823.32
               Mean episode length: 219.44
    Episode_Reward/reaching_object: 1.0054
     Episode_Reward/lifting_object: 168.5935
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0616
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 1.91s
                      Time elapsed: 01:02:14
                               ETA: 00:11:24

################################################################################
                     [1m Learning iteration 1691/2000 [0m                     

                       Computation: 51672 steps/s (collection: 1.803s, learning 0.100s)
             Mean action noise std: 3.38
          Mean value_function loss: 119.3387
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.9311
                       Mean reward: 852.55
               Mean episode length: 226.10
    Episode_Reward/reaching_object: 1.0325
     Episode_Reward/lifting_object: 173.1826
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 1.90s
                      Time elapsed: 01:02:16
                               ETA: 00:11:22

################################################################################
                     [1m Learning iteration 1692/2000 [0m                     

                       Computation: 49582 steps/s (collection: 1.874s, learning 0.109s)
             Mean action noise std: 3.38
          Mean value_function loss: 96.0410
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.9377
                       Mean reward: 893.27
               Mean episode length: 237.30
    Episode_Reward/reaching_object: 1.0279
     Episode_Reward/lifting_object: 172.1186
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 1.98s
                      Time elapsed: 01:02:18
                               ETA: 00:11:20

################################################################################
                     [1m Learning iteration 1693/2000 [0m                     

                       Computation: 49547 steps/s (collection: 1.863s, learning 0.121s)
             Mean action noise std: 3.38
          Mean value_function loss: 113.5765
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.9449
                       Mean reward: 852.74
               Mean episode length: 226.08
    Episode_Reward/reaching_object: 1.0165
     Episode_Reward/lifting_object: 170.2637
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 1.98s
                      Time elapsed: 01:02:20
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 1694/2000 [0m                     

                       Computation: 50279 steps/s (collection: 1.851s, learning 0.104s)
             Mean action noise std: 3.38
          Mean value_function loss: 150.2259
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 56.9541
                       Mean reward: 855.63
               Mean episode length: 228.36
    Episode_Reward/reaching_object: 0.9880
     Episode_Reward/lifting_object: 165.1211
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 1.96s
                      Time elapsed: 01:02:22
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 1695/2000 [0m                     

                       Computation: 48665 steps/s (collection: 1.920s, learning 0.100s)
             Mean action noise std: 3.38
          Mean value_function loss: 116.8454
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 56.9624
                       Mean reward: 880.25
               Mean episode length: 232.71
    Episode_Reward/reaching_object: 0.9970
     Episode_Reward/lifting_object: 166.7864
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 2.02s
                      Time elapsed: 01:02:24
                               ETA: 00:11:13

################################################################################
                     [1m Learning iteration 1696/2000 [0m                     

                       Computation: 50359 steps/s (collection: 1.861s, learning 0.091s)
             Mean action noise std: 3.38
          Mean value_function loss: 109.2591
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 56.9692
                       Mean reward: 833.62
               Mean episode length: 222.26
    Episode_Reward/reaching_object: 1.0152
     Episode_Reward/lifting_object: 170.2431
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 1.95s
                      Time elapsed: 01:02:26
                               ETA: 00:11:11

################################################################################
                     [1m Learning iteration 1697/2000 [0m                     

                       Computation: 49558 steps/s (collection: 1.888s, learning 0.096s)
             Mean action noise std: 3.38
          Mean value_function loss: 122.9623
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.9753
                       Mean reward: 871.32
               Mean episode length: 232.13
    Episode_Reward/reaching_object: 1.0206
     Episode_Reward/lifting_object: 170.7789
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 1.98s
                      Time elapsed: 01:02:28
                               ETA: 00:11:08

################################################################################
                     [1m Learning iteration 1698/2000 [0m                     

                       Computation: 50666 steps/s (collection: 1.834s, learning 0.107s)
             Mean action noise std: 3.39
          Mean value_function loss: 114.7384
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 56.9837
                       Mean reward: 848.04
               Mean episode length: 225.28
    Episode_Reward/reaching_object: 1.0038
     Episode_Reward/lifting_object: 168.3176
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0615
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 1.94s
                      Time elapsed: 01:02:30
                               ETA: 00:11:06

################################################################################
                     [1m Learning iteration 1699/2000 [0m                     

                       Computation: 49286 steps/s (collection: 1.866s, learning 0.128s)
             Mean action noise std: 3.39
          Mean value_function loss: 111.6877
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.9910
                       Mean reward: 894.11
               Mean episode length: 237.11
    Episode_Reward/reaching_object: 1.0321
     Episode_Reward/lifting_object: 173.2125
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 1.99s
                      Time elapsed: 01:02:32
                               ETA: 00:11:04

################################################################################
                     [1m Learning iteration 1700/2000 [0m                     

                       Computation: 49210 steps/s (collection: 1.860s, learning 0.138s)
             Mean action noise std: 3.39
          Mean value_function loss: 112.2657
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 56.9995
                       Mean reward: 860.21
               Mean episode length: 228.60
    Episode_Reward/reaching_object: 0.9994
     Episode_Reward/lifting_object: 167.5607
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0613
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 2.00s
                      Time elapsed: 01:02:34
                               ETA: 00:11:02

################################################################################
                     [1m Learning iteration 1701/2000 [0m                     

                       Computation: 48310 steps/s (collection: 1.946s, learning 0.089s)
             Mean action noise std: 3.39
          Mean value_function loss: 117.5723
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 57.0095
                       Mean reward: 867.30
               Mean episode length: 231.02
    Episode_Reward/reaching_object: 1.0107
     Episode_Reward/lifting_object: 169.3012
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0622
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 2.03s
                      Time elapsed: 01:02:36
                               ETA: 00:10:59

################################################################################
                     [1m Learning iteration 1702/2000 [0m                     

                       Computation: 50973 steps/s (collection: 1.816s, learning 0.113s)
             Mean action noise std: 3.39
          Mean value_function loss: 119.4040
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 57.0190
                       Mean reward: 841.30
               Mean episode length: 223.68
    Episode_Reward/reaching_object: 1.0375
     Episode_Reward/lifting_object: 174.2604
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 1.93s
                      Time elapsed: 01:02:38
                               ETA: 00:10:57

################################################################################
                     [1m Learning iteration 1703/2000 [0m                     

                       Computation: 47252 steps/s (collection: 1.900s, learning 0.180s)
             Mean action noise std: 3.39
          Mean value_function loss: 123.9584
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 57.0287
                       Mean reward: 813.22
               Mean episode length: 217.26
    Episode_Reward/reaching_object: 0.9851
     Episode_Reward/lifting_object: 164.8044
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0608
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 2.08s
                      Time elapsed: 01:02:40
                               ETA: 00:10:55

################################################################################
                     [1m Learning iteration 1704/2000 [0m                     

                       Computation: 48595 steps/s (collection: 1.911s, learning 0.112s)
             Mean action noise std: 3.39
          Mean value_function loss: 110.0359
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 57.0345
                       Mean reward: 844.46
               Mean episode length: 225.78
    Episode_Reward/reaching_object: 1.0094
     Episode_Reward/lifting_object: 168.9641
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 2.02s
                      Time elapsed: 01:02:42
                               ETA: 00:10:53

################################################################################
                     [1m Learning iteration 1705/2000 [0m                     

                       Computation: 50115 steps/s (collection: 1.876s, learning 0.085s)
             Mean action noise std: 3.39
          Mean value_function loss: 278.0533
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.0416
                       Mean reward: 838.40
               Mean episode length: 223.37
    Episode_Reward/reaching_object: 1.0218
     Episode_Reward/lifting_object: 171.2393
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 1.96s
                      Time elapsed: 01:02:44
                               ETA: 00:10:50

################################################################################
                     [1m Learning iteration 1706/2000 [0m                     

                       Computation: 52079 steps/s (collection: 1.795s, learning 0.092s)
             Mean action noise std: 3.40
          Mean value_function loss: 204.9573
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.0496
                       Mean reward: 831.93
               Mean episode length: 221.54
    Episode_Reward/reaching_object: 1.0062
     Episode_Reward/lifting_object: 168.1477
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 1.89s
                      Time elapsed: 01:02:46
                               ETA: 00:10:48

################################################################################
                     [1m Learning iteration 1707/2000 [0m                     

                       Computation: 50930 steps/s (collection: 1.843s, learning 0.088s)
             Mean action noise std: 3.40
          Mean value_function loss: 100.2829
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.0674
                       Mean reward: 838.20
               Mean episode length: 223.03
    Episode_Reward/reaching_object: 1.0430
     Episode_Reward/lifting_object: 175.0794
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 1.93s
                      Time elapsed: 01:02:48
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 1708/2000 [0m                     

                       Computation: 51242 steps/s (collection: 1.823s, learning 0.095s)
             Mean action noise std: 3.40
          Mean value_function loss: 111.1779
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.0814
                       Mean reward: 871.48
               Mean episode length: 231.77
    Episode_Reward/reaching_object: 1.0252
     Episode_Reward/lifting_object: 171.5932
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 1.92s
                      Time elapsed: 01:02:50
                               ETA: 00:10:44

################################################################################
                     [1m Learning iteration 1709/2000 [0m                     

                       Computation: 50036 steps/s (collection: 1.872s, learning 0.093s)
             Mean action noise std: 3.40
          Mean value_function loss: 113.7954
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 57.0919
                       Mean reward: 855.38
               Mean episode length: 227.37
    Episode_Reward/reaching_object: 1.0334
     Episode_Reward/lifting_object: 173.6577
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 1.96s
                      Time elapsed: 01:02:52
                               ETA: 00:10:41

################################################################################
                     [1m Learning iteration 1710/2000 [0m                     

                       Computation: 50448 steps/s (collection: 1.828s, learning 0.121s)
             Mean action noise std: 3.40
          Mean value_function loss: 121.6119
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 57.0998
                       Mean reward: 900.75
               Mean episode length: 238.09
    Episode_Reward/reaching_object: 1.0117
     Episode_Reward/lifting_object: 169.5185
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 1.95s
                      Time elapsed: 01:02:54
                               ETA: 00:10:39

################################################################################
                     [1m Learning iteration 1711/2000 [0m                     

                       Computation: 50998 steps/s (collection: 1.839s, learning 0.089s)
             Mean action noise std: 3.40
          Mean value_function loss: 98.4960
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 57.1095
                       Mean reward: 896.60
               Mean episode length: 236.90
    Episode_Reward/reaching_object: 1.0368
     Episode_Reward/lifting_object: 174.0947
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 1.93s
                      Time elapsed: 01:02:56
                               ETA: 00:10:37

################################################################################
                     [1m Learning iteration 1712/2000 [0m                     

                       Computation: 50243 steps/s (collection: 1.859s, learning 0.098s)
             Mean action noise std: 3.41
          Mean value_function loss: 133.9313
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 57.1205
                       Mean reward: 880.39
               Mean episode length: 233.26
    Episode_Reward/reaching_object: 1.0134
     Episode_Reward/lifting_object: 170.1243
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 1.96s
                      Time elapsed: 01:02:58
                               ETA: 00:10:35

################################################################################
                     [1m Learning iteration 1713/2000 [0m                     

                       Computation: 51207 steps/s (collection: 1.832s, learning 0.088s)
             Mean action noise std: 3.41
          Mean value_function loss: 93.1955
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 57.1362
                       Mean reward: 877.30
               Mean episode length: 232.67
    Episode_Reward/reaching_object: 1.0424
     Episode_Reward/lifting_object: 174.6845
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0642
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 1.92s
                      Time elapsed: 01:03:00
                               ETA: 00:10:32

################################################################################
                     [1m Learning iteration 1714/2000 [0m                     

                       Computation: 51075 steps/s (collection: 1.834s, learning 0.091s)
             Mean action noise std: 3.41
          Mean value_function loss: 94.0474
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 57.1421
                       Mean reward: 866.95
               Mean episode length: 230.00
    Episode_Reward/reaching_object: 1.0453
     Episode_Reward/lifting_object: 176.1193
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 1.92s
                      Time elapsed: 01:03:01
                               ETA: 00:10:30

################################################################################
                     [1m Learning iteration 1715/2000 [0m                     

                       Computation: 50090 steps/s (collection: 1.846s, learning 0.117s)
             Mean action noise std: 3.41
          Mean value_function loss: 100.9140
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.1544
                       Mean reward: 877.85
               Mean episode length: 232.55
    Episode_Reward/reaching_object: 1.0241
     Episode_Reward/lifting_object: 172.3606
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0634
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 1.96s
                      Time elapsed: 01:03:03
                               ETA: 00:10:28

################################################################################
                     [1m Learning iteration 1716/2000 [0m                     

                       Computation: 50254 steps/s (collection: 1.861s, learning 0.095s)
             Mean action noise std: 3.41
          Mean value_function loss: 121.4753
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 57.1675
                       Mean reward: 802.54
               Mean episode length: 214.91
    Episode_Reward/reaching_object: 1.0036
     Episode_Reward/lifting_object: 168.5187
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 1.96s
                      Time elapsed: 01:03:05
                               ETA: 00:10:26

################################################################################
                     [1m Learning iteration 1717/2000 [0m                     

                       Computation: 51114 steps/s (collection: 1.807s, learning 0.116s)
             Mean action noise std: 3.42
          Mean value_function loss: 95.0312
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.1755
                       Mean reward: 863.50
               Mean episode length: 229.16
    Episode_Reward/reaching_object: 1.0412
     Episode_Reward/lifting_object: 175.3808
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0648
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 1.92s
                      Time elapsed: 01:03:07
                               ETA: 00:10:23

################################################################################
                     [1m Learning iteration 1718/2000 [0m                     

                       Computation: 49361 steps/s (collection: 1.847s, learning 0.144s)
             Mean action noise std: 3.42
          Mean value_function loss: 107.2925
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.1867
                       Mean reward: 827.55
               Mean episode length: 221.78
    Episode_Reward/reaching_object: 0.9964
     Episode_Reward/lifting_object: 166.9165
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 1.99s
                      Time elapsed: 01:03:09
                               ETA: 00:10:21

################################################################################
                     [1m Learning iteration 1719/2000 [0m                     

                       Computation: 50671 steps/s (collection: 1.846s, learning 0.094s)
             Mean action noise std: 3.42
          Mean value_function loss: 113.7209
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.1976
                       Mean reward: 909.87
               Mean episode length: 239.44
    Episode_Reward/reaching_object: 1.0317
     Episode_Reward/lifting_object: 173.4414
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0645
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 1.94s
                      Time elapsed: 01:03:11
                               ETA: 00:10:19

################################################################################
                     [1m Learning iteration 1720/2000 [0m                     

                       Computation: 52157 steps/s (collection: 1.791s, learning 0.094s)
             Mean action noise std: 3.42
          Mean value_function loss: 94.3043
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 57.2077
                       Mean reward: 902.49
               Mean episode length: 238.61
    Episode_Reward/reaching_object: 1.0364
     Episode_Reward/lifting_object: 174.0861
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 1.88s
                      Time elapsed: 01:03:13
                               ETA: 00:10:17

################################################################################
                     [1m Learning iteration 1721/2000 [0m                     

                       Computation: 51920 steps/s (collection: 1.802s, learning 0.091s)
             Mean action noise std: 3.42
          Mean value_function loss: 94.1775
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 57.2187
                       Mean reward: 887.40
               Mean episode length: 234.34
    Episode_Reward/reaching_object: 1.0241
     Episode_Reward/lifting_object: 172.1483
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 1.89s
                      Time elapsed: 01:03:15
                               ETA: 00:10:14

################################################################################
                     [1m Learning iteration 1722/2000 [0m                     

                       Computation: 51318 steps/s (collection: 1.823s, learning 0.093s)
             Mean action noise std: 3.42
          Mean value_function loss: 162.2396
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.2271
                       Mean reward: 851.23
               Mean episode length: 226.62
    Episode_Reward/reaching_object: 1.0253
     Episode_Reward/lifting_object: 172.3696
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 1.92s
                      Time elapsed: 01:03:17
                               ETA: 00:10:12

################################################################################
                     [1m Learning iteration 1723/2000 [0m                     

                       Computation: 51419 steps/s (collection: 1.819s, learning 0.092s)
             Mean action noise std: 3.42
          Mean value_function loss: 101.1786
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.2347
                       Mean reward: 871.41
               Mean episode length: 231.80
    Episode_Reward/reaching_object: 1.0235
     Episode_Reward/lifting_object: 171.9436
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0642
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 1.91s
                      Time elapsed: 01:03:19
                               ETA: 00:10:10

################################################################################
                     [1m Learning iteration 1724/2000 [0m                     

                       Computation: 50463 steps/s (collection: 1.855s, learning 0.093s)
             Mean action noise std: 3.43
          Mean value_function loss: 95.4804
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.2436
                       Mean reward: 859.85
               Mean episode length: 228.13
    Episode_Reward/reaching_object: 1.0129
     Episode_Reward/lifting_object: 170.4878
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 1.95s
                      Time elapsed: 01:03:21
                               ETA: 00:10:08

################################################################################
                     [1m Learning iteration 1725/2000 [0m                     

                       Computation: 49594 steps/s (collection: 1.853s, learning 0.129s)
             Mean action noise std: 3.43
          Mean value_function loss: 124.8991
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 57.2518
                       Mean reward: 881.13
               Mean episode length: 233.72
    Episode_Reward/reaching_object: 1.0214
     Episode_Reward/lifting_object: 171.2641
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0642
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 1.98s
                      Time elapsed: 01:03:23
                               ETA: 00:10:05

################################################################################
                     [1m Learning iteration 1726/2000 [0m                     

                       Computation: 50372 steps/s (collection: 1.851s, learning 0.100s)
             Mean action noise std: 3.43
          Mean value_function loss: 151.6751
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.2612
                       Mean reward: 838.17
               Mean episode length: 222.66
    Episode_Reward/reaching_object: 1.0028
     Episode_Reward/lifting_object: 168.6466
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 1.95s
                      Time elapsed: 01:03:25
                               ETA: 00:10:03

################################################################################
                     [1m Learning iteration 1727/2000 [0m                     

                       Computation: 50341 steps/s (collection: 1.852s, learning 0.101s)
             Mean action noise std: 3.43
          Mean value_function loss: 118.9657
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.2718
                       Mean reward: 830.01
               Mean episode length: 220.41
    Episode_Reward/reaching_object: 1.0216
     Episode_Reward/lifting_object: 171.5648
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 1.95s
                      Time elapsed: 01:03:27
                               ETA: 00:10:01

################################################################################
                     [1m Learning iteration 1728/2000 [0m                     

                       Computation: 50888 steps/s (collection: 1.823s, learning 0.109s)
             Mean action noise std: 3.43
          Mean value_function loss: 100.2125
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 57.2808
                       Mean reward: 879.48
               Mean episode length: 232.46
    Episode_Reward/reaching_object: 1.0017
     Episode_Reward/lifting_object: 168.3060
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 1.93s
                      Time elapsed: 01:03:29
                               ETA: 00:09:59

################################################################################
                     [1m Learning iteration 1729/2000 [0m                     

                       Computation: 50002 steps/s (collection: 1.856s, learning 0.110s)
             Mean action noise std: 3.43
          Mean value_function loss: 120.3211
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.2888
                       Mean reward: 876.59
               Mean episode length: 232.03
    Episode_Reward/reaching_object: 1.0128
     Episode_Reward/lifting_object: 170.2632
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 1.97s
                      Time elapsed: 01:03:31
                               ETA: 00:09:56

################################################################################
                     [1m Learning iteration 1730/2000 [0m                     

                       Computation: 51285 steps/s (collection: 1.822s, learning 0.094s)
             Mean action noise std: 3.43
          Mean value_function loss: 148.2235
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.2978
                       Mean reward: 875.07
               Mean episode length: 232.33
    Episode_Reward/reaching_object: 1.0343
     Episode_Reward/lifting_object: 173.8166
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 1.92s
                      Time elapsed: 01:03:32
                               ETA: 00:09:54

################################################################################
                     [1m Learning iteration 1731/2000 [0m                     

                       Computation: 49803 steps/s (collection: 1.872s, learning 0.102s)
             Mean action noise std: 3.44
          Mean value_function loss: 133.6918
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 57.3050
                       Mean reward: 830.17
               Mean episode length: 220.64
    Episode_Reward/reaching_object: 0.9862
     Episode_Reward/lifting_object: 165.5437
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 1.97s
                      Time elapsed: 01:03:34
                               ETA: 00:09:52

################################################################################
                     [1m Learning iteration 1732/2000 [0m                     

                       Computation: 50733 steps/s (collection: 1.852s, learning 0.086s)
             Mean action noise std: 3.44
          Mean value_function loss: 137.6197
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 57.3192
                       Mean reward: 822.75
               Mean episode length: 219.24
    Episode_Reward/reaching_object: 1.0099
     Episode_Reward/lifting_object: 169.5698
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 1.94s
                      Time elapsed: 01:03:36
                               ETA: 00:09:50

################################################################################
                     [1m Learning iteration 1733/2000 [0m                     

                       Computation: 49882 steps/s (collection: 1.861s, learning 0.110s)
             Mean action noise std: 3.44
          Mean value_function loss: 137.4286
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 57.3301
                       Mean reward: 788.56
               Mean episode length: 211.07
    Episode_Reward/reaching_object: 1.0326
     Episode_Reward/lifting_object: 173.5566
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 1.97s
                      Time elapsed: 01:03:38
                               ETA: 00:09:48

################################################################################
                     [1m Learning iteration 1734/2000 [0m                     

                       Computation: 49292 steps/s (collection: 1.841s, learning 0.153s)
             Mean action noise std: 3.44
          Mean value_function loss: 101.7822
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.3354
                       Mean reward: 896.35
               Mean episode length: 236.96
    Episode_Reward/reaching_object: 1.0229
     Episode_Reward/lifting_object: 171.6363
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0648
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 1.99s
                      Time elapsed: 01:03:40
                               ETA: 00:09:45

################################################################################
                     [1m Learning iteration 1735/2000 [0m                     

                       Computation: 49141 steps/s (collection: 1.844s, learning 0.157s)
             Mean action noise std: 3.44
          Mean value_function loss: 119.3189
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.3395
                       Mean reward: 828.41
               Mean episode length: 221.92
    Episode_Reward/reaching_object: 0.9883
     Episode_Reward/lifting_object: 165.0662
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 2.00s
                      Time elapsed: 01:03:42
                               ETA: 00:09:43

################################################################################
                     [1m Learning iteration 1736/2000 [0m                     

                       Computation: 48565 steps/s (collection: 1.840s, learning 0.184s)
             Mean action noise std: 3.44
          Mean value_function loss: 94.2009
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 57.3441
                       Mean reward: 840.29
               Mean episode length: 223.79
    Episode_Reward/reaching_object: 1.0051
     Episode_Reward/lifting_object: 168.6126
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 2.02s
                      Time elapsed: 01:03:44
                               ETA: 00:09:41

################################################################################
                     [1m Learning iteration 1737/2000 [0m                     

                       Computation: 50133 steps/s (collection: 1.839s, learning 0.122s)
             Mean action noise std: 3.44
          Mean value_function loss: 95.3104
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.3506
                       Mean reward: 880.95
               Mean episode length: 233.47
    Episode_Reward/reaching_object: 1.0346
     Episode_Reward/lifting_object: 174.1532
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 1.96s
                      Time elapsed: 01:03:46
                               ETA: 00:09:39

################################################################################
                     [1m Learning iteration 1738/2000 [0m                     

                       Computation: 50985 steps/s (collection: 1.823s, learning 0.105s)
             Mean action noise std: 3.44
          Mean value_function loss: 113.4980
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.3597
                       Mean reward: 873.22
               Mean episode length: 231.06
    Episode_Reward/reaching_object: 1.0439
     Episode_Reward/lifting_object: 175.6969
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0660
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 1.93s
                      Time elapsed: 01:03:48
                               ETA: 00:09:36

################################################################################
                     [1m Learning iteration 1739/2000 [0m                     

                       Computation: 49502 steps/s (collection: 1.865s, learning 0.121s)
             Mean action noise std: 3.45
          Mean value_function loss: 106.7699
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.3735
                       Mean reward: 900.23
               Mean episode length: 238.50
    Episode_Reward/reaching_object: 1.0360
     Episode_Reward/lifting_object: 174.0057
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0657
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 1.99s
                      Time elapsed: 01:03:50
                               ETA: 00:09:34

################################################################################
                     [1m Learning iteration 1740/2000 [0m                     

                       Computation: 49971 steps/s (collection: 1.880s, learning 0.088s)
             Mean action noise std: 3.45
          Mean value_function loss: 115.3311
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.3854
                       Mean reward: 854.29
               Mean episode length: 226.78
    Episode_Reward/reaching_object: 1.0276
     Episode_Reward/lifting_object: 172.5149
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 1.97s
                      Time elapsed: 01:03:52
                               ETA: 00:09:32

################################################################################
                     [1m Learning iteration 1741/2000 [0m                     

                       Computation: 50987 steps/s (collection: 1.828s, learning 0.100s)
             Mean action noise std: 3.45
          Mean value_function loss: 131.1786
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.4006
                       Mean reward: 892.13
               Mean episode length: 236.62
    Episode_Reward/reaching_object: 1.0165
     Episode_Reward/lifting_object: 170.8674
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 1.93s
                      Time elapsed: 01:03:54
                               ETA: 00:09:30

################################################################################
                     [1m Learning iteration 1742/2000 [0m                     

                       Computation: 45382 steps/s (collection: 1.998s, learning 0.168s)
             Mean action noise std: 3.45
          Mean value_function loss: 126.3158
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.4175
                       Mean reward: 833.91
               Mean episode length: 222.88
    Episode_Reward/reaching_object: 1.0371
     Episode_Reward/lifting_object: 174.1117
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0659
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 2.17s
                      Time elapsed: 01:03:56
                               ETA: 00:09:27

################################################################################
                     [1m Learning iteration 1743/2000 [0m                     

                       Computation: 49933 steps/s (collection: 1.876s, learning 0.093s)
             Mean action noise std: 3.45
          Mean value_function loss: 130.5803
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.4261
                       Mean reward: 818.68
               Mean episode length: 218.75
    Episode_Reward/reaching_object: 0.9778
     Episode_Reward/lifting_object: 163.9162
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 1.97s
                      Time elapsed: 01:03:58
                               ETA: 00:09:25

################################################################################
                     [1m Learning iteration 1744/2000 [0m                     

                       Computation: 50529 steps/s (collection: 1.855s, learning 0.091s)
             Mean action noise std: 3.46
          Mean value_function loss: 121.3746
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.4336
                       Mean reward: 871.41
               Mean episode length: 230.56
    Episode_Reward/reaching_object: 1.0051
     Episode_Reward/lifting_object: 168.5852
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0642
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 1.95s
                      Time elapsed: 01:04:00
                               ETA: 00:09:23

################################################################################
                     [1m Learning iteration 1745/2000 [0m                     

                       Computation: 49972 steps/s (collection: 1.874s, learning 0.093s)
             Mean action noise std: 3.46
          Mean value_function loss: 89.7906
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 57.4441
                       Mean reward: 889.91
               Mean episode length: 234.75
    Episode_Reward/reaching_object: 0.9975
     Episode_Reward/lifting_object: 167.4965
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 1.97s
                      Time elapsed: 01:04:02
                               ETA: 00:09:21

################################################################################
                     [1m Learning iteration 1746/2000 [0m                     

                       Computation: 48754 steps/s (collection: 1.921s, learning 0.096s)
             Mean action noise std: 3.46
          Mean value_function loss: 84.4611
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.4569
                       Mean reward: 877.20
               Mean episode length: 232.18
    Episode_Reward/reaching_object: 1.0236
     Episode_Reward/lifting_object: 172.2114
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 2.02s
                      Time elapsed: 01:04:04
                               ETA: 00:09:18

################################################################################
                     [1m Learning iteration 1747/2000 [0m                     

                       Computation: 49409 steps/s (collection: 1.901s, learning 0.089s)
             Mean action noise std: 3.46
          Mean value_function loss: 100.8372
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 57.4671
                       Mean reward: 887.25
               Mean episode length: 234.74
    Episode_Reward/reaching_object: 1.0283
     Episode_Reward/lifting_object: 172.8258
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0661
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 1.99s
                      Time elapsed: 01:04:06
                               ETA: 00:09:16

################################################################################
                     [1m Learning iteration 1748/2000 [0m                     

                       Computation: 51091 steps/s (collection: 1.827s, learning 0.098s)
             Mean action noise std: 3.46
          Mean value_function loss: 98.9924
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 57.4735
                       Mean reward: 888.66
               Mean episode length: 235.12
    Episode_Reward/reaching_object: 1.0140
     Episode_Reward/lifting_object: 170.1981
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 1.92s
                      Time elapsed: 01:04:08
                               ETA: 00:09:14

################################################################################
                     [1m Learning iteration 1749/2000 [0m                     

                       Computation: 48505 steps/s (collection: 1.892s, learning 0.135s)
             Mean action noise std: 3.46
          Mean value_function loss: 101.4174
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 57.4831
                       Mean reward: 845.41
               Mean episode length: 226.27
    Episode_Reward/reaching_object: 1.0176
     Episode_Reward/lifting_object: 171.2316
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 2.03s
                      Time elapsed: 01:04:10
                               ETA: 00:09:12

################################################################################
                     [1m Learning iteration 1750/2000 [0m                     

                       Computation: 47682 steps/s (collection: 1.961s, learning 0.101s)
             Mean action noise std: 3.47
          Mean value_function loss: 109.4014
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.4892
                       Mean reward: 890.27
               Mean episode length: 235.44
    Episode_Reward/reaching_object: 1.0163
     Episode_Reward/lifting_object: 170.8752
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0657
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 2.06s
                      Time elapsed: 01:04:12
                               ETA: 00:09:10

################################################################################
                     [1m Learning iteration 1751/2000 [0m                     

                       Computation: 48586 steps/s (collection: 1.906s, learning 0.117s)
             Mean action noise std: 3.47
          Mean value_function loss: 138.8800
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 57.5002
                       Mean reward: 836.01
               Mean episode length: 223.96
    Episode_Reward/reaching_object: 1.0095
     Episode_Reward/lifting_object: 169.6866
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 2.02s
                      Time elapsed: 01:04:14
                               ETA: 00:09:07

################################################################################
                     [1m Learning iteration 1752/2000 [0m                     

                       Computation: 49875 steps/s (collection: 1.879s, learning 0.092s)
             Mean action noise std: 3.47
          Mean value_function loss: 93.7332
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.5109
                       Mean reward: 892.77
               Mean episode length: 235.93
    Episode_Reward/reaching_object: 1.0244
     Episode_Reward/lifting_object: 172.3633
      Episode_Reward/object_height: 0.0272
        Episode_Reward/action_rate: -0.0663
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 1.97s
                      Time elapsed: 01:04:16
                               ETA: 00:09:05

################################################################################
                     [1m Learning iteration 1753/2000 [0m                     

                       Computation: 49958 steps/s (collection: 1.858s, learning 0.110s)
             Mean action noise std: 3.47
          Mean value_function loss: 110.5218
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.5191
                       Mean reward: 835.19
               Mean episode length: 222.35
    Episode_Reward/reaching_object: 1.0262
     Episode_Reward/lifting_object: 172.8453
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.0663
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 1.97s
                      Time elapsed: 01:04:18
                               ETA: 00:09:03

################################################################################
                     [1m Learning iteration 1754/2000 [0m                     

                       Computation: 49690 steps/s (collection: 1.885s, learning 0.093s)
             Mean action noise std: 3.47
          Mean value_function loss: 109.3726
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 57.5279
                       Mean reward: 888.55
               Mean episode length: 234.91
    Episode_Reward/reaching_object: 1.0123
     Episode_Reward/lifting_object: 170.1545
      Episode_Reward/object_height: 0.0275
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 1.98s
                      Time elapsed: 01:04:20
                               ETA: 00:09:01

################################################################################
                     [1m Learning iteration 1755/2000 [0m                     

                       Computation: 50293 steps/s (collection: 1.841s, learning 0.114s)
             Mean action noise std: 3.47
          Mean value_function loss: 90.1551
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.5374
                       Mean reward: 868.20
               Mean episode length: 230.67
    Episode_Reward/reaching_object: 1.0169
     Episode_Reward/lifting_object: 171.0125
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.0660
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 1.95s
                      Time elapsed: 01:04:22
                               ETA: 00:08:58

################################################################################
                     [1m Learning iteration 1756/2000 [0m                     

                       Computation: 49931 steps/s (collection: 1.883s, learning 0.086s)
             Mean action noise std: 3.47
          Mean value_function loss: 107.1967
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 57.5482
                       Mean reward: 849.02
               Mean episode length: 226.11
    Episode_Reward/reaching_object: 1.0298
     Episode_Reward/lifting_object: 173.4561
      Episode_Reward/object_height: 0.0279
        Episode_Reward/action_rate: -0.0669
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 1.97s
                      Time elapsed: 01:04:24
                               ETA: 00:08:56

################################################################################
                     [1m Learning iteration 1757/2000 [0m                     

                       Computation: 49534 steps/s (collection: 1.881s, learning 0.104s)
             Mean action noise std: 3.48
          Mean value_function loss: 95.2068
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 57.5552
                       Mean reward: 854.17
               Mean episode length: 227.89
    Episode_Reward/reaching_object: 1.0105
     Episode_Reward/lifting_object: 169.7888
      Episode_Reward/object_height: 0.0271
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 1.98s
                      Time elapsed: 01:04:26
                               ETA: 00:08:54

################################################################################
                     [1m Learning iteration 1758/2000 [0m                     

                       Computation: 49861 steps/s (collection: 1.861s, learning 0.110s)
             Mean action noise std: 3.48
          Mean value_function loss: 130.2749
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 57.5649
                       Mean reward: 894.00
               Mean episode length: 236.12
    Episode_Reward/reaching_object: 1.0255
     Episode_Reward/lifting_object: 173.0824
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0667
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 1.97s
                      Time elapsed: 01:04:28
                               ETA: 00:08:52

################################################################################
                     [1m Learning iteration 1759/2000 [0m                     

                       Computation: 48579 steps/s (collection: 1.927s, learning 0.097s)
             Mean action noise std: 3.48
          Mean value_function loss: 141.1655
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 57.5729
                       Mean reward: 804.57
               Mean episode length: 215.43
    Episode_Reward/reaching_object: 0.9986
     Episode_Reward/lifting_object: 167.7943
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0650
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 2.02s
                      Time elapsed: 01:04:30
                               ETA: 00:08:50

################################################################################
                     [1m Learning iteration 1760/2000 [0m                     

                       Computation: 44968 steps/s (collection: 2.025s, learning 0.161s)
             Mean action noise std: 3.48
          Mean value_function loss: 140.5232
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 57.5836
                       Mean reward: 846.42
               Mean episode length: 225.09
    Episode_Reward/reaching_object: 0.9896
     Episode_Reward/lifting_object: 166.7382
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 2.19s
                      Time elapsed: 01:04:32
                               ETA: 00:08:47

################################################################################
                     [1m Learning iteration 1761/2000 [0m                     

                       Computation: 47293 steps/s (collection: 1.979s, learning 0.100s)
             Mean action noise std: 3.48
          Mean value_function loss: 104.2716
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 57.5918
                       Mean reward: 887.47
               Mean episode length: 234.48
    Episode_Reward/reaching_object: 1.0271
     Episode_Reward/lifting_object: 173.4699
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0668
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 2.08s
                      Time elapsed: 01:04:34
                               ETA: 00:08:45

################################################################################
                     [1m Learning iteration 1762/2000 [0m                     

                       Computation: 49717 steps/s (collection: 1.885s, learning 0.092s)
             Mean action noise std: 3.48
          Mean value_function loss: 103.9757
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.5990
                       Mean reward: 876.26
               Mean episode length: 232.87
    Episode_Reward/reaching_object: 1.0037
     Episode_Reward/lifting_object: 168.8494
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0654
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 1.98s
                      Time elapsed: 01:04:36
                               ETA: 00:08:43

################################################################################
                     [1m Learning iteration 1763/2000 [0m                     

                       Computation: 50329 steps/s (collection: 1.869s, learning 0.085s)
             Mean action noise std: 3.48
          Mean value_function loss: 118.7386
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 57.6087
                       Mean reward: 900.37
               Mean episode length: 237.94
    Episode_Reward/reaching_object: 1.0268
     Episode_Reward/lifting_object: 173.4760
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0669
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 1.95s
                      Time elapsed: 01:04:38
                               ETA: 00:08:41

################################################################################
                     [1m Learning iteration 1764/2000 [0m                     

                       Computation: 50447 steps/s (collection: 1.844s, learning 0.105s)
             Mean action noise std: 3.48
          Mean value_function loss: 98.1506
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 57.6131
                       Mean reward: 884.08
               Mean episode length: 233.51
    Episode_Reward/reaching_object: 1.0402
     Episode_Reward/lifting_object: 176.2833
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 1.95s
                      Time elapsed: 01:04:40
                               ETA: 00:08:38

################################################################################
                     [1m Learning iteration 1765/2000 [0m                     

                       Computation: 50329 steps/s (collection: 1.868s, learning 0.085s)
             Mean action noise std: 3.49
          Mean value_function loss: 122.4202
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.6170
                       Mean reward: 820.79
               Mean episode length: 219.26
    Episode_Reward/reaching_object: 0.9830
     Episode_Reward/lifting_object: 165.5832
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 1.95s
                      Time elapsed: 01:04:42
                               ETA: 00:08:36

################################################################################
                     [1m Learning iteration 1766/2000 [0m                     

                       Computation: 49907 steps/s (collection: 1.878s, learning 0.092s)
             Mean action noise std: 3.49
          Mean value_function loss: 107.1678
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 57.6243
                       Mean reward: 922.64
               Mean episode length: 243.38
    Episode_Reward/reaching_object: 1.0323
     Episode_Reward/lifting_object: 174.8163
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0672
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 1.97s
                      Time elapsed: 01:04:44
                               ETA: 00:08:34

################################################################################
                     [1m Learning iteration 1767/2000 [0m                     

                       Computation: 49622 steps/s (collection: 1.888s, learning 0.093s)
             Mean action noise std: 3.49
          Mean value_function loss: 111.1122
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.6334
                       Mean reward: 859.28
               Mean episode length: 228.68
    Episode_Reward/reaching_object: 1.0144
     Episode_Reward/lifting_object: 171.1914
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0662
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 1.98s
                      Time elapsed: 01:04:46
                               ETA: 00:08:32

################################################################################
                     [1m Learning iteration 1768/2000 [0m                     

                       Computation: 49287 steps/s (collection: 1.895s, learning 0.099s)
             Mean action noise std: 3.49
          Mean value_function loss: 139.4115
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.6440
                       Mean reward: 844.75
               Mean episode length: 224.61
    Episode_Reward/reaching_object: 1.0331
     Episode_Reward/lifting_object: 174.9282
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0675
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 1.99s
                      Time elapsed: 01:04:48
                               ETA: 00:08:29

################################################################################
                     [1m Learning iteration 1769/2000 [0m                     

                       Computation: 51134 steps/s (collection: 1.829s, learning 0.094s)
             Mean action noise std: 3.49
          Mean value_function loss: 103.3109
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.6503
                       Mean reward: 867.63
               Mean episode length: 230.18
    Episode_Reward/reaching_object: 0.9960
     Episode_Reward/lifting_object: 168.0762
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0654
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 1.92s
                      Time elapsed: 01:04:50
                               ETA: 00:08:27

################################################################################
                     [1m Learning iteration 1770/2000 [0m                     

                       Computation: 50792 steps/s (collection: 1.829s, learning 0.106s)
             Mean action noise std: 3.49
          Mean value_function loss: 115.7587
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.6575
                       Mean reward: 829.02
               Mean episode length: 220.78
    Episode_Reward/reaching_object: 1.0131
     Episode_Reward/lifting_object: 171.0852
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0663
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 1.94s
                      Time elapsed: 01:04:52
                               ETA: 00:08:25

################################################################################
                     [1m Learning iteration 1771/2000 [0m                     

                       Computation: 50400 steps/s (collection: 1.854s, learning 0.096s)
             Mean action noise std: 3.49
          Mean value_function loss: 120.9058
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 57.6683
                       Mean reward: 839.85
               Mean episode length: 224.14
    Episode_Reward/reaching_object: 1.0282
     Episode_Reward/lifting_object: 173.9047
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0676
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 1.95s
                      Time elapsed: 01:04:54
                               ETA: 00:08:23

################################################################################
                     [1m Learning iteration 1772/2000 [0m                     

                       Computation: 50193 steps/s (collection: 1.856s, learning 0.102s)
             Mean action noise std: 3.50
          Mean value_function loss: 140.2156
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.6750
                       Mean reward: 782.03
               Mean episode length: 210.14
    Episode_Reward/reaching_object: 0.9933
     Episode_Reward/lifting_object: 167.2349
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 1.96s
                      Time elapsed: 01:04:56
                               ETA: 00:08:21

################################################################################
                     [1m Learning iteration 1773/2000 [0m                     

                       Computation: 50071 steps/s (collection: 1.870s, learning 0.094s)
             Mean action noise std: 3.50
          Mean value_function loss: 146.1096
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.6914
                       Mean reward: 780.82
               Mean episode length: 209.62
    Episode_Reward/reaching_object: 0.9876
     Episode_Reward/lifting_object: 166.3410
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0651
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 1.96s
                      Time elapsed: 01:04:58
                               ETA: 00:08:18

################################################################################
                     [1m Learning iteration 1774/2000 [0m                     

                       Computation: 48730 steps/s (collection: 1.892s, learning 0.126s)
             Mean action noise std: 3.50
          Mean value_function loss: 136.7767
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.7032
                       Mean reward: 857.78
               Mean episode length: 227.89
    Episode_Reward/reaching_object: 1.0071
     Episode_Reward/lifting_object: 169.6091
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0662
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 2.02s
                      Time elapsed: 01:05:00
                               ETA: 00:08:16

################################################################################
                     [1m Learning iteration 1775/2000 [0m                     

                       Computation: 47294 steps/s (collection: 1.966s, learning 0.112s)
             Mean action noise std: 3.50
          Mean value_function loss: 120.2472
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.7139
                       Mean reward: 813.33
               Mean episode length: 217.88
    Episode_Reward/reaching_object: 0.9969
     Episode_Reward/lifting_object: 167.4050
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0657
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 2.08s
                      Time elapsed: 01:05:02
                               ETA: 00:08:14

################################################################################
                     [1m Learning iteration 1776/2000 [0m                     

                       Computation: 48515 steps/s (collection: 1.930s, learning 0.096s)
             Mean action noise std: 3.50
          Mean value_function loss: 117.1631
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.7257
                       Mean reward: 811.15
               Mean episode length: 217.29
    Episode_Reward/reaching_object: 1.0156
     Episode_Reward/lifting_object: 170.9651
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0669
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 2.03s
                      Time elapsed: 01:05:04
                               ETA: 00:08:12

################################################################################
                     [1m Learning iteration 1777/2000 [0m                     

                       Computation: 49734 steps/s (collection: 1.874s, learning 0.103s)
             Mean action noise std: 3.51
          Mean value_function loss: 120.2944
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.7379
                       Mean reward: 809.84
               Mean episode length: 216.32
    Episode_Reward/reaching_object: 1.0048
     Episode_Reward/lifting_object: 169.1159
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0666
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 1.98s
                      Time elapsed: 01:05:06
                               ETA: 00:08:09

################################################################################
                     [1m Learning iteration 1778/2000 [0m                     

                       Computation: 50208 steps/s (collection: 1.872s, learning 0.086s)
             Mean action noise std: 3.51
          Mean value_function loss: 110.8702
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 57.7532
                       Mean reward: 904.30
               Mean episode length: 238.30
    Episode_Reward/reaching_object: 1.0230
     Episode_Reward/lifting_object: 172.5091
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0674
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 1.96s
                      Time elapsed: 01:05:08
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 1779/2000 [0m                     

                       Computation: 46269 steps/s (collection: 1.986s, learning 0.139s)
             Mean action noise std: 3.51
          Mean value_function loss: 160.3910
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 57.7656
                       Mean reward: 837.11
               Mean episode length: 222.23
    Episode_Reward/reaching_object: 1.0023
     Episode_Reward/lifting_object: 168.8458
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 2.12s
                      Time elapsed: 01:05:10
                               ETA: 00:08:05

################################################################################
                     [1m Learning iteration 1780/2000 [0m                     

                       Computation: 48495 steps/s (collection: 1.883s, learning 0.144s)
             Mean action noise std: 3.51
          Mean value_function loss: 156.9622
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 57.7694
                       Mean reward: 855.24
               Mean episode length: 226.57
    Episode_Reward/reaching_object: 1.0072
     Episode_Reward/lifting_object: 169.4388
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0667
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 2.03s
                      Time elapsed: 01:05:12
                               ETA: 00:08:03

################################################################################
                     [1m Learning iteration 1781/2000 [0m                     

                       Computation: 47158 steps/s (collection: 1.989s, learning 0.095s)
             Mean action noise std: 3.51
          Mean value_function loss: 131.3021
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 57.7719
                       Mean reward: 856.48
               Mean episode length: 226.95
    Episode_Reward/reaching_object: 0.9844
     Episode_Reward/lifting_object: 165.5727
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 2.08s
                      Time elapsed: 01:05:14
                               ETA: 00:08:01

################################################################################
                     [1m Learning iteration 1782/2000 [0m                     

                       Computation: 49898 steps/s (collection: 1.881s, learning 0.089s)
             Mean action noise std: 3.51
          Mean value_function loss: 121.2951
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 57.7751
                       Mean reward: 879.99
               Mean episode length: 232.76
    Episode_Reward/reaching_object: 1.0254
     Episode_Reward/lifting_object: 173.0576
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 1.97s
                      Time elapsed: 01:05:16
                               ETA: 00:07:58

################################################################################
                     [1m Learning iteration 1783/2000 [0m                     

                       Computation: 49272 steps/s (collection: 1.902s, learning 0.093s)
             Mean action noise std: 3.51
          Mean value_function loss: 126.0038
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.7780
                       Mean reward: 814.88
               Mean episode length: 217.02
    Episode_Reward/reaching_object: 0.9972
     Episode_Reward/lifting_object: 167.6493
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 2.00s
                      Time elapsed: 01:05:18
                               ETA: 00:07:56

################################################################################
                     [1m Learning iteration 1784/2000 [0m                     

                       Computation: 49653 steps/s (collection: 1.881s, learning 0.099s)
             Mean action noise std: 3.51
          Mean value_function loss: 126.6410
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 57.7828
                       Mean reward: 819.34
               Mean episode length: 217.65
    Episode_Reward/reaching_object: 1.0209
     Episode_Reward/lifting_object: 171.9273
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0680
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 1.98s
                      Time elapsed: 01:05:20
                               ETA: 00:07:54

################################################################################
                     [1m Learning iteration 1785/2000 [0m                     

                       Computation: 47967 steps/s (collection: 1.949s, learning 0.101s)
             Mean action noise std: 3.51
          Mean value_function loss: 126.1424
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.7876
                       Mean reward: 860.31
               Mean episode length: 227.65
    Episode_Reward/reaching_object: 1.0181
     Episode_Reward/lifting_object: 171.3001
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0680
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 2.05s
                      Time elapsed: 01:05:22
                               ETA: 00:07:52

################################################################################
                     [1m Learning iteration 1786/2000 [0m                     

                       Computation: 49572 steps/s (collection: 1.895s, learning 0.088s)
             Mean action noise std: 3.51
          Mean value_function loss: 90.9632
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.7932
                       Mean reward: 853.17
               Mean episode length: 226.91
    Episode_Reward/reaching_object: 1.0045
     Episode_Reward/lifting_object: 168.9067
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0673
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 1.98s
                      Time elapsed: 01:05:24
                               ETA: 00:07:49

################################################################################
                     [1m Learning iteration 1787/2000 [0m                     

                       Computation: 49780 steps/s (collection: 1.882s, learning 0.093s)
             Mean action noise std: 3.52
          Mean value_function loss: 121.0960
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 57.7982
                       Mean reward: 866.44
               Mean episode length: 229.49
    Episode_Reward/reaching_object: 1.0282
     Episode_Reward/lifting_object: 173.0238
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0686
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 1.97s
                      Time elapsed: 01:05:26
                               ETA: 00:07:47

################################################################################
                     [1m Learning iteration 1788/2000 [0m                     

                       Computation: 50087 steps/s (collection: 1.861s, learning 0.102s)
             Mean action noise std: 3.52
          Mean value_function loss: 108.2184
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.8111
                       Mean reward: 830.77
               Mean episode length: 221.33
    Episode_Reward/reaching_object: 0.9998
     Episode_Reward/lifting_object: 167.8953
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 1.96s
                      Time elapsed: 01:05:28
                               ETA: 00:07:45

################################################################################
                     [1m Learning iteration 1789/2000 [0m                     

                       Computation: 47294 steps/s (collection: 1.943s, learning 0.135s)
             Mean action noise std: 3.52
          Mean value_function loss: 110.8392
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 57.8236
                       Mean reward: 846.31
               Mean episode length: 225.15
    Episode_Reward/reaching_object: 1.0388
     Episode_Reward/lifting_object: 174.8911
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0693
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 2.08s
                      Time elapsed: 01:05:30
                               ETA: 00:07:43

################################################################################
                     [1m Learning iteration 1790/2000 [0m                     

                       Computation: 47764 steps/s (collection: 1.942s, learning 0.117s)
             Mean action noise std: 3.52
          Mean value_function loss: 113.5668
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.8366
                       Mean reward: 854.23
               Mean episode length: 227.17
    Episode_Reward/reaching_object: 1.0028
     Episode_Reward/lifting_object: 168.0444
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0672
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 2.06s
                      Time elapsed: 01:05:32
                               ETA: 00:07:41

################################################################################
                     [1m Learning iteration 1791/2000 [0m                     

                       Computation: 45589 steps/s (collection: 2.068s, learning 0.088s)
             Mean action noise std: 3.52
          Mean value_function loss: 146.0289
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 57.8486
                       Mean reward: 829.14
               Mean episode length: 221.20
    Episode_Reward/reaching_object: 1.0112
     Episode_Reward/lifting_object: 169.7805
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 2.16s
                      Time elapsed: 01:05:34
                               ETA: 00:07:38

################################################################################
                     [1m Learning iteration 1792/2000 [0m                     

                       Computation: 50189 steps/s (collection: 1.867s, learning 0.092s)
             Mean action noise std: 3.52
          Mean value_function loss: 124.2459
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 57.8564
                       Mean reward: 823.82
               Mean episode length: 219.24
    Episode_Reward/reaching_object: 0.9877
     Episode_Reward/lifting_object: 165.6810
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0661
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 1.96s
                      Time elapsed: 01:05:36
                               ETA: 00:07:36

################################################################################
                     [1m Learning iteration 1793/2000 [0m                     

                       Computation: 51113 steps/s (collection: 1.837s, learning 0.086s)
             Mean action noise std: 3.53
          Mean value_function loss: 130.3197
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.8642
                       Mean reward: 821.65
               Mean episode length: 218.79
    Episode_Reward/reaching_object: 1.0155
     Episode_Reward/lifting_object: 170.2992
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 1.92s
                      Time elapsed: 01:05:38
                               ETA: 00:07:34

################################################################################
                     [1m Learning iteration 1794/2000 [0m                     

                       Computation: 50626 steps/s (collection: 1.843s, learning 0.099s)
             Mean action noise std: 3.53
          Mean value_function loss: 100.8066
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 57.8747
                       Mean reward: 862.61
               Mean episode length: 229.62
    Episode_Reward/reaching_object: 0.9960
     Episode_Reward/lifting_object: 166.3268
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0665
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 1.94s
                      Time elapsed: 01:05:40
                               ETA: 00:07:32

################################################################################
                     [1m Learning iteration 1795/2000 [0m                     

                       Computation: 50985 steps/s (collection: 1.843s, learning 0.085s)
             Mean action noise std: 3.53
          Mean value_function loss: 135.4918
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.8804
                       Mean reward: 851.98
               Mean episode length: 226.32
    Episode_Reward/reaching_object: 1.0157
     Episode_Reward/lifting_object: 170.4535
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 1.93s
                      Time elapsed: 01:05:42
                               ETA: 00:07:30

################################################################################
                     [1m Learning iteration 1796/2000 [0m                     

                       Computation: 49469 steps/s (collection: 1.841s, learning 0.146s)
             Mean action noise std: 3.53
          Mean value_function loss: 140.9117
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.8863
                       Mean reward: 857.31
               Mean episode length: 226.42
    Episode_Reward/reaching_object: 1.0083
     Episode_Reward/lifting_object: 168.9762
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 1.99s
                      Time elapsed: 01:05:44
                               ETA: 00:07:27

################################################################################
                     [1m Learning iteration 1797/2000 [0m                     

                       Computation: 48390 steps/s (collection: 1.922s, learning 0.110s)
             Mean action noise std: 3.53
          Mean value_function loss: 123.5719
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 57.8927
                       Mean reward: 849.61
               Mean episode length: 225.67
    Episode_Reward/reaching_object: 1.0017
     Episode_Reward/lifting_object: 167.5531
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0672
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 2.03s
                      Time elapsed: 01:05:46
                               ETA: 00:07:25

################################################################################
                     [1m Learning iteration 1798/2000 [0m                     

                       Computation: 48299 steps/s (collection: 1.881s, learning 0.154s)
             Mean action noise std: 3.53
          Mean value_function loss: 146.7573
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.8981
                       Mean reward: 813.88
               Mean episode length: 218.25
    Episode_Reward/reaching_object: 0.9883
     Episode_Reward/lifting_object: 165.2878
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0661
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 2.04s
                      Time elapsed: 01:05:48
                               ETA: 00:07:23

################################################################################
                     [1m Learning iteration 1799/2000 [0m                     

                       Computation: 50459 steps/s (collection: 1.851s, learning 0.097s)
             Mean action noise std: 3.53
          Mean value_function loss: 128.4642
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.9052
                       Mean reward: 890.78
               Mean episode length: 235.13
    Episode_Reward/reaching_object: 1.0238
     Episode_Reward/lifting_object: 171.4489
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0685
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 1.95s
                      Time elapsed: 01:05:50
                               ETA: 00:07:21

################################################################################
                     [1m Learning iteration 1800/2000 [0m                     

                       Computation: 51029 steps/s (collection: 1.840s, learning 0.086s)
             Mean action noise std: 3.53
          Mean value_function loss: 142.2979
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.9120
                       Mean reward: 855.78
               Mean episode length: 227.75
    Episode_Reward/reaching_object: 1.0109
     Episode_Reward/lifting_object: 169.2713
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 1.93s
                      Time elapsed: 01:05:52
                               ETA: 00:07:18

################################################################################
                     [1m Learning iteration 1801/2000 [0m                     

                       Computation: 48405 steps/s (collection: 1.934s, learning 0.097s)
             Mean action noise std: 3.54
          Mean value_function loss: 121.2047
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.9234
                       Mean reward: 865.89
               Mean episode length: 229.41
    Episode_Reward/reaching_object: 0.9867
     Episode_Reward/lifting_object: 164.3662
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 2.03s
                      Time elapsed: 01:05:54
                               ETA: 00:07:16

################################################################################
                     [1m Learning iteration 1802/2000 [0m                     

                       Computation: 49870 steps/s (collection: 1.877s, learning 0.094s)
             Mean action noise std: 3.54
          Mean value_function loss: 99.4276
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.9350
                       Mean reward: 880.89
               Mean episode length: 232.62
    Episode_Reward/reaching_object: 1.0516
     Episode_Reward/lifting_object: 176.4459
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0702
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 1.97s
                      Time elapsed: 01:05:56
                               ETA: 00:07:14

################################################################################
                     [1m Learning iteration 1803/2000 [0m                     

                       Computation: 50161 steps/s (collection: 1.840s, learning 0.120s)
             Mean action noise std: 3.54
          Mean value_function loss: 129.7286
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.9461
                       Mean reward: 797.71
               Mean episode length: 213.82
    Episode_Reward/reaching_object: 1.0060
     Episode_Reward/lifting_object: 168.2207
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0674
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 1.96s
                      Time elapsed: 01:05:58
                               ETA: 00:07:12

################################################################################
                     [1m Learning iteration 1804/2000 [0m                     

                       Computation: 47533 steps/s (collection: 1.957s, learning 0.112s)
             Mean action noise std: 3.54
          Mean value_function loss: 136.9938
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 57.9572
                       Mean reward: 826.00
               Mean episode length: 219.32
    Episode_Reward/reaching_object: 1.0190
     Episode_Reward/lifting_object: 170.9410
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0681
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 2.07s
                      Time elapsed: 01:06:00
                               ETA: 00:07:10

################################################################################
                     [1m Learning iteration 1805/2000 [0m                     

                       Computation: 49338 steps/s (collection: 1.892s, learning 0.100s)
             Mean action noise std: 3.54
          Mean value_function loss: 109.2175
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 57.9699
                       Mean reward: 860.40
               Mean episode length: 228.15
    Episode_Reward/reaching_object: 1.0233
     Episode_Reward/lifting_object: 170.9892
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0683
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 1.99s
                      Time elapsed: 01:06:02
                               ETA: 00:07:07

################################################################################
                     [1m Learning iteration 1806/2000 [0m                     

                       Computation: 47660 steps/s (collection: 1.969s, learning 0.094s)
             Mean action noise std: 3.55
          Mean value_function loss: 90.9062
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.9872
                       Mean reward: 840.65
               Mean episode length: 222.93
    Episode_Reward/reaching_object: 1.0293
     Episode_Reward/lifting_object: 172.8123
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0685
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 2.06s
                      Time elapsed: 01:06:04
                               ETA: 00:07:05

################################################################################
                     [1m Learning iteration 1807/2000 [0m                     

                       Computation: 51226 steps/s (collection: 1.829s, learning 0.090s)
             Mean action noise std: 3.55
          Mean value_function loss: 114.5259
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.0036
                       Mean reward: 834.47
               Mean episode length: 221.93
    Episode_Reward/reaching_object: 1.0205
     Episode_Reward/lifting_object: 170.6911
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0680
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 1.92s
                      Time elapsed: 01:06:06
                               ETA: 00:07:03

################################################################################
                     [1m Learning iteration 1808/2000 [0m                     

                       Computation: 49844 steps/s (collection: 1.878s, learning 0.094s)
             Mean action noise std: 3.55
          Mean value_function loss: 110.1669
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.0158
                       Mean reward: 856.88
               Mean episode length: 227.84
    Episode_Reward/reaching_object: 1.0172
     Episode_Reward/lifting_object: 169.9949
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0676
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 1.97s
                      Time elapsed: 01:06:08
                               ETA: 00:07:01

################################################################################
                     [1m Learning iteration 1809/2000 [0m                     

                       Computation: 50013 steps/s (collection: 1.875s, learning 0.091s)
             Mean action noise std: 3.55
          Mean value_function loss: 106.3871
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.0292
                       Mean reward: 869.30
               Mean episode length: 230.72
    Episode_Reward/reaching_object: 1.0214
     Episode_Reward/lifting_object: 170.8035
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 1.97s
                      Time elapsed: 01:06:10
                               ETA: 00:06:58

################################################################################
                     [1m Learning iteration 1810/2000 [0m                     

                       Computation: 48843 steps/s (collection: 1.928s, learning 0.085s)
             Mean action noise std: 3.55
          Mean value_function loss: 126.4234
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 58.0443
                       Mean reward: 849.33
               Mean episode length: 224.89
    Episode_Reward/reaching_object: 1.0050
     Episode_Reward/lifting_object: 168.1032
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0670
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 2.01s
                      Time elapsed: 01:06:12
                               ETA: 00:06:56

################################################################################
                     [1m Learning iteration 1811/2000 [0m                     

                       Computation: 49432 steps/s (collection: 1.880s, learning 0.109s)
             Mean action noise std: 3.56
          Mean value_function loss: 111.8344
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.0526
                       Mean reward: 842.43
               Mean episode length: 223.86
    Episode_Reward/reaching_object: 1.0224
     Episode_Reward/lifting_object: 170.9358
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0679
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 1.99s
                      Time elapsed: 01:06:14
                               ETA: 00:06:54

################################################################################
                     [1m Learning iteration 1812/2000 [0m                     

                       Computation: 49823 steps/s (collection: 1.870s, learning 0.103s)
             Mean action noise std: 3.56
          Mean value_function loss: 115.5025
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 58.0641
                       Mean reward: 837.09
               Mean episode length: 223.70
    Episode_Reward/reaching_object: 1.0123
     Episode_Reward/lifting_object: 169.1272
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0675
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 1.97s
                      Time elapsed: 01:06:16
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1813/2000 [0m                     

                       Computation: 49151 steps/s (collection: 1.870s, learning 0.130s)
             Mean action noise std: 3.56
          Mean value_function loss: 127.2787
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 58.0726
                       Mean reward: 870.80
               Mean episode length: 230.26
    Episode_Reward/reaching_object: 1.0039
     Episode_Reward/lifting_object: 167.6117
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 2.00s
                      Time elapsed: 01:06:18
                               ETA: 00:06:50

################################################################################
                     [1m Learning iteration 1814/2000 [0m                     

                       Computation: 50192 steps/s (collection: 1.851s, learning 0.107s)
             Mean action noise std: 3.56
          Mean value_function loss: 143.0286
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.0799
                       Mean reward: 864.14
               Mean episode length: 229.58
    Episode_Reward/reaching_object: 1.0260
     Episode_Reward/lifting_object: 171.4795
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 1.96s
                      Time elapsed: 01:06:20
                               ETA: 00:06:47

################################################################################
                     [1m Learning iteration 1815/2000 [0m                     

                       Computation: 47004 steps/s (collection: 1.939s, learning 0.153s)
             Mean action noise std: 3.56
          Mean value_function loss: 166.7299
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 58.0880
                       Mean reward: 833.42
               Mean episode length: 224.59
    Episode_Reward/reaching_object: 1.0071
     Episode_Reward/lifting_object: 167.7746
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0672
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 2.09s
                      Time elapsed: 01:06:22
                               ETA: 00:06:45

################################################################################
                     [1m Learning iteration 1816/2000 [0m                     

                       Computation: 48283 steps/s (collection: 1.904s, learning 0.132s)
             Mean action noise std: 3.56
          Mean value_function loss: 142.4884
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.0931
                       Mean reward: 844.83
               Mean episode length: 224.68
    Episode_Reward/reaching_object: 1.0282
     Episode_Reward/lifting_object: 172.0145
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0686
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 2.04s
                      Time elapsed: 01:06:24
                               ETA: 00:06:43

################################################################################
                     [1m Learning iteration 1817/2000 [0m                     

                       Computation: 47998 steps/s (collection: 1.894s, learning 0.154s)
             Mean action noise std: 3.56
          Mean value_function loss: 153.8330
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.1008
                       Mean reward: 844.03
               Mean episode length: 226.71
    Episode_Reward/reaching_object: 0.9790
     Episode_Reward/lifting_object: 162.5535
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 2.05s
                      Time elapsed: 01:06:26
                               ETA: 00:06:41

################################################################################
                     [1m Learning iteration 1818/2000 [0m                     

                       Computation: 49703 steps/s (collection: 1.867s, learning 0.111s)
             Mean action noise std: 3.57
          Mean value_function loss: 109.5301
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 58.1095
                       Mean reward: 903.78
               Mean episode length: 238.19
    Episode_Reward/reaching_object: 1.0173
     Episode_Reward/lifting_object: 169.9950
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0680
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 1.98s
                      Time elapsed: 01:06:28
                               ETA: 00:06:39

################################################################################
                     [1m Learning iteration 1819/2000 [0m                     

                       Computation: 48551 steps/s (collection: 1.873s, learning 0.152s)
             Mean action noise std: 3.57
          Mean value_function loss: 100.0570
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.1177
                       Mean reward: 917.04
               Mean episode length: 241.10
    Episode_Reward/reaching_object: 1.0371
     Episode_Reward/lifting_object: 174.0298
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 2.02s
                      Time elapsed: 01:06:30
                               ETA: 00:06:36

################################################################################
                     [1m Learning iteration 1820/2000 [0m                     

                       Computation: 48102 steps/s (collection: 1.913s, learning 0.131s)
             Mean action noise std: 3.57
          Mean value_function loss: 87.9139
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.1250
                       Mean reward: 868.94
               Mean episode length: 230.34
    Episode_Reward/reaching_object: 1.0097
     Episode_Reward/lifting_object: 168.9521
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0674
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 2.04s
                      Time elapsed: 01:06:32
                               ETA: 00:06:34

################################################################################
                     [1m Learning iteration 1821/2000 [0m                     

                       Computation: 49351 steps/s (collection: 1.872s, learning 0.120s)
             Mean action noise std: 3.57
          Mean value_function loss: 146.8934
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.1338
                       Mean reward: 814.59
               Mean episode length: 218.15
    Episode_Reward/reaching_object: 1.0176
     Episode_Reward/lifting_object: 170.5634
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 1.99s
                      Time elapsed: 01:06:34
                               ETA: 00:06:32

################################################################################
                     [1m Learning iteration 1822/2000 [0m                     

                       Computation: 49942 steps/s (collection: 1.880s, learning 0.089s)
             Mean action noise std: 3.57
          Mean value_function loss: 135.4182
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.1428
                       Mean reward: 863.46
               Mean episode length: 228.60
    Episode_Reward/reaching_object: 0.9957
     Episode_Reward/lifting_object: 166.6948
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0669
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 1.97s
                      Time elapsed: 01:06:36
                               ETA: 00:06:30

################################################################################
                     [1m Learning iteration 1823/2000 [0m                     

                       Computation: 50818 steps/s (collection: 1.841s, learning 0.094s)
             Mean action noise std: 3.57
          Mean value_function loss: 166.8022
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.1506
                       Mean reward: 812.77
               Mean episode length: 218.24
    Episode_Reward/reaching_object: 0.9625
     Episode_Reward/lifting_object: 160.6227
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 1.93s
                      Time elapsed: 01:06:38
                               ETA: 00:06:28

################################################################################
                     [1m Learning iteration 1824/2000 [0m                     

                       Computation: 48232 steps/s (collection: 1.945s, learning 0.094s)
             Mean action noise std: 3.58
          Mean value_function loss: 120.9236
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 58.1644
                       Mean reward: 871.17
               Mean episode length: 230.45
    Episode_Reward/reaching_object: 1.0309
     Episode_Reward/lifting_object: 173.2839
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0690
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 2.04s
                      Time elapsed: 01:06:40
                               ETA: 00:06:25

################################################################################
                     [1m Learning iteration 1825/2000 [0m                     

                       Computation: 49935 steps/s (collection: 1.875s, learning 0.094s)
             Mean action noise std: 3.58
          Mean value_function loss: 148.8564
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.1760
                       Mean reward: 827.51
               Mean episode length: 220.92
    Episode_Reward/reaching_object: 1.0097
     Episode_Reward/lifting_object: 169.3139
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0681
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 1.97s
                      Time elapsed: 01:06:42
                               ETA: 00:06:23

################################################################################
                     [1m Learning iteration 1826/2000 [0m                     

                       Computation: 47925 steps/s (collection: 1.917s, learning 0.135s)
             Mean action noise std: 3.58
          Mean value_function loss: 182.5437
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 58.1867
                       Mean reward: 881.55
               Mean episode length: 233.38
    Episode_Reward/reaching_object: 1.0340
     Episode_Reward/lifting_object: 173.5773
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0697
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 2.05s
                      Time elapsed: 01:06:44
                               ETA: 00:06:21

################################################################################
                     [1m Learning iteration 1827/2000 [0m                     

                       Computation: 48180 steps/s (collection: 1.951s, learning 0.089s)
             Mean action noise std: 3.58
          Mean value_function loss: 173.3386
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 58.1999
                       Mean reward: 918.19
               Mean episode length: 242.71
    Episode_Reward/reaching_object: 1.0321
     Episode_Reward/lifting_object: 173.0351
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0693
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 2.04s
                      Time elapsed: 01:06:46
                               ETA: 00:06:19

################################################################################
                     [1m Learning iteration 1828/2000 [0m                     

                       Computation: 49138 steps/s (collection: 1.875s, learning 0.125s)
             Mean action noise std: 3.58
          Mean value_function loss: 180.1031
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.2140
                       Mean reward: 784.53
               Mean episode length: 211.01
    Episode_Reward/reaching_object: 0.9648
     Episode_Reward/lifting_object: 161.1081
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 2.00s
                      Time elapsed: 01:06:48
                               ETA: 00:06:16

################################################################################
                     [1m Learning iteration 1829/2000 [0m                     

                       Computation: 48629 steps/s (collection: 1.874s, learning 0.147s)
             Mean action noise std: 3.59
          Mean value_function loss: 140.0616
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 58.2249
                       Mean reward: 820.03
               Mean episode length: 219.05
    Episode_Reward/reaching_object: 0.9984
     Episode_Reward/lifting_object: 166.6845
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0673
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 2.02s
                      Time elapsed: 01:06:50
                               ETA: 00:06:14

################################################################################
                     [1m Learning iteration 1830/2000 [0m                     

                       Computation: 48076 steps/s (collection: 1.866s, learning 0.179s)
             Mean action noise std: 3.59
          Mean value_function loss: 108.0085
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 58.2366
                       Mean reward: 876.50
               Mean episode length: 233.02
    Episode_Reward/reaching_object: 1.0235
     Episode_Reward/lifting_object: 171.4571
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0690
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 2.04s
                      Time elapsed: 01:06:52
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 1831/2000 [0m                     

                       Computation: 47497 steps/s (collection: 1.885s, learning 0.185s)
             Mean action noise std: 3.59
          Mean value_function loss: 139.8228
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.2438
                       Mean reward: 821.93
               Mean episode length: 219.32
    Episode_Reward/reaching_object: 0.9984
     Episode_Reward/lifting_object: 167.1783
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0675
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 2.07s
                      Time elapsed: 01:06:54
                               ETA: 00:06:10

################################################################################
                     [1m Learning iteration 1832/2000 [0m                     

                       Computation: 49294 steps/s (collection: 1.889s, learning 0.105s)
             Mean action noise std: 3.59
          Mean value_function loss: 84.7789
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 58.2516
                       Mean reward: 881.83
               Mean episode length: 233.14
    Episode_Reward/reaching_object: 0.9976
     Episode_Reward/lifting_object: 167.4510
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0673
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 1.99s
                      Time elapsed: 01:06:56
                               ETA: 00:06:08

################################################################################
                     [1m Learning iteration 1833/2000 [0m                     

                       Computation: 49916 steps/s (collection: 1.881s, learning 0.088s)
             Mean action noise std: 3.59
          Mean value_function loss: 135.1036
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.2559
                       Mean reward: 877.64
               Mean episode length: 232.01
    Episode_Reward/reaching_object: 1.0263
     Episode_Reward/lifting_object: 172.6801
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0689
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 1.97s
                      Time elapsed: 01:06:58
                               ETA: 00:06:05

################################################################################
                     [1m Learning iteration 1834/2000 [0m                     

                       Computation: 48447 steps/s (collection: 1.935s, learning 0.094s)
             Mean action noise std: 3.59
          Mean value_function loss: 138.5007
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.2616
                       Mean reward: 815.64
               Mean episode length: 216.96
    Episode_Reward/reaching_object: 1.0104
     Episode_Reward/lifting_object: 169.7321
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0681
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 2.03s
                      Time elapsed: 01:07:00
                               ETA: 00:06:03

################################################################################
                     [1m Learning iteration 1835/2000 [0m                     

                       Computation: 50607 steps/s (collection: 1.849s, learning 0.094s)
             Mean action noise std: 3.59
          Mean value_function loss: 113.1951
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.2719
                       Mean reward: 881.62
               Mean episode length: 233.13
    Episode_Reward/reaching_object: 1.0284
     Episode_Reward/lifting_object: 173.1226
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0693
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 1.94s
                      Time elapsed: 01:07:02
                               ETA: 00:06:01

################################################################################
                     [1m Learning iteration 1836/2000 [0m                     

                       Computation: 49931 steps/s (collection: 1.851s, learning 0.118s)
             Mean action noise std: 3.59
          Mean value_function loss: 111.8180
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 58.2807
                       Mean reward: 888.14
               Mean episode length: 234.67
    Episode_Reward/reaching_object: 1.0157
     Episode_Reward/lifting_object: 170.7931
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 1.97s
                      Time elapsed: 01:07:04
                               ETA: 00:05:59

################################################################################
                     [1m Learning iteration 1837/2000 [0m                     

                       Computation: 50177 steps/s (collection: 1.867s, learning 0.092s)
             Mean action noise std: 3.60
          Mean value_function loss: 109.6785
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.2842
                       Mean reward: 873.25
               Mean episode length: 232.44
    Episode_Reward/reaching_object: 1.0221
     Episode_Reward/lifting_object: 171.6238
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0686
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 1.96s
                      Time elapsed: 01:07:06
                               ETA: 00:05:57

################################################################################
                     [1m Learning iteration 1838/2000 [0m                     

                       Computation: 49191 steps/s (collection: 1.909s, learning 0.090s)
             Mean action noise std: 3.60
          Mean value_function loss: 107.7831
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.2920
                       Mean reward: 883.61
               Mean episode length: 233.69
    Episode_Reward/reaching_object: 1.0366
     Episode_Reward/lifting_object: 174.4429
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0695
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 2.00s
                      Time elapsed: 01:07:08
                               ETA: 00:05:54

################################################################################
                     [1m Learning iteration 1839/2000 [0m                     

                       Computation: 47319 steps/s (collection: 1.967s, learning 0.110s)
             Mean action noise std: 3.60
          Mean value_function loss: 99.4706
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.3037
                       Mean reward: 874.88
               Mean episode length: 231.99
    Episode_Reward/reaching_object: 1.0117
     Episode_Reward/lifting_object: 170.0036
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0681
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 2.08s
                      Time elapsed: 01:07:10
                               ETA: 00:05:52

################################################################################
                     [1m Learning iteration 1840/2000 [0m                     

                       Computation: 50661 steps/s (collection: 1.853s, learning 0.087s)
             Mean action noise std: 3.60
          Mean value_function loss: 98.9700
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 58.3119
                       Mean reward: 856.03
               Mean episode length: 226.85
    Episode_Reward/reaching_object: 1.0208
     Episode_Reward/lifting_object: 171.4409
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0686
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 1.94s
                      Time elapsed: 01:07:12
                               ETA: 00:05:50

################################################################################
                     [1m Learning iteration 1841/2000 [0m                     

                       Computation: 49462 steps/s (collection: 1.863s, learning 0.125s)
             Mean action noise std: 3.60
          Mean value_function loss: 124.1095
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.3176
                       Mean reward: 918.70
               Mean episode length: 243.23
    Episode_Reward/reaching_object: 1.0448
     Episode_Reward/lifting_object: 175.3652
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0702
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 1.99s
                      Time elapsed: 01:07:14
                               ETA: 00:05:48

################################################################################
                     [1m Learning iteration 1842/2000 [0m                     

                       Computation: 48794 steps/s (collection: 1.895s, learning 0.120s)
             Mean action noise std: 3.60
          Mean value_function loss: 130.5340
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 58.3288
                       Mean reward: 851.08
               Mean episode length: 226.46
    Episode_Reward/reaching_object: 0.9963
     Episode_Reward/lifting_object: 166.5671
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0673
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 2.01s
                      Time elapsed: 01:07:16
                               ETA: 00:05:46

################################################################################
                     [1m Learning iteration 1843/2000 [0m                     

                       Computation: 50387 steps/s (collection: 1.864s, learning 0.087s)
             Mean action noise std: 3.61
          Mean value_function loss: 121.4213
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.3464
                       Mean reward: 873.92
               Mean episode length: 231.14
    Episode_Reward/reaching_object: 1.0136
     Episode_Reward/lifting_object: 169.6285
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 1.95s
                      Time elapsed: 01:07:18
                               ETA: 00:05:43

################################################################################
                     [1m Learning iteration 1844/2000 [0m                     

                       Computation: 47214 steps/s (collection: 1.992s, learning 0.090s)
             Mean action noise std: 3.61
          Mean value_function loss: 119.7433
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 58.3629
                       Mean reward: 847.15
               Mean episode length: 225.29
    Episode_Reward/reaching_object: 1.0172
     Episode_Reward/lifting_object: 170.5490
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 2.08s
                      Time elapsed: 01:07:20
                               ETA: 00:05:41

################################################################################
                     [1m Learning iteration 1845/2000 [0m                     

                       Computation: 48811 steps/s (collection: 1.905s, learning 0.109s)
             Mean action noise std: 3.61
          Mean value_function loss: 138.7554
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 58.3713
                       Mean reward: 813.29
               Mean episode length: 217.95
    Episode_Reward/reaching_object: 1.0015
     Episode_Reward/lifting_object: 167.5705
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 2.01s
                      Time elapsed: 01:07:22
                               ETA: 00:05:39

################################################################################
                     [1m Learning iteration 1846/2000 [0m                     

                       Computation: 49968 steps/s (collection: 1.877s, learning 0.090s)
             Mean action noise std: 3.61
          Mean value_function loss: 106.4516
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 58.3813
                       Mean reward: 837.70
               Mean episode length: 223.03
    Episode_Reward/reaching_object: 1.0393
     Episode_Reward/lifting_object: 174.4702
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 1.97s
                      Time elapsed: 01:07:24
                               ETA: 00:05:37

################################################################################
                     [1m Learning iteration 1847/2000 [0m                     

                       Computation: 50362 steps/s (collection: 1.858s, learning 0.093s)
             Mean action noise std: 3.61
          Mean value_function loss: 113.6817
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 58.3949
                       Mean reward: 830.86
               Mean episode length: 221.52
    Episode_Reward/reaching_object: 1.0197
     Episode_Reward/lifting_object: 170.9001
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0689
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 1.95s
                      Time elapsed: 01:07:26
                               ETA: 00:05:35

################################################################################
                     [1m Learning iteration 1848/2000 [0m                     

                       Computation: 51151 steps/s (collection: 1.835s, learning 0.087s)
             Mean action noise std: 3.61
          Mean value_function loss: 132.4811
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 58.4041
                       Mean reward: 853.94
               Mean episode length: 226.51
    Episode_Reward/reaching_object: 1.0353
     Episode_Reward/lifting_object: 173.7462
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0695
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 1.92s
                      Time elapsed: 01:07:28
                               ETA: 00:05:32

################################################################################
                     [1m Learning iteration 1849/2000 [0m                     

                       Computation: 51041 steps/s (collection: 1.838s, learning 0.088s)
             Mean action noise std: 3.62
          Mean value_function loss: 149.3255
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.4137
                       Mean reward: 820.15
               Mean episode length: 218.35
    Episode_Reward/reaching_object: 1.0128
     Episode_Reward/lifting_object: 169.4779
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0683
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 1.93s
                      Time elapsed: 01:07:30
                               ETA: 00:05:30

################################################################################
                     [1m Learning iteration 1850/2000 [0m                     

                       Computation: 50975 steps/s (collection: 1.839s, learning 0.089s)
             Mean action noise std: 3.62
          Mean value_function loss: 142.9414
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.4249
                       Mean reward: 874.01
               Mean episode length: 231.95
    Episode_Reward/reaching_object: 1.0160
     Episode_Reward/lifting_object: 170.2109
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0687
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 1.93s
                      Time elapsed: 01:07:32
                               ETA: 00:05:28

################################################################################
                     [1m Learning iteration 1851/2000 [0m                     

                       Computation: 49735 steps/s (collection: 1.872s, learning 0.105s)
             Mean action noise std: 3.62
          Mean value_function loss: 147.9417
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.4314
                       Mean reward: 847.82
               Mean episode length: 225.02
    Episode_Reward/reaching_object: 1.0076
     Episode_Reward/lifting_object: 168.7370
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0680
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 1.98s
                      Time elapsed: 01:07:34
                               ETA: 00:05:26

################################################################################
                     [1m Learning iteration 1852/2000 [0m                     

                       Computation: 49676 steps/s (collection: 1.893s, learning 0.086s)
             Mean action noise std: 3.62
          Mean value_function loss: 139.9816
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.4408
                       Mean reward: 836.19
               Mean episode length: 223.51
    Episode_Reward/reaching_object: 0.9996
     Episode_Reward/lifting_object: 166.9343
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0676
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 1.98s
                      Time elapsed: 01:07:36
                               ETA: 00:05:23

################################################################################
                     [1m Learning iteration 1853/2000 [0m                     

                       Computation: 49950 steps/s (collection: 1.852s, learning 0.116s)
             Mean action noise std: 3.62
          Mean value_function loss: 126.9403
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 58.4554
                       Mean reward: 871.52
               Mean episode length: 231.58
    Episode_Reward/reaching_object: 1.0444
     Episode_Reward/lifting_object: 175.1067
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0703
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 1.97s
                      Time elapsed: 01:07:38
                               ETA: 00:05:21

################################################################################
                     [1m Learning iteration 1854/2000 [0m                     

                       Computation: 48709 steps/s (collection: 1.931s, learning 0.087s)
             Mean action noise std: 3.63
          Mean value_function loss: 135.9404
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 58.4698
                       Mean reward: 840.47
               Mean episode length: 224.12
    Episode_Reward/reaching_object: 1.0155
     Episode_Reward/lifting_object: 169.9906
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 2.02s
                      Time elapsed: 01:07:40
                               ETA: 00:05:19

################################################################################
                     [1m Learning iteration 1855/2000 [0m                     

                       Computation: 49698 steps/s (collection: 1.884s, learning 0.094s)
             Mean action noise std: 3.63
          Mean value_function loss: 138.9300
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.4772
                       Mean reward: 874.40
               Mean episode length: 231.48
    Episode_Reward/reaching_object: 1.0328
     Episode_Reward/lifting_object: 173.0164
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0695
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 1.98s
                      Time elapsed: 01:07:42
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 1856/2000 [0m                     

                       Computation: 50491 steps/s (collection: 1.862s, learning 0.085s)
             Mean action noise std: 3.63
          Mean value_function loss: 144.1536
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 58.4817
                       Mean reward: 863.06
               Mean episode length: 228.76
    Episode_Reward/reaching_object: 1.0237
     Episode_Reward/lifting_object: 171.4765
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 1.95s
                      Time elapsed: 01:07:44
                               ETA: 00:05:15

################################################################################
                     [1m Learning iteration 1857/2000 [0m                     

                       Computation: 50334 steps/s (collection: 1.861s, learning 0.092s)
             Mean action noise std: 3.63
          Mean value_function loss: 186.2406
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 58.4885
                       Mean reward: 802.22
               Mean episode length: 214.57
    Episode_Reward/reaching_object: 1.0067
     Episode_Reward/lifting_object: 168.4236
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0680
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 1.95s
                      Time elapsed: 01:07:46
                               ETA: 00:05:12

################################################################################
                     [1m Learning iteration 1858/2000 [0m                     

                       Computation: 50599 steps/s (collection: 1.859s, learning 0.084s)
             Mean action noise std: 3.63
          Mean value_function loss: 142.2510
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.4973
                       Mean reward: 922.49
               Mean episode length: 243.32
    Episode_Reward/reaching_object: 1.0313
     Episode_Reward/lifting_object: 172.4102
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0698
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 1.94s
                      Time elapsed: 01:07:48
                               ETA: 00:05:10

################################################################################
                     [1m Learning iteration 1859/2000 [0m                     

                       Computation: 48309 steps/s (collection: 1.938s, learning 0.097s)
             Mean action noise std: 3.63
          Mean value_function loss: 115.3437
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.5080
                       Mean reward: 857.16
               Mean episode length: 228.65
    Episode_Reward/reaching_object: 1.0219
     Episode_Reward/lifting_object: 171.0951
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0693
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 2.03s
                      Time elapsed: 01:07:50
                               ETA: 00:05:08

################################################################################
                     [1m Learning iteration 1860/2000 [0m                     

                       Computation: 50898 steps/s (collection: 1.821s, learning 0.110s)
             Mean action noise std: 3.63
          Mean value_function loss: 133.2507
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.5145
                       Mean reward: 867.33
               Mean episode length: 230.77
    Episode_Reward/reaching_object: 1.0358
     Episode_Reward/lifting_object: 174.0039
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0703
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 1.93s
                      Time elapsed: 01:07:52
                               ETA: 00:05:06

################################################################################
                     [1m Learning iteration 1861/2000 [0m                     

                       Computation: 49946 steps/s (collection: 1.832s, learning 0.137s)
             Mean action noise std: 3.63
          Mean value_function loss: 100.8311
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.5182
                       Mean reward: 891.37
               Mean episode length: 235.90
    Episode_Reward/reaching_object: 1.0323
     Episode_Reward/lifting_object: 173.5800
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0700
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 1.97s
                      Time elapsed: 01:07:54
                               ETA: 00:05:04

################################################################################
                     [1m Learning iteration 1862/2000 [0m                     

                       Computation: 49594 steps/s (collection: 1.829s, learning 0.153s)
             Mean action noise std: 3.64
          Mean value_function loss: 131.0827
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 58.5243
                       Mean reward: 853.16
               Mean episode length: 227.09
    Episode_Reward/reaching_object: 1.0014
     Episode_Reward/lifting_object: 167.8113
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 1.98s
                      Time elapsed: 01:07:56
                               ETA: 00:05:01

################################################################################
                     [1m Learning iteration 1863/2000 [0m                     

                       Computation: 48469 steps/s (collection: 1.908s, learning 0.121s)
             Mean action noise std: 3.64
          Mean value_function loss: 104.1224
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 58.5334
                       Mean reward: 860.12
               Mean episode length: 228.63
    Episode_Reward/reaching_object: 1.0147
     Episode_Reward/lifting_object: 170.3661
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0692
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 2.03s
                      Time elapsed: 01:07:58
                               ETA: 00:04:59

################################################################################
                     [1m Learning iteration 1864/2000 [0m                     

                       Computation: 46439 steps/s (collection: 2.009s, learning 0.108s)
             Mean action noise std: 3.64
          Mean value_function loss: 79.8328
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 58.5409
                       Mean reward: 902.81
               Mean episode length: 238.38
    Episode_Reward/reaching_object: 1.0529
     Episode_Reward/lifting_object: 177.0985
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0716
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 2.12s
                      Time elapsed: 01:08:00
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 1865/2000 [0m                     

                       Computation: 50217 steps/s (collection: 1.859s, learning 0.099s)
             Mean action noise std: 3.64
          Mean value_function loss: 127.1435
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.5473
                       Mean reward: 866.58
               Mean episode length: 230.03
    Episode_Reward/reaching_object: 1.0312
     Episode_Reward/lifting_object: 173.3271
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0701
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 1.96s
                      Time elapsed: 01:08:02
                               ETA: 00:04:55

################################################################################
                     [1m Learning iteration 1866/2000 [0m                     

                       Computation: 49576 steps/s (collection: 1.885s, learning 0.098s)
             Mean action noise std: 3.64
          Mean value_function loss: 125.6838
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.5537
                       Mean reward: 858.96
               Mean episode length: 228.84
    Episode_Reward/reaching_object: 1.0193
     Episode_Reward/lifting_object: 171.0268
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 1.98s
                      Time elapsed: 01:08:04
                               ETA: 00:04:53

################################################################################
                     [1m Learning iteration 1867/2000 [0m                     

                       Computation: 49374 steps/s (collection: 1.893s, learning 0.098s)
             Mean action noise std: 3.64
          Mean value_function loss: 159.8900
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.5624
                       Mean reward: 777.22
               Mean episode length: 210.55
    Episode_Reward/reaching_object: 0.9958
     Episode_Reward/lifting_object: 166.5997
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0683
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 1.99s
                      Time elapsed: 01:08:06
                               ETA: 00:04:50

################################################################################
                     [1m Learning iteration 1868/2000 [0m                     

                       Computation: 49475 steps/s (collection: 1.876s, learning 0.111s)
             Mean action noise std: 3.64
          Mean value_function loss: 154.3575
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.5722
                       Mean reward: 847.09
               Mean episode length: 225.55
    Episode_Reward/reaching_object: 1.0135
     Episode_Reward/lifting_object: 169.3976
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0693
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 1.99s
                      Time elapsed: 01:08:08
                               ETA: 00:04:48

################################################################################
                     [1m Learning iteration 1869/2000 [0m                     

                       Computation: 49512 steps/s (collection: 1.880s, learning 0.106s)
             Mean action noise std: 3.65
          Mean value_function loss: 107.8505
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.5817
                       Mean reward: 888.83
               Mean episode length: 234.97
    Episode_Reward/reaching_object: 1.0199
     Episode_Reward/lifting_object: 170.7180
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0697
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 1.99s
                      Time elapsed: 01:08:10
                               ETA: 00:04:46

################################################################################
                     [1m Learning iteration 1870/2000 [0m                     

                       Computation: 48141 steps/s (collection: 1.922s, learning 0.120s)
             Mean action noise std: 3.65
          Mean value_function loss: 131.0963
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 58.5889
                       Mean reward: 852.68
               Mean episode length: 226.67
    Episode_Reward/reaching_object: 1.0031
     Episode_Reward/lifting_object: 168.0666
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0685
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 2.04s
                      Time elapsed: 01:08:12
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 1871/2000 [0m                     

                       Computation: 49557 steps/s (collection: 1.862s, learning 0.121s)
             Mean action noise std: 3.65
          Mean value_function loss: 100.6290
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.5941
                       Mean reward: 892.26
               Mean episode length: 236.20
    Episode_Reward/reaching_object: 1.0255
     Episode_Reward/lifting_object: 171.8556
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0701
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 1.98s
                      Time elapsed: 01:08:14
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 1872/2000 [0m                     

                       Computation: 49736 steps/s (collection: 1.862s, learning 0.114s)
             Mean action noise std: 3.65
          Mean value_function loss: 98.6064
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.5991
                       Mean reward: 870.86
               Mean episode length: 230.67
    Episode_Reward/reaching_object: 1.0255
     Episode_Reward/lifting_object: 171.7815
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0698
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 1.98s
                      Time elapsed: 01:08:16
                               ETA: 00:04:39

################################################################################
                     [1m Learning iteration 1873/2000 [0m                     

                       Computation: 48719 steps/s (collection: 1.857s, learning 0.161s)
             Mean action noise std: 3.65
          Mean value_function loss: 127.6754
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.6069
                       Mean reward: 872.79
               Mean episode length: 230.77
    Episode_Reward/reaching_object: 1.0343
     Episode_Reward/lifting_object: 173.5732
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0704
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 2.02s
                      Time elapsed: 01:08:18
                               ETA: 00:04:37

################################################################################
                     [1m Learning iteration 1874/2000 [0m                     

                       Computation: 47246 steps/s (collection: 1.981s, learning 0.100s)
             Mean action noise std: 3.65
          Mean value_function loss: 117.3433
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.6154
                       Mean reward: 901.22
               Mean episode length: 237.78
    Episode_Reward/reaching_object: 1.0157
     Episode_Reward/lifting_object: 170.1778
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 2.08s
                      Time elapsed: 01:08:20
                               ETA: 00:04:35

################################################################################
                     [1m Learning iteration 1875/2000 [0m                     

                       Computation: 47555 steps/s (collection: 1.947s, learning 0.120s)
             Mean action noise std: 3.65
          Mean value_function loss: 145.4623
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 58.6246
                       Mean reward: 856.48
               Mean episode length: 227.76
    Episode_Reward/reaching_object: 1.0438
     Episode_Reward/lifting_object: 175.0140
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0713
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 2.07s
                      Time elapsed: 01:08:22
                               ETA: 00:04:33

################################################################################
                     [1m Learning iteration 1876/2000 [0m                     

                       Computation: 49359 steps/s (collection: 1.900s, learning 0.092s)
             Mean action noise std: 3.65
          Mean value_function loss: 109.8286
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 58.6277
                       Mean reward: 862.60
               Mean episode length: 228.49
    Episode_Reward/reaching_object: 1.0347
     Episode_Reward/lifting_object: 173.4960
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0708
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 1.99s
                      Time elapsed: 01:08:24
                               ETA: 00:04:31

################################################################################
                     [1m Learning iteration 1877/2000 [0m                     

                       Computation: 49498 steps/s (collection: 1.894s, learning 0.092s)
             Mean action noise std: 3.65
          Mean value_function loss: 104.0180
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 58.6294
                       Mean reward: 910.08
               Mean episode length: 240.65
    Episode_Reward/reaching_object: 1.0615
     Episode_Reward/lifting_object: 178.2598
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0725
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 1.99s
                      Time elapsed: 01:08:26
                               ETA: 00:04:28

################################################################################
                     [1m Learning iteration 1878/2000 [0m                     

                       Computation: 49998 steps/s (collection: 1.859s, learning 0.107s)
             Mean action noise std: 3.65
          Mean value_function loss: 136.2877
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.6322
                       Mean reward: 846.53
               Mean episode length: 225.11
    Episode_Reward/reaching_object: 0.9889
     Episode_Reward/lifting_object: 165.2597
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0679
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 1.97s
                      Time elapsed: 01:08:28
                               ETA: 00:04:26

################################################################################
                     [1m Learning iteration 1879/2000 [0m                     

                       Computation: 46742 steps/s (collection: 2.017s, learning 0.086s)
             Mean action noise std: 3.66
          Mean value_function loss: 130.8250
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 58.6392
                       Mean reward: 867.35
               Mean episode length: 229.38
    Episode_Reward/reaching_object: 1.0301
     Episode_Reward/lifting_object: 172.5657
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0703
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 2.10s
                      Time elapsed: 01:08:30
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1880/2000 [0m                     

                       Computation: 49693 steps/s (collection: 1.889s, learning 0.089s)
             Mean action noise std: 3.66
          Mean value_function loss: 118.0728
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 58.6462
                       Mean reward: 833.71
               Mean episode length: 222.49
    Episode_Reward/reaching_object: 1.0027
     Episode_Reward/lifting_object: 167.4698
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0688
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 1.98s
                      Time elapsed: 01:08:32
                               ETA: 00:04:22

################################################################################
                     [1m Learning iteration 1881/2000 [0m                     

                       Computation: 49533 steps/s (collection: 1.857s, learning 0.128s)
             Mean action noise std: 3.66
          Mean value_function loss: 149.0106
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 58.6513
                       Mean reward: 881.59
               Mean episode length: 232.73
    Episode_Reward/reaching_object: 1.0126
     Episode_Reward/lifting_object: 169.5663
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0692
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 1.98s
                      Time elapsed: 01:08:34
                               ETA: 00:04:20

################################################################################
                     [1m Learning iteration 1882/2000 [0m                     

                       Computation: 49901 steps/s (collection: 1.855s, learning 0.115s)
             Mean action noise std: 3.66
          Mean value_function loss: 150.6735
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 58.6576
                       Mean reward: 834.10
               Mean episode length: 222.02
    Episode_Reward/reaching_object: 0.9857
     Episode_Reward/lifting_object: 164.2324
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0679
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 1.97s
                      Time elapsed: 01:08:36
                               ETA: 00:04:17

################################################################################
                     [1m Learning iteration 1883/2000 [0m                     

                       Computation: 49779 steps/s (collection: 1.832s, learning 0.143s)
             Mean action noise std: 3.66
          Mean value_function loss: 149.0160
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.6685
                       Mean reward: 800.62
               Mean episode length: 214.13
    Episode_Reward/reaching_object: 0.9694
     Episode_Reward/lifting_object: 161.2106
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0666
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 1.97s
                      Time elapsed: 01:08:38
                               ETA: 00:04:15

################################################################################
                     [1m Learning iteration 1884/2000 [0m                     

                       Computation: 45859 steps/s (collection: 2.008s, learning 0.136s)
             Mean action noise std: 3.66
          Mean value_function loss: 97.6476
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 58.6808
                       Mean reward: 876.71
               Mean episode length: 232.22
    Episode_Reward/reaching_object: 1.0309
     Episode_Reward/lifting_object: 172.3778
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0702
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 2.14s
                      Time elapsed: 01:08:40
                               ETA: 00:04:13

################################################################################
                     [1m Learning iteration 1885/2000 [0m                     

                       Computation: 50243 steps/s (collection: 1.846s, learning 0.111s)
             Mean action noise std: 3.66
          Mean value_function loss: 108.4865
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 58.6892
                       Mean reward: 843.86
               Mean episode length: 225.16
    Episode_Reward/reaching_object: 1.0163
     Episode_Reward/lifting_object: 169.6471
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0695
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 1.96s
                      Time elapsed: 01:08:42
                               ETA: 00:04:11

################################################################################
                     [1m Learning iteration 1886/2000 [0m                     

                       Computation: 48255 steps/s (collection: 1.890s, learning 0.147s)
             Mean action noise std: 3.67
          Mean value_function loss: 143.1002
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.7014
                       Mean reward: 869.18
               Mean episode length: 231.71
    Episode_Reward/reaching_object: 1.0420
     Episode_Reward/lifting_object: 173.5928
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0706
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 2.04s
                      Time elapsed: 01:08:44
                               ETA: 00:04:09

################################################################################
                     [1m Learning iteration 1887/2000 [0m                     

                       Computation: 48170 steps/s (collection: 1.935s, learning 0.106s)
             Mean action noise std: 3.67
          Mean value_function loss: 106.0563
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 58.7122
                       Mean reward: 846.31
               Mean episode length: 224.85
    Episode_Reward/reaching_object: 1.0160
     Episode_Reward/lifting_object: 169.3139
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0690
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 2.04s
                      Time elapsed: 01:08:46
                               ETA: 00:04:06

################################################################################
                     [1m Learning iteration 1888/2000 [0m                     

                       Computation: 48045 steps/s (collection: 1.926s, learning 0.120s)
             Mean action noise std: 3.67
          Mean value_function loss: 115.6180
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.7225
                       Mean reward: 857.14
               Mean episode length: 227.37
    Episode_Reward/reaching_object: 1.0148
     Episode_Reward/lifting_object: 168.9634
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0690
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 2.05s
                      Time elapsed: 01:08:48
                               ETA: 00:04:04

################################################################################
                     [1m Learning iteration 1889/2000 [0m                     

                       Computation: 50281 steps/s (collection: 1.864s, learning 0.091s)
             Mean action noise std: 3.67
          Mean value_function loss: 118.2604
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 58.7335
                       Mean reward: 839.65
               Mean episode length: 223.32
    Episode_Reward/reaching_object: 1.0101
     Episode_Reward/lifting_object: 168.2849
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0688
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 1.96s
                      Time elapsed: 01:08:50
                               ETA: 00:04:02

################################################################################
                     [1m Learning iteration 1890/2000 [0m                     

                       Computation: 51229 steps/s (collection: 1.833s, learning 0.086s)
             Mean action noise std: 3.67
          Mean value_function loss: 116.6602
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.7421
                       Mean reward: 884.04
               Mean episode length: 233.18
    Episode_Reward/reaching_object: 1.0243
     Episode_Reward/lifting_object: 170.8720
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0695
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 1.92s
                      Time elapsed: 01:08:52
                               ETA: 00:04:00

################################################################################
                     [1m Learning iteration 1891/2000 [0m                     

                       Computation: 48887 steps/s (collection: 1.897s, learning 0.114s)
             Mean action noise std: 3.67
          Mean value_function loss: 112.2858
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.7487
                       Mean reward: 870.92
               Mean episode length: 231.43
    Episode_Reward/reaching_object: 1.0339
     Episode_Reward/lifting_object: 172.1933
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0703
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 2.01s
                      Time elapsed: 01:08:54
                               ETA: 00:03:58

################################################################################
                     [1m Learning iteration 1892/2000 [0m                     

                       Computation: 47309 steps/s (collection: 1.973s, learning 0.105s)
             Mean action noise std: 3.68
          Mean value_function loss: 137.7907
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.7566
                       Mean reward: 873.66
               Mean episode length: 232.13
    Episode_Reward/reaching_object: 1.0208
     Episode_Reward/lifting_object: 169.1799
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0697
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 2.08s
                      Time elapsed: 01:08:56
                               ETA: 00:03:55

################################################################################
                     [1m Learning iteration 1893/2000 [0m                     

                       Computation: 50025 steps/s (collection: 1.876s, learning 0.089s)
             Mean action noise std: 3.68
          Mean value_function loss: 99.9211
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 58.7644
                       Mean reward: 829.04
               Mean episode length: 220.26
    Episode_Reward/reaching_object: 1.0432
     Episode_Reward/lifting_object: 173.9185
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0709
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 1.97s
                      Time elapsed: 01:08:58
                               ETA: 00:03:53

################################################################################
                     [1m Learning iteration 1894/2000 [0m                     

                       Computation: 49774 steps/s (collection: 1.874s, learning 0.101s)
             Mean action noise std: 3.68
          Mean value_function loss: 118.5878
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.7731
                       Mean reward: 862.05
               Mean episode length: 228.03
    Episode_Reward/reaching_object: 1.0222
     Episode_Reward/lifting_object: 170.2930
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 1.97s
                      Time elapsed: 01:09:00
                               ETA: 00:03:51

################################################################################
                     [1m Learning iteration 1895/2000 [0m                     

                       Computation: 49251 steps/s (collection: 1.903s, learning 0.093s)
             Mean action noise std: 3.68
          Mean value_function loss: 126.3679
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.7826
                       Mean reward: 909.18
               Mean episode length: 238.74
    Episode_Reward/reaching_object: 1.0209
     Episode_Reward/lifting_object: 170.0399
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 2.00s
                      Time elapsed: 01:09:02
                               ETA: 00:03:49

################################################################################
                     [1m Learning iteration 1896/2000 [0m                     

                       Computation: 49109 steps/s (collection: 1.899s, learning 0.103s)
             Mean action noise std: 3.68
          Mean value_function loss: 120.2290
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 58.7953
                       Mean reward: 859.43
               Mean episode length: 227.64
    Episode_Reward/reaching_object: 1.0154
     Episode_Reward/lifting_object: 169.0862
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0692
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 2.00s
                      Time elapsed: 01:09:04
                               ETA: 00:03:47

################################################################################
                     [1m Learning iteration 1897/2000 [0m                     

                       Computation: 49326 steps/s (collection: 1.884s, learning 0.109s)
             Mean action noise std: 3.68
          Mean value_function loss: 160.6333
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 58.8020
                       Mean reward: 872.04
               Mean episode length: 231.76
    Episode_Reward/reaching_object: 1.0226
     Episode_Reward/lifting_object: 170.1247
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0695
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 1.99s
                      Time elapsed: 01:09:06
                               ETA: 00:03:45

################################################################################
                     [1m Learning iteration 1898/2000 [0m                     

                       Computation: 48665 steps/s (collection: 1.837s, learning 0.183s)
             Mean action noise std: 3.68
          Mean value_function loss: 139.2679
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 58.8111
                       Mean reward: 824.13
               Mean episode length: 219.69
    Episode_Reward/reaching_object: 1.0263
     Episode_Reward/lifting_object: 171.1097
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0697
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 2.02s
                      Time elapsed: 01:09:08
                               ETA: 00:03:42

################################################################################
                     [1m Learning iteration 1899/2000 [0m                     

                       Computation: 50958 steps/s (collection: 1.833s, learning 0.097s)
             Mean action noise std: 3.69
          Mean value_function loss: 116.9146
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.8207
                       Mean reward: 904.43
               Mean episode length: 238.04
    Episode_Reward/reaching_object: 1.0322
     Episode_Reward/lifting_object: 172.5911
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0698
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 1.93s
                      Time elapsed: 01:09:10
                               ETA: 00:03:40

################################################################################
                     [1m Learning iteration 1900/2000 [0m                     

                       Computation: 48373 steps/s (collection: 1.918s, learning 0.114s)
             Mean action noise std: 3.69
          Mean value_function loss: 193.6037
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 58.8310
                       Mean reward: 803.22
               Mean episode length: 215.62
    Episode_Reward/reaching_object: 1.0016
     Episode_Reward/lifting_object: 166.9515
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0679
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 2.03s
                      Time elapsed: 01:09:12
                               ETA: 00:03:38

################################################################################
                     [1m Learning iteration 1901/2000 [0m                     

                       Computation: 47699 steps/s (collection: 1.912s, learning 0.149s)
             Mean action noise std: 3.69
          Mean value_function loss: 134.4288
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.8408
                       Mean reward: 871.58
               Mean episode length: 231.21
    Episode_Reward/reaching_object: 1.0213
     Episode_Reward/lifting_object: 170.6603
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0690
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 2.06s
                      Time elapsed: 01:09:14
                               ETA: 00:03:36

################################################################################
                     [1m Learning iteration 1902/2000 [0m                     

                       Computation: 49771 steps/s (collection: 1.880s, learning 0.095s)
             Mean action noise std: 3.69
          Mean value_function loss: 145.1718
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 58.8515
                       Mean reward: 835.61
               Mean episode length: 221.81
    Episode_Reward/reaching_object: 1.0327
     Episode_Reward/lifting_object: 172.8522
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0698
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 1.98s
                      Time elapsed: 01:09:16
                               ETA: 00:03:34

################################################################################
                     [1m Learning iteration 1903/2000 [0m                     

                       Computation: 48477 steps/s (collection: 1.908s, learning 0.120s)
             Mean action noise std: 3.69
          Mean value_function loss: 159.0283
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.8570
                       Mean reward: 864.56
               Mean episode length: 228.85
    Episode_Reward/reaching_object: 0.9952
     Episode_Reward/lifting_object: 166.0249
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0675
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 2.03s
                      Time elapsed: 01:09:18
                               ETA: 00:03:31

################################################################################
                     [1m Learning iteration 1904/2000 [0m                     

                       Computation: 48098 steps/s (collection: 1.932s, learning 0.112s)
             Mean action noise std: 3.69
          Mean value_function loss: 187.4923
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.8612
                       Mean reward: 854.42
               Mean episode length: 227.73
    Episode_Reward/reaching_object: 1.0072
     Episode_Reward/lifting_object: 167.8058
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0679
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 2.04s
                      Time elapsed: 01:09:20
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 1905/2000 [0m                     

                       Computation: 47553 steps/s (collection: 1.976s, learning 0.092s)
             Mean action noise std: 3.70
          Mean value_function loss: 159.0024
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 58.8695
                       Mean reward: 829.18
               Mean episode length: 221.09
    Episode_Reward/reaching_object: 0.9952
     Episode_Reward/lifting_object: 165.9183
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0673
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 2.07s
                      Time elapsed: 01:09:22
                               ETA: 00:03:27

################################################################################
                     [1m Learning iteration 1906/2000 [0m                     

                       Computation: 49039 steps/s (collection: 1.907s, learning 0.097s)
             Mean action noise std: 3.70
          Mean value_function loss: 176.9197
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 58.8792
                       Mean reward: 813.56
               Mean episode length: 218.66
    Episode_Reward/reaching_object: 1.0086
     Episode_Reward/lifting_object: 168.1080
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0681
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 2.00s
                      Time elapsed: 01:09:24
                               ETA: 00:03:25

################################################################################
                     [1m Learning iteration 1907/2000 [0m                     

                       Computation: 50049 steps/s (collection: 1.857s, learning 0.107s)
             Mean action noise std: 3.70
          Mean value_function loss: 165.3752
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.8858
                       Mean reward: 870.43
               Mean episode length: 230.07
    Episode_Reward/reaching_object: 0.9862
     Episode_Reward/lifting_object: 164.3493
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0667
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 1.96s
                      Time elapsed: 01:09:26
                               ETA: 00:03:23

################################################################################
                     [1m Learning iteration 1908/2000 [0m                     

                       Computation: 49372 steps/s (collection: 1.894s, learning 0.098s)
             Mean action noise std: 3.70
          Mean value_function loss: 133.9023
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 58.8932
                       Mean reward: 885.86
               Mean episode length: 234.77
    Episode_Reward/reaching_object: 1.0280
     Episode_Reward/lifting_object: 171.6736
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0692
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 1.99s
                      Time elapsed: 01:09:28
                               ETA: 00:03:20

################################################################################
                     [1m Learning iteration 1909/2000 [0m                     

                       Computation: 50456 steps/s (collection: 1.851s, learning 0.098s)
             Mean action noise std: 3.70
          Mean value_function loss: 186.7804
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.9017
                       Mean reward: 855.29
               Mean episode length: 227.17
    Episode_Reward/reaching_object: 0.9969
     Episode_Reward/lifting_object: 166.2716
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0672
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 1.95s
                      Time elapsed: 01:09:30
                               ETA: 00:03:18

################################################################################
                     [1m Learning iteration 1910/2000 [0m                     

                       Computation: 48860 steps/s (collection: 1.894s, learning 0.118s)
             Mean action noise std: 3.70
          Mean value_function loss: 203.2752
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.9120
                       Mean reward: 798.07
               Mean episode length: 215.48
    Episode_Reward/reaching_object: 0.9739
     Episode_Reward/lifting_object: 161.4020
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 2.01s
                      Time elapsed: 01:09:32
                               ETA: 00:03:16

################################################################################
                     [1m Learning iteration 1911/2000 [0m                     

                       Computation: 49048 steps/s (collection: 1.882s, learning 0.123s)
             Mean action noise std: 3.70
          Mean value_function loss: 154.5307
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.9209
                       Mean reward: 835.80
               Mean episode length: 223.13
    Episode_Reward/reaching_object: 1.0015
     Episode_Reward/lifting_object: 165.8673
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 2.00s
                      Time elapsed: 01:09:34
                               ETA: 00:03:14

################################################################################
                     [1m Learning iteration 1912/2000 [0m                     

                       Computation: 47538 steps/s (collection: 1.942s, learning 0.126s)
             Mean action noise std: 3.71
          Mean value_function loss: 172.5797
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 58.9257
                       Mean reward: 820.94
               Mean episode length: 218.32
    Episode_Reward/reaching_object: 0.9792
     Episode_Reward/lifting_object: 162.7091
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0663
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 2.07s
                      Time elapsed: 01:09:36
                               ETA: 00:03:12

################################################################################
                     [1m Learning iteration 1913/2000 [0m                     

                       Computation: 49291 steps/s (collection: 1.886s, learning 0.109s)
             Mean action noise std: 3.71
          Mean value_function loss: 149.6000
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 58.9318
                       Mean reward: 842.72
               Mean episode length: 223.72
    Episode_Reward/reaching_object: 1.0109
     Episode_Reward/lifting_object: 168.1279
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 1.99s
                      Time elapsed: 01:09:38
                               ETA: 00:03:09

################################################################################
                     [1m Learning iteration 1914/2000 [0m                     

                       Computation: 50353 steps/s (collection: 1.856s, learning 0.096s)
             Mean action noise std: 3.71
          Mean value_function loss: 157.5301
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.9428
                       Mean reward: 851.15
               Mean episode length: 224.99
    Episode_Reward/reaching_object: 1.0033
     Episode_Reward/lifting_object: 166.8666
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 1.95s
                      Time elapsed: 01:09:40
                               ETA: 00:03:07

################################################################################
                     [1m Learning iteration 1915/2000 [0m                     

                       Computation: 50111 steps/s (collection: 1.871s, learning 0.091s)
             Mean action noise std: 3.71
          Mean value_function loss: 178.4317
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.9522
                       Mean reward: 853.27
               Mean episode length: 227.55
    Episode_Reward/reaching_object: 1.0189
     Episode_Reward/lifting_object: 169.2642
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0689
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 1.96s
                      Time elapsed: 01:09:42
                               ETA: 00:03:05

################################################################################
                     [1m Learning iteration 1916/2000 [0m                     

                       Computation: 46369 steps/s (collection: 1.951s, learning 0.169s)
             Mean action noise std: 3.71
          Mean value_function loss: 101.7614
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.9621
                       Mean reward: 900.82
               Mean episode length: 237.39
    Episode_Reward/reaching_object: 1.0366
     Episode_Reward/lifting_object: 172.4489
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0697
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 2.12s
                      Time elapsed: 01:09:44
                               ETA: 00:03:03

################################################################################
                     [1m Learning iteration 1917/2000 [0m                     

                       Computation: 47525 steps/s (collection: 1.953s, learning 0.116s)
             Mean action noise std: 3.71
          Mean value_function loss: 134.5484
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 58.9738
                       Mean reward: 872.84
               Mean episode length: 231.07
    Episode_Reward/reaching_object: 1.0395
     Episode_Reward/lifting_object: 173.2490
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0701
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 2.07s
                      Time elapsed: 01:09:46
                               ETA: 00:03:01

################################################################################
                     [1m Learning iteration 1918/2000 [0m                     

                       Computation: 45348 steps/s (collection: 2.073s, learning 0.095s)
             Mean action noise std: 3.72
          Mean value_function loss: 172.4590
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.9893
                       Mean reward: 835.51
               Mean episode length: 221.83
    Episode_Reward/reaching_object: 1.0007
     Episode_Reward/lifting_object: 166.4682
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 2.17s
                      Time elapsed: 01:09:48
                               ETA: 00:02:58

################################################################################
                     [1m Learning iteration 1919/2000 [0m                     

                       Computation: 48517 steps/s (collection: 1.921s, learning 0.105s)
             Mean action noise std: 3.72
          Mean value_function loss: 145.3588
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 59.0014
                       Mean reward: 846.82
               Mean episode length: 224.67
    Episode_Reward/reaching_object: 1.0241
     Episode_Reward/lifting_object: 170.3520
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0693
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 2.03s
                      Time elapsed: 01:09:50
                               ETA: 00:02:56

################################################################################
                     [1m Learning iteration 1920/2000 [0m                     

                       Computation: 49050 steps/s (collection: 1.910s, learning 0.094s)
             Mean action noise std: 3.72
          Mean value_function loss: 169.1326
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 59.0094
                       Mean reward: 844.24
               Mean episode length: 223.59
    Episode_Reward/reaching_object: 1.0047
     Episode_Reward/lifting_object: 167.0502
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0680
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 2.00s
                      Time elapsed: 01:09:52
                               ETA: 00:02:54

################################################################################
                     [1m Learning iteration 1921/2000 [0m                     

                       Computation: 42683 steps/s (collection: 2.138s, learning 0.165s)
             Mean action noise std: 3.72
          Mean value_function loss: 142.5445
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 59.0230
                       Mean reward: 823.46
               Mean episode length: 220.12
    Episode_Reward/reaching_object: 1.0059
     Episode_Reward/lifting_object: 167.0835
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 2.30s
                      Time elapsed: 01:09:55
                               ETA: 00:02:52

################################################################################
                     [1m Learning iteration 1922/2000 [0m                     

                       Computation: 49395 steps/s (collection: 1.897s, learning 0.093s)
             Mean action noise std: 3.72
          Mean value_function loss: 153.6786
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 59.0337
                       Mean reward: 854.02
               Mean episode length: 226.75
    Episode_Reward/reaching_object: 1.0118
     Episode_Reward/lifting_object: 168.7881
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0687
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 1.99s
                      Time elapsed: 01:09:57
                               ETA: 00:02:50

################################################################################
                     [1m Learning iteration 1923/2000 [0m                     

                       Computation: 48449 steps/s (collection: 1.925s, learning 0.104s)
             Mean action noise std: 3.73
          Mean value_function loss: 165.8043
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 59.0456
                       Mean reward: 895.00
               Mean episode length: 236.42
    Episode_Reward/reaching_object: 1.0244
     Episode_Reward/lifting_object: 170.7452
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 2.03s
                      Time elapsed: 01:09:59
                               ETA: 00:02:48

################################################################################
                     [1m Learning iteration 1924/2000 [0m                     

                       Computation: 48250 steps/s (collection: 1.935s, learning 0.102s)
             Mean action noise std: 3.73
          Mean value_function loss: 164.0516
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 59.0544
                       Mean reward: 836.47
               Mean episode length: 221.59
    Episode_Reward/reaching_object: 1.0009
     Episode_Reward/lifting_object: 166.6008
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 2.04s
                      Time elapsed: 01:10:01
                               ETA: 00:02:45

################################################################################
                     [1m Learning iteration 1925/2000 [0m                     

                       Computation: 49290 steps/s (collection: 1.902s, learning 0.092s)
             Mean action noise std: 3.73
          Mean value_function loss: 148.9495
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 59.0650
                       Mean reward: 820.65
               Mean episode length: 220.02
    Episode_Reward/reaching_object: 1.0263
     Episode_Reward/lifting_object: 171.2063
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 1.99s
                      Time elapsed: 01:10:03
                               ETA: 00:02:43

################################################################################
                     [1m Learning iteration 1926/2000 [0m                     

                       Computation: 48970 steps/s (collection: 1.906s, learning 0.101s)
             Mean action noise std: 3.73
          Mean value_function loss: 134.4075
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.0764
                       Mean reward: 808.48
               Mean episode length: 216.07
    Episode_Reward/reaching_object: 1.0089
     Episode_Reward/lifting_object: 168.1330
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 2.01s
                      Time elapsed: 01:10:05
                               ETA: 00:02:41

################################################################################
                     [1m Learning iteration 1927/2000 [0m                     

                       Computation: 48590 steps/s (collection: 1.933s, learning 0.090s)
             Mean action noise std: 3.73
          Mean value_function loss: 123.6160
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 59.0862
                       Mean reward: 836.54
               Mean episode length: 222.30
    Episode_Reward/reaching_object: 1.0288
     Episode_Reward/lifting_object: 172.2236
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0703
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 2.02s
                      Time elapsed: 01:10:07
                               ETA: 00:02:39

################################################################################
                     [1m Learning iteration 1928/2000 [0m                     

                       Computation: 48660 steps/s (collection: 1.924s, learning 0.096s)
             Mean action noise std: 3.74
          Mean value_function loss: 138.6015
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 59.1023
                       Mean reward: 883.95
               Mean episode length: 233.30
    Episode_Reward/reaching_object: 1.0386
     Episode_Reward/lifting_object: 173.6393
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0710
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 2.02s
                      Time elapsed: 01:10:09
                               ETA: 00:02:37

################################################################################
                     [1m Learning iteration 1929/2000 [0m                     

                       Computation: 49062 steps/s (collection: 1.911s, learning 0.093s)
             Mean action noise std: 3.74
          Mean value_function loss: 170.9493
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 59.1199
                       Mean reward: 790.05
               Mean episode length: 211.16
    Episode_Reward/reaching_object: 1.0004
     Episode_Reward/lifting_object: 167.0660
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0686
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 2.00s
                      Time elapsed: 01:10:11
                               ETA: 00:02:34

################################################################################
                     [1m Learning iteration 1930/2000 [0m                     

                       Computation: 49361 steps/s (collection: 1.892s, learning 0.099s)
             Mean action noise std: 3.74
          Mean value_function loss: 126.0902
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 59.1309
                       Mean reward: 888.93
               Mean episode length: 234.55
    Episode_Reward/reaching_object: 1.0018
     Episode_Reward/lifting_object: 167.2302
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0689
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 1.99s
                      Time elapsed: 01:10:13
                               ETA: 00:02:32

################################################################################
                     [1m Learning iteration 1931/2000 [0m                     

                       Computation: 44174 steps/s (collection: 2.119s, learning 0.106s)
             Mean action noise std: 3.74
          Mean value_function loss: 123.6030
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 59.1354
                       Mean reward: 852.62
               Mean episode length: 226.37
    Episode_Reward/reaching_object: 1.0364
     Episode_Reward/lifting_object: 173.4194
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0711
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 2.23s
                      Time elapsed: 01:10:15
                               ETA: 00:02:30

################################################################################
                     [1m Learning iteration 1932/2000 [0m                     

                       Computation: 48270 steps/s (collection: 1.925s, learning 0.112s)
             Mean action noise std: 3.74
          Mean value_function loss: 173.6474
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.1397
                       Mean reward: 878.77
               Mean episode length: 232.92
    Episode_Reward/reaching_object: 1.0363
     Episode_Reward/lifting_object: 173.3340
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0715
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 2.04s
                      Time elapsed: 01:10:17
                               ETA: 00:02:28

################################################################################
                     [1m Learning iteration 1933/2000 [0m                     

                       Computation: 48648 steps/s (collection: 1.915s, learning 0.106s)
             Mean action noise std: 3.74
          Mean value_function loss: 180.8810
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 59.1478
                       Mean reward: 862.37
               Mean episode length: 229.55
    Episode_Reward/reaching_object: 1.0089
     Episode_Reward/lifting_object: 167.9857
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0697
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 2.02s
                      Time elapsed: 01:10:19
                               ETA: 00:02:26

################################################################################
                     [1m Learning iteration 1934/2000 [0m                     

                       Computation: 47345 steps/s (collection: 1.945s, learning 0.131s)
             Mean action noise std: 3.75
          Mean value_function loss: 132.9685
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.1570
                       Mean reward: 827.26
               Mean episode length: 220.83
    Episode_Reward/reaching_object: 1.0111
     Episode_Reward/lifting_object: 168.6510
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0701
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 2.08s
                      Time elapsed: 01:10:21
                               ETA: 00:02:23

################################################################################
                     [1m Learning iteration 1935/2000 [0m                     

                       Computation: 46754 steps/s (collection: 1.997s, learning 0.106s)
             Mean action noise std: 3.75
          Mean value_function loss: 153.9699
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 59.1658
                       Mean reward: 831.44
               Mean episode length: 221.91
    Episode_Reward/reaching_object: 1.0069
     Episode_Reward/lifting_object: 167.8994
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0698
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 2.10s
                      Time elapsed: 01:10:23
                               ETA: 00:02:21

################################################################################
                     [1m Learning iteration 1936/2000 [0m                     

                       Computation: 45855 steps/s (collection: 2.051s, learning 0.093s)
             Mean action noise std: 3.75
          Mean value_function loss: 126.7761
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.1772
                       Mean reward: 894.84
               Mean episode length: 236.00
    Episode_Reward/reaching_object: 1.0107
     Episode_Reward/lifting_object: 168.8272
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0702
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 2.14s
                      Time elapsed: 01:10:25
                               ETA: 00:02:19

################################################################################
                     [1m Learning iteration 1937/2000 [0m                     

                       Computation: 47216 steps/s (collection: 1.982s, learning 0.100s)
             Mean action noise std: 3.75
          Mean value_function loss: 121.2030
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 59.1910
                       Mean reward: 850.29
               Mean episode length: 225.38
    Episode_Reward/reaching_object: 1.0189
     Episode_Reward/lifting_object: 169.7981
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0706
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 2.08s
                      Time elapsed: 01:10:27
                               ETA: 00:02:17

################################################################################
                     [1m Learning iteration 1938/2000 [0m                     

                       Computation: 48118 steps/s (collection: 1.946s, learning 0.097s)
             Mean action noise std: 3.75
          Mean value_function loss: 158.7574
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.1982
                       Mean reward: 823.02
               Mean episode length: 219.41
    Episode_Reward/reaching_object: 0.9985
     Episode_Reward/lifting_object: 166.3634
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0696
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 2.04s
                      Time elapsed: 01:10:29
                               ETA: 00:02:15

################################################################################
                     [1m Learning iteration 1939/2000 [0m                     

                       Computation: 48198 steps/s (collection: 1.941s, learning 0.099s)
             Mean action noise std: 3.75
          Mean value_function loss: 198.9439
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 59.2031
                       Mean reward: 830.49
               Mean episode length: 221.70
    Episode_Reward/reaching_object: 0.9862
     Episode_Reward/lifting_object: 163.8768
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0689
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 2.04s
                      Time elapsed: 01:10:31
                               ETA: 00:02:13

################################################################################
                     [1m Learning iteration 1940/2000 [0m                     

                       Computation: 42656 steps/s (collection: 2.153s, learning 0.152s)
             Mean action noise std: 3.76
          Mean value_function loss: 146.9724
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 59.2083
                       Mean reward: 834.81
               Mean episode length: 222.12
    Episode_Reward/reaching_object: 0.9920
     Episode_Reward/lifting_object: 165.0164
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0695
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 2.30s
                      Time elapsed: 01:10:34
                               ETA: 00:02:10

################################################################################
                     [1m Learning iteration 1941/2000 [0m                     

                       Computation: 48486 steps/s (collection: 1.931s, learning 0.096s)
             Mean action noise std: 3.76
          Mean value_function loss: 171.7410
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 59.2160
                       Mean reward: 861.76
               Mean episode length: 228.99
    Episode_Reward/reaching_object: 1.0124
     Episode_Reward/lifting_object: 168.5540
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0709
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 2.03s
                      Time elapsed: 01:10:36
                               ETA: 00:02:08

################################################################################
                     [1m Learning iteration 1942/2000 [0m                     

                       Computation: 47019 steps/s (collection: 1.986s, learning 0.105s)
             Mean action noise std: 3.76
          Mean value_function loss: 121.9955
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 59.2247
                       Mean reward: 859.73
               Mean episode length: 227.65
    Episode_Reward/reaching_object: 1.0046
     Episode_Reward/lifting_object: 167.2959
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0704
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 2.09s
                      Time elapsed: 01:10:38
                               ETA: 00:02:06

################################################################################
                     [1m Learning iteration 1943/2000 [0m                     

                       Computation: 47700 steps/s (collection: 1.936s, learning 0.125s)
             Mean action noise std: 3.76
          Mean value_function loss: 172.7113
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 59.2329
                       Mean reward: 846.05
               Mean episode length: 226.01
    Episode_Reward/reaching_object: 1.0197
     Episode_Reward/lifting_object: 169.7121
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0715
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 2.06s
                      Time elapsed: 01:10:40
                               ETA: 00:02:04

################################################################################
                     [1m Learning iteration 1944/2000 [0m                     

                       Computation: 46255 steps/s (collection: 1.936s, learning 0.190s)
             Mean action noise std: 3.76
          Mean value_function loss: 156.4159
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 59.2408
                       Mean reward: 865.31
               Mean episode length: 229.60
    Episode_Reward/reaching_object: 1.0128
     Episode_Reward/lifting_object: 168.6761
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0711
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 2.13s
                      Time elapsed: 01:10:42
                               ETA: 00:02:02

################################################################################
                     [1m Learning iteration 1945/2000 [0m                     

                       Computation: 48614 steps/s (collection: 1.909s, learning 0.114s)
             Mean action noise std: 3.76
          Mean value_function loss: 138.0634
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.2452
                       Mean reward: 872.66
               Mean episode length: 231.91
    Episode_Reward/reaching_object: 1.0405
     Episode_Reward/lifting_object: 173.6479
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 2.02s
                      Time elapsed: 01:10:44
                               ETA: 00:01:59

################################################################################
                     [1m Learning iteration 1946/2000 [0m                     

                       Computation: 48035 steps/s (collection: 1.927s, learning 0.119s)
             Mean action noise std: 3.76
          Mean value_function loss: 149.9386
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.2525
                       Mean reward: 862.15
               Mean episode length: 228.37
    Episode_Reward/reaching_object: 1.0219
     Episode_Reward/lifting_object: 170.1565
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0721
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 2.05s
                      Time elapsed: 01:10:46
                               ETA: 00:01:57

################################################################################
                     [1m Learning iteration 1947/2000 [0m                     

                       Computation: 47341 steps/s (collection: 1.956s, learning 0.121s)
             Mean action noise std: 3.77
          Mean value_function loss: 129.9570
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 59.2582
                       Mean reward: 898.63
               Mean episode length: 236.97
    Episode_Reward/reaching_object: 1.0201
     Episode_Reward/lifting_object: 170.3181
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0717
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 2.08s
                      Time elapsed: 01:10:48
                               ETA: 00:01:55

################################################################################
                     [1m Learning iteration 1948/2000 [0m                     

                       Computation: 47487 steps/s (collection: 1.955s, learning 0.115s)
             Mean action noise std: 3.77
          Mean value_function loss: 141.8193
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 59.2665
                       Mean reward: 817.45
               Mean episode length: 219.09
    Episode_Reward/reaching_object: 0.9958
     Episode_Reward/lifting_object: 165.8828
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0707
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 2.07s
                      Time elapsed: 01:10:50
                               ETA: 00:01:53

################################################################################
                     [1m Learning iteration 1949/2000 [0m                     

                       Computation: 45741 steps/s (collection: 2.048s, learning 0.101s)
             Mean action noise std: 3.77
          Mean value_function loss: 160.3607
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 59.2754
                       Mean reward: 836.64
               Mean episode length: 222.09
    Episode_Reward/reaching_object: 1.0269
     Episode_Reward/lifting_object: 171.4288
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0730
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 2.15s
                      Time elapsed: 01:10:52
                               ETA: 00:01:51

################################################################################
                     [1m Learning iteration 1950/2000 [0m                     

                       Computation: 47861 steps/s (collection: 1.957s, learning 0.097s)
             Mean action noise std: 3.77
          Mean value_function loss: 172.8036
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 59.2808
                       Mean reward: 870.36
               Mean episode length: 231.24
    Episode_Reward/reaching_object: 0.9914
     Episode_Reward/lifting_object: 165.3091
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0704
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 2.05s
                      Time elapsed: 01:10:54
                               ETA: 00:01:49

################################################################################
                     [1m Learning iteration 1951/2000 [0m                     

                       Computation: 48268 steps/s (collection: 1.933s, learning 0.104s)
             Mean action noise std: 3.77
          Mean value_function loss: 114.2398
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 59.2850
                       Mean reward: 904.21
               Mean episode length: 238.56
    Episode_Reward/reaching_object: 1.0374
     Episode_Reward/lifting_object: 173.7198
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0735
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 2.04s
                      Time elapsed: 01:10:57
                               ETA: 00:01:46

################################################################################
                     [1m Learning iteration 1952/2000 [0m                     

                       Computation: 47894 steps/s (collection: 1.954s, learning 0.098s)
             Mean action noise std: 3.77
          Mean value_function loss: 124.1181
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.2905
                       Mean reward: 874.77
               Mean episode length: 231.58
    Episode_Reward/reaching_object: 1.0279
     Episode_Reward/lifting_object: 171.7394
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0730
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 2.05s
                      Time elapsed: 01:10:59
                               ETA: 00:01:44

################################################################################
                     [1m Learning iteration 1953/2000 [0m                     

                       Computation: 48127 steps/s (collection: 1.935s, learning 0.108s)
             Mean action noise std: 3.77
          Mean value_function loss: 143.1019
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 59.2965
                       Mean reward: 844.57
               Mean episode length: 224.55
    Episode_Reward/reaching_object: 0.9881
     Episode_Reward/lifting_object: 164.8576
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0705
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 2.04s
                      Time elapsed: 01:11:01
                               ETA: 00:01:42

################################################################################
                     [1m Learning iteration 1954/2000 [0m                     

                       Computation: 48511 steps/s (collection: 1.923s, learning 0.104s)
             Mean action noise std: 3.77
          Mean value_function loss: 169.4295
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 59.3035
                       Mean reward: 821.85
               Mean episode length: 218.92
    Episode_Reward/reaching_object: 0.9851
     Episode_Reward/lifting_object: 164.2830
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0704
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 2.03s
                      Time elapsed: 01:11:03
                               ETA: 00:01:40

################################################################################
                     [1m Learning iteration 1955/2000 [0m                     

                       Computation: 48105 steps/s (collection: 1.930s, learning 0.113s)
             Mean action noise std: 3.77
          Mean value_function loss: 174.8892
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 59.3119
                       Mean reward: 794.07
               Mean episode length: 213.27
    Episode_Reward/reaching_object: 0.9748
     Episode_Reward/lifting_object: 162.2508
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 2.04s
                      Time elapsed: 01:11:05
                               ETA: 00:01:38

################################################################################
                     [1m Learning iteration 1956/2000 [0m                     

                       Computation: 48072 steps/s (collection: 1.932s, learning 0.113s)
             Mean action noise std: 3.78
          Mean value_function loss: 178.9862
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 59.3158
                       Mean reward: 830.46
               Mean episode length: 221.26
    Episode_Reward/reaching_object: 0.9871
     Episode_Reward/lifting_object: 164.7221
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0708
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 2.04s
                      Time elapsed: 01:11:07
                               ETA: 00:01:35

################################################################################
                     [1m Learning iteration 1957/2000 [0m                     

                       Computation: 45725 steps/s (collection: 2.033s, learning 0.117s)
             Mean action noise std: 3.78
          Mean value_function loss: 127.8237
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 59.3194
                       Mean reward: 848.54
               Mean episode length: 225.30
    Episode_Reward/reaching_object: 1.0091
     Episode_Reward/lifting_object: 168.0125
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0723
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 2.15s
                      Time elapsed: 01:11:09
                               ETA: 00:01:33

################################################################################
                     [1m Learning iteration 1958/2000 [0m                     

                       Computation: 48829 steps/s (collection: 1.898s, learning 0.115s)
             Mean action noise std: 3.78
          Mean value_function loss: 125.7874
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 59.3271
                       Mean reward: 857.55
               Mean episode length: 227.12
    Episode_Reward/reaching_object: 0.9890
     Episode_Reward/lifting_object: 164.5661
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0711
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 2.01s
                      Time elapsed: 01:11:11
                               ETA: 00:01:31

################################################################################
                     [1m Learning iteration 1959/2000 [0m                     

                       Computation: 46810 steps/s (collection: 1.961s, learning 0.140s)
             Mean action noise std: 3.78
          Mean value_function loss: 141.2958
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 59.3399
                       Mean reward: 879.39
               Mean episode length: 232.90
    Episode_Reward/reaching_object: 1.0474
     Episode_Reward/lifting_object: 175.3138
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0749
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 2.10s
                      Time elapsed: 01:11:13
                               ETA: 00:01:29

################################################################################
                     [1m Learning iteration 1960/2000 [0m                     

                       Computation: 47057 steps/s (collection: 1.969s, learning 0.121s)
             Mean action noise std: 3.78
          Mean value_function loss: 153.9088
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 59.3510
                       Mean reward: 870.59
               Mean episode length: 230.57
    Episode_Reward/reaching_object: 1.0294
     Episode_Reward/lifting_object: 171.8683
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0734
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 2.09s
                      Time elapsed: 01:11:15
                               ETA: 00:01:27

################################################################################
                     [1m Learning iteration 1961/2000 [0m                     

                       Computation: 49782 steps/s (collection: 1.883s, learning 0.092s)
             Mean action noise std: 3.78
          Mean value_function loss: 187.2765
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 59.3575
                       Mean reward: 844.03
               Mean episode length: 223.80
    Episode_Reward/reaching_object: 0.9950
     Episode_Reward/lifting_object: 165.9452
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0713
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 1.97s
                      Time elapsed: 01:11:17
                               ETA: 00:01:25

################################################################################
                     [1m Learning iteration 1962/2000 [0m                     

                       Computation: 48884 steps/s (collection: 1.910s, learning 0.101s)
             Mean action noise std: 3.78
          Mean value_function loss: 157.8062
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 59.3644
                       Mean reward: 800.61
               Mean episode length: 214.48
    Episode_Reward/reaching_object: 0.9951
     Episode_Reward/lifting_object: 165.6085
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0714
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 2.01s
                      Time elapsed: 01:11:19
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 1963/2000 [0m                     

                       Computation: 46264 steps/s (collection: 1.979s, learning 0.146s)
             Mean action noise std: 3.79
          Mean value_function loss: 142.0077
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.3709
                       Mean reward: 839.09
               Mean episode length: 222.94
    Episode_Reward/reaching_object: 1.0103
     Episode_Reward/lifting_object: 168.8823
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0723
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 2.12s
                      Time elapsed: 01:11:21
                               ETA: 00:01:20

################################################################################
                     [1m Learning iteration 1964/2000 [0m                     

                       Computation: 48764 steps/s (collection: 1.916s, learning 0.100s)
             Mean action noise std: 3.79
          Mean value_function loss: 134.7329
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 59.3777
                       Mean reward: 856.15
               Mean episode length: 227.80
    Episode_Reward/reaching_object: 1.0002
     Episode_Reward/lifting_object: 167.2888
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0717
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 2.02s
                      Time elapsed: 01:11:23
                               ETA: 00:01:18

################################################################################
                     [1m Learning iteration 1965/2000 [0m                     

                       Computation: 47675 steps/s (collection: 1.968s, learning 0.094s)
             Mean action noise std: 3.79
          Mean value_function loss: 133.7573
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 59.3868
                       Mean reward: 823.69
               Mean episode length: 219.62
    Episode_Reward/reaching_object: 1.0221
     Episode_Reward/lifting_object: 170.7376
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0730
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 2.06s
                      Time elapsed: 01:11:25
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 1966/2000 [0m                     

                       Computation: 47509 steps/s (collection: 1.969s, learning 0.101s)
             Mean action noise std: 3.79
          Mean value_function loss: 147.2420
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 59.3914
                       Mean reward: 845.32
               Mean episode length: 224.97
    Episode_Reward/reaching_object: 1.0044
     Episode_Reward/lifting_object: 167.8009
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0718
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 2.07s
                      Time elapsed: 01:11:27
                               ETA: 00:01:14

################################################################################
                     [1m Learning iteration 1967/2000 [0m                     

                       Computation: 48258 steps/s (collection: 1.927s, learning 0.110s)
             Mean action noise std: 3.79
          Mean value_function loss: 141.0106
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 59.3985
                       Mean reward: 853.86
               Mean episode length: 227.08
    Episode_Reward/reaching_object: 1.0262
     Episode_Reward/lifting_object: 171.5230
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 2.04s
                      Time elapsed: 01:11:29
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 1968/2000 [0m                     

                       Computation: 48836 steps/s (collection: 1.922s, learning 0.091s)
             Mean action noise std: 3.79
          Mean value_function loss: 145.9175
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 59.4055
                       Mean reward: 806.82
               Mean episode length: 216.49
    Episode_Reward/reaching_object: 0.9989
     Episode_Reward/lifting_object: 166.5959
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0717
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 2.01s
                      Time elapsed: 01:11:31
                               ETA: 00:01:09

################################################################################
                     [1m Learning iteration 1969/2000 [0m                     

                       Computation: 48815 steps/s (collection: 1.918s, learning 0.096s)
             Mean action noise std: 3.79
          Mean value_function loss: 135.6627
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 59.4152
                       Mean reward: 823.13
               Mean episode length: 219.50
    Episode_Reward/reaching_object: 1.0136
     Episode_Reward/lifting_object: 168.9147
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0726
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 2.01s
                      Time elapsed: 01:11:33
                               ETA: 00:01:07

################################################################################
                     [1m Learning iteration 1970/2000 [0m                     

                       Computation: 47600 steps/s (collection: 1.953s, learning 0.112s)
             Mean action noise std: 3.80
          Mean value_function loss: 149.4347
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 59.4266
                       Mean reward: 844.72
               Mean episode length: 224.03
    Episode_Reward/reaching_object: 1.0342
     Episode_Reward/lifting_object: 172.7120
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0738
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 2.07s
                      Time elapsed: 01:11:35
                               ETA: 00:01:05

################################################################################
                     [1m Learning iteration 1971/2000 [0m                     

                       Computation: 46500 steps/s (collection: 2.008s, learning 0.106s)
             Mean action noise std: 3.80
          Mean value_function loss: 142.0610
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 59.4366
                       Mean reward: 857.84
               Mean episode length: 227.89
    Episode_Reward/reaching_object: 1.0098
     Episode_Reward/lifting_object: 167.7719
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0725
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 2.11s
                      Time elapsed: 01:11:38
                               ETA: 00:01:03

################################################################################
                     [1m Learning iteration 1972/2000 [0m                     

                       Computation: 46371 steps/s (collection: 1.982s, learning 0.138s)
             Mean action noise std: 3.80
          Mean value_function loss: 156.8401
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 59.4413
                       Mean reward: 837.89
               Mean episode length: 222.95
    Episode_Reward/reaching_object: 1.0104
     Episode_Reward/lifting_object: 168.1154
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0726
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 2.12s
                      Time elapsed: 01:11:40
                               ETA: 00:01:01

################################################################################
                     [1m Learning iteration 1973/2000 [0m                     

                       Computation: 48198 steps/s (collection: 1.922s, learning 0.117s)
             Mean action noise std: 3.80
          Mean value_function loss: 160.6873
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.4465
                       Mean reward: 827.46
               Mean episode length: 221.60
    Episode_Reward/reaching_object: 0.9946
     Episode_Reward/lifting_object: 164.9426
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0717
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 2.04s
                      Time elapsed: 01:11:42
                               ETA: 00:00:58

################################################################################
                     [1m Learning iteration 1974/2000 [0m                     

                       Computation: 46660 steps/s (collection: 2.009s, learning 0.098s)
             Mean action noise std: 3.80
          Mean value_function loss: 120.8407
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 59.4540
                       Mean reward: 853.97
               Mean episode length: 225.89
    Episode_Reward/reaching_object: 0.9999
     Episode_Reward/lifting_object: 165.8068
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0721
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 2.11s
                      Time elapsed: 01:11:44
                               ETA: 00:00:56

################################################################################
                     [1m Learning iteration 1975/2000 [0m                     

                       Computation: 48971 steps/s (collection: 1.893s, learning 0.114s)
             Mean action noise std: 3.80
          Mean value_function loss: 159.7106
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 59.4616
                       Mean reward: 848.94
               Mean episode length: 227.07
    Episode_Reward/reaching_object: 1.0148
     Episode_Reward/lifting_object: 168.5418
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0729
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 2.01s
                      Time elapsed: 01:11:46
                               ETA: 00:00:54

################################################################################
                     [1m Learning iteration 1976/2000 [0m                     

                       Computation: 44772 steps/s (collection: 2.033s, learning 0.163s)
             Mean action noise std: 3.81
          Mean value_function loss: 141.2415
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 59.4733
                       Mean reward: 753.41
               Mean episode length: 205.89
    Episode_Reward/reaching_object: 0.9947
     Episode_Reward/lifting_object: 165.0453
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0715
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 2.20s
                      Time elapsed: 01:11:48
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1977/2000 [0m                     

                       Computation: 49005 steps/s (collection: 1.911s, learning 0.095s)
             Mean action noise std: 3.81
          Mean value_function loss: 117.1588
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 59.4805
                       Mean reward: 867.07
               Mean episode length: 230.32
    Episode_Reward/reaching_object: 1.0300
     Episode_Reward/lifting_object: 171.4911
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 2.01s
                      Time elapsed: 01:11:50
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1978/2000 [0m                     

                       Computation: 47862 steps/s (collection: 1.961s, learning 0.093s)
             Mean action noise std: 3.81
          Mean value_function loss: 139.7965
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 59.4901
                       Mean reward: 820.01
               Mean episode length: 218.67
    Episode_Reward/reaching_object: 1.0211
     Episode_Reward/lifting_object: 170.0029
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0730
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 2.05s
                      Time elapsed: 01:11:52
                               ETA: 00:00:47

################################################################################
                     [1m Learning iteration 1979/2000 [0m                     

                       Computation: 48654 steps/s (collection: 1.929s, learning 0.092s)
             Mean action noise std: 3.81
          Mean value_function loss: 156.5268
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 59.4993
                       Mean reward: 836.97
               Mean episode length: 223.19
    Episode_Reward/reaching_object: 1.0225
     Episode_Reward/lifting_object: 170.5792
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 2.02s
                      Time elapsed: 01:11:54
                               ETA: 00:00:45

################################################################################
                     [1m Learning iteration 1980/2000 [0m                     

                       Computation: 49411 steps/s (collection: 1.895s, learning 0.094s)
             Mean action noise std: 3.81
          Mean value_function loss: 139.9160
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 59.5047
                       Mean reward: 827.15
               Mean episode length: 220.10
    Episode_Reward/reaching_object: 0.9900
     Episode_Reward/lifting_object: 164.4956
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0713
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 1.99s
                      Time elapsed: 01:11:56
                               ETA: 00:00:43

################################################################################
                     [1m Learning iteration 1981/2000 [0m                     

                       Computation: 46368 steps/s (collection: 1.964s, learning 0.156s)
             Mean action noise std: 3.81
          Mean value_function loss: 149.2964
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 59.5161
                       Mean reward: 846.58
               Mean episode length: 225.16
    Episode_Reward/reaching_object: 1.0005
     Episode_Reward/lifting_object: 166.5123
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0721
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 2.12s
                      Time elapsed: 01:11:58
                               ETA: 00:00:41

################################################################################
                     [1m Learning iteration 1982/2000 [0m                     

                       Computation: 47663 steps/s (collection: 1.957s, learning 0.105s)
             Mean action noise std: 3.81
          Mean value_function loss: 155.9066
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 59.5273
                       Mean reward: 795.02
               Mean episode length: 212.60
    Episode_Reward/reaching_object: 0.9749
     Episode_Reward/lifting_object: 161.9576
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0702
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 2.06s
                      Time elapsed: 01:12:00
                               ETA: 00:00:39

################################################################################
                     [1m Learning iteration 1983/2000 [0m                     

                       Computation: 48482 steps/s (collection: 1.931s, learning 0.097s)
             Mean action noise std: 3.82
          Mean value_function loss: 170.2304
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 59.5343
                       Mean reward: 812.99
               Mean episode length: 217.27
    Episode_Reward/reaching_object: 0.9979
     Episode_Reward/lifting_object: 166.0676
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0719
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 2.03s
                      Time elapsed: 01:12:02
                               ETA: 00:00:37

################################################################################
                     [1m Learning iteration 1984/2000 [0m                     

                       Computation: 47635 steps/s (collection: 1.960s, learning 0.104s)
             Mean action noise std: 3.82
          Mean value_function loss: 155.8868
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 59.5453
                       Mean reward: 833.72
               Mean episode length: 223.62
    Episode_Reward/reaching_object: 1.0025
     Episode_Reward/lifting_object: 166.6059
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0724
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 2.06s
                      Time elapsed: 01:12:04
                               ETA: 00:00:34

################################################################################
                     [1m Learning iteration 1985/2000 [0m                     

                       Computation: 48807 steps/s (collection: 1.917s, learning 0.098s)
             Mean action noise std: 3.82
          Mean value_function loss: 164.1260
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 59.5609
                       Mean reward: 831.32
               Mean episode length: 220.94
    Episode_Reward/reaching_object: 0.9919
     Episode_Reward/lifting_object: 165.3384
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0717
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 2.01s
                      Time elapsed: 01:12:06
                               ETA: 00:00:32

################################################################################
                     [1m Learning iteration 1986/2000 [0m                     

                       Computation: 47987 steps/s (collection: 1.954s, learning 0.095s)
             Mean action noise std: 3.82
          Mean value_function loss: 177.9609
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 59.5702
                       Mean reward: 841.22
               Mean episode length: 223.51
    Episode_Reward/reaching_object: 0.9985
     Episode_Reward/lifting_object: 166.3911
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0723
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 2.05s
                      Time elapsed: 01:12:08
                               ETA: 00:00:30

################################################################################
                     [1m Learning iteration 1987/2000 [0m                     

                       Computation: 46316 steps/s (collection: 2.024s, learning 0.098s)
             Mean action noise std: 3.83
          Mean value_function loss: 222.4352
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 59.5812
                       Mean reward: 805.61
               Mean episode length: 215.99
    Episode_Reward/reaching_object: 0.9861
     Episode_Reward/lifting_object: 163.8414
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0717
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 2.12s
                      Time elapsed: 01:12:11
                               ETA: 00:00:28

################################################################################
                     [1m Learning iteration 1988/2000 [0m                     

                       Computation: 48368 steps/s (collection: 1.940s, learning 0.093s)
             Mean action noise std: 3.83
          Mean value_function loss: 180.1334
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 59.5928
                       Mean reward: 774.21
               Mean episode length: 208.40
    Episode_Reward/reaching_object: 0.9876
     Episode_Reward/lifting_object: 163.8297
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0719
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 2.03s
                      Time elapsed: 01:12:13
                               ETA: 00:00:26

################################################################################
                     [1m Learning iteration 1989/2000 [0m                     

                       Computation: 47639 steps/s (collection: 1.940s, learning 0.123s)
             Mean action noise std: 3.83
          Mean value_function loss: 120.4013
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 59.5996
                       Mean reward: 807.08
               Mean episode length: 217.38
    Episode_Reward/reaching_object: 1.0068
     Episode_Reward/lifting_object: 167.3815
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0730
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 2.06s
                      Time elapsed: 01:12:15
                               ETA: 00:00:23

################################################################################
                     [1m Learning iteration 1990/2000 [0m                     

                       Computation: 45702 steps/s (collection: 1.945s, learning 0.206s)
             Mean action noise std: 3.83
          Mean value_function loss: 149.1173
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 59.6093
                       Mean reward: 809.58
               Mean episode length: 215.83
    Episode_Reward/reaching_object: 1.0129
     Episode_Reward/lifting_object: 168.3773
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0738
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 2.15s
                      Time elapsed: 01:12:17
                               ETA: 00:00:21

################################################################################
                     [1m Learning iteration 1991/2000 [0m                     

                       Computation: 47241 steps/s (collection: 1.985s, learning 0.096s)
             Mean action noise std: 3.83
          Mean value_function loss: 112.2966
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 59.6196
                       Mean reward: 851.30
               Mean episode length: 226.64
    Episode_Reward/reaching_object: 1.0212
     Episode_Reward/lifting_object: 170.0328
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0741
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 2.08s
                      Time elapsed: 01:12:19
                               ETA: 00:00:19

################################################################################
                     [1m Learning iteration 1992/2000 [0m                     

                       Computation: 46779 steps/s (collection: 2.011s, learning 0.091s)
             Mean action noise std: 3.83
          Mean value_function loss: 145.0796
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 59.6291
                       Mean reward: 834.75
               Mean episode length: 221.40
    Episode_Reward/reaching_object: 0.9984
     Episode_Reward/lifting_object: 166.0075
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0727
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 2.10s
                      Time elapsed: 01:12:21
                               ETA: 00:00:17

################################################################################
                     [1m Learning iteration 1993/2000 [0m                     

                       Computation: 48618 steps/s (collection: 1.900s, learning 0.122s)
             Mean action noise std: 3.83
          Mean value_function loss: 108.0989
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 59.6372
                       Mean reward: 877.82
               Mean episode length: 231.89
    Episode_Reward/reaching_object: 1.0231
     Episode_Reward/lifting_object: 170.7028
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0743
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 2.02s
                      Time elapsed: 01:12:23
                               ETA: 00:00:15

################################################################################
                     [1m Learning iteration 1994/2000 [0m                     

                       Computation: 48631 steps/s (collection: 1.903s, learning 0.119s)
             Mean action noise std: 3.84
          Mean value_function loss: 142.8028
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 59.6421
                       Mean reward: 866.36
               Mean episode length: 229.03
    Episode_Reward/reaching_object: 1.0322
     Episode_Reward/lifting_object: 172.3427
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0751
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 2.02s
                      Time elapsed: 01:12:25
                               ETA: 00:00:13

################################################################################
                     [1m Learning iteration 1995/2000 [0m                     

                       Computation: 46797 steps/s (collection: 2.000s, learning 0.101s)
             Mean action noise std: 3.84
          Mean value_function loss: 117.5612
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 59.6494
                       Mean reward: 857.47
               Mean episode length: 227.50
    Episode_Reward/reaching_object: 1.0308
     Episode_Reward/lifting_object: 172.0234
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0749
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 2.10s
                      Time elapsed: 01:12:27
                               ETA: 00:00:10

################################################################################
                     [1m Learning iteration 1996/2000 [0m                     

                       Computation: 47915 steps/s (collection: 1.951s, learning 0.101s)
             Mean action noise std: 3.84
          Mean value_function loss: 120.7086
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.6588
                       Mean reward: 851.84
               Mean episode length: 226.07
    Episode_Reward/reaching_object: 1.0454
     Episode_Reward/lifting_object: 174.7514
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0759
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 2.05s
                      Time elapsed: 01:12:29
                               ETA: 00:00:08

################################################################################
                     [1m Learning iteration 1997/2000 [0m                     

                       Computation: 47935 steps/s (collection: 1.952s, learning 0.099s)
             Mean action noise std: 3.84
          Mean value_function loss: 125.7409
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 59.6679
                       Mean reward: 792.60
               Mean episode length: 211.33
    Episode_Reward/reaching_object: 1.0149
     Episode_Reward/lifting_object: 169.5647
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0741
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 2.05s
                      Time elapsed: 01:12:31
                               ETA: 00:00:06

################################################################################
                     [1m Learning iteration 1998/2000 [0m                     

                       Computation: 48323 steps/s (collection: 1.938s, learning 0.096s)
             Mean action noise std: 3.84
          Mean value_function loss: 135.3263
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.6756
                       Mean reward: 861.45
               Mean episode length: 228.38
    Episode_Reward/reaching_object: 1.0213
     Episode_Reward/lifting_object: 170.5533
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0746
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 2.03s
                      Time elapsed: 01:12:33
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1999/2000 [0m                     

                       Computation: 49045 steps/s (collection: 1.907s, learning 0.097s)
             Mean action noise std: 3.84
          Mean value_function loss: 153.6442
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 59.6822
                       Mean reward: 839.69
               Mean episode length: 223.14
    Episode_Reward/reaching_object: 0.9966
     Episode_Reward/lifting_object: 166.3846
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 2.00s
                      Time elapsed: 01:12:35
                               ETA: 00:00:02

