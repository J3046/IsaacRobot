################################################################################
                      [1m Learning iteration 0/2000 [0m                       

                       Computation: 10161 steps/s (collection: 9.427s, learning 0.247s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0064
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 36.9324
                       Mean reward: 0.00
               Mean episode length: 21.21
    Episode_Reward/reaching_object: 0.0009
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0003
          Episode_Reward/joint_vel: -0.0004
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 9.67s
                      Time elapsed: 00:00:09
                               ETA: 05:22:27

################################################################################
                      [1m Learning iteration 1/2000 [0m                       

                       Computation: 14256 steps/s (collection: 6.770s, learning 0.126s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 37.0596
                       Mean reward: 0.01
               Mean episode length: 45.83
    Episode_Reward/reaching_object: 0.0026
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0008
          Episode_Reward/joint_vel: -0.0011
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 6.90s
                      Time elapsed: 00:00:16
                               ETA: 04:36:01

################################################################################
                      [1m Learning iteration 2/2000 [0m                       

                       Computation: 15138 steps/s (collection: 6.346s, learning 0.147s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 37.1224
                       Mean reward: 0.01
               Mean episode length: 69.78
    Episode_Reward/reaching_object: 0.0042
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0013
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 6.49s
                      Time elapsed: 00:00:23
                               ETA: 04:15:59

################################################################################
                      [1m Learning iteration 3/2000 [0m                       

                       Computation: 14858 steps/s (collection: 6.463s, learning 0.154s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 37.1799
                       Mean reward: 0.02
               Mean episode length: 93.42
    Episode_Reward/reaching_object: 0.0064
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 6.62s
                      Time elapsed: 00:00:29
                               ETA: 04:06:57

################################################################################
                      [1m Learning iteration 4/2000 [0m                       

                       Computation: 14691 steps/s (collection: 6.531s, learning 0.160s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 37.1980
                       Mean reward: 0.03
               Mean episode length: 117.74
    Episode_Reward/reaching_object: 0.0089
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0033
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 6.69s
                      Time elapsed: 00:00:36
                               ETA: 04:01:58

################################################################################
                      [1m Learning iteration 5/2000 [0m                       

                       Computation: 14956 steps/s (collection: 6.412s, learning 0.161s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 37.2111
                       Mean reward: 0.04
               Mean episode length: 141.96
    Episode_Reward/reaching_object: 0.0120
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0040
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 6.57s
                      Time elapsed: 00:00:42
                               ETA: 03:57:58

################################################################################
                      [1m Learning iteration 6/2000 [0m                       

                       Computation: 15067 steps/s (collection: 6.403s, learning 0.121s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 37.1997
                       Mean reward: 0.05
               Mean episode length: 165.77
    Episode_Reward/reaching_object: 0.0150
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0048
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 6.52s
                      Time elapsed: 00:00:49
                               ETA: 03:54:51

################################################################################
                      [1m Learning iteration 7/2000 [0m                       

                       Computation: 14909 steps/s (collection: 6.432s, learning 0.162s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0069
                 Mean entropy loss: 37.2275
                       Mean reward: 0.07
               Mean episode length: 189.36
    Episode_Reward/reaching_object: 0.0197
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0055
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 6.59s
                      Time elapsed: 00:00:56
                               ETA: 03:52:46

################################################################################
                      [1m Learning iteration 8/2000 [0m                       

                       Computation: 18633 steps/s (collection: 5.165s, learning 0.111s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0078
                 Mean entropy loss: 37.2516
                       Mean reward: 0.09
               Mean episode length: 213.10
    Episode_Reward/reaching_object: 0.0252
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0062
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 5.28s
                      Time elapsed: 00:01:01
                               ETA: 03:46:15

################################################################################
                      [1m Learning iteration 9/2000 [0m                       

                       Computation: 58241 steps/s (collection: 1.596s, learning 0.092s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 37.2711
                       Mean reward: 0.12
               Mean episode length: 237.13
    Episode_Reward/reaching_object: 0.0319
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 1.69s
                      Time elapsed: 00:01:03
                               ETA: 03:29:08

################################################################################
                      [1m Learning iteration 10/2000 [0m                      

                       Computation: 56812 steps/s (collection: 1.632s, learning 0.098s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 37.2871
                       Mean reward: 0.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0426
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 1.73s
                      Time elapsed: 00:01:04
                               ETA: 03:15:14

################################################################################
                      [1m Learning iteration 11/2000 [0m                      

                       Computation: 58606 steps/s (collection: 1.589s, learning 0.088s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 37.3496
                       Mean reward: 0.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0490
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 1.68s
                      Time elapsed: 00:01:06
                               ETA: 03:03:31

################################################################################
                      [1m Learning iteration 12/2000 [0m                      

                       Computation: 56813 steps/s (collection: 1.638s, learning 0.093s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 37.3936
                       Mean reward: 0.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0619
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 1.73s
                      Time elapsed: 00:01:08
                               ETA: 02:53:43

################################################################################
                      [1m Learning iteration 13/2000 [0m                      

                       Computation: 55482 steps/s (collection: 1.663s, learning 0.109s)
             Mean action noise std: 1.02
          Mean value_function loss: 1.2051
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.4837
                       Mean reward: 0.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0790
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 1.77s
                      Time elapsed: 00:01:09
                               ETA: 02:45:25

################################################################################
                      [1m Learning iteration 14/2000 [0m                      

                       Computation: 56417 steps/s (collection: 1.647s, learning 0.095s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.3739
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.5720
                       Mean reward: 0.54
               Mean episode length: 249.66
    Episode_Reward/reaching_object: 0.1053
     Episode_Reward/lifting_object: -0.1374
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 1.74s
                      Time elapsed: 00:01:11
                               ETA: 02:38:09

################################################################################
                      [1m Learning iteration 15/2000 [0m                      

                       Computation: 54841 steps/s (collection: 1.707s, learning 0.086s)
             Mean action noise std: 1.04
          Mean value_function loss: 1.2258
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 37.8034
                       Mean reward: -0.60
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 0.1295
     Episode_Reward/lifting_object: -0.1208
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 1.79s
                      Time elapsed: 00:01:13
                               ETA: 02:31:54

################################################################################
                      [1m Learning iteration 16/2000 [0m                      

                       Computation: 54211 steps/s (collection: 1.726s, learning 0.088s)
             Mean action noise std: 1.04
          Mean value_function loss: 1.4115
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.9256
                       Mean reward: -0.34
               Mean episode length: 249.57
    Episode_Reward/reaching_object: 0.1424
     Episode_Reward/lifting_object: -0.2513
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 1.81s
                      Time elapsed: 00:01:15
                               ETA: 02:26:25

################################################################################
                      [1m Learning iteration 17/2000 [0m                      

                       Computation: 53376 steps/s (collection: 1.755s, learning 0.087s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.6491
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.0923
                       Mean reward: 0.11
               Mean episode length: 249.92
    Episode_Reward/reaching_object: 0.1849
     Episode_Reward/lifting_object: -0.0672
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 1.84s
                      Time elapsed: 00:01:17
                               ETA: 02:21:36

################################################################################
                      [1m Learning iteration 18/2000 [0m                      

                       Computation: 53227 steps/s (collection: 1.747s, learning 0.100s)
             Mean action noise std: 1.06
          Mean value_function loss: 0.5743
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.2303
                       Mean reward: 0.38
               Mean episode length: 249.74
    Episode_Reward/reaching_object: 0.2054
     Episode_Reward/lifting_object: -0.1599
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 1.85s
                      Time elapsed: 00:01:18
                               ETA: 02:17:17

################################################################################
                      [1m Learning iteration 19/2000 [0m                      

                       Computation: 53024 steps/s (collection: 1.760s, learning 0.094s)
             Mean action noise std: 1.06
          Mean value_function loss: 0.3176
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.4000
                       Mean reward: 0.69
               Mean episode length: 249.55
    Episode_Reward/reaching_object: 0.2139
     Episode_Reward/lifting_object: -0.0503
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 1.85s
                      Time elapsed: 00:01:20
                               ETA: 02:13:25

################################################################################
                      [1m Learning iteration 20/2000 [0m                      

                       Computation: 52808 steps/s (collection: 1.767s, learning 0.095s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.3575
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.5468
                       Mean reward: 1.10
               Mean episode length: 249.63
    Episode_Reward/reaching_object: 0.2281
     Episode_Reward/lifting_object: -0.0579
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 1.86s
                      Time elapsed: 00:01:22
                               ETA: 02:09:56

################################################################################
                      [1m Learning iteration 21/2000 [0m                      

                       Computation: 53678 steps/s (collection: 1.741s, learning 0.090s)
             Mean action noise std: 1.07
          Mean value_function loss: 1.7717
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.6773
                       Mean reward: 0.56
               Mean episode length: 249.63
    Episode_Reward/reaching_object: 0.2272
     Episode_Reward/lifting_object: -0.2279
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 1.83s
                      Time elapsed: 00:01:24
                               ETA: 02:06:42

################################################################################
                      [1m Learning iteration 22/2000 [0m                      

                       Computation: 52193 steps/s (collection: 1.784s, learning 0.099s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.3996
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.8190
                       Mean reward: 0.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2116
     Episode_Reward/lifting_object: -0.0845
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 1.88s
                      Time elapsed: 00:01:26
                               ETA: 02:03:50

################################################################################
                      [1m Learning iteration 23/2000 [0m                      

                       Computation: 54044 steps/s (collection: 1.710s, learning 0.109s)
             Mean action noise std: 1.09
          Mean value_function loss: 1.1198
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.9838
                       Mean reward: 0.26
               Mean episode length: 249.92
    Episode_Reward/reaching_object: 0.1937
     Episode_Reward/lifting_object: -0.0263
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0081
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 1.82s
                      Time elapsed: 00:01:28
                               ETA: 02:01:07

################################################################################
                      [1m Learning iteration 24/2000 [0m                      

                       Computation: 53259 steps/s (collection: 1.736s, learning 0.110s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.1323
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.1426
                       Mean reward: 0.76
               Mean episode length: 249.77
    Episode_Reward/reaching_object: 0.1762
     Episode_Reward/lifting_object: -0.0877
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0081
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 1.85s
                      Time elapsed: 00:01:30
                               ETA: 01:58:38

################################################################################
                      [1m Learning iteration 25/2000 [0m                      

                       Computation: 51257 steps/s (collection: 1.820s, learning 0.098s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0010
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 39.3081
                       Mean reward: 0.72
               Mean episode length: 249.92
    Episode_Reward/reaching_object: 0.1614
     Episode_Reward/lifting_object: -0.0083
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0082
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 1.92s
                      Time elapsed: 00:01:31
                               ETA: 01:56:27

################################################################################
                      [1m Learning iteration 26/2000 [0m                      

                       Computation: 52598 steps/s (collection: 1.767s, learning 0.102s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 39.4127
                       Mean reward: 0.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1541
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0083
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 1.87s
                      Time elapsed: 00:01:33
                               ETA: 01:54:21

################################################################################
                      [1m Learning iteration 27/2000 [0m                      

                       Computation: 55275 steps/s (collection: 1.692s, learning 0.087s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 39.4802
                       Mean reward: 0.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1443
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0084
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 1.78s
                      Time elapsed: 00:01:35
                               ETA: 01:52:18

################################################################################
                      [1m Learning iteration 28/2000 [0m                      

                       Computation: 55110 steps/s (collection: 1.691s, learning 0.093s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0268
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 39.5401
                       Mean reward: 0.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1357
     Episode_Reward/lifting_object: -0.0152
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0085
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 1.78s
                      Time elapsed: 00:01:37
                               ETA: 01:50:24

################################################################################
                      [1m Learning iteration 29/2000 [0m                      

                       Computation: 53407 steps/s (collection: 1.752s, learning 0.089s)
             Mean action noise std: 1.11
          Mean value_function loss: 2.5500
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 39.5995
                       Mean reward: 0.65
               Mean episode length: 249.60
    Episode_Reward/reaching_object: 0.1416
     Episode_Reward/lifting_object: -0.1101
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 1.84s
                      Time elapsed: 00:01:39
                               ETA: 01:48:41

################################################################################
                      [1m Learning iteration 30/2000 [0m                      

                       Computation: 54044 steps/s (collection: 1.722s, learning 0.097s)
             Mean action noise std: 1.12
          Mean value_function loss: 3.1603
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.7640
                       Mean reward: 0.66
               Mean episode length: 249.68
    Episode_Reward/reaching_object: 0.1471
     Episode_Reward/lifting_object: -0.1234
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 1.82s
                      Time elapsed: 00:01:41
                               ETA: 01:47:03

################################################################################
                      [1m Learning iteration 31/2000 [0m                      

                       Computation: 52037 steps/s (collection: 1.797s, learning 0.093s)
             Mean action noise std: 1.13
          Mean value_function loss: 1.0888
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.9103
                       Mean reward: 0.54
               Mean episode length: 249.55
    Episode_Reward/reaching_object: 0.1523
     Episode_Reward/lifting_object: -0.2124
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 1.89s
                      Time elapsed: 00:01:42
                               ETA: 01:45:35

################################################################################
                      [1m Learning iteration 32/2000 [0m                      

                       Computation: 50581 steps/s (collection: 1.841s, learning 0.102s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.3986
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.0526
                       Mean reward: 0.60
               Mean episode length: 249.50
    Episode_Reward/reaching_object: 0.1670
     Episode_Reward/lifting_object: -0.1467
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 1.94s
                      Time elapsed: 00:01:44
                               ETA: 01:44:16

################################################################################
                      [1m Learning iteration 33/2000 [0m                      

                       Computation: 51878 steps/s (collection: 1.776s, learning 0.119s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.1386
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.2011
                       Mean reward: 0.50
               Mean episode length: 249.71
    Episode_Reward/reaching_object: 0.1805
     Episode_Reward/lifting_object: -0.0351
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 1.89s
                      Time elapsed: 00:01:46
                               ETA: 01:42:58

################################################################################
                      [1m Learning iteration 34/2000 [0m                      

                       Computation: 52289 steps/s (collection: 1.778s, learning 0.102s)
             Mean action noise std: 1.14
          Mean value_function loss: 1.2985
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 40.3264
                       Mean reward: 0.75
               Mean episode length: 249.71
    Episode_Reward/reaching_object: 0.1857
     Episode_Reward/lifting_object: -0.1220
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 1.88s
                      Time elapsed: 00:01:48
                               ETA: 01:41:44

################################################################################
                      [1m Learning iteration 35/2000 [0m                      

                       Computation: 52336 steps/s (collection: 1.787s, learning 0.092s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.6260
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 40.4276
                       Mean reward: -0.05
               Mean episode length: 249.17
    Episode_Reward/reaching_object: 0.1969
     Episode_Reward/lifting_object: -0.0887
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 1.88s
                      Time elapsed: 00:01:50
                               ETA: 01:40:34

################################################################################
                      [1m Learning iteration 36/2000 [0m                      

                       Computation: 51523 steps/s (collection: 1.815s, learning 0.093s)
             Mean action noise std: 1.15
          Mean value_function loss: 2.6091
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.5392
                       Mean reward: -0.50
               Mean episode length: 248.52
    Episode_Reward/reaching_object: 0.2026
     Episode_Reward/lifting_object: -0.3252
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 1.91s
                      Time elapsed: 00:01:52
                               ETA: 01:39:29

################################################################################
                      [1m Learning iteration 37/2000 [0m                      

                       Computation: 52384 steps/s (collection: 1.790s, learning 0.087s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.3002
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.6405
                       Mean reward: 0.35
               Mean episode length: 249.40
    Episode_Reward/reaching_object: 0.2158
     Episode_Reward/lifting_object: -0.0931
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 1.88s
                      Time elapsed: 00:01:54
                               ETA: 01:38:26

################################################################################
                      [1m Learning iteration 38/2000 [0m                      

                       Computation: 52206 steps/s (collection: 1.775s, learning 0.108s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.3563
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.7767
                       Mean reward: 0.58
               Mean episode length: 249.57
    Episode_Reward/reaching_object: 0.2041
     Episode_Reward/lifting_object: -0.0781
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 1.88s
                      Time elapsed: 00:01:56
                               ETA: 01:37:27

################################################################################
                      [1m Learning iteration 39/2000 [0m                      

                       Computation: 51133 steps/s (collection: 1.813s, learning 0.110s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 40.8968
                       Mean reward: 0.83
               Mean episode length: 248.98
    Episode_Reward/reaching_object: 0.1956
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 1.92s
                      Time elapsed: 00:01:58
                               ETA: 01:36:32

################################################################################
                      [1m Learning iteration 40/2000 [0m                      

                       Computation: 53237 steps/s (collection: 1.748s, learning 0.099s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0012
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 41.0218
                       Mean reward: 0.81
               Mean episode length: 249.57
    Episode_Reward/reaching_object: 0.1916
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 1.85s
                      Time elapsed: 00:01:59
                               ETA: 01:35:36

################################################################################
                      [1m Learning iteration 41/2000 [0m                      

                       Computation: 49757 steps/s (collection: 1.859s, learning 0.117s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0164
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 41.0871
                       Mean reward: 0.84
               Mean episode length: 249.90
    Episode_Reward/reaching_object: 0.1845
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 1.98s
                      Time elapsed: 00:02:01
                               ETA: 01:34:49

################################################################################
                      [1m Learning iteration 42/2000 [0m                      

                       Computation: 52560 steps/s (collection: 1.778s, learning 0.092s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 41.1272
                       Mean reward: 0.78
               Mean episode length: 249.31
    Episode_Reward/reaching_object: 0.1813
     Episode_Reward/lifting_object: -0.0098
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 1.87s
                      Time elapsed: 00:02:03
                               ETA: 01:33:59

################################################################################
                      [1m Learning iteration 43/2000 [0m                      

                       Computation: 53927 steps/s (collection: 1.734s, learning 0.088s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0529
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 41.2360
                       Mean reward: 0.80
               Mean episode length: 249.98
    Episode_Reward/reaching_object: 0.1703
     Episode_Reward/lifting_object: -0.0174
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 1.82s
                      Time elapsed: 00:02:05
                               ETA: 01:33:09

################################################################################
                      [1m Learning iteration 44/2000 [0m                      

                       Computation: 50934 steps/s (collection: 1.836s, learning 0.094s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 41.2857
                       Mean reward: 0.77
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.1710
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 1.93s
                      Time elapsed: 00:02:07
                               ETA: 01:32:26

################################################################################
                      [1m Learning iteration 45/2000 [0m                      

                       Computation: 51984 steps/s (collection: 1.796s, learning 0.095s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.3045
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 41.4071
                       Mean reward: 0.66
               Mean episode length: 249.78
    Episode_Reward/reaching_object: 0.1779
     Episode_Reward/lifting_object: -0.0256
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 1.89s
                      Time elapsed: 00:02:09
                               ETA: 01:31:43

################################################################################
                      [1m Learning iteration 46/2000 [0m                      

                       Computation: 52897 steps/s (collection: 1.749s, learning 0.109s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.7708
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.4581
                       Mean reward: 0.82
               Mean episode length: 249.72
    Episode_Reward/reaching_object: 0.1815
     Episode_Reward/lifting_object: -0.0672
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 1.86s
                      Time elapsed: 00:02:11
                               ETA: 01:31:00

################################################################################
                      [1m Learning iteration 47/2000 [0m                      

                       Computation: 51752 steps/s (collection: 1.791s, learning 0.109s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0286
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.5897
                       Mean reward: 0.71
               Mean episode length: 249.35
    Episode_Reward/reaching_object: 0.1833
     Episode_Reward/lifting_object: -0.0942
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 1.90s
                      Time elapsed: 00:02:13
                               ETA: 01:30:21

################################################################################
                      [1m Learning iteration 48/2000 [0m                      

                       Computation: 52503 steps/s (collection: 1.779s, learning 0.093s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.3316
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 41.8431
                       Mean reward: 0.69
               Mean episode length: 249.36
    Episode_Reward/reaching_object: 0.1988
     Episode_Reward/lifting_object: -0.0283
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 1.87s
                      Time elapsed: 00:02:15
                               ETA: 01:29:42

################################################################################
                      [1m Learning iteration 49/2000 [0m                      

                       Computation: 51593 steps/s (collection: 1.813s, learning 0.092s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.5430
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.9041
                       Mean reward: 0.23
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 0.2037
     Episode_Reward/lifting_object: -0.0939
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 1.91s
                      Time elapsed: 00:02:17
                               ETA: 01:29:06

################################################################################
                      [1m Learning iteration 50/2000 [0m                      

                       Computation: 53226 steps/s (collection: 1.760s, learning 0.087s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0566
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 42.0185
                       Mean reward: 0.91
               Mean episode length: 249.06
    Episode_Reward/reaching_object: 0.2186
     Episode_Reward/lifting_object: -0.0534
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 1.85s
                      Time elapsed: 00:02:18
                               ETA: 01:28:29

################################################################################
                      [1m Learning iteration 51/2000 [0m                      

                       Computation: 53910 steps/s (collection: 1.725s, learning 0.098s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.1932
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 42.1033
                       Mean reward: 0.86
               Mean episode length: 249.65
    Episode_Reward/reaching_object: 0.2315
     Episode_Reward/lifting_object: -0.0321
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 1.82s
                      Time elapsed: 00:02:20
                               ETA: 01:27:53

################################################################################
                      [1m Learning iteration 52/2000 [0m                      

                       Computation: 52833 steps/s (collection: 1.774s, learning 0.087s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.6736
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 42.1780
                       Mean reward: -0.16
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.2251
     Episode_Reward/lifting_object: -0.1410
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 1.86s
                      Time elapsed: 00:02:22
                               ETA: 01:27:19

################################################################################
                      [1m Learning iteration 53/2000 [0m                      

                       Computation: 52737 steps/s (collection: 1.763s, learning 0.101s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.3282
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 42.2644
                       Mean reward: 1.12
               Mean episode length: 248.90
    Episode_Reward/reaching_object: 0.2350
     Episode_Reward/lifting_object: -0.0401
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 1.86s
                      Time elapsed: 00:02:24
                               ETA: 01:26:47

################################################################################
                      [1m Learning iteration 54/2000 [0m                      

                       Computation: 53093 steps/s (collection: 1.761s, learning 0.091s)
             Mean action noise std: 1.24
          Mean value_function loss: 1.8364
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 42.3496
                       Mean reward: 0.79
               Mean episode length: 249.20
    Episode_Reward/reaching_object: 0.2200
     Episode_Reward/lifting_object: -0.2218
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 1.85s
                      Time elapsed: 00:02:26
                               ETA: 01:26:15

################################################################################
                      [1m Learning iteration 55/2000 [0m                      

                       Computation: 53209 steps/s (collection: 1.735s, learning 0.112s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.4558
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 42.4239
                       Mean reward: 0.67
               Mean episode length: 248.73
    Episode_Reward/reaching_object: 0.2308
     Episode_Reward/lifting_object: -0.1556
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 1.85s
                      Time elapsed: 00:02:28
                               ETA: 01:25:44

################################################################################
                      [1m Learning iteration 56/2000 [0m                      

                       Computation: 53908 steps/s (collection: 1.717s, learning 0.107s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.2061
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 42.4908
                       Mean reward: 1.00
               Mean episode length: 248.43
    Episode_Reward/reaching_object: 0.2324
     Episode_Reward/lifting_object: -0.1353
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 1.82s
                      Time elapsed: 00:02:29
                               ETA: 01:25:13

################################################################################
                      [1m Learning iteration 57/2000 [0m                      

                       Computation: 52236 steps/s (collection: 1.775s, learning 0.107s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.4289
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 42.5634
                       Mean reward: 1.15
               Mean episode length: 249.04
    Episode_Reward/reaching_object: 0.2528
     Episode_Reward/lifting_object: -0.0608
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 1.88s
                      Time elapsed: 00:02:31
                               ETA: 01:24:46

################################################################################
                      [1m Learning iteration 58/2000 [0m                      

                       Computation: 53869 steps/s (collection: 1.727s, learning 0.098s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0369
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 42.6452
                       Mean reward: 0.45
               Mean episode length: 249.22
    Episode_Reward/reaching_object: 0.2426
     Episode_Reward/lifting_object: -0.0598
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 1.82s
                      Time elapsed: 00:02:33
                               ETA: 01:24:17

################################################################################
                      [1m Learning iteration 59/2000 [0m                      

                       Computation: 54346 steps/s (collection: 1.712s, learning 0.097s)
             Mean action noise std: 1.26
          Mean value_function loss: 1.4585
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 42.7783
                       Mean reward: -0.31
               Mean episode length: 249.86
    Episode_Reward/reaching_object: 0.2369
     Episode_Reward/lifting_object: -0.1668
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 1.81s
                      Time elapsed: 00:02:35
                               ETA: 01:23:48

################################################################################
                      [1m Learning iteration 60/2000 [0m                      

                       Computation: 53543 steps/s (collection: 1.741s, learning 0.095s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0770
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 42.8276
                       Mean reward: 0.82
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 0.2198
     Episode_Reward/lifting_object: -0.0239
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 1.84s
                      Time elapsed: 00:02:37
                               ETA: 01:23:22

################################################################################
                      [1m Learning iteration 61/2000 [0m                      

                       Computation: 52324 steps/s (collection: 1.789s, learning 0.090s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0090
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 42.9177
                       Mean reward: 0.96
               Mean episode length: 249.78
    Episode_Reward/reaching_object: 0.2067
     Episode_Reward/lifting_object: -0.0212
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 1.88s
                      Time elapsed: 00:02:39
                               ETA: 01:22:57

################################################################################
                      [1m Learning iteration 62/2000 [0m                      

                       Computation: 54979 steps/s (collection: 1.682s, learning 0.106s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0010
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 43.0833
                       Mean reward: 0.81
               Mean episode length: 249.82
    Episode_Reward/reaching_object: 0.1962
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 1.79s
                      Time elapsed: 00:02:40
                               ETA: 01:22:31

################################################################################
                      [1m Learning iteration 63/2000 [0m                      

                       Computation: 53526 steps/s (collection: 1.743s, learning 0.094s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0010
               Mean surrogate loss: -0.0081
                 Mean entropy loss: 43.1308
                       Mean reward: 0.87
               Mean episode length: 249.78
    Episode_Reward/reaching_object: 0.1771
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0119
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 1.84s
                      Time elapsed: 00:02:42
                               ETA: 01:22:07

################################################################################
                      [1m Learning iteration 64/2000 [0m                      

                       Computation: 50357 steps/s (collection: 1.867s, learning 0.085s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.7967
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 43.2314
                       Mean reward: -0.19
               Mean episode length: 249.34
    Episode_Reward/reaching_object: 0.1772
     Episode_Reward/lifting_object: -0.1127
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0119
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 1.95s
                      Time elapsed: 00:02:44
                               ETA: 01:21:46

################################################################################
                      [1m Learning iteration 65/2000 [0m                      

                       Computation: 53502 steps/s (collection: 1.749s, learning 0.089s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.1202
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 43.2753
                       Mean reward: 0.29
               Mean episode length: 249.01
    Episode_Reward/reaching_object: 0.1749
     Episode_Reward/lifting_object: -0.0587
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 1.84s
                      Time elapsed: 00:02:46
                               ETA: 01:21:23

################################################################################
                      [1m Learning iteration 66/2000 [0m                      

                       Computation: 51769 steps/s (collection: 1.805s, learning 0.094s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.1111
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 43.3734
                       Mean reward: 0.86
               Mean episode length: 249.75
    Episode_Reward/reaching_object: 0.1744
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 1.90s
                      Time elapsed: 00:02:48
                               ETA: 01:21:03

################################################################################
                      [1m Learning iteration 67/2000 [0m                      

                       Computation: 50219 steps/s (collection: 1.866s, learning 0.091s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 43.4262
                       Mean reward: 0.87
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.1850
     Episode_Reward/lifting_object: -0.0400
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 1.96s
                      Time elapsed: 00:02:50
                               ETA: 01:20:45

################################################################################
                      [1m Learning iteration 68/2000 [0m                      

                       Computation: 50534 steps/s (collection: 1.858s, learning 0.088s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0335
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 43.5198
                       Mean reward: 0.80
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.1841
     Episode_Reward/lifting_object: -0.0111
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 1.95s
                      Time elapsed: 00:02:52
                               ETA: 01:20:26

################################################################################
                      [1m Learning iteration 69/2000 [0m                      

                       Computation: 22007 steps/s (collection: 4.049s, learning 0.417s)
             Mean action noise std: 1.29
          Mean value_function loss: 1.5388
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 43.5633
                       Mean reward: -0.38
               Mean episode length: 244.60
    Episode_Reward/reaching_object: 0.2113
     Episode_Reward/lifting_object: -0.1390
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 4.47s
                      Time elapsed: 00:02:56
                               ETA: 01:21:18

################################################################################
                      [1m Learning iteration 70/2000 [0m                      

                       Computation: 15582 steps/s (collection: 5.778s, learning 0.531s)
             Mean action noise std: 1.30
          Mean value_function loss: 1.7371
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 43.6424
                       Mean reward: 0.37
               Mean episode length: 245.16
    Episode_Reward/reaching_object: 0.2469
     Episode_Reward/lifting_object: -0.2722
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 6.31s
                      Time elapsed: 00:03:03
                               ETA: 01:22:58

################################################################################
                      [1m Learning iteration 71/2000 [0m                      

                       Computation: 16345 steps/s (collection: 5.531s, learning 0.483s)
             Mean action noise std: 1.30
          Mean value_function loss: 2.1458
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 43.7085
                       Mean reward: 0.08
               Mean episode length: 241.18
    Episode_Reward/reaching_object: 0.2579
     Episode_Reward/lifting_object: -0.1851
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 6.01s
                      Time elapsed: 00:03:09
                               ETA: 01:24:28

################################################################################
                      [1m Learning iteration 72/2000 [0m                      

                       Computation: 15541 steps/s (collection: 6.069s, learning 0.256s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.7592
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 43.7811
                       Mean reward: 0.67
               Mean episode length: 241.66
    Episode_Reward/reaching_object: 0.2815
     Episode_Reward/lifting_object: -0.1651
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 6.33s
                      Time elapsed: 00:03:15
                               ETA: 01:26:03

################################################################################
                      [1m Learning iteration 73/2000 [0m                      

                       Computation: 15546 steps/s (collection: 5.828s, learning 0.495s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.3679
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 43.8559
                       Mean reward: 0.38
               Mean episode length: 242.21
    Episode_Reward/reaching_object: 0.2914
     Episode_Reward/lifting_object: -0.1155
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 6.32s
                      Time elapsed: 00:03:21
                               ETA: 01:27:35

################################################################################
                      [1m Learning iteration 74/2000 [0m                      

                       Computation: 14158 steps/s (collection: 6.490s, learning 0.453s)
             Mean action noise std: 1.31
          Mean value_function loss: 1.0039
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 43.9285
                       Mean reward: 0.73
               Mean episode length: 244.63
    Episode_Reward/reaching_object: 0.2989
     Episode_Reward/lifting_object: -0.2150
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 6.94s
                      Time elapsed: 00:03:28
                               ETA: 01:29:21

################################################################################
                      [1m Learning iteration 75/2000 [0m                      

                       Computation: 16739 steps/s (collection: 5.494s, learning 0.379s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.6679
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 44.0004
                       Mean reward: 1.23
               Mean episode length: 245.83
    Episode_Reward/reaching_object: 0.2778
     Episode_Reward/lifting_object: -0.1178
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 5.87s
                      Time elapsed: 00:03:34
                               ETA: 01:30:36

################################################################################
                      [1m Learning iteration 76/2000 [0m                      

                       Computation: 16932 steps/s (collection: 5.371s, learning 0.435s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0956
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 44.0755
                       Mean reward: 1.38
               Mean episode length: 246.77
    Episode_Reward/reaching_object: 0.2911
     Episode_Reward/lifting_object: -0.0111
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 5.81s
                      Time elapsed: 00:03:40
                               ETA: 01:31:48

################################################################################
                      [1m Learning iteration 77/2000 [0m                      

                       Computation: 15847 steps/s (collection: 5.686s, learning 0.517s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 44.1488
                       Mean reward: 1.29
               Mean episode length: 246.68
    Episode_Reward/reaching_object: 0.2780
     Episode_Reward/lifting_object: -0.0208
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 6.20s
                      Time elapsed: 00:03:46
                               ETA: 01:33:07

################################################################################
                      [1m Learning iteration 78/2000 [0m                      

                       Computation: 14401 steps/s (collection: 6.398s, learning 0.428s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0555
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 44.2529
                       Mean reward: 1.16
               Mean episode length: 248.33
    Episode_Reward/reaching_object: 0.2675
     Episode_Reward/lifting_object: -0.0159
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 6.83s
                      Time elapsed: 00:03:53
                               ETA: 01:34:40

################################################################################
                      [1m Learning iteration 79/2000 [0m                      

                       Computation: 15967 steps/s (collection: 5.752s, learning 0.404s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.2572
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 44.3007
                       Mean reward: 1.05
               Mean episode length: 247.46
    Episode_Reward/reaching_object: 0.2446
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0131
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 6.16s
                      Time elapsed: 00:03:59
                               ETA: 01:35:54

################################################################################
                      [1m Learning iteration 80/2000 [0m                      

                       Computation: 18208 steps/s (collection: 5.152s, learning 0.247s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 44.4332
                       Mean reward: 1.09
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.2344
     Episode_Reward/lifting_object: -0.0339
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0132
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 5.40s
                      Time elapsed: 00:04:05
                               ETA: 01:36:48

################################################################################
                      [1m Learning iteration 81/2000 [0m                      

                       Computation: 17903 steps/s (collection: 5.142s, learning 0.348s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0045
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 44.6079
                       Mean reward: 0.95
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.2110
     Episode_Reward/lifting_object: 0.0048
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0133
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 5.49s
                      Time elapsed: 00:04:10
                               ETA: 01:37:42

################################################################################
                      [1m Learning iteration 82/2000 [0m                      

                       Computation: 17425 steps/s (collection: 5.183s, learning 0.458s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0317
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 44.5983
                       Mean reward: 0.72
               Mean episode length: 246.98
    Episode_Reward/reaching_object: 0.2100
     Episode_Reward/lifting_object: -0.0098
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 5.64s
                      Time elapsed: 00:04:16
                               ETA: 01:38:39

################################################################################
                      [1m Learning iteration 83/2000 [0m                      

                       Computation: 15343 steps/s (collection: 5.910s, learning 0.497s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.5474
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 44.6277
                       Mean reward: 0.95
               Mean episode length: 246.35
    Episode_Reward/reaching_object: 0.2091
     Episode_Reward/lifting_object: -0.0468
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 6.41s
                      Time elapsed: 00:04:22
                               ETA: 01:39:52

################################################################################
                      [1m Learning iteration 84/2000 [0m                      

                       Computation: 15292 steps/s (collection: 5.978s, learning 0.451s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.6678
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 44.6989
                       Mean reward: 0.30
               Mean episode length: 246.57
    Episode_Reward/reaching_object: 0.2114
     Episode_Reward/lifting_object: -0.0949
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 6.43s
                      Time elapsed: 00:04:28
                               ETA: 01:41:03

################################################################################
                      [1m Learning iteration 85/2000 [0m                      

                       Computation: 16810 steps/s (collection: 5.499s, learning 0.349s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.4768
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 44.7688
                       Mean reward: 0.92
               Mean episode length: 244.88
    Episode_Reward/reaching_object: 0.2099
     Episode_Reward/lifting_object: -0.1225
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 5.85s
                      Time elapsed: 00:04:34
                               ETA: 01:42:00

################################################################################
                      [1m Learning iteration 86/2000 [0m                      

                       Computation: 15977 steps/s (collection: 5.635s, learning 0.518s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 44.8458
                       Mean reward: 1.07
               Mean episode length: 240.47
    Episode_Reward/reaching_object: 0.2417
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 6.15s
                      Time elapsed: 00:04:40
                               ETA: 01:43:01

################################################################################
                      [1m Learning iteration 87/2000 [0m                      

                       Computation: 16468 steps/s (collection: 5.573s, learning 0.396s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.1943
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 45.0114
                       Mean reward: 1.22
               Mean episode length: 236.84
    Episode_Reward/reaching_object: 0.2445
     Episode_Reward/lifting_object: -0.0280
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 5.97s
                      Time elapsed: 00:04:46
                               ETA: 01:43:58

################################################################################
                      [1m Learning iteration 88/2000 [0m                      

                       Computation: 14346 steps/s (collection: 6.267s, learning 0.585s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.1110
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 45.0607
                       Mean reward: 0.75
               Mean episode length: 238.88
    Episode_Reward/reaching_object: 0.2644
     Episode_Reward/lifting_object: -0.0222
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 6.85s
                      Time elapsed: 00:04:53
                               ETA: 01:45:12

################################################################################
                      [1m Learning iteration 89/2000 [0m                      

                       Computation: 14522 steps/s (collection: 6.306s, learning 0.464s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.6669
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 45.1851
                       Mean reward: -0.12
               Mean episode length: 237.44
    Episode_Reward/reaching_object: 0.2738
     Episode_Reward/lifting_object: -0.0851
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 6.77s
                      Time elapsed: 00:05:00
                               ETA: 01:46:22

################################################################################
                      [1m Learning iteration 90/2000 [0m                      

                       Computation: 17272 steps/s (collection: 5.440s, learning 0.252s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.3866
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 45.2269
                       Mean reward: 1.27
               Mean episode length: 232.82
    Episode_Reward/reaching_object: 0.2684
     Episode_Reward/lifting_object: -0.0098
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 5.69s
                      Time elapsed: 00:05:06
                               ETA: 01:47:08

################################################################################
                      [1m Learning iteration 91/2000 [0m                      

                       Computation: 15534 steps/s (collection: 5.940s, learning 0.388s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.4786
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 45.2927
                       Mean reward: -0.13
               Mean episode length: 238.99
    Episode_Reward/reaching_object: 0.2843
     Episode_Reward/lifting_object: -0.1228
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 6.33s
                      Time elapsed: 00:05:12
                               ETA: 01:48:06

################################################################################
                      [1m Learning iteration 92/2000 [0m                      

                       Computation: 16339 steps/s (collection: 5.519s, learning 0.497s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.3200
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 45.3424
                       Mean reward: 0.67
               Mean episode length: 231.10
    Episode_Reward/reaching_object: 0.2925
     Episode_Reward/lifting_object: -0.0924
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 6.02s
                      Time elapsed: 00:05:18
                               ETA: 01:48:56

################################################################################
                      [1m Learning iteration 93/2000 [0m                      

                       Computation: 15124 steps/s (collection: 6.081s, learning 0.419s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 45.4084
                       Mean reward: 1.42
               Mean episode length: 231.62
    Episode_Reward/reaching_object: 0.3023
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 6.50s
                      Time elapsed: 00:05:25
                               ETA: 01:49:55

################################################################################
                      [1m Learning iteration 94/2000 [0m                      

                       Computation: 16934 steps/s (collection: 5.418s, learning 0.387s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 45.4961
                       Mean reward: 1.33
               Mean episode length: 233.01
    Episode_Reward/reaching_object: 0.2946
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 5.80s
                      Time elapsed: 00:05:30
                               ETA: 01:50:39

################################################################################
                      [1m Learning iteration 95/2000 [0m                      

                       Computation: 16837 steps/s (collection: 5.449s, learning 0.389s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0058
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 45.5638
                       Mean reward: 1.50
               Mean episode length: 231.65
    Episode_Reward/reaching_object: 0.3132
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 5.84s
                      Time elapsed: 00:05:36
                               ETA: 01:51:22

################################################################################
                      [1m Learning iteration 96/2000 [0m                      

                       Computation: 16829 steps/s (collection: 5.419s, learning 0.422s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0219
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 45.6295
                       Mean reward: 1.28
               Mean episode length: 232.77
    Episode_Reward/reaching_object: 0.3009
     Episode_Reward/lifting_object: -0.0098
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 5.84s
                      Time elapsed: 00:05:42
                               ETA: 01:52:04

################################################################################
                      [1m Learning iteration 97/2000 [0m                      

                       Computation: 16486 steps/s (collection: 5.493s, learning 0.470s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0091
                 Mean entropy loss: 45.7356
                       Mean reward: 1.36
               Mean episode length: 230.88
    Episode_Reward/reaching_object: 0.2897
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 5.96s
                      Time elapsed: 00:05:48
                               ETA: 01:52:48

################################################################################
                      [1m Learning iteration 98/2000 [0m                      

                       Computation: 17941 steps/s (collection: 5.249s, learning 0.230s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.4096
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 45.7819
                       Mean reward: 0.42
               Mean episode length: 224.58
    Episode_Reward/reaching_object: 0.3007
     Episode_Reward/lifting_object: -0.0643
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 5.48s
                      Time elapsed: 00:05:54
                               ETA: 01:53:22

################################################################################
                      [1m Learning iteration 99/2000 [0m                      

                       Computation: 14687 steps/s (collection: 6.210s, learning 0.483s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.3966
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 45.8126
                       Mean reward: 1.26
               Mean episode length: 223.51
    Episode_Reward/reaching_object: 0.3041
     Episode_Reward/lifting_object: -0.0785
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 6.69s
                      Time elapsed: 00:06:00
                               ETA: 01:54:17

################################################################################
                     [1m Learning iteration 100/2000 [0m                      

                       Computation: 16865 steps/s (collection: 5.391s, learning 0.437s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.1755
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 45.8689
                       Mean reward: 1.41
               Mean episode length: 224.29
    Episode_Reward/reaching_object: 0.3089
     Episode_Reward/lifting_object: -0.0360
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 5.83s
                      Time elapsed: 00:06:06
                               ETA: 01:54:55

################################################################################
                     [1m Learning iteration 101/2000 [0m                      

                       Computation: 14242 steps/s (collection: 6.453s, learning 0.449s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.3435
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 45.9342
                       Mean reward: 1.25
               Mean episode length: 214.86
    Episode_Reward/reaching_object: 0.3166
     Episode_Reward/lifting_object: -0.0494
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 6.90s
                      Time elapsed: 00:06:13
                               ETA: 01:55:53

################################################################################
                     [1m Learning iteration 102/2000 [0m                      

                       Computation: 14039 steps/s (collection: 6.601s, learning 0.400s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0499
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 46.0050
                       Mean reward: 1.52
               Mean episode length: 208.30
    Episode_Reward/reaching_object: 0.3234
     Episode_Reward/lifting_object: -0.0152
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 7.00s
                      Time elapsed: 00:06:20
                               ETA: 01:56:51

################################################################################
                     [1m Learning iteration 103/2000 [0m                      

                       Computation: 16093 steps/s (collection: 5.722s, learning 0.386s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.2084
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 46.1220
                       Mean reward: 1.66
               Mean episode length: 212.43
    Episode_Reward/reaching_object: 0.3277
     Episode_Reward/lifting_object: -0.0430
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.7500
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 6.11s
                      Time elapsed: 00:06:26
                               ETA: 01:57:31

################################################################################
                     [1m Learning iteration 104/2000 [0m                      

                       Computation: 15696 steps/s (collection: 5.774s, learning 0.489s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.0277
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 46.1558
                       Mean reward: 1.58
               Mean episode length: 210.16
    Episode_Reward/reaching_object: 0.3357
     Episode_Reward/lifting_object: -0.0237
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 2.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.2917
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 6.26s
                      Time elapsed: 00:06:32
                               ETA: 01:58:13

################################################################################
                     [1m Learning iteration 105/2000 [0m                      

                       Computation: 17147 steps/s (collection: 5.360s, learning 0.373s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.1949
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 46.2391
                       Mean reward: 1.33
               Mean episode length: 207.09
    Episode_Reward/reaching_object: 0.3417
     Episode_Reward/lifting_object: -0.0452
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.2917
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 5.73s
                      Time elapsed: 00:06:38
                               ETA: 01:58:45

################################################################################
                     [1m Learning iteration 106/2000 [0m                      

                       Computation: 18557 steps/s (collection: 5.160s, learning 0.137s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.1194
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 46.2819
                       Mean reward: 1.65
               Mean episode length: 206.95
    Episode_Reward/reaching_object: 0.3560
     Episode_Reward/lifting_object: -0.0515
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 5.30s
                      Time elapsed: 00:06:43
                               ETA: 01:59:08

################################################################################
                     [1m Learning iteration 107/2000 [0m                      

                       Computation: 48892 steps/s (collection: 1.920s, learning 0.091s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.2194
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 46.4014
                       Mean reward: 1.41
               Mean episode length: 206.00
    Episode_Reward/reaching_object: 0.3563
     Episode_Reward/lifting_object: -0.0418
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.0417
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 2.01s
                      Time elapsed: 00:06:45
                               ETA: 01:58:34

################################################################################
                     [1m Learning iteration 108/2000 [0m                      

                       Computation: 50935 steps/s (collection: 1.832s, learning 0.098s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.3365
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 46.4356
                       Mean reward: 0.77
               Mean episode length: 217.38
    Episode_Reward/reaching_object: 0.3529
     Episode_Reward/lifting_object: -0.0728
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 1.93s
                      Time elapsed: 00:06:47
                               ETA: 01:57:58

################################################################################
                     [1m Learning iteration 109/2000 [0m                      

                       Computation: 51734 steps/s (collection: 1.810s, learning 0.090s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.0057
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 46.4798
                       Mean reward: 1.75
               Mean episode length: 213.44
    Episode_Reward/reaching_object: 0.3698
     Episode_Reward/lifting_object: -0.0064
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 3.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 1.90s
                      Time elapsed: 00:06:49
                               ETA: 01:57:23

################################################################################
                     [1m Learning iteration 110/2000 [0m                      

                       Computation: 47358 steps/s (collection: 1.917s, learning 0.159s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.0391
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 46.5448
                       Mean reward: 1.91
               Mean episode length: 218.62
    Episode_Reward/reaching_object: 0.3716
     Episode_Reward/lifting_object: 0.0056
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 2.08s
                      Time elapsed: 00:06:51
                               ETA: 01:56:51

################################################################################
                     [1m Learning iteration 111/2000 [0m                      

                       Computation: 48926 steps/s (collection: 1.909s, learning 0.100s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.1711
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 46.5921
                       Mean reward: 0.96
               Mean episode length: 222.10
    Episode_Reward/reaching_object: 0.3560
     Episode_Reward/lifting_object: -0.0315
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 2.01s
                      Time elapsed: 00:06:53
                               ETA: 01:56:19

################################################################################
                     [1m Learning iteration 112/2000 [0m                      

                       Computation: 50049 steps/s (collection: 1.871s, learning 0.093s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.1913
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 46.6233
                       Mean reward: 1.80
               Mean episode length: 224.01
    Episode_Reward/reaching_object: 0.3626
     Episode_Reward/lifting_object: -0.0348
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.6667
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 1.96s
                      Time elapsed: 00:06:55
                               ETA: 01:55:46

################################################################################
                     [1m Learning iteration 113/2000 [0m                      

                       Computation: 50683 steps/s (collection: 1.850s, learning 0.090s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.2325
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 46.6629
                       Mean reward: 1.65
               Mean episode length: 217.54
    Episode_Reward/reaching_object: 0.3514
     Episode_Reward/lifting_object: -0.0018
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 1.94s
                      Time elapsed: 00:06:57
                               ETA: 01:55:14

################################################################################
                     [1m Learning iteration 114/2000 [0m                      

                       Computation: 46730 steps/s (collection: 1.992s, learning 0.112s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.0237
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 46.7917
                       Mean reward: 1.61
               Mean episode length: 214.23
    Episode_Reward/reaching_object: 0.3509
     Episode_Reward/lifting_object: -0.0817
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 2.10s
                      Time elapsed: 00:06:59
                               ETA: 01:54:44

################################################################################
                     [1m Learning iteration 115/2000 [0m                      

                       Computation: 51106 steps/s (collection: 1.836s, learning 0.088s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.1528
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 46.9022
                       Mean reward: 1.30
               Mean episode length: 220.19
    Episode_Reward/reaching_object: 0.3526
     Episode_Reward/lifting_object: -0.0302
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 1.92s
                      Time elapsed: 00:07:01
                               ETA: 01:54:13

################################################################################
                     [1m Learning iteration 116/2000 [0m                      

                       Computation: 50529 steps/s (collection: 1.798s, learning 0.147s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.3776
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 46.9298
                       Mean reward: 1.04
               Mean episode length: 211.67
    Episode_Reward/reaching_object: 0.3626
     Episode_Reward/lifting_object: -0.0301
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 1.95s
                      Time elapsed: 00:07:03
                               ETA: 01:53:42

################################################################################
                     [1m Learning iteration 117/2000 [0m                      

                       Computation: 50674 steps/s (collection: 1.809s, learning 0.131s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 46.9822
                       Mean reward: 1.27
               Mean episode length: 211.97
    Episode_Reward/reaching_object: 0.3671
     Episode_Reward/lifting_object: -0.0200
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.9583
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 1.94s
                      Time elapsed: 00:07:05
                               ETA: 01:53:11

################################################################################
                     [1m Learning iteration 118/2000 [0m                      

                       Computation: 44236 steps/s (collection: 2.132s, learning 0.090s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.0369
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 47.0816
                       Mean reward: 1.62
               Mean episode length: 216.68
    Episode_Reward/reaching_object: 0.3688
     Episode_Reward/lifting_object: -0.0080
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.0417
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 2.22s
                      Time elapsed: 00:07:07
                               ETA: 01:52:46

################################################################################
                     [1m Learning iteration 119/2000 [0m                      

                       Computation: 44724 steps/s (collection: 2.073s, learning 0.125s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.1115
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 47.1490
                       Mean reward: 1.47
               Mean episode length: 210.51
    Episode_Reward/reaching_object: 0.3796
     Episode_Reward/lifting_object: -0.0072
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 5.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 2.20s
                      Time elapsed: 00:07:10
                               ETA: 01:52:20

################################################################################
                     [1m Learning iteration 120/2000 [0m                      

                       Computation: 48184 steps/s (collection: 1.866s, learning 0.175s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.8569
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 47.1753
                       Mean reward: 1.46
               Mean episode length: 209.29
    Episode_Reward/reaching_object: 0.3941
     Episode_Reward/lifting_object: -0.0609
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 2.04s
                      Time elapsed: 00:07:12
                               ETA: 01:51:53

################################################################################
                     [1m Learning iteration 121/2000 [0m                      

                       Computation: 43557 steps/s (collection: 2.075s, learning 0.182s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.7577
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.2120
                       Mean reward: 1.60
               Mean episode length: 213.53
    Episode_Reward/reaching_object: 0.3953
     Episode_Reward/lifting_object: -0.0685
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.0417
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 2.26s
                      Time elapsed: 00:07:14
                               ETA: 01:51:29

################################################################################
                     [1m Learning iteration 122/2000 [0m                      

                       Computation: 45014 steps/s (collection: 2.097s, learning 0.087s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.0667
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.2605
                       Mean reward: 2.18
               Mean episode length: 206.55
    Episode_Reward/reaching_object: 0.4180
     Episode_Reward/lifting_object: -0.0545
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 2.18s
                      Time elapsed: 00:07:16
                               ETA: 01:51:04

################################################################################
                     [1m Learning iteration 123/2000 [0m                      

                       Computation: 45462 steps/s (collection: 2.035s, learning 0.128s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.2099
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 47.3538
                       Mean reward: 1.96
               Mean episode length: 218.30
    Episode_Reward/reaching_object: 0.4235
     Episode_Reward/lifting_object: -0.0233
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 2.16s
                      Time elapsed: 00:07:18
                               ETA: 01:50:40

################################################################################
                     [1m Learning iteration 124/2000 [0m                      

                       Computation: 45851 steps/s (collection: 2.008s, learning 0.136s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.0243
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 47.3756
                       Mean reward: 2.27
               Mean episode length: 222.70
    Episode_Reward/reaching_object: 0.4572
     Episode_Reward/lifting_object: -0.0226
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 2.14s
                      Time elapsed: 00:07:20
                               ETA: 01:50:15

################################################################################
                     [1m Learning iteration 125/2000 [0m                      

                       Computation: 51017 steps/s (collection: 1.818s, learning 0.109s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.0480
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 47.4595
                       Mean reward: 2.19
               Mean episode length: 228.58
    Episode_Reward/reaching_object: 0.4624
     Episode_Reward/lifting_object: 0.0112
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 1.93s
                      Time elapsed: 00:07:22
                               ETA: 01:49:48

################################################################################
                     [1m Learning iteration 126/2000 [0m                      

                       Computation: 50018 steps/s (collection: 1.851s, learning 0.114s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.1697
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 47.5756
                       Mean reward: 2.29
               Mean episode length: 228.76
    Episode_Reward/reaching_object: 0.4795
     Episode_Reward/lifting_object: -0.0301
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 1.97s
                      Time elapsed: 00:07:24
                               ETA: 01:49:22

################################################################################
                     [1m Learning iteration 127/2000 [0m                      

                       Computation: 48750 steps/s (collection: 1.894s, learning 0.122s)
             Mean action noise std: 1.51
          Mean value_function loss: 1.2380
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 47.6058
                       Mean reward: 1.74
               Mean episode length: 221.42
    Episode_Reward/reaching_object: 0.4789
     Episode_Reward/lifting_object: -0.0312
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 2.02s
                      Time elapsed: 00:07:26
                               ETA: 01:48:56

################################################################################
                     [1m Learning iteration 128/2000 [0m                      

                       Computation: 41686 steps/s (collection: 2.130s, learning 0.229s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.3783
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 47.6557
                       Mean reward: 1.83
               Mean episode length: 221.74
    Episode_Reward/reaching_object: 0.4792
     Episode_Reward/lifting_object: -0.0751
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 2.36s
                      Time elapsed: 00:07:29
                               ETA: 01:48:37

################################################################################
                     [1m Learning iteration 129/2000 [0m                      

                       Computation: 38341 steps/s (collection: 2.435s, learning 0.129s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.2373
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.6985
                       Mean reward: 2.30
               Mean episode length: 226.92
    Episode_Reward/reaching_object: 0.5073
     Episode_Reward/lifting_object: -0.0448
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 2.56s
                      Time elapsed: 00:07:31
                               ETA: 01:48:20

################################################################################
                     [1m Learning iteration 130/2000 [0m                      

                       Computation: 41678 steps/s (collection: 2.208s, learning 0.151s)
             Mean action noise std: 1.52
          Mean value_function loss: 1.0042
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 47.7932
                       Mean reward: 2.12
               Mean episode length: 222.82
    Episode_Reward/reaching_object: 0.4783
     Episode_Reward/lifting_object: -0.0713
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 2.36s
                      Time elapsed: 00:07:34
                               ETA: 01:48:00

################################################################################
                     [1m Learning iteration 131/2000 [0m                      

                       Computation: 42234 steps/s (collection: 2.125s, learning 0.203s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.4772
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 47.8247
                       Mean reward: 2.37
               Mean episode length: 223.67
    Episode_Reward/reaching_object: 0.4940
     Episode_Reward/lifting_object: -0.0472
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 2.33s
                      Time elapsed: 00:07:36
                               ETA: 01:47:41

################################################################################
                     [1m Learning iteration 132/2000 [0m                      

                       Computation: 42154 steps/s (collection: 2.206s, learning 0.126s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.0428
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 47.8611
                       Mean reward: 2.29
               Mean episode length: 223.11
    Episode_Reward/reaching_object: 0.5115
     Episode_Reward/lifting_object: -0.0058
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 2.33s
                      Time elapsed: 00:07:38
                               ETA: 01:47:22

################################################################################
                     [1m Learning iteration 133/2000 [0m                      

                       Computation: 48316 steps/s (collection: 1.909s, learning 0.125s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.1668
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 47.9338
                       Mean reward: 2.53
               Mean episode length: 225.34
    Episode_Reward/reaching_object: 0.5425
     Episode_Reward/lifting_object: -0.0345
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 2.03s
                      Time elapsed: 00:07:40
                               ETA: 01:46:58

################################################################################
                     [1m Learning iteration 134/2000 [0m                      

                       Computation: 42945 steps/s (collection: 2.180s, learning 0.110s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.3152
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 47.9929
                       Mean reward: 2.65
               Mean episode length: 222.78
    Episode_Reward/reaching_object: 0.5490
     Episode_Reward/lifting_object: -0.0275
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 2.29s
                      Time elapsed: 00:07:42
                               ETA: 01:46:39

################################################################################
                     [1m Learning iteration 135/2000 [0m                      

                       Computation: 45765 steps/s (collection: 2.029s, learning 0.119s)
             Mean action noise std: 1.54
          Mean value_function loss: 1.0466
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.0536
                       Mean reward: 2.40
               Mean episode length: 215.86
    Episode_Reward/reaching_object: 0.5127
     Episode_Reward/lifting_object: -0.0749
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 2.15s
                      Time elapsed: 00:07:45
                               ETA: 01:46:18

################################################################################
                     [1m Learning iteration 136/2000 [0m                      

                       Computation: 48762 steps/s (collection: 1.929s, learning 0.087s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.1583
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.0777
                       Mean reward: 1.51
               Mean episode length: 212.71
    Episode_Reward/reaching_object: 0.5469
     Episode_Reward/lifting_object: -0.0590
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 2.02s
                      Time elapsed: 00:07:47
                               ETA: 01:45:56

################################################################################
                     [1m Learning iteration 137/2000 [0m                      

                       Computation: 47717 steps/s (collection: 1.972s, learning 0.088s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.1793
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 48.1562
                       Mean reward: 0.56
               Mean episode length: 213.74
    Episode_Reward/reaching_object: 0.5344
     Episode_Reward/lifting_object: -0.1172
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 2.06s
                      Time elapsed: 00:07:49
                               ETA: 01:45:34

################################################################################
                     [1m Learning iteration 138/2000 [0m                      

                       Computation: 49289 steps/s (collection: 1.890s, learning 0.104s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.4954
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 48.2508
                       Mean reward: 2.61
               Mean episode length: 222.59
    Episode_Reward/reaching_object: 0.5480
     Episode_Reward/lifting_object: -0.0628
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.9583
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 1.99s
                      Time elapsed: 00:07:51
                               ETA: 01:45:12

################################################################################
                     [1m Learning iteration 139/2000 [0m                      

                       Computation: 48156 steps/s (collection: 1.914s, learning 0.127s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.0691
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.2724
                       Mean reward: 2.69
               Mean episode length: 224.16
    Episode_Reward/reaching_object: 0.5614
     Episode_Reward/lifting_object: -0.0373
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 2.04s
                      Time elapsed: 00:07:53
                               ETA: 01:44:50

################################################################################
                     [1m Learning iteration 140/2000 [0m                      

                       Computation: 49326 steps/s (collection: 1.854s, learning 0.139s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.2299
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.3191
                       Mean reward: 2.38
               Mean episode length: 231.06
    Episode_Reward/reaching_object: 0.6076
     Episode_Reward/lifting_object: -0.0459
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 1.99s
                      Time elapsed: 00:07:55
                               ETA: 01:44:29

################################################################################
                     [1m Learning iteration 141/2000 [0m                      

                       Computation: 49265 steps/s (collection: 1.839s, learning 0.156s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.0463
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 48.3410
                       Mean reward: 3.08
               Mean episode length: 223.53
    Episode_Reward/reaching_object: 0.6207
     Episode_Reward/lifting_object: 0.0307
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 2.00s
                      Time elapsed: 00:07:57
                               ETA: 01:44:07

################################################################################
                     [1m Learning iteration 142/2000 [0m                      

                       Computation: 48918 steps/s (collection: 1.872s, learning 0.138s)
             Mean action noise std: 1.56
          Mean value_function loss: 2.1771
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.3766
                       Mean reward: 2.23
               Mean episode length: 232.55
    Episode_Reward/reaching_object: 0.6166
     Episode_Reward/lifting_object: -0.0629
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 2.01s
                      Time elapsed: 00:07:59
                               ETA: 01:43:46

################################################################################
                     [1m Learning iteration 143/2000 [0m                      

                       Computation: 49085 steps/s (collection: 1.860s, learning 0.143s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.8203
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.3997
                       Mean reward: 0.38
               Mean episode length: 225.63
    Episode_Reward/reaching_object: 0.6201
     Episode_Reward/lifting_object: -0.1695
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 2.00s
                      Time elapsed: 00:08:01
                               ETA: 01:43:26

################################################################################
                     [1m Learning iteration 144/2000 [0m                      

                       Computation: 48931 steps/s (collection: 1.860s, learning 0.149s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.2398
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.4418
                       Mean reward: 2.94
               Mean episode length: 227.84
    Episode_Reward/reaching_object: 0.6281
     Episode_Reward/lifting_object: -0.0621
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 2.01s
                      Time elapsed: 00:08:03
                               ETA: 01:43:05

################################################################################
                     [1m Learning iteration 145/2000 [0m                      

                       Computation: 46781 steps/s (collection: 2.001s, learning 0.101s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.4344
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 48.5314
                       Mean reward: 2.81
               Mean episode length: 228.68
    Episode_Reward/reaching_object: 0.6376
     Episode_Reward/lifting_object: -0.0315
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 2.10s
                      Time elapsed: 00:08:05
                               ETA: 01:42:46

################################################################################
                     [1m Learning iteration 146/2000 [0m                      

                       Computation: 48899 steps/s (collection: 1.922s, learning 0.089s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.1305
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.5585
                       Mean reward: 3.33
               Mean episode length: 231.36
    Episode_Reward/reaching_object: 0.6562
     Episode_Reward/lifting_object: -0.0484
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 2.01s
                      Time elapsed: 00:08:07
                               ETA: 01:42:26

################################################################################
                     [1m Learning iteration 147/2000 [0m                      

                       Computation: 49471 steps/s (collection: 1.891s, learning 0.096s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.0988
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 48.6243
                       Mean reward: 3.15
               Mean episode length: 235.71
    Episode_Reward/reaching_object: 0.6856
     Episode_Reward/lifting_object: 0.0038
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 1.99s
                      Time elapsed: 00:08:09
                               ETA: 01:42:07

################################################################################
                     [1m Learning iteration 148/2000 [0m                      

                       Computation: 47455 steps/s (collection: 1.961s, learning 0.111s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.1887
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.7037
                       Mean reward: 2.98
               Mean episode length: 234.61
    Episode_Reward/reaching_object: 0.6703
     Episode_Reward/lifting_object: -0.0319
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 2.07s
                      Time elapsed: 00:08:11
                               ETA: 01:41:48

################################################################################
                     [1m Learning iteration 149/2000 [0m                      

                       Computation: 47258 steps/s (collection: 1.948s, learning 0.132s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.9527
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 48.7380
                       Mean reward: 3.15
               Mean episode length: 222.46
    Episode_Reward/reaching_object: 0.6681
     Episode_Reward/lifting_object: -0.0383
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 2.08s
                      Time elapsed: 00:08:13
                               ETA: 01:41:30

################################################################################
                     [1m Learning iteration 150/2000 [0m                      

                       Computation: 46616 steps/s (collection: 1.999s, learning 0.110s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.6668
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.7962
                       Mean reward: 2.23
               Mean episode length: 212.22
    Episode_Reward/reaching_object: 0.6522
     Episode_Reward/lifting_object: -0.0437
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 2.11s
                      Time elapsed: 00:08:15
                               ETA: 01:41:12

################################################################################
                     [1m Learning iteration 151/2000 [0m                      

                       Computation: 47376 steps/s (collection: 1.966s, learning 0.109s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.2955
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 48.8170
                       Mean reward: 3.33
               Mean episode length: 229.49
    Episode_Reward/reaching_object: 0.6824
     Episode_Reward/lifting_object: -0.0949
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 2.07s
                      Time elapsed: 00:08:17
                               ETA: 01:40:54

################################################################################
                     [1m Learning iteration 152/2000 [0m                      

                       Computation: 48809 steps/s (collection: 1.912s, learning 0.102s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.3223
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.8428
                       Mean reward: 3.42
               Mean episode length: 230.43
    Episode_Reward/reaching_object: 0.7086
     Episode_Reward/lifting_object: -0.0548
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 2.01s
                      Time elapsed: 00:08:19
                               ETA: 01:40:35

################################################################################
                     [1m Learning iteration 153/2000 [0m                      

                       Computation: 49487 steps/s (collection: 1.874s, learning 0.113s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.2355
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 48.9078
                       Mean reward: 3.42
               Mean episode length: 229.76
    Episode_Reward/reaching_object: 0.7143
     Episode_Reward/lifting_object: -0.0268
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 1.99s
                      Time elapsed: 00:08:21
                               ETA: 01:40:17

################################################################################
                     [1m Learning iteration 154/2000 [0m                      

                       Computation: 50436 steps/s (collection: 1.856s, learning 0.093s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.1840
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 48.9280
                       Mean reward: 2.74
               Mean episode length: 222.21
    Episode_Reward/reaching_object: 0.7066
     Episode_Reward/lifting_object: -0.0681
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 1.95s
                      Time elapsed: 00:08:23
                               ETA: 01:39:58

################################################################################
                     [1m Learning iteration 155/2000 [0m                      

                       Computation: 50034 steps/s (collection: 1.856s, learning 0.109s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.5231
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 48.9638
                       Mean reward: 2.84
               Mean episode length: 233.63
    Episode_Reward/reaching_object: 0.7349
     Episode_Reward/lifting_object: -0.0355
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 1.96s
                      Time elapsed: 00:08:25
                               ETA: 01:39:39

################################################################################
                     [1m Learning iteration 156/2000 [0m                      

                       Computation: 49737 steps/s (collection: 1.868s, learning 0.109s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.0727
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.9806
                       Mean reward: 3.39
               Mean episode length: 229.29
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: -0.0152
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 1.98s
                      Time elapsed: 00:08:27
                               ETA: 01:39:21

################################################################################
                     [1m Learning iteration 157/2000 [0m                      

                       Computation: 48726 steps/s (collection: 1.932s, learning 0.086s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.4814
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 49.0239
                       Mean reward: 3.56
               Mean episode length: 224.42
    Episode_Reward/reaching_object: 0.7811
     Episode_Reward/lifting_object: 0.0309
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 2.02s
                      Time elapsed: 00:08:29
                               ETA: 01:39:04

################################################################################
                     [1m Learning iteration 158/2000 [0m                      

                       Computation: 49623 steps/s (collection: 1.894s, learning 0.087s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.2402
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 49.0475
                       Mean reward: 4.29
               Mean episode length: 240.75
    Episode_Reward/reaching_object: 0.8034
     Episode_Reward/lifting_object: -0.0012
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 1.98s
                      Time elapsed: 00:08:31
                               ETA: 01:38:46

################################################################################
                     [1m Learning iteration 159/2000 [0m                      

                       Computation: 48498 steps/s (collection: 1.935s, learning 0.092s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.1524
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 49.0789
                       Mean reward: 3.82
               Mean episode length: 234.36
    Episode_Reward/reaching_object: 0.7714
     Episode_Reward/lifting_object: -0.0127
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 2.03s
                      Time elapsed: 00:08:33
                               ETA: 01:38:29

################################################################################
                     [1m Learning iteration 160/2000 [0m                      

                       Computation: 49745 steps/s (collection: 1.891s, learning 0.085s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.7335
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.1338
                       Mean reward: 3.94
               Mean episode length: 239.12
    Episode_Reward/reaching_object: 0.8067
     Episode_Reward/lifting_object: -0.0026
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 1.98s
                      Time elapsed: 00:08:35
                               ETA: 01:38:12

################################################################################
                     [1m Learning iteration 161/2000 [0m                      

                       Computation: 48179 steps/s (collection: 1.949s, learning 0.091s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.1776
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 49.1583
                       Mean reward: 3.40
               Mean episode length: 224.10
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: -0.0230
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 2.04s
                      Time elapsed: 00:08:37
                               ETA: 01:37:56

################################################################################
                     [1m Learning iteration 162/2000 [0m                      

                       Computation: 48437 steps/s (collection: 1.942s, learning 0.088s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.8763
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 49.2087
                       Mean reward: 4.04
               Mean episode length: 230.44
    Episode_Reward/reaching_object: 0.7474
     Episode_Reward/lifting_object: 0.0296
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 2.03s
                      Time elapsed: 00:08:39
                               ETA: 01:37:39

################################################################################
                     [1m Learning iteration 163/2000 [0m                      

                       Computation: 49486 steps/s (collection: 1.894s, learning 0.093s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.1595
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 49.2254
                       Mean reward: 3.37
               Mean episode length: 228.25
    Episode_Reward/reaching_object: 0.7468
     Episode_Reward/lifting_object: 0.0087
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 1.99s
                      Time elapsed: 00:08:41
                               ETA: 01:37:23

################################################################################
                     [1m Learning iteration 164/2000 [0m                      

                       Computation: 49432 steps/s (collection: 1.896s, learning 0.093s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.2472
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 49.2773
                       Mean reward: 2.34
               Mean episode length: 230.21
    Episode_Reward/reaching_object: 0.7884
     Episode_Reward/lifting_object: -0.0921
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 1.99s
                      Time elapsed: 00:08:43
                               ETA: 01:37:06

################################################################################
                     [1m Learning iteration 165/2000 [0m                      

                       Computation: 49150 steps/s (collection: 1.888s, learning 0.112s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.1127
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 49.3438
                       Mean reward: 3.66
               Mean episode length: 233.90
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: -0.0263
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 2.00s
                      Time elapsed: 00:08:45
                               ETA: 01:36:50

################################################################################
                     [1m Learning iteration 166/2000 [0m                      

                       Computation: 47915 steps/s (collection: 1.941s, learning 0.111s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.2664
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 49.4222
                       Mean reward: 3.85
               Mean episode length: 227.67
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 0.0001
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 2.05s
                      Time elapsed: 00:08:47
                               ETA: 01:36:35

################################################################################
                     [1m Learning iteration 167/2000 [0m                      

                       Computation: 46643 steps/s (collection: 2.016s, learning 0.092s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.3179
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 49.4790
                       Mean reward: 4.05
               Mean episode length: 227.58
    Episode_Reward/reaching_object: 0.7619
     Episode_Reward/lifting_object: 0.0355
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 2.11s
                      Time elapsed: 00:08:49
                               ETA: 01:36:20

################################################################################
                     [1m Learning iteration 168/2000 [0m                      

                       Computation: 46709 steps/s (collection: 1.997s, learning 0.108s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.6739
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 49.5161
                       Mean reward: 3.43
               Mean episode length: 220.91
    Episode_Reward/reaching_object: 0.7389
     Episode_Reward/lifting_object: -0.0174
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 2.10s
                      Time elapsed: 00:08:51
                               ETA: 01:36:05

################################################################################
                     [1m Learning iteration 169/2000 [0m                      

                       Computation: 47288 steps/s (collection: 1.979s, learning 0.100s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.7833
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 49.5531
                       Mean reward: 4.03
               Mean episode length: 228.73
    Episode_Reward/reaching_object: 0.7884
     Episode_Reward/lifting_object: -0.0756
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 2.08s
                      Time elapsed: 00:08:53
                               ETA: 01:35:51

################################################################################
                     [1m Learning iteration 170/2000 [0m                      

                       Computation: 46744 steps/s (collection: 2.001s, learning 0.102s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.4788
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 49.5767
                       Mean reward: 3.82
               Mean episode length: 221.53
    Episode_Reward/reaching_object: 0.7888
     Episode_Reward/lifting_object: 0.0012
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 2.10s
                      Time elapsed: 00:08:56
                               ETA: 01:35:37

################################################################################
                     [1m Learning iteration 171/2000 [0m                      

                       Computation: 46853 steps/s (collection: 2.004s, learning 0.094s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.2369
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 49.6302
                       Mean reward: 4.17
               Mean episode length: 232.46
    Episode_Reward/reaching_object: 0.7944
     Episode_Reward/lifting_object: 0.0528
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 2.10s
                      Time elapsed: 00:08:58
                               ETA: 01:35:22

################################################################################
                     [1m Learning iteration 172/2000 [0m                      

                       Computation: 44782 steps/s (collection: 2.069s, learning 0.126s)
             Mean action noise std: 1.64
          Mean value_function loss: 1.6647
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 49.7006
                       Mean reward: 3.89
               Mean episode length: 225.11
    Episode_Reward/reaching_object: 0.7882
     Episode_Reward/lifting_object: -0.0395
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 2.20s
                      Time elapsed: 00:09:00
                               ETA: 01:35:09

################################################################################
                     [1m Learning iteration 173/2000 [0m                      

                       Computation: 47176 steps/s (collection: 1.990s, learning 0.094s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.4349
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.7181
                       Mean reward: 3.50
               Mean episode length: 228.52
    Episode_Reward/reaching_object: 0.8023
     Episode_Reward/lifting_object: -0.0588
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 2.08s
                      Time elapsed: 00:09:02
                               ETA: 01:34:55

################################################################################
                     [1m Learning iteration 174/2000 [0m                      

                       Computation: 47214 steps/s (collection: 1.954s, learning 0.128s)
             Mean action noise std: 1.64
          Mean value_function loss: 1.2436
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 49.7528
                       Mean reward: 2.51
               Mean episode length: 227.17
    Episode_Reward/reaching_object: 0.7902
     Episode_Reward/lifting_object: -0.0928
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 2.08s
                      Time elapsed: 00:09:04
                               ETA: 01:34:41

################################################################################
                     [1m Learning iteration 175/2000 [0m                      

                       Computation: 47385 steps/s (collection: 1.959s, learning 0.116s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.4905
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.7885
                       Mean reward: 4.29
               Mean episode length: 230.94
    Episode_Reward/reaching_object: 0.8100
     Episode_Reward/lifting_object: 0.0056
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 2.07s
                      Time elapsed: 00:09:06
                               ETA: 01:34:28

################################################################################
                     [1m Learning iteration 176/2000 [0m                      

                       Computation: 47113 steps/s (collection: 1.968s, learning 0.118s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.5340
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 49.8333
                       Mean reward: 3.58
               Mean episode length: 225.75
    Episode_Reward/reaching_object: 0.8024
     Episode_Reward/lifting_object: -0.0352
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 2.09s
                      Time elapsed: 00:09:08
                               ETA: 01:34:14

################################################################################
                     [1m Learning iteration 177/2000 [0m                      

                       Computation: 44061 steps/s (collection: 2.131s, learning 0.100s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.3519
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 49.8760
                       Mean reward: 3.95
               Mean episode length: 227.75
    Episode_Reward/reaching_object: 0.8052
     Episode_Reward/lifting_object: 0.0152
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 2.23s
                      Time elapsed: 00:09:10
                               ETA: 01:34:02

################################################################################
                     [1m Learning iteration 178/2000 [0m                      

                       Computation: 45300 steps/s (collection: 2.061s, learning 0.110s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.2273
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.9441
                       Mean reward: 3.86
               Mean episode length: 223.20
    Episode_Reward/reaching_object: 0.7970
     Episode_Reward/lifting_object: 0.0600
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 2.17s
                      Time elapsed: 00:09:13
                               ETA: 01:33:49

################################################################################
                     [1m Learning iteration 179/2000 [0m                      

                       Computation: 39269 steps/s (collection: 2.317s, learning 0.187s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.3061
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 49.9897
                       Mean reward: 3.93
               Mean episode length: 229.69
    Episode_Reward/reaching_object: 0.8322
     Episode_Reward/lifting_object: -0.0122
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 2.50s
                      Time elapsed: 00:09:15
                               ETA: 01:33:40

################################################################################
                     [1m Learning iteration 180/2000 [0m                      

                       Computation: 39325 steps/s (collection: 2.299s, learning 0.201s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.4297
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 50.0546
                       Mean reward: 4.02
               Mean episode length: 231.29
    Episode_Reward/reaching_object: 0.8403
     Episode_Reward/lifting_object: 0.0336
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 2.50s
                      Time elapsed: 00:09:18
                               ETA: 01:33:31

################################################################################
                     [1m Learning iteration 181/2000 [0m                      

                       Computation: 45678 steps/s (collection: 2.039s, learning 0.114s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.5724
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.0769
                       Mean reward: 4.91
               Mean episode length: 242.64
    Episode_Reward/reaching_object: 0.8582
     Episode_Reward/lifting_object: -0.0054
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 2.15s
                      Time elapsed: 00:09:20
                               ETA: 01:33:19

################################################################################
                     [1m Learning iteration 182/2000 [0m                      

                       Computation: 45067 steps/s (collection: 2.062s, learning 0.120s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.3964
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 50.1021
                       Mean reward: 3.70
               Mean episode length: 239.28
    Episode_Reward/reaching_object: 0.8492
     Episode_Reward/lifting_object: 0.0567
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 2.18s
                      Time elapsed: 00:09:22
                               ETA: 01:33:07

################################################################################
                     [1m Learning iteration 183/2000 [0m                      

                       Computation: 43569 steps/s (collection: 2.132s, learning 0.124s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.3252
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 50.1467
                       Mean reward: 4.47
               Mean episode length: 231.19
    Episode_Reward/reaching_object: 0.8544
     Episode_Reward/lifting_object: 0.0408
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 2.26s
                      Time elapsed: 00:09:24
                               ETA: 01:32:56

################################################################################
                     [1m Learning iteration 184/2000 [0m                      

                       Computation: 46544 steps/s (collection: 2.003s, learning 0.109s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.2495
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 50.2151
                       Mean reward: 3.92
               Mean episode length: 231.39
    Episode_Reward/reaching_object: 0.8271
     Episode_Reward/lifting_object: 0.0379
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 2.11s
                      Time elapsed: 00:09:26
                               ETA: 01:32:43

################################################################################
                     [1m Learning iteration 185/2000 [0m                      

                       Computation: 47860 steps/s (collection: 1.960s, learning 0.094s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.4077
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 50.2593
                       Mean reward: 3.83
               Mean episode length: 231.87
    Episode_Reward/reaching_object: 0.8095
     Episode_Reward/lifting_object: 0.0004
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 2.05s
                      Time elapsed: 00:09:28
                               ETA: 01:32:31

################################################################################
                     [1m Learning iteration 186/2000 [0m                      

                       Computation: 44976 steps/s (collection: 2.089s, learning 0.097s)
             Mean action noise std: 1.68
          Mean value_function loss: 1.5294
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 50.2969
                       Mean reward: 4.52
               Mean episode length: 234.21
    Episode_Reward/reaching_object: 0.8281
     Episode_Reward/lifting_object: 0.0483
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0268
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 2.19s
                      Time elapsed: 00:09:31
                               ETA: 01:32:19

################################################################################
                     [1m Learning iteration 187/2000 [0m                      

                       Computation: 47407 steps/s (collection: 1.971s, learning 0.103s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.4651
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.3161
                       Mean reward: 4.53
               Mean episode length: 226.88
    Episode_Reward/reaching_object: 0.8113
     Episode_Reward/lifting_object: 0.0768
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 2.07s
                      Time elapsed: 00:09:33
                               ETA: 01:32:06

################################################################################
                     [1m Learning iteration 188/2000 [0m                      

                       Computation: 47752 steps/s (collection: 1.971s, learning 0.088s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.8701
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 50.3711
                       Mean reward: 3.12
               Mean episode length: 235.15
    Episode_Reward/reaching_object: 0.8506
     Episode_Reward/lifting_object: 0.0130
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0271
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 2.06s
                      Time elapsed: 00:09:35
                               ETA: 01:31:54

################################################################################
                     [1m Learning iteration 189/2000 [0m                      

                       Computation: 45165 steps/s (collection: 2.075s, learning 0.102s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.4178
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 50.4157
                       Mean reward: 4.98
               Mean episode length: 237.29
    Episode_Reward/reaching_object: 0.8348
     Episode_Reward/lifting_object: 0.0681
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 2.18s
                      Time elapsed: 00:09:37
                               ETA: 01:31:43

################################################################################
                     [1m Learning iteration 190/2000 [0m                      

                       Computation: 45935 steps/s (collection: 1.971s, learning 0.169s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.1798
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 50.4910
                       Mean reward: 2.86
               Mean episode length: 239.57
    Episode_Reward/reaching_object: 0.8349
     Episode_Reward/lifting_object: 0.0202
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 2.14s
                      Time elapsed: 00:09:39
                               ETA: 01:31:31

################################################################################
                     [1m Learning iteration 191/2000 [0m                      

                       Computation: 46707 steps/s (collection: 1.949s, learning 0.156s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.2905
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 50.5612
                       Mean reward: 3.98
               Mean episode length: 239.44
    Episode_Reward/reaching_object: 0.8478
     Episode_Reward/lifting_object: 0.0895
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 2.10s
                      Time elapsed: 00:09:41
                               ETA: 01:31:19

################################################################################
                     [1m Learning iteration 192/2000 [0m                      

                       Computation: 47605 steps/s (collection: 1.960s, learning 0.105s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.3038
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 50.6074
                       Mean reward: 3.26
               Mean episode length: 240.43
    Episode_Reward/reaching_object: 0.8109
     Episode_Reward/lifting_object: -0.0259
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 2.06s
                      Time elapsed: 00:09:43
                               ETA: 01:31:07

################################################################################
                     [1m Learning iteration 193/2000 [0m                      

                       Computation: 46410 steps/s (collection: 2.011s, learning 0.108s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.2512
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 50.6638
                       Mean reward: 4.19
               Mean episode length: 240.30
    Episode_Reward/reaching_object: 0.7752
     Episode_Reward/lifting_object: 0.0988
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 2.12s
                      Time elapsed: 00:09:45
                               ETA: 01:30:56

################################################################################
                     [1m Learning iteration 194/2000 [0m                      

                       Computation: 45606 steps/s (collection: 2.044s, learning 0.112s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.3413
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 50.7164
                       Mean reward: 3.34
               Mean episode length: 242.10
    Episode_Reward/reaching_object: 0.7811
     Episode_Reward/lifting_object: 0.0706
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 2.16s
                      Time elapsed: 00:09:47
                               ETA: 01:30:45

################################################################################
                     [1m Learning iteration 195/2000 [0m                      

                       Computation: 44350 steps/s (collection: 2.114s, learning 0.103s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.3662
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 50.7703
                       Mean reward: 4.42
               Mean episode length: 236.91
    Episode_Reward/reaching_object: 0.7931
     Episode_Reward/lifting_object: 0.1359
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 2.22s
                      Time elapsed: 00:09:50
                               ETA: 01:30:34

################################################################################
                     [1m Learning iteration 196/2000 [0m                      

                       Computation: 49396 steps/s (collection: 1.894s, learning 0.096s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.2588
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 50.8212
                       Mean reward: 3.80
               Mean episode length: 234.03
    Episode_Reward/reaching_object: 0.8248
     Episode_Reward/lifting_object: 0.1312
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 1.99s
                      Time elapsed: 00:09:52
                               ETA: 01:30:22

################################################################################
                     [1m Learning iteration 197/2000 [0m                      

                       Computation: 47868 steps/s (collection: 1.951s, learning 0.103s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.2284
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 50.8677
                       Mean reward: 4.53
               Mean episode length: 240.15
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: -0.0099
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 2.05s
                      Time elapsed: 00:09:54
                               ETA: 01:30:10

################################################################################
                     [1m Learning iteration 198/2000 [0m                      

                       Computation: 49155 steps/s (collection: 1.901s, learning 0.099s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.2428
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 50.9422
                       Mean reward: 4.04
               Mean episode length: 231.99
    Episode_Reward/reaching_object: 0.7471
     Episode_Reward/lifting_object: 0.1337
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 2.00s
                      Time elapsed: 00:09:56
                               ETA: 01:29:58

################################################################################
                     [1m Learning iteration 199/2000 [0m                      

                       Computation: 48004 steps/s (collection: 1.956s, learning 0.092s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.3476
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 50.9961
                       Mean reward: 4.05
               Mean episode length: 239.38
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 0.0898
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 2.05s
                      Time elapsed: 00:09:58
                               ETA: 01:29:47

################################################################################
                     [1m Learning iteration 200/2000 [0m                      

                       Computation: 48507 steps/s (collection: 1.935s, learning 0.092s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.3752
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 51.0470
                       Mean reward: 3.77
               Mean episode length: 243.89
    Episode_Reward/reaching_object: 0.7722
     Episode_Reward/lifting_object: 0.0881
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 2.03s
                      Time elapsed: 00:10:00
                               ETA: 01:29:35

################################################################################
                     [1m Learning iteration 201/2000 [0m                      

                       Computation: 47029 steps/s (collection: 1.995s, learning 0.096s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.2688
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 51.1159
                       Mean reward: 3.86
               Mean episode length: 236.96
    Episode_Reward/reaching_object: 0.7978
     Episode_Reward/lifting_object: 0.1454
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 2.09s
                      Time elapsed: 00:10:02
                               ETA: 01:29:24

################################################################################
                     [1m Learning iteration 202/2000 [0m                      

                       Computation: 44002 steps/s (collection: 2.137s, learning 0.098s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.4506
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 51.1442
                       Mean reward: 4.00
               Mean episode length: 235.76
    Episode_Reward/reaching_object: 0.7725
     Episode_Reward/lifting_object: 0.0922
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 2.23s
                      Time elapsed: 00:10:04
                               ETA: 01:29:15

################################################################################
                     [1m Learning iteration 203/2000 [0m                      

                       Computation: 44556 steps/s (collection: 2.113s, learning 0.094s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.3102
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 51.1648
                       Mean reward: 4.08
               Mean episode length: 236.72
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 0.1260
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 2.21s
                      Time elapsed: 00:10:06
                               ETA: 01:29:05

################################################################################
                     [1m Learning iteration 204/2000 [0m                      

                       Computation: 44150 steps/s (collection: 2.096s, learning 0.131s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.3740
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 51.2124
                       Mean reward: 4.41
               Mean episode length: 233.70
    Episode_Reward/reaching_object: 0.7852
     Episode_Reward/lifting_object: 0.0978
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 2.23s
                      Time elapsed: 00:10:09
                               ETA: 01:28:55

################################################################################
                     [1m Learning iteration 205/2000 [0m                      

                       Computation: 44549 steps/s (collection: 2.104s, learning 0.103s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.2634
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 51.2660
                       Mean reward: 4.73
               Mean episode length: 229.43
    Episode_Reward/reaching_object: 0.7740
     Episode_Reward/lifting_object: 0.1690
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 2.21s
                      Time elapsed: 00:10:11
                               ETA: 01:28:46

################################################################################
                     [1m Learning iteration 206/2000 [0m                      

                       Computation: 44831 steps/s (collection: 2.044s, learning 0.149s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.4272
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 51.3116
                       Mean reward: 3.72
               Mean episode length: 227.34
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 0.0768
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 2.19s
                      Time elapsed: 00:10:13
                               ETA: 01:28:36

################################################################################
                     [1m Learning iteration 207/2000 [0m                      

                       Computation: 42800 steps/s (collection: 2.206s, learning 0.091s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.6654
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 51.3485
                       Mean reward: 4.06
               Mean episode length: 226.67
    Episode_Reward/reaching_object: 0.7429
     Episode_Reward/lifting_object: 0.0960
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 2.30s
                      Time elapsed: 00:10:15
                               ETA: 01:28:27

################################################################################
                     [1m Learning iteration 208/2000 [0m                      

                       Computation: 46455 steps/s (collection: 2.013s, learning 0.104s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.3730
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 51.3879
                       Mean reward: 4.10
               Mean episode length: 224.89
    Episode_Reward/reaching_object: 0.7342
     Episode_Reward/lifting_object: 0.1804
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 2.12s
                      Time elapsed: 00:10:17
                               ETA: 01:28:17

################################################################################
                     [1m Learning iteration 209/2000 [0m                      

                       Computation: 45768 steps/s (collection: 2.058s, learning 0.090s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.6020
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 51.4470
                       Mean reward: 4.46
               Mean episode length: 231.17
    Episode_Reward/reaching_object: 0.7571
     Episode_Reward/lifting_object: 0.0849
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 2.15s
                      Time elapsed: 00:10:19
                               ETA: 01:28:07

################################################################################
                     [1m Learning iteration 210/2000 [0m                      

                       Computation: 47811 steps/s (collection: 1.968s, learning 0.089s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.4462
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 51.4777
                       Mean reward: 3.89
               Mean episode length: 233.24
    Episode_Reward/reaching_object: 0.7757
     Episode_Reward/lifting_object: 0.1219
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 2.06s
                      Time elapsed: 00:10:22
                               ETA: 01:27:57

################################################################################
                     [1m Learning iteration 211/2000 [0m                      

                       Computation: 43471 steps/s (collection: 2.162s, learning 0.099s)
             Mean action noise std: 1.76
          Mean value_function loss: 1.1032
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.5236
                       Mean reward: 3.90
               Mean episode length: 233.80
    Episode_Reward/reaching_object: 0.7536
     Episode_Reward/lifting_object: 0.0329
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 2.26s
                      Time elapsed: 00:10:24
                               ETA: 01:27:48

################################################################################
                     [1m Learning iteration 212/2000 [0m                      

                       Computation: 48078 steps/s (collection: 1.946s, learning 0.099s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.4354
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 51.5586
                       Mean reward: 4.60
               Mean episode length: 223.12
    Episode_Reward/reaching_object: 0.7536
     Episode_Reward/lifting_object: 0.1420
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 2.04s
                      Time elapsed: 00:10:26
                               ETA: 01:27:37

################################################################################
                     [1m Learning iteration 213/2000 [0m                      

                       Computation: 49049 steps/s (collection: 1.916s, learning 0.088s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.5464
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 51.6252
                       Mean reward: 4.44
               Mean episode length: 228.00
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 0.1287
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 2.00s
                      Time elapsed: 00:10:28
                               ETA: 01:27:27

################################################################################
                     [1m Learning iteration 214/2000 [0m                      

                       Computation: 47463 steps/s (collection: 1.955s, learning 0.117s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.6109
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 51.6749
                       Mean reward: 5.08
               Mean episode length: 224.88
    Episode_Reward/reaching_object: 0.7534
     Episode_Reward/lifting_object: 0.1959
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 2.07s
                      Time elapsed: 00:10:30
                               ETA: 01:27:16

################################################################################
                     [1m Learning iteration 215/2000 [0m                      

                       Computation: 48408 steps/s (collection: 1.917s, learning 0.114s)
             Mean action noise std: 1.77
          Mean value_function loss: 1.1647
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 51.7211
                       Mean reward: 3.78
               Mean episode length: 232.97
    Episode_Reward/reaching_object: 0.7577
     Episode_Reward/lifting_object: 0.0727
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 2.03s
                      Time elapsed: 00:10:32
                               ETA: 01:27:06

################################################################################
                     [1m Learning iteration 216/2000 [0m                      

                       Computation: 49344 steps/s (collection: 1.887s, learning 0.105s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.7684
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 51.7577
                       Mean reward: 4.47
               Mean episode length: 226.87
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 0.1817
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 1.99s
                      Time elapsed: 00:10:34
                               ETA: 01:26:55

################################################################################
                     [1m Learning iteration 217/2000 [0m                      

                       Computation: 46725 steps/s (collection: 1.996s, learning 0.108s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.4267
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 51.8239
                       Mean reward: 4.62
               Mean episode length: 235.04
    Episode_Reward/reaching_object: 0.7542
     Episode_Reward/lifting_object: 0.1958
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 2.10s
                      Time elapsed: 00:10:36
                               ETA: 01:26:46

################################################################################
                     [1m Learning iteration 218/2000 [0m                      

                       Computation: 47986 steps/s (collection: 1.934s, learning 0.115s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.3737
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 51.8931
                       Mean reward: 3.82
               Mean episode length: 239.31
    Episode_Reward/reaching_object: 0.7766
     Episode_Reward/lifting_object: 0.1437
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 2.05s
                      Time elapsed: 00:10:38
                               ETA: 01:26:36

################################################################################
                     [1m Learning iteration 219/2000 [0m                      

                       Computation: 48032 steps/s (collection: 1.914s, learning 0.133s)
             Mean action noise std: 1.79
          Mean value_function loss: 1.9815
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 51.9348
                       Mean reward: 3.84
               Mean episode length: 238.23
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 0.1097
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 2.05s
                      Time elapsed: 00:10:40
                               ETA: 01:26:26

################################################################################
                     [1m Learning iteration 220/2000 [0m                      

                       Computation: 45886 steps/s (collection: 2.013s, learning 0.129s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.8675
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 51.9750
                       Mean reward: 5.11
               Mean episode length: 234.09
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 0.1397
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 2.14s
                      Time elapsed: 00:10:42
                               ETA: 01:26:17

################################################################################
                     [1m Learning iteration 221/2000 [0m                      

                       Computation: 49063 steps/s (collection: 1.905s, learning 0.099s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.7293
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 52.0256
                       Mean reward: 4.29
               Mean episode length: 232.21
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 0.2566
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 2.00s
                      Time elapsed: 00:10:44
                               ETA: 01:26:07

################################################################################
                     [1m Learning iteration 222/2000 [0m                      

                       Computation: 47572 steps/s (collection: 1.952s, learning 0.115s)
             Mean action noise std: 1.80
          Mean value_function loss: 1.0221
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 52.0663
                       Mean reward: 4.90
               Mean episode length: 237.26
    Episode_Reward/reaching_object: 0.7798
     Episode_Reward/lifting_object: 0.2468
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 2.07s
                      Time elapsed: 00:10:46
                               ETA: 01:25:57

################################################################################
                     [1m Learning iteration 223/2000 [0m                      

                       Computation: 49624 steps/s (collection: 1.870s, learning 0.111s)
             Mean action noise std: 1.80
          Mean value_function loss: 1.1879
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 52.1132
                       Mean reward: 4.84
               Mean episode length: 230.52
    Episode_Reward/reaching_object: 0.7711
     Episode_Reward/lifting_object: 0.3505
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 1.98s
                      Time elapsed: 00:10:48
                               ETA: 01:25:47

################################################################################
                     [1m Learning iteration 224/2000 [0m                      

                       Computation: 49318 steps/s (collection: 1.898s, learning 0.096s)
             Mean action noise std: 1.80
          Mean value_function loss: 1.2819
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 52.1637
                       Mean reward: 5.44
               Mean episode length: 237.54
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 0.3009
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 1.99s
                      Time elapsed: 00:10:50
                               ETA: 01:25:37

################################################################################
                     [1m Learning iteration 225/2000 [0m                      

                       Computation: 46899 steps/s (collection: 1.971s, learning 0.125s)
             Mean action noise std: 1.81
          Mean value_function loss: 1.5176
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 52.2172
                       Mean reward: 4.28
               Mean episode length: 241.27
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 0.2923
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 2.10s
                      Time elapsed: 00:10:52
                               ETA: 01:25:28

################################################################################
                     [1m Learning iteration 226/2000 [0m                      

                       Computation: 46655 steps/s (collection: 2.000s, learning 0.107s)
             Mean action noise std: 1.81
          Mean value_function loss: 1.0755
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 52.2496
                       Mean reward: 6.01
               Mean episode length: 233.99
    Episode_Reward/reaching_object: 0.7504
     Episode_Reward/lifting_object: 0.2276
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 2.11s
                      Time elapsed: 00:10:55
                               ETA: 01:25:19

################################################################################
                     [1m Learning iteration 227/2000 [0m                      

                       Computation: 47329 steps/s (collection: 1.981s, learning 0.096s)
             Mean action noise std: 1.81
          Mean value_function loss: 2.2926
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 52.3011
                       Mean reward: 4.83
               Mean episode length: 229.35
    Episode_Reward/reaching_object: 0.6884
     Episode_Reward/lifting_object: 0.2555
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0334
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 2.08s
                      Time elapsed: 00:10:57
                               ETA: 01:25:09

################################################################################
                     [1m Learning iteration 228/2000 [0m                      

                       Computation: 42723 steps/s (collection: 2.209s, learning 0.092s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.9086
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 52.3491
                       Mean reward: 4.48
               Mean episode length: 229.02
    Episode_Reward/reaching_object: 0.7015
     Episode_Reward/lifting_object: 0.3841
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 2.30s
                      Time elapsed: 00:10:59
                               ETA: 01:25:02

################################################################################
                     [1m Learning iteration 229/2000 [0m                      

                       Computation: 48543 steps/s (collection: 1.932s, learning 0.093s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.8477
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 52.4171
                       Mean reward: 3.88
               Mean episode length: 225.61
    Episode_Reward/reaching_object: 0.6681
     Episode_Reward/lifting_object: 0.2592
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 2.03s
                      Time elapsed: 00:11:01
                               ETA: 01:24:53

################################################################################
                     [1m Learning iteration 230/2000 [0m                      

                       Computation: 45333 steps/s (collection: 2.009s, learning 0.160s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.9259
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 52.4705
                       Mean reward: 6.04
               Mean episode length: 226.45
    Episode_Reward/reaching_object: 0.6730
     Episode_Reward/lifting_object: 0.4008
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 2.17s
                      Time elapsed: 00:11:03
                               ETA: 01:24:44

################################################################################
                     [1m Learning iteration 231/2000 [0m                      

                       Computation: 48102 steps/s (collection: 1.956s, learning 0.088s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.9829
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 52.5267
                       Mean reward: 4.15
               Mean episode length: 222.76
    Episode_Reward/reaching_object: 0.6533
     Episode_Reward/lifting_object: 0.4073
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 2.04s
                      Time elapsed: 00:11:05
                               ETA: 01:24:35

################################################################################
                     [1m Learning iteration 232/2000 [0m                      

                       Computation: 49058 steps/s (collection: 1.904s, learning 0.100s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.9174
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 52.5743
                       Mean reward: 5.35
               Mean episode length: 223.69
    Episode_Reward/reaching_object: 0.6476
     Episode_Reward/lifting_object: 0.3977
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 2.00s
                      Time elapsed: 00:11:07
                               ETA: 01:24:26

################################################################################
                     [1m Learning iteration 233/2000 [0m                      

                       Computation: 49448 steps/s (collection: 1.892s, learning 0.096s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.9704
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 52.6140
                       Mean reward: 4.31
               Mean episode length: 226.80
    Episode_Reward/reaching_object: 0.6142
     Episode_Reward/lifting_object: 0.3653
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 1.99s
                      Time elapsed: 00:11:09
                               ETA: 01:24:16

################################################################################
                     [1m Learning iteration 234/2000 [0m                      

                       Computation: 48090 steps/s (collection: 1.937s, learning 0.108s)
             Mean action noise std: 1.84
          Mean value_function loss: 1.1494
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 52.6731
                       Mean reward: 3.20
               Mean episode length: 221.65
    Episode_Reward/reaching_object: 0.6146
     Episode_Reward/lifting_object: 0.3246
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 2.04s
                      Time elapsed: 00:11:11
                               ETA: 01:24:07

################################################################################
                     [1m Learning iteration 235/2000 [0m                      

                       Computation: 48255 steps/s (collection: 1.912s, learning 0.126s)
             Mean action noise std: 1.84
          Mean value_function loss: 0.8318
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 52.7343
                       Mean reward: 5.09
               Mean episode length: 209.16
    Episode_Reward/reaching_object: 0.6082
     Episode_Reward/lifting_object: 0.3712
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 2.04s
                      Time elapsed: 00:11:13
                               ETA: 01:23:58

################################################################################
                     [1m Learning iteration 236/2000 [0m                      

                       Computation: 48093 steps/s (collection: 1.938s, learning 0.106s)
             Mean action noise std: 1.85
          Mean value_function loss: 1.2548
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 52.7791
                       Mean reward: 4.90
               Mean episode length: 195.89
    Episode_Reward/reaching_object: 0.5574
     Episode_Reward/lifting_object: 0.3766
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 2.04s
                      Time elapsed: 00:11:15
                               ETA: 01:23:49

################################################################################
                     [1m Learning iteration 237/2000 [0m                      

                       Computation: 46312 steps/s (collection: 1.949s, learning 0.174s)
             Mean action noise std: 1.85
          Mean value_function loss: 1.3251
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 52.8156
                       Mean reward: 4.14
               Mean episode length: 189.59
    Episode_Reward/reaching_object: 0.5747
     Episode_Reward/lifting_object: 0.4290
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 2.12s
                      Time elapsed: 00:11:17
                               ETA: 01:23:41

################################################################################
                     [1m Learning iteration 238/2000 [0m                      

                       Computation: 47878 steps/s (collection: 1.933s, learning 0.120s)
             Mean action noise std: 1.85
          Mean value_function loss: 1.6514
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 52.8467
                       Mean reward: 5.09
               Mean episode length: 202.45
    Episode_Reward/reaching_object: 0.5781
     Episode_Reward/lifting_object: 0.3574
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 2.05s
                      Time elapsed: 00:11:19
                               ETA: 01:23:32

################################################################################
                     [1m Learning iteration 239/2000 [0m                      

                       Computation: 49372 steps/s (collection: 1.892s, learning 0.099s)
             Mean action noise std: 1.85
          Mean value_function loss: 1.4041
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 52.8737
                       Mean reward: 5.40
               Mean episode length: 204.39
    Episode_Reward/reaching_object: 0.5813
     Episode_Reward/lifting_object: 0.3731
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 1.99s
                      Time elapsed: 00:11:21
                               ETA: 01:23:23

################################################################################
                     [1m Learning iteration 240/2000 [0m                      

                       Computation: 47490 steps/s (collection: 1.974s, learning 0.096s)
             Mean action noise std: 1.86
          Mean value_function loss: 1.4977
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 52.9152
                       Mean reward: 4.47
               Mean episode length: 201.02
    Episode_Reward/reaching_object: 0.5810
     Episode_Reward/lifting_object: 0.3515
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 2.07s
                      Time elapsed: 00:11:24
                               ETA: 01:23:15

################################################################################
                     [1m Learning iteration 241/2000 [0m                      

                       Computation: 47655 steps/s (collection: 1.960s, learning 0.103s)
             Mean action noise std: 1.86
          Mean value_function loss: 1.0807
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 52.9529
                       Mean reward: 4.79
               Mean episode length: 187.87
    Episode_Reward/reaching_object: 0.5735
     Episode_Reward/lifting_object: 0.3501
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 2.06s
                      Time elapsed: 00:11:26
                               ETA: 01:23:06

################################################################################
                     [1m Learning iteration 242/2000 [0m                      

                       Computation: 48189 steps/s (collection: 1.953s, learning 0.087s)
             Mean action noise std: 1.86
          Mean value_function loss: 1.3570
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 52.9964
                       Mean reward: 4.63
               Mean episode length: 193.96
    Episode_Reward/reaching_object: 0.5969
     Episode_Reward/lifting_object: 0.3484
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 2.04s
                      Time elapsed: 00:11:28
                               ETA: 01:22:58

################################################################################
                     [1m Learning iteration 243/2000 [0m                      

                       Computation: 47775 steps/s (collection: 1.972s, learning 0.086s)
             Mean action noise std: 1.86
          Mean value_function loss: 2.5997
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.0295
                       Mean reward: 5.73
               Mean episode length: 196.07
    Episode_Reward/reaching_object: 0.6155
     Episode_Reward/lifting_object: 0.5046
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 2.06s
                      Time elapsed: 00:11:30
                               ETA: 01:22:49

################################################################################
                     [1m Learning iteration 244/2000 [0m                      

                       Computation: 45708 steps/s (collection: 2.037s, learning 0.114s)
             Mean action noise std: 1.86
          Mean value_function loss: 1.2657
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 53.0447
                       Mean reward: 6.19
               Mean episode length: 196.82
    Episode_Reward/reaching_object: 0.6224
     Episode_Reward/lifting_object: 0.3673
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 2.15s
                      Time elapsed: 00:11:32
                               ETA: 01:22:42

################################################################################
                     [1m Learning iteration 245/2000 [0m                      

                       Computation: 47026 steps/s (collection: 2.004s, learning 0.086s)
             Mean action noise std: 1.87
          Mean value_function loss: 1.2434
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 53.0904
                       Mean reward: 4.97
               Mean episode length: 212.70
    Episode_Reward/reaching_object: 0.6375
     Episode_Reward/lifting_object: 0.3406
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 2.09s
                      Time elapsed: 00:11:34
                               ETA: 01:22:34

################################################################################
                     [1m Learning iteration 246/2000 [0m                      

                       Computation: 46115 steps/s (collection: 2.016s, learning 0.116s)
             Mean action noise std: 1.87
          Mean value_function loss: 1.7389
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 53.1427
                       Mean reward: 4.17
               Mean episode length: 214.14
    Episode_Reward/reaching_object: 0.6572
     Episode_Reward/lifting_object: 0.4617
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 2.13s
                      Time elapsed: 00:11:36
                               ETA: 01:22:26

################################################################################
                     [1m Learning iteration 247/2000 [0m                      

                       Computation: 46027 steps/s (collection: 2.021s, learning 0.115s)
             Mean action noise std: 1.87
          Mean value_function loss: 5.3427
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 53.1850
                       Mean reward: 6.26
               Mean episode length: 210.94
    Episode_Reward/reaching_object: 0.6540
     Episode_Reward/lifting_object: 0.5238
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 2.14s
                      Time elapsed: 00:11:38
                               ETA: 01:22:18

################################################################################
                     [1m Learning iteration 248/2000 [0m                      

                       Computation: 47248 steps/s (collection: 1.971s, learning 0.110s)
             Mean action noise std: 1.88
          Mean value_function loss: 2.0538
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.1955
                       Mean reward: 4.88
               Mean episode length: 219.97
    Episode_Reward/reaching_object: 0.6452
     Episode_Reward/lifting_object: 0.2437
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 2.08s
                      Time elapsed: 00:11:40
                               ETA: 01:22:10

################################################################################
                     [1m Learning iteration 249/2000 [0m                      

                       Computation: 47203 steps/s (collection: 1.962s, learning 0.120s)
             Mean action noise std: 1.88
          Mean value_function loss: 2.5519
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.2249
                       Mean reward: 5.44
               Mean episode length: 217.78
    Episode_Reward/reaching_object: 0.6668
     Episode_Reward/lifting_object: 0.5446
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 2.08s
                      Time elapsed: 00:11:42
                               ETA: 01:22:02

################################################################################
                     [1m Learning iteration 250/2000 [0m                      

                       Computation: 48293 steps/s (collection: 1.930s, learning 0.106s)
             Mean action noise std: 1.88
          Mean value_function loss: 1.5494
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 53.2623
                       Mean reward: 5.62
               Mean episode length: 214.30
    Episode_Reward/reaching_object: 0.6704
     Episode_Reward/lifting_object: 0.4336
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 2.04s
                      Time elapsed: 00:11:44
                               ETA: 01:21:54

################################################################################
                     [1m Learning iteration 251/2000 [0m                      

                       Computation: 47079 steps/s (collection: 1.969s, learning 0.120s)
             Mean action noise std: 1.89
          Mean value_function loss: 1.1760
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 53.3209
                       Mean reward: 5.29
               Mean episode length: 217.47
    Episode_Reward/reaching_object: 0.6508
     Episode_Reward/lifting_object: 0.4307
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 2.09s
                      Time elapsed: 00:11:46
                               ETA: 01:21:46

################################################################################
                     [1m Learning iteration 252/2000 [0m                      

                       Computation: 46864 steps/s (collection: 1.958s, learning 0.140s)
             Mean action noise std: 1.89
          Mean value_function loss: 1.7461
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 53.3728
                       Mean reward: 7.37
               Mean episode length: 217.40
    Episode_Reward/reaching_object: 0.6586
     Episode_Reward/lifting_object: 0.7280
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 2.10s
                      Time elapsed: 00:11:49
                               ETA: 01:21:39

################################################################################
                     [1m Learning iteration 253/2000 [0m                      

                       Computation: 48976 steps/s (collection: 1.881s, learning 0.127s)
             Mean action noise std: 1.89
          Mean value_function loss: 1.6195
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 53.4104
                       Mean reward: 5.00
               Mean episode length: 220.31
    Episode_Reward/reaching_object: 0.6589
     Episode_Reward/lifting_object: 0.5173
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 2.01s
                      Time elapsed: 00:11:51
                               ETA: 01:21:30

################################################################################
                     [1m Learning iteration 254/2000 [0m                      

                       Computation: 49027 steps/s (collection: 1.879s, learning 0.126s)
             Mean action noise std: 1.90
          Mean value_function loss: 1.9903
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 53.4661
                       Mean reward: 6.24
               Mean episode length: 223.82
    Episode_Reward/reaching_object: 0.6286
     Episode_Reward/lifting_object: 0.5147
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 2.01s
                      Time elapsed: 00:11:53
                               ETA: 01:21:22

################################################################################
                     [1m Learning iteration 255/2000 [0m                      

                       Computation: 48650 steps/s (collection: 1.900s, learning 0.121s)
             Mean action noise std: 1.90
          Mean value_function loss: 1.5964
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 53.5300
                       Mean reward: 6.26
               Mean episode length: 223.57
    Episode_Reward/reaching_object: 0.6521
     Episode_Reward/lifting_object: 0.7162
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 2.02s
                      Time elapsed: 00:11:55
                               ETA: 01:21:14

################################################################################
                     [1m Learning iteration 256/2000 [0m                      

                       Computation: 48236 steps/s (collection: 1.886s, learning 0.152s)
             Mean action noise std: 1.90
          Mean value_function loss: 2.9209
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 53.5646
                       Mean reward: 5.47
               Mean episode length: 231.81
    Episode_Reward/reaching_object: 0.6424
     Episode_Reward/lifting_object: 0.4968
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 2.04s
                      Time elapsed: 00:11:57
                               ETA: 01:21:06

################################################################################
                     [1m Learning iteration 257/2000 [0m                      

                       Computation: 46563 steps/s (collection: 2.019s, learning 0.092s)
             Mean action noise std: 1.91
          Mean value_function loss: 7.0694
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.6049
                       Mean reward: 6.10
               Mean episode length: 231.18
    Episode_Reward/reaching_object: 0.7016
     Episode_Reward/lifting_object: 0.5991
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 2.11s
                      Time elapsed: 00:11:59
                               ETA: 01:20:59

################################################################################
                     [1m Learning iteration 258/2000 [0m                      

                       Computation: 48565 steps/s (collection: 1.935s, learning 0.089s)
             Mean action noise std: 1.91
          Mean value_function loss: 9.3560
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 53.6392
                       Mean reward: 7.87
               Mean episode length: 229.20
    Episode_Reward/reaching_object: 0.6872
     Episode_Reward/lifting_object: 0.7919
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 2.02s
                      Time elapsed: 00:12:01
                               ETA: 01:20:51

################################################################################
                     [1m Learning iteration 259/2000 [0m                      

                       Computation: 49434 steps/s (collection: 1.904s, learning 0.085s)
             Mean action noise std: 1.91
          Mean value_function loss: 9.0881
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 53.6532
                       Mean reward: 7.14
               Mean episode length: 235.45
    Episode_Reward/reaching_object: 0.6898
     Episode_Reward/lifting_object: 0.7133
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 1.99s
                      Time elapsed: 00:12:03
                               ETA: 01:20:43

################################################################################
                     [1m Learning iteration 260/2000 [0m                      

                       Computation: 49501 steps/s (collection: 1.896s, learning 0.089s)
             Mean action noise std: 1.91
          Mean value_function loss: 1.7329
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 53.6907
                       Mean reward: 6.26
               Mean episode length: 222.34
    Episode_Reward/reaching_object: 0.6797
     Episode_Reward/lifting_object: 0.7021
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 1.99s
                      Time elapsed: 00:12:05
                               ETA: 01:20:34

################################################################################
                     [1m Learning iteration 261/2000 [0m                      

                       Computation: 45881 steps/s (collection: 1.962s, learning 0.181s)
             Mean action noise std: 1.92
          Mean value_function loss: 1.8015
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 53.7435
                       Mean reward: 6.89
               Mean episode length: 217.53
    Episode_Reward/reaching_object: 0.6861
     Episode_Reward/lifting_object: 0.8092
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 2.14s
                      Time elapsed: 00:12:07
                               ETA: 01:20:27

################################################################################
                     [1m Learning iteration 262/2000 [0m                      

                       Computation: 44923 steps/s (collection: 2.090s, learning 0.098s)
             Mean action noise std: 1.92
          Mean value_function loss: 2.5999
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.7883
                       Mean reward: 6.50
               Mean episode length: 210.68
    Episode_Reward/reaching_object: 0.6615
     Episode_Reward/lifting_object: 0.6924
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 2.19s
                      Time elapsed: 00:12:09
                               ETA: 01:20:21

################################################################################
                     [1m Learning iteration 263/2000 [0m                      

                       Computation: 46885 steps/s (collection: 1.965s, learning 0.132s)
             Mean action noise std: 1.92
          Mean value_function loss: 1.7675
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 53.8277
                       Mean reward: 7.00
               Mean episode length: 208.38
    Episode_Reward/reaching_object: 0.6356
     Episode_Reward/lifting_object: 0.1706
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 2.10s
                      Time elapsed: 00:12:11
                               ETA: 01:20:14

################################################################################
                     [1m Learning iteration 264/2000 [0m                      

                       Computation: 44261 steps/s (collection: 2.124s, learning 0.097s)
             Mean action noise std: 1.92
          Mean value_function loss: 5.3096
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 53.8688
                       Mean reward: 6.36
               Mean episode length: 206.67
    Episode_Reward/reaching_object: 0.6573
     Episode_Reward/lifting_object: 0.5903
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 2.22s
                      Time elapsed: 00:12:13
                               ETA: 01:20:07

################################################################################
                     [1m Learning iteration 265/2000 [0m                      

                       Computation: 48544 steps/s (collection: 1.929s, learning 0.096s)
             Mean action noise std: 1.93
          Mean value_function loss: 2.8518
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 53.8831
                       Mean reward: 6.72
               Mean episode length: 202.79
    Episode_Reward/reaching_object: 0.6337
     Episode_Reward/lifting_object: 0.5309
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 2.03s
                      Time elapsed: 00:12:15
                               ETA: 01:20:00

################################################################################
                     [1m Learning iteration 266/2000 [0m                      

                       Computation: 47866 steps/s (collection: 1.934s, learning 0.120s)
             Mean action noise std: 1.93
          Mean value_function loss: 1.9153
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 53.9162
                       Mean reward: 6.44
               Mean episode length: 214.51
    Episode_Reward/reaching_object: 0.6628
     Episode_Reward/lifting_object: 0.6603
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 2.05s
                      Time elapsed: 00:12:17
                               ETA: 01:19:52

################################################################################
                     [1m Learning iteration 267/2000 [0m                      

                       Computation: 49305 steps/s (collection: 1.899s, learning 0.095s)
             Mean action noise std: 1.93
          Mean value_function loss: 1.8209
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 53.9527
                       Mean reward: 7.57
               Mean episode length: 210.99
    Episode_Reward/reaching_object: 0.6592
     Episode_Reward/lifting_object: 0.6913
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 1.99s
                      Time elapsed: 00:12:19
                               ETA: 01:19:44

################################################################################
                     [1m Learning iteration 268/2000 [0m                      

                       Computation: 48139 steps/s (collection: 1.924s, learning 0.118s)
             Mean action noise std: 1.93
          Mean value_function loss: 1.7617
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 53.9898
                       Mean reward: 7.57
               Mean episode length: 211.44
    Episode_Reward/reaching_object: 0.6626
     Episode_Reward/lifting_object: 0.7770
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 2.04s
                      Time elapsed: 00:12:22
                               ETA: 01:19:37

################################################################################
                     [1m Learning iteration 269/2000 [0m                      

                       Computation: 47816 steps/s (collection: 1.953s, learning 0.103s)
             Mean action noise std: 1.94
          Mean value_function loss: 4.5602
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.0267
                       Mean reward: 7.56
               Mean episode length: 205.77
    Episode_Reward/reaching_object: 0.6531
     Episode_Reward/lifting_object: 0.7513
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 2.06s
                      Time elapsed: 00:12:24
                               ETA: 01:19:30

################################################################################
                     [1m Learning iteration 270/2000 [0m                      

                       Computation: 47494 steps/s (collection: 1.935s, learning 0.135s)
             Mean action noise std: 1.94
          Mean value_function loss: 2.1048
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 54.0490
                       Mean reward: 5.88
               Mean episode length: 208.46
    Episode_Reward/reaching_object: 0.6741
     Episode_Reward/lifting_object: 0.6254
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 2.07s
                      Time elapsed: 00:12:26
                               ETA: 01:19:23

################################################################################
                     [1m Learning iteration 271/2000 [0m                      

                       Computation: 47058 steps/s (collection: 2.003s, learning 0.086s)
             Mean action noise std: 1.94
          Mean value_function loss: 2.0335
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 54.0824
                       Mean reward: 5.92
               Mean episode length: 211.72
    Episode_Reward/reaching_object: 0.6877
     Episode_Reward/lifting_object: 0.6911
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 2.09s
                      Time elapsed: 00:12:28
                               ETA: 01:19:16

################################################################################
                     [1m Learning iteration 272/2000 [0m                      

                       Computation: 48541 steps/s (collection: 1.925s, learning 0.100s)
             Mean action noise std: 1.94
          Mean value_function loss: 2.5011
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.1104
                       Mean reward: 6.07
               Mean episode length: 212.03
    Episode_Reward/reaching_object: 0.6615
     Episode_Reward/lifting_object: 0.5998
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 2.03s
                      Time elapsed: 00:12:30
                               ETA: 01:19:08

################################################################################
                     [1m Learning iteration 273/2000 [0m                      

                       Computation: 48067 steps/s (collection: 1.940s, learning 0.106s)
             Mean action noise std: 1.95
          Mean value_function loss: 1.8910
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 54.1362
                       Mean reward: 7.90
               Mean episode length: 221.16
    Episode_Reward/reaching_object: 0.6799
     Episode_Reward/lifting_object: 0.7777
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 2.05s
                      Time elapsed: 00:12:32
                               ETA: 01:19:01

################################################################################
                     [1m Learning iteration 274/2000 [0m                      

                       Computation: 48960 steps/s (collection: 1.910s, learning 0.097s)
             Mean action noise std: 1.95
          Mean value_function loss: 5.3556
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.1753
                       Mean reward: 6.14
               Mean episode length: 217.98
    Episode_Reward/reaching_object: 0.6531
     Episode_Reward/lifting_object: 0.8298
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 2.01s
                      Time elapsed: 00:12:34
                               ETA: 01:18:54

################################################################################
                     [1m Learning iteration 275/2000 [0m                      

                       Computation: 47496 steps/s (collection: 1.979s, learning 0.091s)
             Mean action noise std: 1.95
          Mean value_function loss: 2.2565
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 54.2038
                       Mean reward: 5.16
               Mean episode length: 217.82
    Episode_Reward/reaching_object: 0.6706
     Episode_Reward/lifting_object: 0.6612
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 2.07s
                      Time elapsed: 00:12:36
                               ETA: 01:18:47

################################################################################
                     [1m Learning iteration 276/2000 [0m                      

                       Computation: 48042 steps/s (collection: 1.954s, learning 0.092s)
             Mean action noise std: 1.95
          Mean value_function loss: 3.0258
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 54.2467
                       Mean reward: 7.72
               Mean episode length: 219.86
    Episode_Reward/reaching_object: 0.6524
     Episode_Reward/lifting_object: 0.6984
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 2.05s
                      Time elapsed: 00:12:38
                               ETA: 01:18:40

################################################################################
                     [1m Learning iteration 277/2000 [0m                      

                       Computation: 47986 steps/s (collection: 1.958s, learning 0.091s)
             Mean action noise std: 1.96
          Mean value_function loss: 2.4398
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 54.2766
                       Mean reward: 6.13
               Mean episode length: 209.82
    Episode_Reward/reaching_object: 0.6701
     Episode_Reward/lifting_object: 1.0410
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 2.05s
                      Time elapsed: 00:12:40
                               ETA: 01:18:33

################################################################################
                     [1m Learning iteration 278/2000 [0m                      

                       Computation: 48490 steps/s (collection: 1.939s, learning 0.088s)
             Mean action noise std: 1.96
          Mean value_function loss: 2.4033
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 54.3042
                       Mean reward: 8.48
               Mean episode length: 229.22
    Episode_Reward/reaching_object: 0.6842
     Episode_Reward/lifting_object: 1.0477
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 2.03s
                      Time elapsed: 00:12:42
                               ETA: 01:18:26

################################################################################
                     [1m Learning iteration 279/2000 [0m                      

                       Computation: 48486 steps/s (collection: 1.940s, learning 0.087s)
             Mean action noise std: 1.96
          Mean value_function loss: 2.8295
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 54.3321
                       Mean reward: 8.01
               Mean episode length: 217.08
    Episode_Reward/reaching_object: 0.6659
     Episode_Reward/lifting_object: 0.7335
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 2.03s
                      Time elapsed: 00:12:44
                               ETA: 01:18:19

################################################################################
                     [1m Learning iteration 280/2000 [0m                      

                       Computation: 47868 steps/s (collection: 1.945s, learning 0.109s)
             Mean action noise std: 1.96
          Mean value_function loss: 3.4431
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 54.3729
                       Mean reward: 8.98
               Mean episode length: 210.58
    Episode_Reward/reaching_object: 0.6772
     Episode_Reward/lifting_object: 1.0289
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 2.05s
                      Time elapsed: 00:12:46
                               ETA: 01:18:12

################################################################################
                     [1m Learning iteration 281/2000 [0m                      

                       Computation: 46576 steps/s (collection: 1.987s, learning 0.124s)
             Mean action noise std: 1.97
          Mean value_function loss: 6.5861
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 54.4223
                       Mean reward: 7.38
               Mean episode length: 209.49
    Episode_Reward/reaching_object: 0.6651
     Episode_Reward/lifting_object: 0.9089
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 2.11s
                      Time elapsed: 00:12:48
                               ETA: 01:18:05

################################################################################
                     [1m Learning iteration 282/2000 [0m                      

                       Computation: 48435 steps/s (collection: 1.935s, learning 0.094s)
             Mean action noise std: 1.97
          Mean value_function loss: 2.3552
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 54.4793
                       Mean reward: 8.33
               Mean episode length: 217.43
    Episode_Reward/reaching_object: 0.6433
     Episode_Reward/lifting_object: 0.7134
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 2.03s
                      Time elapsed: 00:12:50
                               ETA: 01:17:58

################################################################################
                     [1m Learning iteration 283/2000 [0m                      

                       Computation: 48192 steps/s (collection: 1.927s, learning 0.113s)
             Mean action noise std: 1.97
          Mean value_function loss: 3.6449
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 54.5185
                       Mean reward: 6.59
               Mean episode length: 212.22
    Episode_Reward/reaching_object: 0.6643
     Episode_Reward/lifting_object: 0.9171
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 2.04s
                      Time elapsed: 00:12:52
                               ETA: 01:17:51

################################################################################
                     [1m Learning iteration 284/2000 [0m                      

                       Computation: 48386 steps/s (collection: 1.918s, learning 0.114s)
             Mean action noise std: 1.98
          Mean value_function loss: 4.2147
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 54.5542
                       Mean reward: 7.83
               Mean episode length: 209.25
    Episode_Reward/reaching_object: 0.6598
     Episode_Reward/lifting_object: 1.1326
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 2.03s
                      Time elapsed: 00:12:54
                               ETA: 01:17:45

################################################################################
                     [1m Learning iteration 285/2000 [0m                      

                       Computation: 47811 steps/s (collection: 1.968s, learning 0.089s)
             Mean action noise std: 1.98
          Mean value_function loss: 3.0070
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 54.5990
                       Mean reward: 9.54
               Mean episode length: 205.58
    Episode_Reward/reaching_object: 0.6627
     Episode_Reward/lifting_object: 0.9640
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 2.06s
                      Time elapsed: 00:12:56
                               ETA: 01:17:38

################################################################################
                     [1m Learning iteration 286/2000 [0m                      

                       Computation: 48399 steps/s (collection: 1.942s, learning 0.089s)
             Mean action noise std: 1.98
          Mean value_function loss: 2.9262
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 54.6408
                       Mean reward: 9.04
               Mean episode length: 218.62
    Episode_Reward/reaching_object: 0.6621
     Episode_Reward/lifting_object: 1.0851
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 2.03s
                      Time elapsed: 00:12:58
                               ETA: 01:17:31

################################################################################
                     [1m Learning iteration 287/2000 [0m                      

                       Computation: 47985 steps/s (collection: 1.962s, learning 0.087s)
             Mean action noise std: 1.99
          Mean value_function loss: 4.0529
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.6899
                       Mean reward: 7.33
               Mean episode length: 205.84
    Episode_Reward/reaching_object: 0.6396
     Episode_Reward/lifting_object: 0.9747
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 2.05s
                      Time elapsed: 00:13:00
                               ETA: 01:17:24

################################################################################
                     [1m Learning iteration 288/2000 [0m                      

                       Computation: 49088 steps/s (collection: 1.915s, learning 0.088s)
             Mean action noise std: 1.99
          Mean value_function loss: 2.8469
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.7238
                       Mean reward: 8.70
               Mean episode length: 209.40
    Episode_Reward/reaching_object: 0.6644
     Episode_Reward/lifting_object: 0.9738
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 2.00s
                      Time elapsed: 00:13:02
                               ETA: 01:17:17

################################################################################
                     [1m Learning iteration 289/2000 [0m                      

                       Computation: 46898 steps/s (collection: 1.997s, learning 0.099s)
             Mean action noise std: 1.99
          Mean value_function loss: 3.1646
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 54.7466
                       Mean reward: 8.54
               Mean episode length: 194.08
    Episode_Reward/reaching_object: 0.6388
     Episode_Reward/lifting_object: 0.9373
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 2.10s
                      Time elapsed: 00:13:05
                               ETA: 01:17:11

################################################################################
                     [1m Learning iteration 290/2000 [0m                      

                       Computation: 46832 steps/s (collection: 1.958s, learning 0.142s)
             Mean action noise std: 1.99
          Mean value_function loss: 4.6619
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 54.7787
                       Mean reward: 7.31
               Mean episode length: 192.22
    Episode_Reward/reaching_object: 0.6558
     Episode_Reward/lifting_object: 1.0333
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 2.10s
                      Time elapsed: 00:13:07
                               ETA: 01:17:05

################################################################################
                     [1m Learning iteration 291/2000 [0m                      

                       Computation: 45477 steps/s (collection: 1.981s, learning 0.181s)
             Mean action noise std: 2.00
          Mean value_function loss: 3.3051
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 54.8132
                       Mean reward: 8.88
               Mean episode length: 212.93
    Episode_Reward/reaching_object: 0.6558
     Episode_Reward/lifting_object: 0.9434
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 2.16s
                      Time elapsed: 00:13:09
                               ETA: 01:16:59

################################################################################
                     [1m Learning iteration 292/2000 [0m                      

                       Computation: 47834 steps/s (collection: 1.931s, learning 0.125s)
             Mean action noise std: 2.00
          Mean value_function loss: 6.9863
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 54.8333
                       Mean reward: 10.59
               Mean episode length: 208.12
    Episode_Reward/reaching_object: 0.6403
     Episode_Reward/lifting_object: 1.1104
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 2.06s
                      Time elapsed: 00:13:11
                               ETA: 01:16:52

################################################################################
                     [1m Learning iteration 293/2000 [0m                      

                       Computation: 49203 steps/s (collection: 1.905s, learning 0.093s)
             Mean action noise std: 2.00
          Mean value_function loss: 2.7902
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 54.8575
                       Mean reward: 7.89
               Mean episode length: 204.96
    Episode_Reward/reaching_object: 0.6455
     Episode_Reward/lifting_object: 1.0839
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 2.00s
                      Time elapsed: 00:13:13
                               ETA: 01:16:46

################################################################################
                     [1m Learning iteration 294/2000 [0m                      

                       Computation: 48298 steps/s (collection: 1.944s, learning 0.092s)
             Mean action noise std: 2.00
          Mean value_function loss: 6.0552
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.8906
                       Mean reward: 5.63
               Mean episode length: 203.83
    Episode_Reward/reaching_object: 0.6292
     Episode_Reward/lifting_object: 1.1622
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 2.04s
                      Time elapsed: 00:13:15
                               ETA: 01:16:39

################################################################################
                     [1m Learning iteration 295/2000 [0m                      

                       Computation: 48504 steps/s (collection: 1.938s, learning 0.089s)
             Mean action noise std: 2.00
          Mean value_function loss: 6.9915
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 54.9199
                       Mean reward: 8.86
               Mean episode length: 212.54
    Episode_Reward/reaching_object: 0.6518
     Episode_Reward/lifting_object: 1.1619
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 2.03s
                      Time elapsed: 00:13:17
                               ETA: 01:16:33

################################################################################
                     [1m Learning iteration 296/2000 [0m                      

                       Computation: 47685 steps/s (collection: 1.971s, learning 0.091s)
             Mean action noise std: 2.01
          Mean value_function loss: 3.8549
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 54.9648
                       Mean reward: 8.94
               Mean episode length: 206.79
    Episode_Reward/reaching_object: 0.6527
     Episode_Reward/lifting_object: 0.8485
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 2.06s
                      Time elapsed: 00:13:19
                               ETA: 01:16:26

################################################################################
                     [1m Learning iteration 297/2000 [0m                      

                       Computation: 48587 steps/s (collection: 1.928s, learning 0.095s)
             Mean action noise std: 2.01
          Mean value_function loss: 6.0397
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 55.0108
                       Mean reward: 11.32
               Mean episode length: 207.49
    Episode_Reward/reaching_object: 0.6522
     Episode_Reward/lifting_object: 1.3368
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 2.02s
                      Time elapsed: 00:13:21
                               ETA: 01:16:20

################################################################################
                     [1m Learning iteration 298/2000 [0m                      

                       Computation: 46423 steps/s (collection: 1.985s, learning 0.133s)
             Mean action noise std: 2.01
          Mean value_function loss: 6.5901
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.0316
                       Mean reward: 9.44
               Mean episode length: 209.41
    Episode_Reward/reaching_object: 0.6600
     Episode_Reward/lifting_object: 1.1974
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 2.12s
                      Time elapsed: 00:13:23
                               ETA: 01:16:14

################################################################################
                     [1m Learning iteration 299/2000 [0m                      

                       Computation: 47515 steps/s (collection: 1.961s, learning 0.108s)
             Mean action noise std: 2.02
          Mean value_function loss: 10.2016
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.0620
                       Mean reward: 8.75
               Mean episode length: 210.35
    Episode_Reward/reaching_object: 0.6549
     Episode_Reward/lifting_object: 1.1529
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 2.07s
                      Time elapsed: 00:13:25
                               ETA: 01:16:08

################################################################################
                     [1m Learning iteration 300/2000 [0m                      

                       Computation: 46715 steps/s (collection: 2.016s, learning 0.088s)
             Mean action noise std: 2.02
          Mean value_function loss: 4.5201
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 55.1106
                       Mean reward: 10.57
               Mean episode length: 206.36
    Episode_Reward/reaching_object: 0.6545
     Episode_Reward/lifting_object: 1.4047
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 2.10s
                      Time elapsed: 00:13:27
                               ETA: 01:16:02

################################################################################
                     [1m Learning iteration 301/2000 [0m                      

                       Computation: 48032 steps/s (collection: 1.958s, learning 0.089s)
             Mean action noise std: 2.02
          Mean value_function loss: 4.2067
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 55.1599
                       Mean reward: 8.24
               Mean episode length: 209.41
    Episode_Reward/reaching_object: 0.6700
     Episode_Reward/lifting_object: 1.5481
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 2.05s
                      Time elapsed: 00:13:29
                               ETA: 01:15:55

################################################################################
                     [1m Learning iteration 302/2000 [0m                      

                       Computation: 47997 steps/s (collection: 1.939s, learning 0.109s)
             Mean action noise std: 2.03
          Mean value_function loss: 5.2198
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 55.1938
                       Mean reward: 9.67
               Mean episode length: 216.82
    Episode_Reward/reaching_object: 0.6682
     Episode_Reward/lifting_object: 1.1364
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 2.05s
                      Time elapsed: 00:13:31
                               ETA: 01:15:49

################################################################################
                     [1m Learning iteration 303/2000 [0m                      

                       Computation: 47533 steps/s (collection: 1.964s, learning 0.105s)
             Mean action noise std: 2.03
          Mean value_function loss: 10.1938
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 55.2042
                       Mean reward: 10.79
               Mean episode length: 213.14
    Episode_Reward/reaching_object: 0.6454
     Episode_Reward/lifting_object: 1.3727
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 2.07s
                      Time elapsed: 00:13:33
                               ETA: 01:15:43

################################################################################
                     [1m Learning iteration 304/2000 [0m                      

                       Computation: 45888 steps/s (collection: 2.053s, learning 0.089s)
             Mean action noise std: 2.03
          Mean value_function loss: 6.7384
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 55.2072
                       Mean reward: 10.51
               Mean episode length: 206.91
    Episode_Reward/reaching_object: 0.6592
     Episode_Reward/lifting_object: 0.9640
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 2.14s
                      Time elapsed: 00:13:36
                               ETA: 01:15:37

################################################################################
                     [1m Learning iteration 305/2000 [0m                      

                       Computation: 48029 steps/s (collection: 1.941s, learning 0.106s)
             Mean action noise std: 2.03
          Mean value_function loss: 6.4521
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.2225
                       Mean reward: 10.05
               Mean episode length: 202.55
    Episode_Reward/reaching_object: 0.6435
     Episode_Reward/lifting_object: 1.3053
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 2.05s
                      Time elapsed: 00:13:38
                               ETA: 01:15:31

################################################################################
                     [1m Learning iteration 306/2000 [0m                      

                       Computation: 46864 steps/s (collection: 1.955s, learning 0.143s)
             Mean action noise std: 2.03
          Mean value_function loss: 4.0830
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 55.2460
                       Mean reward: 10.38
               Mean episode length: 206.18
    Episode_Reward/reaching_object: 0.6341
     Episode_Reward/lifting_object: 1.3682
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 2.10s
                      Time elapsed: 00:13:40
                               ETA: 01:15:25

################################################################################
                     [1m Learning iteration 307/2000 [0m                      

                       Computation: 45229 steps/s (collection: 2.033s, learning 0.141s)
             Mean action noise std: 2.03
          Mean value_function loss: 5.4304
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 55.2727
                       Mean reward: 10.09
               Mean episode length: 211.61
    Episode_Reward/reaching_object: 0.6470
     Episode_Reward/lifting_object: 1.4714
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 2.17s
                      Time elapsed: 00:13:42
                               ETA: 01:15:20

################################################################################
                     [1m Learning iteration 308/2000 [0m                      

                       Computation: 45320 steps/s (collection: 2.065s, learning 0.105s)
             Mean action noise std: 2.04
          Mean value_function loss: 8.9656
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 55.3092
                       Mean reward: 12.33
               Mean episode length: 216.09
    Episode_Reward/reaching_object: 0.6315
     Episode_Reward/lifting_object: 1.5519
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 2.17s
                      Time elapsed: 00:13:44
                               ETA: 01:15:15

################################################################################
                     [1m Learning iteration 309/2000 [0m                      

                       Computation: 46844 steps/s (collection: 2.009s, learning 0.089s)
             Mean action noise std: 2.04
          Mean value_function loss: 10.6614
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 55.3615
                       Mean reward: 13.29
               Mean episode length: 211.79
    Episode_Reward/reaching_object: 0.6431
     Episode_Reward/lifting_object: 1.3342
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0438
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 2.10s
                      Time elapsed: 00:13:46
                               ETA: 01:15:09

################################################################################
                     [1m Learning iteration 310/2000 [0m                      

                       Computation: 45110 steps/s (collection: 2.061s, learning 0.119s)
             Mean action noise std: 2.05
          Mean value_function loss: 6.7442
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 55.4256
                       Mean reward: 13.07
               Mean episode length: 208.35
    Episode_Reward/reaching_object: 0.6491
     Episode_Reward/lifting_object: 1.1635
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 2.18s
                      Time elapsed: 00:13:48
                               ETA: 01:15:03

################################################################################
                     [1m Learning iteration 311/2000 [0m                      

                       Computation: 46787 steps/s (collection: 2.004s, learning 0.097s)
             Mean action noise std: 2.05
          Mean value_function loss: 7.3647
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 55.4765
                       Mean reward: 13.62
               Mean episode length: 216.59
    Episode_Reward/reaching_object: 0.6709
     Episode_Reward/lifting_object: 1.5658
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 2.10s
                      Time elapsed: 00:13:50
                               ETA: 01:14:58

################################################################################
                     [1m Learning iteration 312/2000 [0m                      

                       Computation: 45678 steps/s (collection: 2.030s, learning 0.122s)
             Mean action noise std: 2.05
          Mean value_function loss: 5.9364
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 55.5017
                       Mean reward: 9.26
               Mean episode length: 216.22
    Episode_Reward/reaching_object: 0.6611
     Episode_Reward/lifting_object: 1.6171
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 2.15s
                      Time elapsed: 00:13:53
                               ETA: 01:14:52

################################################################################
                     [1m Learning iteration 313/2000 [0m                      

                       Computation: 44868 steps/s (collection: 2.062s, learning 0.129s)
             Mean action noise std: 2.05
          Mean value_function loss: 8.7550
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.5189
                       Mean reward: 10.56
               Mean episode length: 204.56
    Episode_Reward/reaching_object: 0.6600
     Episode_Reward/lifting_object: 1.4697
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 2.19s
                      Time elapsed: 00:13:55
                               ETA: 01:14:47

################################################################################
                     [1m Learning iteration 314/2000 [0m                      

                       Computation: 45882 steps/s (collection: 2.024s, learning 0.118s)
             Mean action noise std: 2.05
          Mean value_function loss: 7.4695
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 55.5436
                       Mean reward: 11.94
               Mean episode length: 218.03
    Episode_Reward/reaching_object: 0.6397
     Episode_Reward/lifting_object: 1.5964
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 2.14s
                      Time elapsed: 00:13:57
                               ETA: 01:14:42

################################################################################
                     [1m Learning iteration 315/2000 [0m                      

                       Computation: 43957 steps/s (collection: 2.115s, learning 0.122s)
             Mean action noise std: 2.06
          Mean value_function loss: 6.2724
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 55.5843
                       Mean reward: 13.09
               Mean episode length: 203.28
    Episode_Reward/reaching_object: 0.6416
     Episode_Reward/lifting_object: 1.7153
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 2.24s
                      Time elapsed: 00:13:59
                               ETA: 01:14:37

################################################################################
                     [1m Learning iteration 316/2000 [0m                      

                       Computation: 46873 steps/s (collection: 1.994s, learning 0.104s)
             Mean action noise std: 2.06
          Mean value_function loss: 5.6992
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.6192
                       Mean reward: 10.68
               Mean episode length: 200.68
    Episode_Reward/reaching_object: 0.6247
     Episode_Reward/lifting_object: 1.7189
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 2.10s
                      Time elapsed: 00:14:01
                               ETA: 01:14:31

################################################################################
                     [1m Learning iteration 317/2000 [0m                      

                       Computation: 44364 steps/s (collection: 2.115s, learning 0.101s)
             Mean action noise std: 2.06
          Mean value_function loss: 5.7460
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 55.6411
                       Mean reward: 10.23
               Mean episode length: 191.08
    Episode_Reward/reaching_object: 0.6273
     Episode_Reward/lifting_object: 1.3273
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 2.22s
                      Time elapsed: 00:14:03
                               ETA: 01:14:26

################################################################################
                     [1m Learning iteration 318/2000 [0m                      

                       Computation: 45988 steps/s (collection: 2.048s, learning 0.090s)
             Mean action noise std: 2.06
          Mean value_function loss: 10.4624
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 55.6785
                       Mean reward: 10.83
               Mean episode length: 192.00
    Episode_Reward/reaching_object: 0.6087
     Episode_Reward/lifting_object: 1.4590
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 2.14s
                      Time elapsed: 00:14:06
                               ETA: 01:14:21

################################################################################
                     [1m Learning iteration 319/2000 [0m                      

                       Computation: 45849 steps/s (collection: 2.043s, learning 0.102s)
             Mean action noise std: 2.07
          Mean value_function loss: 4.9396
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 55.7181
                       Mean reward: 10.15
               Mean episode length: 186.37
    Episode_Reward/reaching_object: 0.6060
     Episode_Reward/lifting_object: 1.4942
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 2.14s
                      Time elapsed: 00:14:08
                               ETA: 01:14:16

################################################################################
                     [1m Learning iteration 320/2000 [0m                      

                       Computation: 45837 steps/s (collection: 2.039s, learning 0.106s)
             Mean action noise std: 2.07
          Mean value_function loss: 20.7833
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.7450
                       Mean reward: 8.33
               Mean episode length: 191.28
    Episode_Reward/reaching_object: 0.6426
     Episode_Reward/lifting_object: 1.6447
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 2.14s
                      Time elapsed: 00:14:10
                               ETA: 01:14:10

################################################################################
                     [1m Learning iteration 321/2000 [0m                      

                       Computation: 43630 steps/s (collection: 2.122s, learning 0.131s)
             Mean action noise std: 2.07
          Mean value_function loss: 5.1432
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 55.7778
                       Mean reward: 11.86
               Mean episode length: 200.29
    Episode_Reward/reaching_object: 0.6147
     Episode_Reward/lifting_object: 1.4517
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0438
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 2.25s
                      Time elapsed: 00:14:12
                               ETA: 01:14:05

################################################################################
                     [1m Learning iteration 322/2000 [0m                      

                       Computation: 43669 steps/s (collection: 2.116s, learning 0.135s)
             Mean action noise std: 2.08
          Mean value_function loss: 6.2233
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 55.8189
                       Mean reward: 12.46
               Mean episode length: 198.83
    Episode_Reward/reaching_object: 0.6184
     Episode_Reward/lifting_object: 1.8859
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 2.25s
                      Time elapsed: 00:14:14
                               ETA: 01:14:01

################################################################################
                     [1m Learning iteration 323/2000 [0m                      

                       Computation: 46007 steps/s (collection: 2.045s, learning 0.092s)
             Mean action noise std: 2.08
          Mean value_function loss: 10.5903
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 55.8541
                       Mean reward: 13.58
               Mean episode length: 179.11
    Episode_Reward/reaching_object: 0.6335
     Episode_Reward/lifting_object: 1.9658
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 2.14s
                      Time elapsed: 00:14:17
                               ETA: 01:13:56

################################################################################
                     [1m Learning iteration 324/2000 [0m                      

                       Computation: 47286 steps/s (collection: 1.972s, learning 0.107s)
             Mean action noise std: 2.08
          Mean value_function loss: 10.0500
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 55.8972
                       Mean reward: 10.44
               Mean episode length: 196.68
    Episode_Reward/reaching_object: 0.6156
     Episode_Reward/lifting_object: 1.6285
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 2.08s
                      Time elapsed: 00:14:19
                               ETA: 01:13:50

################################################################################
                     [1m Learning iteration 325/2000 [0m                      

                       Computation: 46694 steps/s (collection: 2.014s, learning 0.091s)
             Mean action noise std: 2.09
          Mean value_function loss: 7.5518
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 55.9347
                       Mean reward: 12.62
               Mean episode length: 201.59
    Episode_Reward/reaching_object: 0.6026
     Episode_Reward/lifting_object: 1.7969
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 2.11s
                      Time elapsed: 00:14:21
                               ETA: 01:13:45

################################################################################
                     [1m Learning iteration 326/2000 [0m                      

                       Computation: 42603 steps/s (collection: 2.175s, learning 0.133s)
             Mean action noise std: 2.09
          Mean value_function loss: 10.6707
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 55.9662
                       Mean reward: 9.68
               Mean episode length: 186.28
    Episode_Reward/reaching_object: 0.5918
     Episode_Reward/lifting_object: 1.7712
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 2.31s
                      Time elapsed: 00:14:23
                               ETA: 01:13:40

################################################################################
                     [1m Learning iteration 327/2000 [0m                      

                       Computation: 45226 steps/s (collection: 2.077s, learning 0.097s)
             Mean action noise std: 2.09
          Mean value_function loss: 12.4269
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 56.0009
                       Mean reward: 16.53
               Mean episode length: 189.33
    Episode_Reward/reaching_object: 0.6175
     Episode_Reward/lifting_object: 2.2134
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 2.17s
                      Time elapsed: 00:14:25
                               ETA: 01:13:35

################################################################################
                     [1m Learning iteration 328/2000 [0m                      

                       Computation: 45188 steps/s (collection: 2.068s, learning 0.107s)
             Mean action noise std: 2.09
          Mean value_function loss: 8.5906
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 56.0455
                       Mean reward: 16.12
               Mean episode length: 195.16
    Episode_Reward/reaching_object: 0.5869
     Episode_Reward/lifting_object: 1.8686
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 2.18s
                      Time elapsed: 00:14:27
                               ETA: 01:13:30

################################################################################
                     [1m Learning iteration 329/2000 [0m                      

                       Computation: 43711 steps/s (collection: 2.105s, learning 0.144s)
             Mean action noise std: 2.10
          Mean value_function loss: 14.0116
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 56.0815
                       Mean reward: 16.14
               Mean episode length: 204.58
    Episode_Reward/reaching_object: 0.6121
     Episode_Reward/lifting_object: 1.8069
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 2.25s
                      Time elapsed: 00:14:30
                               ETA: 01:13:26

################################################################################
                     [1m Learning iteration 330/2000 [0m                      

                       Computation: 41532 steps/s (collection: 2.146s, learning 0.221s)
             Mean action noise std: 2.10
          Mean value_function loss: 10.5117
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 56.1277
                       Mean reward: 15.70
               Mean episode length: 186.60
    Episode_Reward/reaching_object: 0.6139
     Episode_Reward/lifting_object: 1.7222
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 2.37s
                      Time elapsed: 00:14:32
                               ETA: 01:13:22

################################################################################
                     [1m Learning iteration 331/2000 [0m                      

                       Computation: 39800 steps/s (collection: 2.320s, learning 0.150s)
             Mean action noise std: 2.10
          Mean value_function loss: 11.0321
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 56.1615
                       Mean reward: 14.28
               Mean episode length: 198.84
    Episode_Reward/reaching_object: 0.5973
     Episode_Reward/lifting_object: 1.9174
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 2.47s
                      Time elapsed: 00:14:34
                               ETA: 01:13:18

################################################################################
                     [1m Learning iteration 332/2000 [0m                      

                       Computation: 38221 steps/s (collection: 2.434s, learning 0.138s)
             Mean action noise std: 2.10
          Mean value_function loss: 7.7324
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 56.1806
                       Mean reward: 13.71
               Mean episode length: 193.65
    Episode_Reward/reaching_object: 0.6484
     Episode_Reward/lifting_object: 2.1825
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 2.57s
                      Time elapsed: 00:14:37
                               ETA: 01:13:15

################################################################################
                     [1m Learning iteration 333/2000 [0m                      

                       Computation: 17877 steps/s (collection: 5.375s, learning 0.124s)
             Mean action noise std: 2.11
          Mean value_function loss: 12.8975
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.2101
                       Mean reward: 7.41
               Mean episode length: 188.37
    Episode_Reward/reaching_object: 0.6045
     Episode_Reward/lifting_object: 1.7717
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 11.3750
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 5.50s
                      Time elapsed: 00:14:43
                               ETA: 01:13:27

################################################################################
                     [1m Learning iteration 334/2000 [0m                      

                       Computation: 14067 steps/s (collection: 6.854s, learning 0.134s)
             Mean action noise std: 2.11
          Mean value_function loss: 13.6002
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 56.2372
                       Mean reward: 13.71
               Mean episode length: 193.91
    Episode_Reward/reaching_object: 0.6243
     Episode_Reward/lifting_object: 1.8051
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 6.99s
                      Time elapsed: 00:14:50
                               ETA: 01:13:46

################################################################################
                     [1m Learning iteration 335/2000 [0m                      

                       Computation: 14255 steps/s (collection: 6.760s, learning 0.136s)
             Mean action noise std: 2.11
          Mean value_function loss: 13.8187
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 56.2799
                       Mean reward: 15.43
               Mean episode length: 197.32
    Episode_Reward/reaching_object: 0.6222
     Episode_Reward/lifting_object: 2.3490
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 6.90s
                      Time elapsed: 00:14:56
                               ETA: 01:14:04

################################################################################
                     [1m Learning iteration 336/2000 [0m                      

                       Computation: 14030 steps/s (collection: 6.884s, learning 0.123s)
             Mean action noise std: 2.12
          Mean value_function loss: 31.2950
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 56.3192
                       Mean reward: 13.92
               Mean episode length: 180.15
    Episode_Reward/reaching_object: 0.6187
     Episode_Reward/lifting_object: 2.5988
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 7.01s
                      Time elapsed: 00:15:03
                               ETA: 01:14:23

################################################################################
                     [1m Learning iteration 337/2000 [0m                      

                       Computation: 13623 steps/s (collection: 7.041s, learning 0.175s)
             Mean action noise std: 2.12
          Mean value_function loss: 9.3810
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 56.3555
                       Mean reward: 19.17
               Mean episode length: 202.68
    Episode_Reward/reaching_object: 0.6048
     Episode_Reward/lifting_object: 2.2786
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 7.22s
                      Time elapsed: 00:15:11
                               ETA: 01:14:42

################################################################################
                     [1m Learning iteration 338/2000 [0m                      

                       Computation: 14011 steps/s (collection: 6.906s, learning 0.110s)
             Mean action noise std: 2.12
          Mean value_function loss: 31.4986
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 56.3741
                       Mean reward: 10.44
               Mean episode length: 190.74
    Episode_Reward/reaching_object: 0.6116
     Episode_Reward/lifting_object: 1.9626
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 7.02s
                      Time elapsed: 00:15:18
                               ETA: 01:15:01

################################################################################
                     [1m Learning iteration 339/2000 [0m                      

                       Computation: 13778 steps/s (collection: 6.983s, learning 0.151s)
             Mean action noise std: 2.12
          Mean value_function loss: 11.7093
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 56.4116
                       Mean reward: 15.06
               Mean episode length: 192.57
    Episode_Reward/reaching_object: 0.6107
     Episode_Reward/lifting_object: 1.8221
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 7.13s
                      Time elapsed: 00:15:25
                               ETA: 01:15:20

################################################################################
                     [1m Learning iteration 340/2000 [0m                      

                       Computation: 13528 steps/s (collection: 7.136s, learning 0.130s)
             Mean action noise std: 2.13
          Mean value_function loss: 17.8041
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 56.4423
                       Mean reward: 9.70
               Mean episode length: 186.13
    Episode_Reward/reaching_object: 0.6173
     Episode_Reward/lifting_object: 2.2631
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 7.27s
                      Time elapsed: 00:15:32
                               ETA: 01:15:39

################################################################################
                     [1m Learning iteration 341/2000 [0m                      

                       Computation: 12328 steps/s (collection: 7.885s, learning 0.089s)
             Mean action noise std: 2.13
          Mean value_function loss: 20.1791
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 56.4820
                       Mean reward: 13.63
               Mean episode length: 183.72
    Episode_Reward/reaching_object: 0.5969
     Episode_Reward/lifting_object: 1.9187
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 7.97s
                      Time elapsed: 00:15:40
                               ETA: 01:16:02

################################################################################
                     [1m Learning iteration 342/2000 [0m                      

                       Computation: 45734 steps/s (collection: 2.060s, learning 0.089s)
             Mean action noise std: 2.13
          Mean value_function loss: 12.8200
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.5187
                       Mean reward: 6.44
               Mean episode length: 195.71
    Episode_Reward/reaching_object: 0.6068
     Episode_Reward/lifting_object: 2.0122
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 2.15s
                      Time elapsed: 00:15:42
                               ETA: 01:15:56

################################################################################
                     [1m Learning iteration 343/2000 [0m                      

                       Computation: 43031 steps/s (collection: 2.139s, learning 0.145s)
             Mean action noise std: 2.14
          Mean value_function loss: 16.2708
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 56.5463
                       Mean reward: 16.36
               Mean episode length: 203.00
    Episode_Reward/reaching_object: 0.6258
     Episode_Reward/lifting_object: 2.3533
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 2.28s
                      Time elapsed: 00:15:44
                               ETA: 01:15:51

################################################################################
                     [1m Learning iteration 344/2000 [0m                      

                       Computation: 45773 steps/s (collection: 2.053s, learning 0.095s)
             Mean action noise std: 2.14
          Mean value_function loss: 13.7303
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 56.5850
                       Mean reward: 15.35
               Mean episode length: 197.27
    Episode_Reward/reaching_object: 0.6076
     Episode_Reward/lifting_object: 2.0248
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 2.15s
                      Time elapsed: 00:15:47
                               ETA: 01:15:46

################################################################################
                     [1m Learning iteration 345/2000 [0m                      

                       Computation: 47517 steps/s (collection: 1.980s, learning 0.089s)
             Mean action noise std: 2.14
          Mean value_function loss: 10.4169
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 56.6101
                       Mean reward: 14.73
               Mean episode length: 195.00
    Episode_Reward/reaching_object: 0.6191
     Episode_Reward/lifting_object: 2.6663
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 2.07s
                      Time elapsed: 00:15:49
                               ETA: 01:15:40

################################################################################
                     [1m Learning iteration 346/2000 [0m                      

                       Computation: 46784 steps/s (collection: 1.962s, learning 0.139s)
             Mean action noise std: 2.14
          Mean value_function loss: 9.8288
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 56.6278
                       Mean reward: 16.99
               Mean episode length: 188.82
    Episode_Reward/reaching_object: 0.6180
     Episode_Reward/lifting_object: 2.5622
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 2.10s
                      Time elapsed: 00:15:51
                               ETA: 01:15:34

################################################################################
                     [1m Learning iteration 347/2000 [0m                      

                       Computation: 44251 steps/s (collection: 2.122s, learning 0.100s)
             Mean action noise std: 2.14
          Mean value_function loss: 13.8877
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.6513
                       Mean reward: 14.89
               Mean episode length: 197.85
    Episode_Reward/reaching_object: 0.6114
     Episode_Reward/lifting_object: 2.4715
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 2.22s
                      Time elapsed: 00:15:53
                               ETA: 01:15:29

################################################################################
                     [1m Learning iteration 348/2000 [0m                      

                       Computation: 47755 steps/s (collection: 1.969s, learning 0.090s)
             Mean action noise std: 2.15
          Mean value_function loss: 18.0062
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 56.6715
                       Mean reward: 16.01
               Mean episode length: 196.82
    Episode_Reward/reaching_object: 0.6021
     Episode_Reward/lifting_object: 2.5114
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 2.06s
                      Time elapsed: 00:15:55
                               ETA: 01:15:23

################################################################################
                     [1m Learning iteration 349/2000 [0m                      

                       Computation: 43174 steps/s (collection: 2.115s, learning 0.162s)
             Mean action noise std: 2.15
          Mean value_function loss: 8.5803
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 56.7034
                       Mean reward: 19.07
               Mean episode length: 193.95
    Episode_Reward/reaching_object: 0.6037
     Episode_Reward/lifting_object: 2.9071
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 2.28s
                      Time elapsed: 00:15:57
                               ETA: 01:15:18

################################################################################
                     [1m Learning iteration 350/2000 [0m                      

                       Computation: 48608 steps/s (collection: 1.921s, learning 0.102s)
             Mean action noise std: 2.15
          Mean value_function loss: 12.5794
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 56.7302
                       Mean reward: 12.53
               Mean episode length: 194.07
    Episode_Reward/reaching_object: 0.6056
     Episode_Reward/lifting_object: 2.7152
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 2.02s
                      Time elapsed: 00:15:59
                               ETA: 01:15:12

################################################################################
                     [1m Learning iteration 351/2000 [0m                      

                       Computation: 47066 steps/s (collection: 1.979s, learning 0.110s)
             Mean action noise std: 2.15
          Mean value_function loss: 18.3891
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 56.7501
                       Mean reward: 9.72
               Mean episode length: 182.53
    Episode_Reward/reaching_object: 0.5770
     Episode_Reward/lifting_object: 2.2226
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 2.09s
                      Time elapsed: 00:16:01
                               ETA: 01:15:06

################################################################################
                     [1m Learning iteration 352/2000 [0m                      

                       Computation: 46643 steps/s (collection: 1.982s, learning 0.125s)
             Mean action noise std: 2.15
          Mean value_function loss: 14.7012
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.7687
                       Mean reward: 16.57
               Mean episode length: 192.78
    Episode_Reward/reaching_object: 0.5986
     Episode_Reward/lifting_object: 2.9523
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 2.11s
                      Time elapsed: 00:16:04
                               ETA: 01:15:00

################################################################################
                     [1m Learning iteration 353/2000 [0m                      

                       Computation: 47361 steps/s (collection: 1.983s, learning 0.093s)
             Mean action noise std: 2.15
          Mean value_function loss: 12.3798
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 56.7797
                       Mean reward: 18.34
               Mean episode length: 194.25
    Episode_Reward/reaching_object: 0.5926
     Episode_Reward/lifting_object: 2.6874
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 2.08s
                      Time elapsed: 00:16:06
                               ETA: 01:14:55

################################################################################
                     [1m Learning iteration 354/2000 [0m                      

                       Computation: 44148 steps/s (collection: 2.065s, learning 0.162s)
             Mean action noise std: 2.15
          Mean value_function loss: 17.0905
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 56.7900
                       Mean reward: 15.81
               Mean episode length: 182.57
    Episode_Reward/reaching_object: 0.5944
     Episode_Reward/lifting_object: 2.3724
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 2.23s
                      Time elapsed: 00:16:08
                               ETA: 01:14:49

################################################################################
                     [1m Learning iteration 355/2000 [0m                      

                       Computation: 47992 steps/s (collection: 1.944s, learning 0.105s)
             Mean action noise std: 2.16
          Mean value_function loss: 17.1304
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 56.8203
                       Mean reward: 11.69
               Mean episode length: 174.69
    Episode_Reward/reaching_object: 0.5772
     Episode_Reward/lifting_object: 2.2497
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 11.3750
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 2.05s
                      Time elapsed: 00:16:10
                               ETA: 01:14:44

################################################################################
                     [1m Learning iteration 356/2000 [0m                      

                       Computation: 47437 steps/s (collection: 1.985s, learning 0.087s)
             Mean action noise std: 2.16
          Mean value_function loss: 13.3223
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 56.8565
                       Mean reward: 14.83
               Mean episode length: 179.67
    Episode_Reward/reaching_object: 0.5817
     Episode_Reward/lifting_object: 2.4982
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 2.07s
                      Time elapsed: 00:16:12
                               ETA: 01:14:38

################################################################################
                     [1m Learning iteration 357/2000 [0m                      

                       Computation: 46312 steps/s (collection: 2.030s, learning 0.093s)
             Mean action noise std: 2.16
          Mean value_function loss: 13.0634
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 56.8702
                       Mean reward: 10.72
               Mean episode length: 194.56
    Episode_Reward/reaching_object: 0.5790
     Episode_Reward/lifting_object: 2.2283
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 2.12s
                      Time elapsed: 00:16:14
                               ETA: 01:14:32

################################################################################
                     [1m Learning iteration 358/2000 [0m                      

                       Computation: 46492 steps/s (collection: 2.020s, learning 0.094s)
             Mean action noise std: 2.16
          Mean value_function loss: 9.1841
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 56.8791
                       Mean reward: 21.36
               Mean episode length: 181.76
    Episode_Reward/reaching_object: 0.5959
     Episode_Reward/lifting_object: 3.3036
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 2.11s
                      Time elapsed: 00:16:16
                               ETA: 01:14:27

################################################################################
                     [1m Learning iteration 359/2000 [0m                      

                       Computation: 46291 steps/s (collection: 2.027s, learning 0.097s)
             Mean action noise std: 2.16
          Mean value_function loss: 21.2734
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 56.8902
                       Mean reward: 12.18
               Mean episode length: 184.31
    Episode_Reward/reaching_object: 0.5878
     Episode_Reward/lifting_object: 3.0221
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 2.12s
                      Time elapsed: 00:16:18
                               ETA: 01:14:21

################################################################################
                     [1m Learning iteration 360/2000 [0m                      

                       Computation: 46577 steps/s (collection: 2.024s, learning 0.087s)
             Mean action noise std: 2.17
          Mean value_function loss: 19.8114
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 56.9148
                       Mean reward: 19.13
               Mean episode length: 182.27
    Episode_Reward/reaching_object: 0.5749
     Episode_Reward/lifting_object: 3.0307
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 2.11s
                      Time elapsed: 00:16:20
                               ETA: 01:14:16

################################################################################
                     [1m Learning iteration 361/2000 [0m                      

                       Computation: 47305 steps/s (collection: 1.969s, learning 0.109s)
             Mean action noise std: 2.17
          Mean value_function loss: 21.7831
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 56.9526
                       Mean reward: 17.19
               Mean episode length: 185.08
    Episode_Reward/reaching_object: 0.5697
     Episode_Reward/lifting_object: 2.7551
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 2.08s
                      Time elapsed: 00:16:23
                               ETA: 01:14:10

################################################################################
                     [1m Learning iteration 362/2000 [0m                      

                       Computation: 47458 steps/s (collection: 1.952s, learning 0.120s)
             Mean action noise std: 2.17
          Mean value_function loss: 26.2653
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 56.9809
                       Mean reward: 14.87
               Mean episode length: 157.94
    Episode_Reward/reaching_object: 0.5330
     Episode_Reward/lifting_object: 2.6473
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 16.7500
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 2.07s
                      Time elapsed: 00:16:25
                               ETA: 01:14:05

################################################################################
                     [1m Learning iteration 363/2000 [0m                      

                       Computation: 47252 steps/s (collection: 1.983s, learning 0.098s)
             Mean action noise std: 2.17
          Mean value_function loss: 28.7050
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 56.9980
                       Mean reward: 16.75
               Mean episode length: 166.92
    Episode_Reward/reaching_object: 0.5343
     Episode_Reward/lifting_object: 2.9173
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 2.08s
                      Time elapsed: 00:16:27
                               ETA: 01:13:59

################################################################################
                     [1m Learning iteration 364/2000 [0m                      

                       Computation: 47048 steps/s (collection: 1.984s, learning 0.106s)
             Mean action noise std: 2.18
          Mean value_function loss: 19.3834
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 57.0288
                       Mean reward: 22.55
               Mean episode length: 174.20
    Episode_Reward/reaching_object: 0.5586
     Episode_Reward/lifting_object: 3.3578
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 14.2917
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 2.09s
                      Time elapsed: 00:16:29
                               ETA: 01:13:54

################################################################################
                     [1m Learning iteration 365/2000 [0m                      

                       Computation: 45795 steps/s (collection: 2.050s, learning 0.097s)
             Mean action noise std: 2.18
          Mean value_function loss: 26.6520
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.0570
                       Mean reward: 13.92
               Mean episode length: 160.59
    Episode_Reward/reaching_object: 0.5590
     Episode_Reward/lifting_object: 2.9028
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 2.15s
                      Time elapsed: 00:16:31
                               ETA: 01:13:48

################################################################################
                     [1m Learning iteration 366/2000 [0m                      

                       Computation: 43988 steps/s (collection: 2.146s, learning 0.089s)
             Mean action noise std: 2.18
          Mean value_function loss: 22.2325
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 57.0784
                       Mean reward: 16.65
               Mean episode length: 168.25
    Episode_Reward/reaching_object: 0.5449
     Episode_Reward/lifting_object: 2.8103
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 2.23s
                      Time elapsed: 00:16:33
                               ETA: 01:13:44

################################################################################
                     [1m Learning iteration 367/2000 [0m                      

                       Computation: 47193 steps/s (collection: 1.970s, learning 0.113s)
             Mean action noise std: 2.18
          Mean value_function loss: 18.3896
               Mean surrogate loss: 0.0103
                 Mean entropy loss: 57.0859
                       Mean reward: 15.31
               Mean episode length: 173.47
    Episode_Reward/reaching_object: 0.5622
     Episode_Reward/lifting_object: 3.0968
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 2.08s
                      Time elapsed: 00:16:35
                               ETA: 01:13:38

################################################################################
                     [1m Learning iteration 368/2000 [0m                      

                       Computation: 48162 steps/s (collection: 1.946s, learning 0.095s)
             Mean action noise std: 2.18
          Mean value_function loss: 11.7547
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 57.0878
                       Mean reward: 16.31
               Mean episode length: 192.78
    Episode_Reward/reaching_object: 0.5823
     Episode_Reward/lifting_object: 2.4894
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 2.04s
                      Time elapsed: 00:16:37
                               ETA: 01:13:32

################################################################################
                     [1m Learning iteration 369/2000 [0m                      

                       Computation: 48085 steps/s (collection: 1.951s, learning 0.093s)
             Mean action noise std: 2.18
          Mean value_function loss: 12.5753
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 57.0938
                       Mean reward: 17.85
               Mean episode length: 191.27
    Episode_Reward/reaching_object: 0.6171
     Episode_Reward/lifting_object: 3.5353
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 9.9583
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 2.04s
                      Time elapsed: 00:16:39
                               ETA: 01:13:27

################################################################################
                     [1m Learning iteration 370/2000 [0m                      

                       Computation: 46423 steps/s (collection: 2.024s, learning 0.093s)
             Mean action noise std: 2.18
          Mean value_function loss: 15.9403
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 57.1107
                       Mean reward: 19.07
               Mean episode length: 198.22
    Episode_Reward/reaching_object: 0.6261
     Episode_Reward/lifting_object: 3.6311
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 2.12s
                      Time elapsed: 00:16:41
                               ETA: 01:13:22

################################################################################
                     [1m Learning iteration 371/2000 [0m                      

                       Computation: 44622 steps/s (collection: 2.019s, learning 0.184s)
             Mean action noise std: 2.18
          Mean value_function loss: 15.7227
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 57.1284
                       Mean reward: 20.28
               Mean episode length: 195.25
    Episode_Reward/reaching_object: 0.6234
     Episode_Reward/lifting_object: 3.4451
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 2.20s
                      Time elapsed: 00:16:44
                               ETA: 01:13:17

################################################################################
                     [1m Learning iteration 372/2000 [0m                      

                       Computation: 47199 steps/s (collection: 1.989s, learning 0.094s)
             Mean action noise std: 2.18
          Mean value_function loss: 16.0142
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.1457
                       Mean reward: 16.35
               Mean episode length: 195.18
    Episode_Reward/reaching_object: 0.6151
     Episode_Reward/lifting_object: 3.3883
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 2.08s
                      Time elapsed: 00:16:46
                               ETA: 01:13:11

################################################################################
                     [1m Learning iteration 373/2000 [0m                      

                       Computation: 45897 steps/s (collection: 2.037s, learning 0.105s)
             Mean action noise std: 2.19
          Mean value_function loss: 37.5781
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 57.1576
                       Mean reward: 18.82
               Mean episode length: 181.67
    Episode_Reward/reaching_object: 0.6200
     Episode_Reward/lifting_object: 3.1938
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 2.14s
                      Time elapsed: 00:16:48
                               ETA: 01:13:06

################################################################################
                     [1m Learning iteration 374/2000 [0m                      

                       Computation: 44068 steps/s (collection: 2.055s, learning 0.176s)
             Mean action noise std: 2.19
          Mean value_function loss: 22.3265
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 57.1811
                       Mean reward: 20.02
               Mean episode length: 181.30
    Episode_Reward/reaching_object: 0.6111
     Episode_Reward/lifting_object: 3.4204
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 2.23s
                      Time elapsed: 00:16:50
                               ETA: 01:13:01

################################################################################
                     [1m Learning iteration 375/2000 [0m                      

                       Computation: 44034 steps/s (collection: 2.132s, learning 0.100s)
             Mean action noise std: 2.19
          Mean value_function loss: 24.8039
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 57.2103
                       Mean reward: 21.90
               Mean episode length: 184.13
    Episode_Reward/reaching_object: 0.6064
     Episode_Reward/lifting_object: 3.1656
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 2.23s
                      Time elapsed: 00:16:52
                               ETA: 01:12:57

################################################################################
                     [1m Learning iteration 376/2000 [0m                      

                       Computation: 43223 steps/s (collection: 2.184s, learning 0.091s)
             Mean action noise std: 2.19
          Mean value_function loss: 32.3517
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 57.2367
                       Mean reward: 24.82
               Mean episode length: 181.37
    Episode_Reward/reaching_object: 0.5985
     Episode_Reward/lifting_object: 3.8973
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 2.27s
                      Time elapsed: 00:16:55
                               ETA: 01:12:52

################################################################################
                     [1m Learning iteration 377/2000 [0m                      

                       Computation: 39499 steps/s (collection: 2.291s, learning 0.198s)
             Mean action noise std: 2.20
          Mean value_function loss: 37.8575
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 57.2633
                       Mean reward: 25.27
               Mean episode length: 186.61
    Episode_Reward/reaching_object: 0.5908
     Episode_Reward/lifting_object: 3.9070
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 2.49s
                      Time elapsed: 00:16:57
                               ETA: 01:12:49

################################################################################
                     [1m Learning iteration 378/2000 [0m                      

                       Computation: 45312 steps/s (collection: 2.076s, learning 0.093s)
             Mean action noise std: 2.20
          Mean value_function loss: 23.4410
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 57.2856
                       Mean reward: 17.10
               Mean episode length: 172.46
    Episode_Reward/reaching_object: 0.5903
     Episode_Reward/lifting_object: 3.7253
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 12.6667
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 2.17s
                      Time elapsed: 00:16:59
                               ETA: 01:12:44

################################################################################
                     [1m Learning iteration 379/2000 [0m                      

                       Computation: 44162 steps/s (collection: 2.128s, learning 0.098s)
             Mean action noise std: 2.20
          Mean value_function loss: 18.0752
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.2915
                       Mean reward: 24.11
               Mean episode length: 187.22
    Episode_Reward/reaching_object: 0.6052
     Episode_Reward/lifting_object: 4.1800
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 2.23s
                      Time elapsed: 00:17:01
                               ETA: 01:12:39

################################################################################
                     [1m Learning iteration 380/2000 [0m                      

                       Computation: 43335 steps/s (collection: 2.170s, learning 0.098s)
             Mean action noise std: 2.20
          Mean value_function loss: 25.9402
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 57.3051
                       Mean reward: 23.17
               Mean episode length: 186.77
    Episode_Reward/reaching_object: 0.6200
     Episode_Reward/lifting_object: 4.2916
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 2.27s
                      Time elapsed: 00:17:04
                               ETA: 01:12:35

################################################################################
                     [1m Learning iteration 381/2000 [0m                      

                       Computation: 45982 steps/s (collection: 2.047s, learning 0.090s)
             Mean action noise std: 2.20
          Mean value_function loss: 28.8303
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 57.3275
                       Mean reward: 13.71
               Mean episode length: 174.21
    Episode_Reward/reaching_object: 0.5903
     Episode_Reward/lifting_object: 3.5963
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 2.14s
                      Time elapsed: 00:17:06
                               ETA: 01:12:30

################################################################################
                     [1m Learning iteration 382/2000 [0m                      

                       Computation: 43747 steps/s (collection: 2.072s, learning 0.175s)
             Mean action noise std: 2.20
          Mean value_function loss: 21.2370
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 57.3547
                       Mean reward: 24.24
               Mean episode length: 168.31
    Episode_Reward/reaching_object: 0.5991
     Episode_Reward/lifting_object: 3.9228
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 2.25s
                      Time elapsed: 00:17:08
                               ETA: 01:12:25

################################################################################
                     [1m Learning iteration 383/2000 [0m                      

                       Computation: 45651 steps/s (collection: 2.040s, learning 0.113s)
             Mean action noise std: 2.21
          Mean value_function loss: 26.3005
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 57.3817
                       Mean reward: 24.21
               Mean episode length: 185.49
    Episode_Reward/reaching_object: 0.6186
     Episode_Reward/lifting_object: 3.6830
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 2.15s
                      Time elapsed: 00:17:10
                               ETA: 01:12:20

################################################################################
                     [1m Learning iteration 384/2000 [0m                      

                       Computation: 46540 steps/s (collection: 2.024s, learning 0.089s)
             Mean action noise std: 2.21
          Mean value_function loss: 18.1400
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 57.4008
                       Mean reward: 19.89
               Mean episode length: 176.23
    Episode_Reward/reaching_object: 0.6156
     Episode_Reward/lifting_object: 3.8183
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 2.11s
                      Time elapsed: 00:17:12
                               ETA: 01:12:15

################################################################################
                     [1m Learning iteration 385/2000 [0m                      

                       Computation: 47127 steps/s (collection: 1.987s, learning 0.099s)
             Mean action noise std: 2.21
          Mean value_function loss: 17.4187
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.4223
                       Mean reward: 22.00
               Mean episode length: 189.13
    Episode_Reward/reaching_object: 0.6202
     Episode_Reward/lifting_object: 3.9745
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 2.09s
                      Time elapsed: 00:17:15
                               ETA: 01:12:10

################################################################################
                     [1m Learning iteration 386/2000 [0m                      

                       Computation: 46809 steps/s (collection: 1.994s, learning 0.107s)
             Mean action noise std: 2.21
          Mean value_function loss: 22.2687
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 57.4352
                       Mean reward: 30.27
               Mean episode length: 197.81
    Episode_Reward/reaching_object: 0.6268
     Episode_Reward/lifting_object: 4.4710
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 2.10s
                      Time elapsed: 00:17:17
                               ETA: 01:12:05

################################################################################
                     [1m Learning iteration 387/2000 [0m                      

                       Computation: 45297 steps/s (collection: 2.082s, learning 0.089s)
             Mean action noise std: 2.21
          Mean value_function loss: 25.2479
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 57.4395
                       Mean reward: 20.44
               Mean episode length: 188.89
    Episode_Reward/reaching_object: 0.6042
     Episode_Reward/lifting_object: 4.4204
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 2.17s
                      Time elapsed: 00:17:19
                               ETA: 01:12:00

################################################################################
                     [1m Learning iteration 388/2000 [0m                      

                       Computation: 45764 steps/s (collection: 2.045s, learning 0.104s)
             Mean action noise std: 2.21
          Mean value_function loss: 23.9020
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 57.4468
                       Mean reward: 21.23
               Mean episode length: 179.81
    Episode_Reward/reaching_object: 0.6090
     Episode_Reward/lifting_object: 4.4394
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 2.15s
                      Time elapsed: 00:17:21
                               ETA: 01:11:55

################################################################################
                     [1m Learning iteration 389/2000 [0m                      

                       Computation: 47659 steps/s (collection: 1.971s, learning 0.092s)
             Mean action noise std: 2.21
          Mean value_function loss: 20.3473
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 57.4607
                       Mean reward: 22.81
               Mean episode length: 174.04
    Episode_Reward/reaching_object: 0.5893
     Episode_Reward/lifting_object: 3.6358
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 2.06s
                      Time elapsed: 00:17:23
                               ETA: 01:11:50

################################################################################
                     [1m Learning iteration 390/2000 [0m                      

                       Computation: 45430 steps/s (collection: 2.027s, learning 0.136s)
             Mean action noise std: 2.21
          Mean value_function loss: 29.4924
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 57.4802
                       Mean reward: 28.25
               Mean episode length: 172.55
    Episode_Reward/reaching_object: 0.6075
     Episode_Reward/lifting_object: 4.3211
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 13.7917
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 2.16s
                      Time elapsed: 00:17:25
                               ETA: 01:11:45

################################################################################
                     [1m Learning iteration 391/2000 [0m                      

                       Computation: 44627 steps/s (collection: 2.075s, learning 0.128s)
             Mean action noise std: 2.22
          Mean value_function loss: 29.2603
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 57.4996
                       Mean reward: 22.65
               Mean episode length: 168.82
    Episode_Reward/reaching_object: 0.6016
     Episode_Reward/lifting_object: 4.5286
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 13.7083
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 2.20s
                      Time elapsed: 00:17:27
                               ETA: 01:11:40

################################################################################
                     [1m Learning iteration 392/2000 [0m                      

                       Computation: 46176 steps/s (collection: 2.034s, learning 0.095s)
             Mean action noise std: 2.22
          Mean value_function loss: 54.0319
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 57.5163
                       Mean reward: 26.56
               Mean episode length: 184.60
    Episode_Reward/reaching_object: 0.6385
     Episode_Reward/lifting_object: 4.4546
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 2.13s
                      Time elapsed: 00:17:29
                               ETA: 01:11:36

################################################################################
                     [1m Learning iteration 393/2000 [0m                      

                       Computation: 45653 steps/s (collection: 2.041s, learning 0.113s)
             Mean action noise std: 2.22
          Mean value_function loss: 32.5991
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 57.5276
                       Mean reward: 28.95
               Mean episode length: 179.36
    Episode_Reward/reaching_object: 0.6247
     Episode_Reward/lifting_object: 4.9308
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 2.15s
                      Time elapsed: 00:17:32
                               ETA: 01:11:31

################################################################################
                     [1m Learning iteration 394/2000 [0m                      

                       Computation: 44348 steps/s (collection: 2.101s, learning 0.116s)
             Mean action noise std: 2.22
          Mean value_function loss: 35.9621
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 57.5484
                       Mean reward: 20.94
               Mean episode length: 178.59
    Episode_Reward/reaching_object: 0.6246
     Episode_Reward/lifting_object: 4.7548
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 2.22s
                      Time elapsed: 00:17:34
                               ETA: 01:11:26

################################################################################
                     [1m Learning iteration 395/2000 [0m                      

                       Computation: 44782 steps/s (collection: 2.081s, learning 0.114s)
             Mean action noise std: 2.22
          Mean value_function loss: 43.3875
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 57.5692
                       Mean reward: 23.73
               Mean episode length: 170.40
    Episode_Reward/reaching_object: 0.6172
     Episode_Reward/lifting_object: 4.7211
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.0833
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 2.20s
                      Time elapsed: 00:17:36
                               ETA: 01:11:22

################################################################################
                     [1m Learning iteration 396/2000 [0m                      

                       Computation: 45094 steps/s (collection: 2.055s, learning 0.125s)
             Mean action noise std: 2.22
          Mean value_function loss: 30.0429
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 57.5856
                       Mean reward: 28.48
               Mean episode length: 165.01
    Episode_Reward/reaching_object: 0.6036
     Episode_Reward/lifting_object: 4.2747
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 2.18s
                      Time elapsed: 00:17:38
                               ETA: 01:11:17

################################################################################
                     [1m Learning iteration 397/2000 [0m                      

                       Computation: 45273 steps/s (collection: 2.082s, learning 0.089s)
             Mean action noise std: 2.22
          Mean value_function loss: 36.2493
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 57.6053
                       Mean reward: 29.83
               Mean episode length: 164.33
    Episode_Reward/reaching_object: 0.5943
     Episode_Reward/lifting_object: 4.2533
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 2.17s
                      Time elapsed: 00:17:40
                               ETA: 01:11:12

################################################################################
                     [1m Learning iteration 398/2000 [0m                      

                       Computation: 44428 steps/s (collection: 2.110s, learning 0.103s)
             Mean action noise std: 2.23
          Mean value_function loss: 40.1310
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 57.6278
                       Mean reward: 25.82
               Mean episode length: 172.04
    Episode_Reward/reaching_object: 0.5973
     Episode_Reward/lifting_object: 4.8154
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 2.21s
                      Time elapsed: 00:17:43
                               ETA: 01:11:08

################################################################################
                     [1m Learning iteration 399/2000 [0m                      

                       Computation: 44797 steps/s (collection: 2.101s, learning 0.093s)
             Mean action noise std: 2.23
          Mean value_function loss: 55.5313
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 57.6433
                       Mean reward: 31.09
               Mean episode length: 181.82
    Episode_Reward/reaching_object: 0.6354
     Episode_Reward/lifting_object: 5.3617
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 2.19s
                      Time elapsed: 00:17:45
                               ETA: 01:11:03

################################################################################
                     [1m Learning iteration 400/2000 [0m                      

                       Computation: 45172 steps/s (collection: 2.076s, learning 0.101s)
             Mean action noise std: 2.23
          Mean value_function loss: 41.5791
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 57.6665
                       Mean reward: 28.07
               Mean episode length: 182.53
    Episode_Reward/reaching_object: 0.6416
     Episode_Reward/lifting_object: 5.6997
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 2.18s
                      Time elapsed: 00:17:47
                               ETA: 01:10:59

################################################################################
                     [1m Learning iteration 401/2000 [0m                      

                       Computation: 44131 steps/s (collection: 2.113s, learning 0.115s)
             Mean action noise std: 2.23
          Mean value_function loss: 61.0491
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 57.6863
                       Mean reward: 28.10
               Mean episode length: 174.78
    Episode_Reward/reaching_object: 0.6229
     Episode_Reward/lifting_object: 5.6248
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 2.23s
                      Time elapsed: 00:17:49
                               ETA: 01:10:54

################################################################################
                     [1m Learning iteration 402/2000 [0m                      

                       Computation: 44813 steps/s (collection: 2.095s, learning 0.099s)
             Mean action noise std: 2.23
          Mean value_function loss: 55.1832
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 57.6967
                       Mean reward: 36.19
               Mean episode length: 171.67
    Episode_Reward/reaching_object: 0.6188
     Episode_Reward/lifting_object: 5.4426
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 2.19s
                      Time elapsed: 00:17:51
                               ETA: 01:10:50

################################################################################
                     [1m Learning iteration 403/2000 [0m                      

                       Computation: 44259 steps/s (collection: 2.094s, learning 0.128s)
             Mean action noise std: 2.23
          Mean value_function loss: 41.5371
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 57.7179
                       Mean reward: 28.61
               Mean episode length: 175.30
    Episode_Reward/reaching_object: 0.6174
     Episode_Reward/lifting_object: 5.0429
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 16.2500
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 2.22s
                      Time elapsed: 00:17:54
                               ETA: 01:10:45

################################################################################
                     [1m Learning iteration 404/2000 [0m                      

                       Computation: 42657 steps/s (collection: 2.149s, learning 0.156s)
             Mean action noise std: 2.24
          Mean value_function loss: 35.9550
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 57.7356
                       Mean reward: 37.63
               Mean episode length: 174.51
    Episode_Reward/reaching_object: 0.5975
     Episode_Reward/lifting_object: 5.4738
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 15.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 2.30s
                      Time elapsed: 00:17:56
                               ETA: 01:10:41

################################################################################
                     [1m Learning iteration 405/2000 [0m                      

                       Computation: 41102 steps/s (collection: 2.196s, learning 0.196s)
             Mean action noise std: 2.24
          Mean value_function loss: 35.8574
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.7414
                       Mean reward: 33.13
               Mean episode length: 173.63
    Episode_Reward/reaching_object: 0.6114
     Episode_Reward/lifting_object: 4.7820
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 2.39s
                      Time elapsed: 00:17:58
                               ETA: 01:10:38

################################################################################
                     [1m Learning iteration 406/2000 [0m                      

                       Computation: 45064 steps/s (collection: 2.081s, learning 0.101s)
             Mean action noise std: 2.24
          Mean value_function loss: 56.6184
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 57.7554
                       Mean reward: 28.10
               Mean episode length: 170.10
    Episode_Reward/reaching_object: 0.5967
     Episode_Reward/lifting_object: 5.5407
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 2.18s
                      Time elapsed: 00:18:00
                               ETA: 01:10:33

################################################################################
                     [1m Learning iteration 407/2000 [0m                      

                       Computation: 47447 steps/s (collection: 1.972s, learning 0.100s)
             Mean action noise std: 2.24
          Mean value_function loss: 31.9886
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 57.7784
                       Mean reward: 29.50
               Mean episode length: 166.88
    Episode_Reward/reaching_object: 0.5878
     Episode_Reward/lifting_object: 5.2173
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 15.2917
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 2.07s
                      Time elapsed: 00:18:03
                               ETA: 01:10:28

################################################################################
                     [1m Learning iteration 408/2000 [0m                      

                       Computation: 46576 steps/s (collection: 2.017s, learning 0.094s)
             Mean action noise std: 2.24
          Mean value_function loss: 57.9566
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 57.7967
                       Mean reward: 36.00
               Mean episode length: 168.96
    Episode_Reward/reaching_object: 0.5973
     Episode_Reward/lifting_object: 6.1924
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 2.11s
                      Time elapsed: 00:18:05
                               ETA: 01:10:23

################################################################################
                     [1m Learning iteration 409/2000 [0m                      

                       Computation: 46947 steps/s (collection: 1.998s, learning 0.096s)
             Mean action noise std: 2.24
          Mean value_function loss: 60.1921
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 57.8138
                       Mean reward: 36.30
               Mean episode length: 168.27
    Episode_Reward/reaching_object: 0.5819
     Episode_Reward/lifting_object: 5.3157
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 2.09s
                      Time elapsed: 00:18:07
                               ETA: 01:10:19

################################################################################
                     [1m Learning iteration 410/2000 [0m                      

                       Computation: 45163 steps/s (collection: 2.077s, learning 0.100s)
             Mean action noise std: 2.24
          Mean value_function loss: 40.2663
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.8307
                       Mean reward: 23.66
               Mean episode length: 155.58
    Episode_Reward/reaching_object: 0.5841
     Episode_Reward/lifting_object: 5.2258
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 15.7083
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 2.18s
                      Time elapsed: 00:18:09
                               ETA: 01:10:14

################################################################################
                     [1m Learning iteration 411/2000 [0m                      

                       Computation: 45024 steps/s (collection: 2.063s, learning 0.120s)
             Mean action noise std: 2.25
          Mean value_function loss: 54.1348
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 57.8426
                       Mean reward: 26.98
               Mean episode length: 157.29
    Episode_Reward/reaching_object: 0.5655
     Episode_Reward/lifting_object: 5.2813
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 16.7917
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 2.18s
                      Time elapsed: 00:18:11
                               ETA: 01:10:10

################################################################################
                     [1m Learning iteration 412/2000 [0m                      

                       Computation: 45651 steps/s (collection: 2.014s, learning 0.140s)
             Mean action noise std: 2.25
          Mean value_function loss: 51.4500
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.8610
                       Mean reward: 34.51
               Mean episode length: 169.90
    Episode_Reward/reaching_object: 0.5964
     Episode_Reward/lifting_object: 5.8946
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 2.15s
                      Time elapsed: 00:18:13
                               ETA: 01:10:05

################################################################################
                     [1m Learning iteration 413/2000 [0m                      

                       Computation: 45747 steps/s (collection: 2.045s, learning 0.104s)
             Mean action noise std: 2.25
          Mean value_function loss: 56.7653
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 57.8817
                       Mean reward: 37.16
               Mean episode length: 169.99
    Episode_Reward/reaching_object: 0.5878
     Episode_Reward/lifting_object: 5.8665
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 2.15s
                      Time elapsed: 00:18:15
                               ETA: 01:10:01

################################################################################
                     [1m Learning iteration 414/2000 [0m                      

                       Computation: 45998 steps/s (collection: 2.048s, learning 0.089s)
             Mean action noise std: 2.25
          Mean value_function loss: 70.6569
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 57.9035
                       Mean reward: 36.99
               Mean episode length: 171.92
    Episode_Reward/reaching_object: 0.5841
     Episode_Reward/lifting_object: 5.6319
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 16.2500
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 2.14s
                      Time elapsed: 00:18:18
                               ETA: 01:09:56

################################################################################
                     [1m Learning iteration 415/2000 [0m                      

                       Computation: 40540 steps/s (collection: 2.318s, learning 0.107s)
             Mean action noise std: 2.25
          Mean value_function loss: 79.5278
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.9310
                       Mean reward: 34.69
               Mean episode length: 160.87
    Episode_Reward/reaching_object: 0.5978
     Episode_Reward/lifting_object: 6.1130
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 18.8750
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 2.42s
                      Time elapsed: 00:18:20
                               ETA: 01:09:52

################################################################################
                     [1m Learning iteration 416/2000 [0m                      

                       Computation: 43831 steps/s (collection: 2.114s, learning 0.128s)
             Mean action noise std: 2.26
          Mean value_function loss: 51.0018
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 57.9519
                       Mean reward: 17.84
               Mean episode length: 162.61
    Episode_Reward/reaching_object: 0.5840
     Episode_Reward/lifting_object: 6.0570
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 16.5833
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 2.24s
                      Time elapsed: 00:18:22
                               ETA: 01:09:48

################################################################################
                     [1m Learning iteration 417/2000 [0m                      

                       Computation: 45780 steps/s (collection: 2.023s, learning 0.124s)
             Mean action noise std: 2.26
          Mean value_function loss: 52.8095
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 57.9734
                       Mean reward: 41.34
               Mean episode length: 154.88
    Episode_Reward/reaching_object: 0.5633
     Episode_Reward/lifting_object: 6.6541
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 18.4167
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 2.15s
                      Time elapsed: 00:18:24
                               ETA: 01:09:44

################################################################################
                     [1m Learning iteration 418/2000 [0m                      

                       Computation: 45054 steps/s (collection: 2.068s, learning 0.114s)
             Mean action noise std: 2.26
          Mean value_function loss: 68.2282
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.9957
                       Mean reward: 34.95
               Mean episode length: 156.62
    Episode_Reward/reaching_object: 0.5774
     Episode_Reward/lifting_object: 6.3137
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 19.2917
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 2.18s
                      Time elapsed: 00:18:27
                               ETA: 01:09:39

################################################################################
                     [1m Learning iteration 419/2000 [0m                      

                       Computation: 45516 steps/s (collection: 2.052s, learning 0.107s)
             Mean action noise std: 2.26
          Mean value_function loss: 57.3337
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.0111
                       Mean reward: 27.85
               Mean episode length: 134.90
    Episode_Reward/reaching_object: 0.5629
     Episode_Reward/lifting_object: 5.9349
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 20.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 2.16s
                      Time elapsed: 00:18:29
                               ETA: 01:09:35

################################################################################
                     [1m Learning iteration 420/2000 [0m                      

                       Computation: 44957 steps/s (collection: 2.079s, learning 0.108s)
             Mean action noise std: 2.26
          Mean value_function loss: 52.8464
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 58.0293
                       Mean reward: 25.48
               Mean episode length: 164.01
    Episode_Reward/reaching_object: 0.5643
     Episode_Reward/lifting_object: 6.0989
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 20.5833
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 2.19s
                      Time elapsed: 00:18:31
                               ETA: 01:09:31

################################################################################
                     [1m Learning iteration 421/2000 [0m                      

                       Computation: 44275 steps/s (collection: 2.089s, learning 0.132s)
             Mean action noise std: 2.26
          Mean value_function loss: 60.8333
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.0466
                       Mean reward: 39.91
               Mean episode length: 154.82
    Episode_Reward/reaching_object: 0.5832
     Episode_Reward/lifting_object: 6.7118
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 18.6667
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 2.22s
                      Time elapsed: 00:18:33
                               ETA: 01:09:26

################################################################################
                     [1m Learning iteration 422/2000 [0m                      

                       Computation: 43421 steps/s (collection: 2.149s, learning 0.115s)
             Mean action noise std: 2.26
          Mean value_function loss: 115.3736
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.0570
                       Mean reward: 39.41
               Mean episode length: 160.12
    Episode_Reward/reaching_object: 0.5490
     Episode_Reward/lifting_object: 6.6807
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 18.2083
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 2.26s
                      Time elapsed: 00:18:35
                               ETA: 01:09:22

################################################################################
                     [1m Learning iteration 423/2000 [0m                      

                       Computation: 44353 steps/s (collection: 2.109s, learning 0.108s)
             Mean action noise std: 2.27
          Mean value_function loss: 64.7494
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 58.0710
                       Mean reward: 38.14
               Mean episode length: 158.01
    Episode_Reward/reaching_object: 0.5578
     Episode_Reward/lifting_object: 5.8127
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 18.7083
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 2.22s
                      Time elapsed: 00:18:38
                               ETA: 01:09:18

################################################################################
                     [1m Learning iteration 424/2000 [0m                      

                       Computation: 45255 steps/s (collection: 2.080s, learning 0.092s)
             Mean action noise std: 2.27
          Mean value_function loss: 51.4691
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.0944
                       Mean reward: 38.04
               Mean episode length: 167.39
    Episode_Reward/reaching_object: 0.5760
     Episode_Reward/lifting_object: 7.0495
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 17.4583
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 2.17s
                      Time elapsed: 00:18:40
                               ETA: 01:09:14

################################################################################
                     [1m Learning iteration 425/2000 [0m                      

                       Computation: 44530 steps/s (collection: 2.091s, learning 0.117s)
             Mean action noise std: 2.27
          Mean value_function loss: 77.6038
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 58.1092
                       Mean reward: 37.47
               Mean episode length: 152.55
    Episode_Reward/reaching_object: 0.5834
     Episode_Reward/lifting_object: 6.3523
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 17.5000
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 2.21s
                      Time elapsed: 00:18:42
                               ETA: 01:09:10

################################################################################
                     [1m Learning iteration 426/2000 [0m                      

                       Computation: 45418 steps/s (collection: 2.077s, learning 0.088s)
             Mean action noise std: 2.27
          Mean value_function loss: 58.4383
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 58.1198
                       Mean reward: 44.90
               Mean episode length: 155.63
    Episode_Reward/reaching_object: 0.5860
     Episode_Reward/lifting_object: 7.8993
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 19.2917
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 2.16s
                      Time elapsed: 00:18:44
                               ETA: 01:09:05

################################################################################
                     [1m Learning iteration 427/2000 [0m                      

                       Computation: 44295 steps/s (collection: 2.099s, learning 0.121s)
             Mean action noise std: 2.27
          Mean value_function loss: 59.0813
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 58.1342
                       Mean reward: 40.72
               Mean episode length: 150.71
    Episode_Reward/reaching_object: 0.5759
     Episode_Reward/lifting_object: 6.9850
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 20.2917
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 2.22s
                      Time elapsed: 00:18:46
                               ETA: 01:09:01

################################################################################
                     [1m Learning iteration 428/2000 [0m                      

                       Computation: 43773 steps/s (collection: 2.153s, learning 0.093s)
             Mean action noise std: 2.27
          Mean value_function loss: 69.9569
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 58.1487
                       Mean reward: 33.96
               Mean episode length: 164.80
    Episode_Reward/reaching_object: 0.5530
     Episode_Reward/lifting_object: 6.8822
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 17.5833
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 2.25s
                      Time elapsed: 00:18:49
                               ETA: 01:08:57

################################################################################
                     [1m Learning iteration 429/2000 [0m                      

                       Computation: 45552 steps/s (collection: 2.050s, learning 0.108s)
             Mean action noise std: 2.27
          Mean value_function loss: 63.9696
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 58.1583
                       Mean reward: 39.84
               Mean episode length: 147.50
    Episode_Reward/reaching_object: 0.5464
     Episode_Reward/lifting_object: 7.2736
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 19.7917
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 2.16s
                      Time elapsed: 00:18:51
                               ETA: 01:08:53

################################################################################
                     [1m Learning iteration 430/2000 [0m                      

                       Computation: 45021 steps/s (collection: 2.078s, learning 0.105s)
             Mean action noise std: 2.27
          Mean value_function loss: 83.1091
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 58.1674
                       Mean reward: 45.19
               Mean episode length: 151.18
    Episode_Reward/reaching_object: 0.5764
     Episode_Reward/lifting_object: 7.6680
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 19.4583
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 2.18s
                      Time elapsed: 00:18:53
                               ETA: 01:08:48

################################################################################
                     [1m Learning iteration 431/2000 [0m                      

                       Computation: 44245 steps/s (collection: 2.099s, learning 0.123s)
             Mean action noise std: 2.27
          Mean value_function loss: 64.4283
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.1803
                       Mean reward: 39.60
               Mean episode length: 152.22
    Episode_Reward/reaching_object: 0.5576
     Episode_Reward/lifting_object: 7.8122
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 18.8750
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 2.22s
                      Time elapsed: 00:18:55
                               ETA: 01:08:44

################################################################################
                     [1m Learning iteration 432/2000 [0m                      

                       Computation: 41286 steps/s (collection: 2.289s, learning 0.092s)
             Mean action noise std: 2.28
          Mean value_function loss: 98.7593
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 58.1896
                       Mean reward: 43.78
               Mean episode length: 153.31
    Episode_Reward/reaching_object: 0.5784
     Episode_Reward/lifting_object: 8.1013
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 19.5000
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 2.38s
                      Time elapsed: 00:18:58
                               ETA: 01:08:41

################################################################################
                     [1m Learning iteration 433/2000 [0m                      

                       Computation: 43450 steps/s (collection: 2.159s, learning 0.104s)
             Mean action noise std: 2.28
          Mean value_function loss: 90.2541
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 58.1928
                       Mean reward: 39.42
               Mean episode length: 157.15
    Episode_Reward/reaching_object: 0.5632
     Episode_Reward/lifting_object: 7.7584
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 19.0833
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 2.26s
                      Time elapsed: 00:19:00
                               ETA: 01:08:37

################################################################################
                     [1m Learning iteration 434/2000 [0m                      

                       Computation: 43150 steps/s (collection: 2.187s, learning 0.091s)
             Mean action noise std: 2.28
          Mean value_function loss: 88.7324
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 58.2008
                       Mean reward: 40.24
               Mean episode length: 150.38
    Episode_Reward/reaching_object: 0.5929
     Episode_Reward/lifting_object: 8.1938
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 20.3750
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 2.28s
                      Time elapsed: 00:19:02
                               ETA: 01:08:33

################################################################################
                     [1m Learning iteration 435/2000 [0m                      

                       Computation: 45625 steps/s (collection: 2.068s, learning 0.087s)
             Mean action noise std: 2.28
          Mean value_function loss: 93.0338
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 58.2119
                       Mean reward: 38.65
               Mean episode length: 147.52
    Episode_Reward/reaching_object: 0.5944
     Episode_Reward/lifting_object: 8.1879
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 18.9167
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 2.15s
                      Time elapsed: 00:19:04
                               ETA: 01:08:29

################################################################################
                     [1m Learning iteration 436/2000 [0m                      

                       Computation: 41977 steps/s (collection: 2.257s, learning 0.085s)
             Mean action noise std: 2.28
          Mean value_function loss: 81.2111
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.2210
                       Mean reward: 50.62
               Mean episode length: 160.81
    Episode_Reward/reaching_object: 0.5748
     Episode_Reward/lifting_object: 8.2856
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 20.9167
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 2.34s
                      Time elapsed: 00:19:07
                               ETA: 01:08:25

################################################################################
                     [1m Learning iteration 437/2000 [0m                      

                       Computation: 43377 steps/s (collection: 2.175s, learning 0.092s)
             Mean action noise std: 2.28
          Mean value_function loss: 85.6184
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 58.2327
                       Mean reward: 34.51
               Mean episode length: 152.12
    Episode_Reward/reaching_object: 0.5754
     Episode_Reward/lifting_object: 8.0061
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 5.5000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 21.7083
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 2.27s
                      Time elapsed: 00:19:09
                               ETA: 01:08:21

################################################################################
                     [1m Learning iteration 438/2000 [0m                      

                       Computation: 44798 steps/s (collection: 2.106s, learning 0.089s)
             Mean action noise std: 2.28
          Mean value_function loss: 113.8753
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 58.2465
                       Mean reward: 40.39
               Mean episode length: 149.55
    Episode_Reward/reaching_object: 0.5563
     Episode_Reward/lifting_object: 8.3915
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 21.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 2.19s
                      Time elapsed: 00:19:11
                               ETA: 01:08:17

################################################################################
                     [1m Learning iteration 439/2000 [0m                      

                       Computation: 45990 steps/s (collection: 2.051s, learning 0.086s)
             Mean action noise std: 2.28
          Mean value_function loss: 82.8241
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 58.2618
                       Mean reward: 43.36
               Mean episode length: 155.56
    Episode_Reward/reaching_object: 0.5679
     Episode_Reward/lifting_object: 7.7658
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 20.3333
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 2.14s
                      Time elapsed: 00:19:13
                               ETA: 01:08:13

################################################################################
                     [1m Learning iteration 440/2000 [0m                      

                       Computation: 44764 steps/s (collection: 2.081s, learning 0.115s)
             Mean action noise std: 2.28
          Mean value_function loss: 78.8041
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 58.2728
                       Mean reward: 44.91
               Mean episode length: 153.98
    Episode_Reward/reaching_object: 0.5844
     Episode_Reward/lifting_object: 9.2491
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 20.9167
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 2.20s
                      Time elapsed: 00:19:15
                               ETA: 01:08:08

################################################################################
                     [1m Learning iteration 441/2000 [0m                      

                       Computation: 44896 steps/s (collection: 2.078s, learning 0.112s)
             Mean action noise std: 2.28
          Mean value_function loss: 83.7573
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 58.2779
                       Mean reward: 39.78
               Mean episode length: 146.15
    Episode_Reward/reaching_object: 0.5758
     Episode_Reward/lifting_object: 8.8999
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 21.0833
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 2.19s
                      Time elapsed: 00:19:18
                               ETA: 01:08:04

################################################################################
                     [1m Learning iteration 442/2000 [0m                      

                       Computation: 44244 steps/s (collection: 2.097s, learning 0.125s)
             Mean action noise std: 2.28
          Mean value_function loss: 80.1431
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 58.2829
                       Mean reward: 31.82
               Mean episode length: 136.09
    Episode_Reward/reaching_object: 0.5635
     Episode_Reward/lifting_object: 8.6115
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 23.9583
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 2.22s
                      Time elapsed: 00:19:20
                               ETA: 01:08:00

################################################################################
                     [1m Learning iteration 443/2000 [0m                      

                       Computation: 41638 steps/s (collection: 2.215s, learning 0.146s)
             Mean action noise std: 2.28
          Mean value_function loss: 110.8662
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 58.2859
                       Mean reward: 47.17
               Mean episode length: 150.50
    Episode_Reward/reaching_object: 0.5669
     Episode_Reward/lifting_object: 9.2062
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 21.0417
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 2.36s
                      Time elapsed: 00:19:22
                               ETA: 01:07:57

################################################################################
                     [1m Learning iteration 444/2000 [0m                      

                       Computation: 39002 steps/s (collection: 2.361s, learning 0.160s)
             Mean action noise std: 2.28
          Mean value_function loss: 91.1040
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 58.2871
                       Mean reward: 54.52
               Mean episode length: 144.92
    Episode_Reward/reaching_object: 0.5606
     Episode_Reward/lifting_object: 9.7297
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 20.7917
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 2.52s
                      Time elapsed: 00:19:25
                               ETA: 01:07:54

################################################################################
                     [1m Learning iteration 445/2000 [0m                      

                       Computation: 43091 steps/s (collection: 2.164s, learning 0.118s)
             Mean action noise std: 2.28
          Mean value_function loss: 115.4179
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 58.2882
                       Mean reward: 48.07
               Mean episode length: 151.41
    Episode_Reward/reaching_object: 0.5537
     Episode_Reward/lifting_object: 8.8477
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 20.1667
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 2.28s
                      Time elapsed: 00:19:27
                               ETA: 01:07:50

################################################################################
                     [1m Learning iteration 446/2000 [0m                      

                       Computation: 43502 steps/s (collection: 2.159s, learning 0.101s)
             Mean action noise std: 2.28
          Mean value_function loss: 101.1758
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 58.2890
                       Mean reward: 46.50
               Mean episode length: 156.73
    Episode_Reward/reaching_object: 0.5805
     Episode_Reward/lifting_object: 9.9374
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 21.6250
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 2.26s
                      Time elapsed: 00:19:29
                               ETA: 01:07:46

################################################################################
                     [1m Learning iteration 447/2000 [0m                      

                       Computation: 43499 steps/s (collection: 2.127s, learning 0.133s)
             Mean action noise std: 2.28
          Mean value_function loss: 101.4373
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 58.2899
                       Mean reward: 54.60
               Mean episode length: 148.68
    Episode_Reward/reaching_object: 0.5815
     Episode_Reward/lifting_object: 9.7589
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 20.2083
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 2.26s
                      Time elapsed: 00:19:31
                               ETA: 01:07:42

################################################################################
                     [1m Learning iteration 448/2000 [0m                      

                       Computation: 43159 steps/s (collection: 2.184s, learning 0.094s)
             Mean action noise std: 2.28
          Mean value_function loss: 70.1341
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 58.2906
                       Mean reward: 45.37
               Mean episode length: 154.25
    Episode_Reward/reaching_object: 0.5707
     Episode_Reward/lifting_object: 8.9743
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 17.1250
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 2.28s
                      Time elapsed: 00:19:34
                               ETA: 01:07:38

################################################################################
                     [1m Learning iteration 449/2000 [0m                      

                       Computation: 42693 steps/s (collection: 2.204s, learning 0.099s)
             Mean action noise std: 2.28
          Mean value_function loss: 80.1293
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 58.2925
                       Mean reward: 55.31
               Mean episode length: 155.34
    Episode_Reward/reaching_object: 0.5649
     Episode_Reward/lifting_object: 9.3703
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 18.2917
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 2.30s
                      Time elapsed: 00:19:36
                               ETA: 01:07:35

################################################################################
                     [1m Learning iteration 450/2000 [0m                      

                       Computation: 44884 steps/s (collection: 2.099s, learning 0.091s)
             Mean action noise std: 2.29
          Mean value_function loss: 90.1545
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 58.2966
                       Mean reward: 53.89
               Mean episode length: 154.11
    Episode_Reward/reaching_object: 0.5912
     Episode_Reward/lifting_object: 10.0384
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 19.5833
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 2.19s
                      Time elapsed: 00:19:38
                               ETA: 01:07:31

################################################################################
                     [1m Learning iteration 451/2000 [0m                      

                       Computation: 42214 steps/s (collection: 2.219s, learning 0.110s)
             Mean action noise std: 2.29
          Mean value_function loss: 79.2661
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 58.3008
                       Mean reward: 42.37
               Mean episode length: 165.02
    Episode_Reward/reaching_object: 0.6074
     Episode_Reward/lifting_object: 9.4450
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 16.5417
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 2.33s
                      Time elapsed: 00:19:41
                               ETA: 01:07:27

################################################################################
                     [1m Learning iteration 452/2000 [0m                      

                       Computation: 44338 steps/s (collection: 2.129s, learning 0.088s)
             Mean action noise std: 2.29
          Mean value_function loss: 77.5141
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.3085
                       Mean reward: 49.73
               Mean episode length: 161.87
    Episode_Reward/reaching_object: 0.5892
     Episode_Reward/lifting_object: 9.5308
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 17.9583
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 2.22s
                      Time elapsed: 00:19:43
                               ETA: 01:07:23

################################################################################
                     [1m Learning iteration 453/2000 [0m                      

                       Computation: 43645 steps/s (collection: 2.158s, learning 0.094s)
             Mean action noise std: 2.29
          Mean value_function loss: 80.3730
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 58.3179
                       Mean reward: 62.59
               Mean episode length: 165.96
    Episode_Reward/reaching_object: 0.6171
     Episode_Reward/lifting_object: 10.9413
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 19.0833
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 2.25s
                      Time elapsed: 00:19:45
                               ETA: 01:07:19

################################################################################
                     [1m Learning iteration 454/2000 [0m                      

                       Computation: 42759 steps/s (collection: 2.213s, learning 0.086s)
             Mean action noise std: 2.29
          Mean value_function loss: 76.6776
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 58.3233
                       Mean reward: 59.70
               Mean episode length: 157.93
    Episode_Reward/reaching_object: 0.5929
     Episode_Reward/lifting_object: 10.9403
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 18.0833
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 2.30s
                      Time elapsed: 00:19:47
                               ETA: 01:07:16

################################################################################
                     [1m Learning iteration 455/2000 [0m                      

                       Computation: 44415 steps/s (collection: 2.079s, learning 0.134s)
             Mean action noise std: 2.29
          Mean value_function loss: 92.2092
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.3305
                       Mean reward: 61.01
               Mean episode length: 162.09
    Episode_Reward/reaching_object: 0.6100
     Episode_Reward/lifting_object: 11.6981
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 16.8750
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 2.21s
                      Time elapsed: 00:19:50
                               ETA: 01:07:12

################################################################################
                     [1m Learning iteration 456/2000 [0m                      

                       Computation: 43922 steps/s (collection: 2.133s, learning 0.105s)
             Mean action noise std: 2.29
          Mean value_function loss: 80.8002
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 58.3373
                       Mean reward: 67.17
               Mean episode length: 172.70
    Episode_Reward/reaching_object: 0.5924
     Episode_Reward/lifting_object: 11.1032
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 17.8333
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 2.24s
                      Time elapsed: 00:19:52
                               ETA: 01:07:08

################################################################################
                     [1m Learning iteration 457/2000 [0m                      

                       Computation: 42179 steps/s (collection: 2.243s, learning 0.088s)
             Mean action noise std: 2.29
          Mean value_function loss: 98.4260
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 58.3446
                       Mean reward: 54.66
               Mean episode length: 161.53
    Episode_Reward/reaching_object: 0.5827
     Episode_Reward/lifting_object: 10.8547
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 17.0417
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 2.33s
                      Time elapsed: 00:19:54
                               ETA: 01:07:04

################################################################################
                     [1m Learning iteration 458/2000 [0m                      

                       Computation: 38942 steps/s (collection: 2.320s, learning 0.204s)
             Mean action noise std: 2.29
          Mean value_function loss: 133.3136
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 58.3533
                       Mean reward: 61.10
               Mean episode length: 160.49
    Episode_Reward/reaching_object: 0.5793
     Episode_Reward/lifting_object: 10.8215
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 19.9583
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 2.52s
                      Time elapsed: 00:19:57
                               ETA: 01:07:01

################################################################################
                     [1m Learning iteration 459/2000 [0m                      

                       Computation: 43547 steps/s (collection: 2.163s, learning 0.095s)
             Mean action noise std: 2.29
          Mean value_function loss: 220.4340
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 58.3660
                       Mean reward: 69.34
               Mean episode length: 156.46
    Episode_Reward/reaching_object: 0.5942
     Episode_Reward/lifting_object: 11.4806
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 17.9167
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 2.26s
                      Time elapsed: 00:19:59
                               ETA: 01:06:58

################################################################################
                     [1m Learning iteration 460/2000 [0m                      

                       Computation: 41648 steps/s (collection: 2.242s, learning 0.119s)
             Mean action noise std: 2.29
          Mean value_function loss: 84.4232
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 58.3809
                       Mean reward: 61.45
               Mean episode length: 179.95
    Episode_Reward/reaching_object: 0.5921
     Episode_Reward/lifting_object: 10.5819
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 17.9167
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 2.36s
                      Time elapsed: 00:20:01
                               ETA: 01:06:54

################################################################################
                     [1m Learning iteration 461/2000 [0m                      

                       Computation: 42273 steps/s (collection: 2.179s, learning 0.146s)
             Mean action noise std: 2.29
          Mean value_function loss: 102.7442
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 58.3971
                       Mean reward: 54.61
               Mean episode length: 158.54
    Episode_Reward/reaching_object: 0.5654
     Episode_Reward/lifting_object: 10.5618
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 18.0417
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 2.33s
                      Time elapsed: 00:20:04
                               ETA: 01:06:51

################################################################################
                     [1m Learning iteration 462/2000 [0m                      

                       Computation: 41898 steps/s (collection: 2.228s, learning 0.118s)
             Mean action noise std: 2.30
          Mean value_function loss: 95.1282
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.4088
                       Mean reward: 68.24
               Mean episode length: 156.52
    Episode_Reward/reaching_object: 0.5940
     Episode_Reward/lifting_object: 11.9456
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 18.7500
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 2.35s
                      Time elapsed: 00:20:06
                               ETA: 01:06:47

################################################################################
                     [1m Learning iteration 463/2000 [0m                      

                       Computation: 42247 steps/s (collection: 2.226s, learning 0.101s)
             Mean action noise std: 2.30
          Mean value_function loss: 89.1941
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.4156
                       Mean reward: 71.83
               Mean episode length: 166.52
    Episode_Reward/reaching_object: 0.6195
     Episode_Reward/lifting_object: 12.4882
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 18.1667
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 2.33s
                      Time elapsed: 00:20:08
                               ETA: 01:06:44

################################################################################
                     [1m Learning iteration 464/2000 [0m                      

                       Computation: 44321 steps/s (collection: 2.103s, learning 0.115s)
             Mean action noise std: 2.30
          Mean value_function loss: 92.2311
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 58.4220
                       Mean reward: 74.17
               Mean episode length: 153.69
    Episode_Reward/reaching_object: 0.6131
     Episode_Reward/lifting_object: 12.6316
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 20.1667
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 2.22s
                      Time elapsed: 00:20:11
                               ETA: 01:06:40

################################################################################
                     [1m Learning iteration 465/2000 [0m                      

                       Computation: 44542 steps/s (collection: 2.113s, learning 0.094s)
             Mean action noise std: 2.30
          Mean value_function loss: 88.7253
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.4263
                       Mean reward: 71.37
               Mean episode length: 159.80
    Episode_Reward/reaching_object: 0.6162
     Episode_Reward/lifting_object: 12.6727
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 20.2083
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 2.21s
                      Time elapsed: 00:20:13
                               ETA: 01:06:36

################################################################################
                     [1m Learning iteration 466/2000 [0m                      

                       Computation: 42369 steps/s (collection: 2.225s, learning 0.095s)
             Mean action noise std: 2.30
          Mean value_function loss: 96.6184
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 58.4327
                       Mean reward: 69.94
               Mean episode length: 163.59
    Episode_Reward/reaching_object: 0.6116
     Episode_Reward/lifting_object: 12.2758
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 20.3333
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 2.32s
                      Time elapsed: 00:20:15
                               ETA: 01:06:32

################################################################################
                     [1m Learning iteration 467/2000 [0m                      

                       Computation: 42666 steps/s (collection: 2.179s, learning 0.125s)
             Mean action noise std: 2.30
          Mean value_function loss: 104.5653
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 58.4369
                       Mean reward: 67.11
               Mean episode length: 146.67
    Episode_Reward/reaching_object: 0.6328
     Episode_Reward/lifting_object: 13.2868
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 19.4167
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 2.30s
                      Time elapsed: 00:20:17
                               ETA: 01:06:29

################################################################################
                     [1m Learning iteration 468/2000 [0m                      

                       Computation: 40220 steps/s (collection: 2.307s, learning 0.137s)
             Mean action noise std: 2.30
          Mean value_function loss: 105.5812
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 58.4379
                       Mean reward: 71.92
               Mean episode length: 155.31
    Episode_Reward/reaching_object: 0.5961
     Episode_Reward/lifting_object: 13.2805
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 19.7917
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 2.44s
                      Time elapsed: 00:20:20
                               ETA: 01:06:26

################################################################################
                     [1m Learning iteration 469/2000 [0m                      

                       Computation: 41166 steps/s (collection: 2.270s, learning 0.118s)
             Mean action noise std: 2.30
          Mean value_function loss: 146.5763
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 58.4403
                       Mean reward: 70.45
               Mean episode length: 168.62
    Episode_Reward/reaching_object: 0.6167
     Episode_Reward/lifting_object: 13.0003
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 20.5833
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 2.39s
                      Time elapsed: 00:20:22
                               ETA: 01:06:22

################################################################################
                     [1m Learning iteration 470/2000 [0m                      

                       Computation: 41463 steps/s (collection: 2.269s, learning 0.102s)
             Mean action noise std: 2.30
          Mean value_function loss: 154.7254
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.4505
                       Mean reward: 61.72
               Mean episode length: 144.87
    Episode_Reward/reaching_object: 0.6248
     Episode_Reward/lifting_object: 13.4255
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 20.4583
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 2.37s
                      Time elapsed: 00:20:25
                               ETA: 01:06:19

################################################################################
                     [1m Learning iteration 471/2000 [0m                      

                       Computation: 39494 steps/s (collection: 2.375s, learning 0.114s)
             Mean action noise std: 2.30
          Mean value_function loss: 116.6096
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 58.4588
                       Mean reward: 67.24
               Mean episode length: 155.24
    Episode_Reward/reaching_object: 0.5930
     Episode_Reward/lifting_object: 12.6469
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 21.2917
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 2.49s
                      Time elapsed: 00:20:27
                               ETA: 01:06:16

################################################################################
                     [1m Learning iteration 472/2000 [0m                      

                       Computation: 43615 steps/s (collection: 2.158s, learning 0.096s)
             Mean action noise std: 2.30
          Mean value_function loss: 148.4061
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 58.4603
                       Mean reward: 77.10
               Mean episode length: 151.69
    Episode_Reward/reaching_object: 0.6255
     Episode_Reward/lifting_object: 14.1589
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 22.7500
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 2.25s
                      Time elapsed: 00:20:29
                               ETA: 01:06:12

################################################################################
                     [1m Learning iteration 473/2000 [0m                      

                       Computation: 44799 steps/s (collection: 2.103s, learning 0.091s)
             Mean action noise std: 2.30
          Mean value_function loss: 127.8542
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 58.4647
                       Mean reward: 74.68
               Mean episode length: 160.09
    Episode_Reward/reaching_object: 0.6275
     Episode_Reward/lifting_object: 14.3768
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 20.7500
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 2.19s
                      Time elapsed: 00:20:31
                               ETA: 01:06:08

################################################################################
                     [1m Learning iteration 474/2000 [0m                      

                       Computation: 41891 steps/s (collection: 2.222s, learning 0.125s)
             Mean action noise std: 2.30
          Mean value_function loss: 120.1216
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 58.4724
                       Mean reward: 74.35
               Mean episode length: 160.80
    Episode_Reward/reaching_object: 0.6136
     Episode_Reward/lifting_object: 13.4515
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 20.4583
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 2.35s
                      Time elapsed: 00:20:34
                               ETA: 01:06:05

################################################################################
                     [1m Learning iteration 475/2000 [0m                      

                       Computation: 42419 steps/s (collection: 2.227s, learning 0.090s)
             Mean action noise std: 2.30
          Mean value_function loss: 113.7128
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.4792
                       Mean reward: 75.29
               Mean episode length: 136.59
    Episode_Reward/reaching_object: 0.6359
     Episode_Reward/lifting_object: 15.8025
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 20.8750
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 2.32s
                      Time elapsed: 00:20:36
                               ETA: 01:06:01

################################################################################
                     [1m Learning iteration 476/2000 [0m                      

                       Computation: 45048 steps/s (collection: 2.092s, learning 0.091s)
             Mean action noise std: 2.30
          Mean value_function loss: 117.8323
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 58.4891
                       Mean reward: 72.44
               Mean episode length: 150.55
    Episode_Reward/reaching_object: 0.6505
     Episode_Reward/lifting_object: 15.1242
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 18.7083
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 2.18s
                      Time elapsed: 00:20:38
                               ETA: 01:05:57

################################################################################
                     [1m Learning iteration 477/2000 [0m                      

                       Computation: 43285 steps/s (collection: 2.149s, learning 0.122s)
             Mean action noise std: 2.30
          Mean value_function loss: 111.9433
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.4965
                       Mean reward: 75.55
               Mean episode length: 147.95
    Episode_Reward/reaching_object: 0.6146
     Episode_Reward/lifting_object: 14.8748
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 21.4167
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 2.27s
                      Time elapsed: 00:20:41
                               ETA: 01:05:54

################################################################################
                     [1m Learning iteration 478/2000 [0m                      

                       Computation: 43384 steps/s (collection: 2.173s, learning 0.093s)
             Mean action noise std: 2.30
          Mean value_function loss: 119.0081
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 58.5012
                       Mean reward: 75.63
               Mean episode length: 144.60
    Episode_Reward/reaching_object: 0.6112
     Episode_Reward/lifting_object: 14.8133
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 21.7083
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 2.27s
                      Time elapsed: 00:20:43
                               ETA: 01:05:50

################################################################################
                     [1m Learning iteration 479/2000 [0m                      

                       Computation: 43854 steps/s (collection: 2.149s, learning 0.093s)
             Mean action noise std: 2.30
          Mean value_function loss: 135.5202
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 58.5063
                       Mean reward: 103.38
               Mean episode length: 151.99
    Episode_Reward/reaching_object: 0.6173
     Episode_Reward/lifting_object: 14.9275
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 22.2500
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 2.24s
                      Time elapsed: 00:20:45
                               ETA: 01:05:46

################################################################################
                     [1m Learning iteration 480/2000 [0m                      

                       Computation: 42837 steps/s (collection: 2.185s, learning 0.110s)
             Mean action noise std: 2.31
          Mean value_function loss: 124.5538
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 58.5156
                       Mean reward: 72.08
               Mean episode length: 156.15
    Episode_Reward/reaching_object: 0.6311
     Episode_Reward/lifting_object: 15.0554
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 22.6667
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 2.29s
                      Time elapsed: 00:20:47
                               ETA: 01:05:43

################################################################################
                     [1m Learning iteration 481/2000 [0m                      

                       Computation: 40635 steps/s (collection: 2.287s, learning 0.132s)
             Mean action noise std: 2.31
          Mean value_function loss: 111.1212
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 58.5216
                       Mean reward: 88.99
               Mean episode length: 158.43
    Episode_Reward/reaching_object: 0.6555
     Episode_Reward/lifting_object: 15.8290
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 21.7917
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 2.42s
                      Time elapsed: 00:20:50
                               ETA: 01:05:40

################################################################################
                     [1m Learning iteration 482/2000 [0m                      

                       Computation: 42023 steps/s (collection: 2.241s, learning 0.098s)
             Mean action noise std: 2.31
          Mean value_function loss: 134.6934
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 58.5252
                       Mean reward: 84.43
               Mean episode length: 152.38
    Episode_Reward/reaching_object: 0.6326
     Episode_Reward/lifting_object: 15.7482
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 21.3750
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 2.34s
                      Time elapsed: 00:20:52
                               ETA: 01:05:36

################################################################################
                     [1m Learning iteration 483/2000 [0m                      

                       Computation: 41215 steps/s (collection: 2.267s, learning 0.118s)
             Mean action noise std: 2.31
          Mean value_function loss: 121.1016
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 58.5305
                       Mean reward: 74.71
               Mean episode length: 142.15
    Episode_Reward/reaching_object: 0.6475
     Episode_Reward/lifting_object: 16.1751
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 23.4167
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 2.39s
                      Time elapsed: 00:20:55
                               ETA: 01:05:33

################################################################################
                     [1m Learning iteration 484/2000 [0m                      

                       Computation: 34867 steps/s (collection: 2.722s, learning 0.098s)
             Mean action noise std: 2.31
          Mean value_function loss: 159.7552
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.5348
                       Mean reward: 69.75
               Mean episode length: 147.75
    Episode_Reward/reaching_object: 0.6251
     Episode_Reward/lifting_object: 15.7923
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 23.5417
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 2.82s
                      Time elapsed: 00:20:57
                               ETA: 01:05:31

################################################################################
                     [1m Learning iteration 485/2000 [0m                      

                       Computation: 42838 steps/s (collection: 2.198s, learning 0.097s)
             Mean action noise std: 2.31
          Mean value_function loss: 167.1501
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 58.5419
                       Mean reward: 81.78
               Mean episode length: 150.88
    Episode_Reward/reaching_object: 0.6218
     Episode_Reward/lifting_object: 14.9345
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 22.7500
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 2.29s
                      Time elapsed: 00:21:00
                               ETA: 01:05:28

################################################################################
                     [1m Learning iteration 486/2000 [0m                      

                       Computation: 41965 steps/s (collection: 2.220s, learning 0.123s)
             Mean action noise std: 2.31
          Mean value_function loss: 141.7800
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 58.5476
                       Mean reward: 78.97
               Mean episode length: 147.10
    Episode_Reward/reaching_object: 0.6094
     Episode_Reward/lifting_object: 15.3781
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 24.4583
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 2.34s
                      Time elapsed: 00:21:02
                               ETA: 01:05:24

################################################################################
                     [1m Learning iteration 487/2000 [0m                      

                       Computation: 43144 steps/s (collection: 2.163s, learning 0.116s)
             Mean action noise std: 2.31
          Mean value_function loss: 130.5435
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.5510
                       Mean reward: 80.50
               Mean episode length: 135.06
    Episode_Reward/reaching_object: 0.6429
     Episode_Reward/lifting_object: 16.9353
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 25.7917
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 2.28s
                      Time elapsed: 00:21:04
                               ETA: 01:05:21

################################################################################
                     [1m Learning iteration 488/2000 [0m                      

                       Computation: 38803 steps/s (collection: 2.372s, learning 0.161s)
             Mean action noise std: 2.31
          Mean value_function loss: 129.7181
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 58.5530
                       Mean reward: 81.30
               Mean episode length: 136.41
    Episode_Reward/reaching_object: 0.6237
     Episode_Reward/lifting_object: 16.8613
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 23.8750
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 2.53s
                      Time elapsed: 00:21:07
                               ETA: 01:05:18

################################################################################
                     [1m Learning iteration 489/2000 [0m                      

                       Computation: 37720 steps/s (collection: 2.486s, learning 0.120s)
             Mean action noise std: 2.31
          Mean value_function loss: 123.8745
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 58.5537
                       Mean reward: 96.18
               Mean episode length: 141.95
    Episode_Reward/reaching_object: 0.6308
     Episode_Reward/lifting_object: 17.2360
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 4.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 25.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 2.61s
                      Time elapsed: 00:21:09
                               ETA: 01:05:15

################################################################################
                     [1m Learning iteration 490/2000 [0m                      

                       Computation: 38300 steps/s (collection: 2.398s, learning 0.168s)
             Mean action noise std: 2.31
          Mean value_function loss: 152.3758
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 58.5587
                       Mean reward: 80.44
               Mean episode length: 128.78
    Episode_Reward/reaching_object: 0.6173
     Episode_Reward/lifting_object: 16.7312
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 26.0417
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 2.57s
                      Time elapsed: 00:21:12
                               ETA: 01:05:13

################################################################################
                     [1m Learning iteration 491/2000 [0m                      

                       Computation: 37626 steps/s (collection: 2.479s, learning 0.134s)
             Mean action noise std: 2.31
          Mean value_function loss: 165.4853
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.5672
                       Mean reward: 78.87
               Mean episode length: 134.15
    Episode_Reward/reaching_object: 0.6030
     Episode_Reward/lifting_object: 14.7897
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 25.7083
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 2.61s
                      Time elapsed: 00:21:15
                               ETA: 01:05:10

################################################################################
                     [1m Learning iteration 492/2000 [0m                      

                       Computation: 35289 steps/s (collection: 2.682s, learning 0.104s)
             Mean action noise std: 2.31
          Mean value_function loss: 142.9238
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 58.5712
                       Mean reward: 78.31
               Mean episode length: 141.35
    Episode_Reward/reaching_object: 0.6155
     Episode_Reward/lifting_object: 16.9059
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 25.1667
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 2.79s
                      Time elapsed: 00:21:17
                               ETA: 01:05:08

################################################################################
                     [1m Learning iteration 493/2000 [0m                      

                       Computation: 43063 steps/s (collection: 2.191s, learning 0.092s)
             Mean action noise std: 2.31
          Mean value_function loss: 164.9175
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 58.5732
                       Mean reward: 76.65
               Mean episode length: 129.99
    Episode_Reward/reaching_object: 0.6083
     Episode_Reward/lifting_object: 16.2872
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 25.2083
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 2.28s
                      Time elapsed: 00:21:20
                               ETA: 01:05:05

################################################################################
                     [1m Learning iteration 494/2000 [0m                      

                       Computation: 43663 steps/s (collection: 2.157s, learning 0.094s)
             Mean action noise std: 2.31
          Mean value_function loss: 171.2283
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 58.5759
                       Mean reward: 91.42
               Mean episode length: 141.96
    Episode_Reward/reaching_object: 0.6041
     Episode_Reward/lifting_object: 16.1367
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 25.5833
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 2.25s
                      Time elapsed: 00:21:22
                               ETA: 01:05:01

################################################################################
                     [1m Learning iteration 495/2000 [0m                      

                       Computation: 43783 steps/s (collection: 2.143s, learning 0.102s)
             Mean action noise std: 2.31
          Mean value_function loss: 162.5573
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 58.5794
                       Mean reward: 64.88
               Mean episode length: 129.04
    Episode_Reward/reaching_object: 0.6038
     Episode_Reward/lifting_object: 15.8000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 25.3333
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 2.25s
                      Time elapsed: 00:21:24
                               ETA: 01:04:57

################################################################################
                     [1m Learning iteration 496/2000 [0m                      

                       Computation: 44069 steps/s (collection: 2.139s, learning 0.092s)
             Mean action noise std: 2.31
          Mean value_function loss: 189.5195
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 58.5830
                       Mean reward: 85.42
               Mean episode length: 125.49
    Episode_Reward/reaching_object: 0.6030
     Episode_Reward/lifting_object: 15.7589
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 25.8750
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 2.23s
                      Time elapsed: 00:21:26
                               ETA: 01:04:54

################################################################################
                     [1m Learning iteration 497/2000 [0m                      

                       Computation: 44070 steps/s (collection: 2.139s, learning 0.092s)
             Mean action noise std: 2.31
          Mean value_function loss: 164.9707
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 58.5865
                       Mean reward: 95.44
               Mean episode length: 139.67
    Episode_Reward/reaching_object: 0.6220
     Episode_Reward/lifting_object: 17.1394
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 25.4167
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 2.23s
                      Time elapsed: 00:21:29
                               ETA: 01:04:50

################################################################################
                     [1m Learning iteration 498/2000 [0m                      

                       Computation: 40364 steps/s (collection: 2.267s, learning 0.168s)
             Mean action noise std: 2.31
          Mean value_function loss: 154.7353
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 58.5915
                       Mean reward: 93.24
               Mean episode length: 135.47
    Episode_Reward/reaching_object: 0.6221
     Episode_Reward/lifting_object: 17.4414
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 24.3750
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 2.44s
                      Time elapsed: 00:21:31
                               ETA: 01:04:47

################################################################################
                     [1m Learning iteration 499/2000 [0m                      

                       Computation: 42448 steps/s (collection: 2.225s, learning 0.091s)
             Mean action noise std: 2.31
          Mean value_function loss: 167.3201
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 58.5953
                       Mean reward: 96.95
               Mean episode length: 145.69
    Episode_Reward/reaching_object: 0.5890
     Episode_Reward/lifting_object: 16.7361
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 25.0833
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 2.32s
                      Time elapsed: 00:21:33
                               ETA: 01:04:44

################################################################################
                     [1m Learning iteration 500/2000 [0m                      

                       Computation: 43378 steps/s (collection: 2.155s, learning 0.112s)
             Mean action noise std: 2.31
          Mean value_function loss: 163.2151
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 58.5977
                       Mean reward: 80.20
               Mean episode length: 133.08
    Episode_Reward/reaching_object: 0.6021
     Episode_Reward/lifting_object: 16.4419
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 27.8333
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 2.27s
                      Time elapsed: 00:21:36
                               ETA: 01:04:40

################################################################################
                     [1m Learning iteration 501/2000 [0m                      

                       Computation: 42033 steps/s (collection: 2.213s, learning 0.126s)
             Mean action noise std: 2.31
          Mean value_function loss: 170.1880
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 58.6001
                       Mean reward: 92.54
               Mean episode length: 131.15
    Episode_Reward/reaching_object: 0.6124
     Episode_Reward/lifting_object: 17.5499
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 26.1250
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 2.34s
                      Time elapsed: 00:21:38
                               ETA: 01:04:37

################################################################################
                     [1m Learning iteration 502/2000 [0m                      

                       Computation: 41579 steps/s (collection: 2.267s, learning 0.098s)
             Mean action noise std: 2.31
          Mean value_function loss: 167.7684
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.6022
                       Mean reward: 92.94
               Mean episode length: 134.15
    Episode_Reward/reaching_object: 0.6074
     Episode_Reward/lifting_object: 17.4145
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 4.1250
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 26.3333
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 2.36s
                      Time elapsed: 00:21:40
                               ETA: 01:04:34

################################################################################
                     [1m Learning iteration 503/2000 [0m                      

                       Computation: 40310 steps/s (collection: 2.327s, learning 0.112s)
             Mean action noise std: 2.31
          Mean value_function loss: 162.9086
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 58.6055
                       Mean reward: 95.35
               Mean episode length: 136.11
    Episode_Reward/reaching_object: 0.5883
     Episode_Reward/lifting_object: 17.2424
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 26.7083
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 2.44s
                      Time elapsed: 00:21:43
                               ETA: 01:04:31

################################################################################
                     [1m Learning iteration 504/2000 [0m                      

                       Computation: 42140 steps/s (collection: 2.198s, learning 0.135s)
             Mean action noise std: 2.32
          Mean value_function loss: 155.6742
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 58.6106
                       Mean reward: 96.48
               Mean episode length: 133.76
    Episode_Reward/reaching_object: 0.6068
     Episode_Reward/lifting_object: 18.8153
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 26.3333
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 2.33s
                      Time elapsed: 00:21:45
                               ETA: 01:04:27

################################################################################
                     [1m Learning iteration 505/2000 [0m                      

                       Computation: 41650 steps/s (collection: 2.252s, learning 0.108s)
             Mean action noise std: 2.32
          Mean value_function loss: 159.2942
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 58.6153
                       Mean reward: 113.63
               Mean episode length: 129.19
    Episode_Reward/reaching_object: 0.6009
     Episode_Reward/lifting_object: 18.5546
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 28.8750
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 2.36s
                      Time elapsed: 00:21:47
                               ETA: 01:04:24

################################################################################
                     [1m Learning iteration 506/2000 [0m                      

                       Computation: 43762 steps/s (collection: 2.156s, learning 0.091s)
             Mean action noise std: 2.32
          Mean value_function loss: 169.7695
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 58.6188
                       Mean reward: 92.30
               Mean episode length: 139.62
    Episode_Reward/reaching_object: 0.5834
     Episode_Reward/lifting_object: 17.3651
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 27.4583
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 2.25s
                      Time elapsed: 00:21:50
                               ETA: 01:04:20

################################################################################
                     [1m Learning iteration 507/2000 [0m                      

                       Computation: 42175 steps/s (collection: 2.180s, learning 0.151s)
             Mean action noise std: 2.32
          Mean value_function loss: 174.4008
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 58.6219
                       Mean reward: 74.50
               Mean episode length: 129.31
    Episode_Reward/reaching_object: 0.6065
     Episode_Reward/lifting_object: 19.1146
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 28.5000
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 2.33s
                      Time elapsed: 00:21:52
                               ETA: 01:04:17

################################################################################
                     [1m Learning iteration 508/2000 [0m                      

                       Computation: 43085 steps/s (collection: 2.189s, learning 0.093s)
             Mean action noise std: 2.32
          Mean value_function loss: 168.1547
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 58.6237
                       Mean reward: 106.31
               Mean episode length: 126.07
    Episode_Reward/reaching_object: 0.5813
     Episode_Reward/lifting_object: 19.0077
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 28.7083
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 2.28s
                      Time elapsed: 00:21:54
                               ETA: 01:04:14

################################################################################
                     [1m Learning iteration 509/2000 [0m                      

                       Computation: 38389 steps/s (collection: 2.400s, learning 0.161s)
             Mean action noise std: 2.32
          Mean value_function loss: 192.1438
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 58.6239
                       Mean reward: 92.55
               Mean episode length: 129.03
    Episode_Reward/reaching_object: 0.5955
     Episode_Reward/lifting_object: 18.5907
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 28.1667
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 2.56s
                      Time elapsed: 00:21:57
                               ETA: 01:04:11

################################################################################
                     [1m Learning iteration 510/2000 [0m                      

                       Computation: 41767 steps/s (collection: 2.218s, learning 0.136s)
             Mean action noise std: 2.32
          Mean value_function loss: 167.5641
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 58.6245
                       Mean reward: 98.86
               Mean episode length: 123.79
    Episode_Reward/reaching_object: 0.5854
     Episode_Reward/lifting_object: 18.4749
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 30.4167
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 2.35s
                      Time elapsed: 00:21:59
                               ETA: 01:04:08

################################################################################
                     [1m Learning iteration 511/2000 [0m                      

                       Computation: 41880 steps/s (collection: 2.201s, learning 0.146s)
             Mean action noise std: 2.32
          Mean value_function loss: 169.5783
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 58.6251
                       Mean reward: 79.91
               Mean episode length: 127.73
    Episode_Reward/reaching_object: 0.5867
     Episode_Reward/lifting_object: 18.1196
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 27.8750
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 2.35s
                      Time elapsed: 00:22:02
                               ETA: 01:04:04

################################################################################
                     [1m Learning iteration 512/2000 [0m                      

                       Computation: 38842 steps/s (collection: 2.323s, learning 0.208s)
             Mean action noise std: 2.32
          Mean value_function loss: 166.1170
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 58.6263
                       Mean reward: 90.22
               Mean episode length: 128.51
    Episode_Reward/reaching_object: 0.6028
     Episode_Reward/lifting_object: 19.0468
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 25.6250
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 2.53s
                      Time elapsed: 00:22:04
                               ETA: 01:04:02

################################################################################
                     [1m Learning iteration 513/2000 [0m                      

                       Computation: 39459 steps/s (collection: 2.372s, learning 0.119s)
             Mean action noise std: 2.32
          Mean value_function loss: 199.1540
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 58.6274
                       Mean reward: 93.35
               Mean episode length: 119.18
    Episode_Reward/reaching_object: 0.6066
     Episode_Reward/lifting_object: 20.2845
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 25.2500
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 2.49s
                      Time elapsed: 00:22:07
                               ETA: 01:03:59

################################################################################
                     [1m Learning iteration 514/2000 [0m                      

                       Computation: 42116 steps/s (collection: 2.237s, learning 0.097s)
             Mean action noise std: 2.32
          Mean value_function loss: 195.1240
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 58.6281
                       Mean reward: 117.19
               Mean episode length: 133.31
    Episode_Reward/reaching_object: 0.6024
     Episode_Reward/lifting_object: 19.8011
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 26.2083
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 2.33s
                      Time elapsed: 00:22:09
                               ETA: 01:03:56

################################################################################
                     [1m Learning iteration 515/2000 [0m                      

                       Computation: 42815 steps/s (collection: 2.186s, learning 0.110s)
             Mean action noise std: 2.32
          Mean value_function loss: 205.8468
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 58.6286
                       Mean reward: 104.04
               Mean episode length: 135.33
    Episode_Reward/reaching_object: 0.6334
     Episode_Reward/lifting_object: 20.5096
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 26.7500
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 2.30s
                      Time elapsed: 00:22:11
                               ETA: 01:03:52

################################################################################
                     [1m Learning iteration 516/2000 [0m                      

                       Computation: 41938 steps/s (collection: 2.245s, learning 0.099s)
             Mean action noise std: 2.32
          Mean value_function loss: 178.0335
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 58.6311
                       Mean reward: 113.24
               Mean episode length: 131.57
    Episode_Reward/reaching_object: 0.6307
     Episode_Reward/lifting_object: 21.7546
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 29.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 2.34s
                      Time elapsed: 00:22:14
                               ETA: 01:03:49

################################################################################
                     [1m Learning iteration 517/2000 [0m                      

                       Computation: 42814 steps/s (collection: 2.205s, learning 0.091s)
             Mean action noise std: 2.32
          Mean value_function loss: 200.3313
               Mean surrogate loss: 0.0110
                 Mean entropy loss: 58.6338
                       Mean reward: 115.25
               Mean episode length: 131.34
    Episode_Reward/reaching_object: 0.6313
     Episode_Reward/lifting_object: 21.8271
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 26.5833
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 2.30s
                      Time elapsed: 00:22:16
                               ETA: 01:03:45

################################################################################
                     [1m Learning iteration 518/2000 [0m                      

                       Computation: 41489 steps/s (collection: 2.251s, learning 0.118s)
             Mean action noise std: 2.32
          Mean value_function loss: 190.5670
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 58.6351
                       Mean reward: 107.83
               Mean episode length: 132.19
    Episode_Reward/reaching_object: 0.6136
     Episode_Reward/lifting_object: 20.6883
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 27.2083
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 2.37s
                      Time elapsed: 00:22:18
                               ETA: 01:03:42

################################################################################
                     [1m Learning iteration 519/2000 [0m                      

                       Computation: 43563 steps/s (collection: 2.153s, learning 0.104s)
             Mean action noise std: 2.32
          Mean value_function loss: 174.9480
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 58.6370
                       Mean reward: 119.75
               Mean episode length: 126.94
    Episode_Reward/reaching_object: 0.6126
     Episode_Reward/lifting_object: 21.7656
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 25.2083
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 2.26s
                      Time elapsed: 00:22:21
                               ETA: 01:03:39

################################################################################
                     [1m Learning iteration 520/2000 [0m                      

                       Computation: 42738 steps/s (collection: 2.195s, learning 0.105s)
             Mean action noise std: 2.32
          Mean value_function loss: 205.6742
               Mean surrogate loss: 0.0107
                 Mean entropy loss: 58.6380
                       Mean reward: 115.94
               Mean episode length: 137.70
    Episode_Reward/reaching_object: 0.6380
     Episode_Reward/lifting_object: 22.5194
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 27.3333
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 2.30s
                      Time elapsed: 00:22:23
                               ETA: 01:03:35

################################################################################
                     [1m Learning iteration 521/2000 [0m                      

                       Computation: 40740 steps/s (collection: 2.270s, learning 0.143s)
             Mean action noise std: 2.32
          Mean value_function loss: 216.2499
               Mean surrogate loss: 0.0132
                 Mean entropy loss: 58.6383
                       Mean reward: 103.58
               Mean episode length: 145.70
    Episode_Reward/reaching_object: 0.6388
     Episode_Reward/lifting_object: 22.4339
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 26.5417
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 2.41s
                      Time elapsed: 00:22:25
                               ETA: 01:03:32

################################################################################
                     [1m Learning iteration 522/2000 [0m                      

                       Computation: 40413 steps/s (collection: 2.330s, learning 0.103s)
             Mean action noise std: 2.32
          Mean value_function loss: 165.5053
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 58.6383
                       Mean reward: 104.57
               Mean episode length: 123.65
    Episode_Reward/reaching_object: 0.6242
     Episode_Reward/lifting_object: 21.4271
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 27.5000
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 2.43s
                      Time elapsed: 00:22:28
                               ETA: 01:03:29

################################################################################
                     [1m Learning iteration 523/2000 [0m                      

                       Computation: 41541 steps/s (collection: 2.263s, learning 0.104s)
             Mean action noise std: 2.32
          Mean value_function loss: 186.1392
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 58.6384
                       Mean reward: 118.99
               Mean episode length: 136.40
    Episode_Reward/reaching_object: 0.6283
     Episode_Reward/lifting_object: 21.8257
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 26.7917
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 2.37s
                      Time elapsed: 00:22:30
                               ETA: 01:03:26

################################################################################
                     [1m Learning iteration 524/2000 [0m                      

                       Computation: 41270 steps/s (collection: 2.257s, learning 0.125s)
             Mean action noise std: 2.32
          Mean value_function loss: 196.6005
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 58.6384
                       Mean reward: 118.41
               Mean episode length: 135.56
    Episode_Reward/reaching_object: 0.6310
     Episode_Reward/lifting_object: 22.5312
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 28.7917
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 2.38s
                      Time elapsed: 00:22:32
                               ETA: 01:03:23

################################################################################
                     [1m Learning iteration 525/2000 [0m                      

                       Computation: 42202 steps/s (collection: 2.223s, learning 0.107s)
             Mean action noise std: 2.32
          Mean value_function loss: 223.7990
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 58.6385
                       Mean reward: 111.97
               Mean episode length: 127.21
    Episode_Reward/reaching_object: 0.6382
     Episode_Reward/lifting_object: 23.4885
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 26.5833
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 2.33s
                      Time elapsed: 00:22:35
                               ETA: 01:03:20

################################################################################
                     [1m Learning iteration 526/2000 [0m                      

                       Computation: 43013 steps/s (collection: 2.179s, learning 0.106s)
             Mean action noise std: 2.32
          Mean value_function loss: 199.2353
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 58.6391
                       Mean reward: 131.22
               Mean episode length: 137.68
    Episode_Reward/reaching_object: 0.6439
     Episode_Reward/lifting_object: 24.4231
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 26.8750
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 2.29s
                      Time elapsed: 00:22:37
                               ETA: 01:03:16

################################################################################
                     [1m Learning iteration 527/2000 [0m                      

                       Computation: 41287 steps/s (collection: 2.208s, learning 0.173s)
             Mean action noise std: 2.32
          Mean value_function loss: 186.3714
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.6402
                       Mean reward: 126.10
               Mean episode length: 139.69
    Episode_Reward/reaching_object: 0.6206
     Episode_Reward/lifting_object: 22.2071
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 26.5000
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 2.38s
                      Time elapsed: 00:22:39
                               ETA: 01:03:13

################################################################################
                     [1m Learning iteration 528/2000 [0m                      

                       Computation: 39188 steps/s (collection: 2.405s, learning 0.103s)
             Mean action noise std: 2.32
          Mean value_function loss: 225.8197
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.6427
                       Mean reward: 112.11
               Mean episode length: 128.75
    Episode_Reward/reaching_object: 0.6075
     Episode_Reward/lifting_object: 22.4982
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 26.3333
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 2.51s
                      Time elapsed: 00:22:42
                               ETA: 01:03:11

################################################################################
                     [1m Learning iteration 529/2000 [0m                      

                       Computation: 34084 steps/s (collection: 2.767s, learning 0.118s)
             Mean action noise std: 2.32
          Mean value_function loss: 189.3934
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 58.6460
                       Mean reward: 137.04
               Mean episode length: 138.43
    Episode_Reward/reaching_object: 0.6434
     Episode_Reward/lifting_object: 24.7273
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 2.9583
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 26.4167
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 2.88s
                      Time elapsed: 00:22:45
                               ETA: 01:03:09

################################################################################
                     [1m Learning iteration 530/2000 [0m                      

                       Computation: 41642 steps/s (collection: 2.236s, learning 0.125s)
             Mean action noise std: 2.32
          Mean value_function loss: 192.6412
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 58.6480
                       Mean reward: 138.15
               Mean episode length: 135.75
    Episode_Reward/reaching_object: 0.6228
     Episode_Reward/lifting_object: 23.5773
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 26.8333
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 2.36s
                      Time elapsed: 00:22:47
                               ETA: 01:03:06

################################################################################
                     [1m Learning iteration 531/2000 [0m                      

                       Computation: 37254 steps/s (collection: 2.498s, learning 0.141s)
             Mean action noise std: 2.32
          Mean value_function loss: 256.7794
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 58.6492
                       Mean reward: 117.99
               Mean episode length: 135.35
    Episode_Reward/reaching_object: 0.6468
     Episode_Reward/lifting_object: 25.1367
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 26.5417
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 2.64s
                      Time elapsed: 00:22:50
                               ETA: 01:03:03

################################################################################
                     [1m Learning iteration 532/2000 [0m                      

                       Computation: 42360 steps/s (collection: 2.205s, learning 0.116s)
             Mean action noise std: 2.32
          Mean value_function loss: 206.6328
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 58.6497
                       Mean reward: 105.85
               Mean episode length: 126.25
    Episode_Reward/reaching_object: 0.6105
     Episode_Reward/lifting_object: 22.3099
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 26.9167
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 2.32s
                      Time elapsed: 00:22:52
                               ETA: 01:03:00

################################################################################
                     [1m Learning iteration 533/2000 [0m                      

                       Computation: 41977 steps/s (collection: 2.222s, learning 0.120s)
             Mean action noise std: 2.32
          Mean value_function loss: 204.9516
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 58.6509
                       Mean reward: 132.39
               Mean episode length: 142.26
    Episode_Reward/reaching_object: 0.6395
     Episode_Reward/lifting_object: 25.7880
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 27.3750
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 2.34s
                      Time elapsed: 00:22:54
                               ETA: 01:02:57

################################################################################
                     [1m Learning iteration 534/2000 [0m                      

                       Computation: 38945 steps/s (collection: 2.406s, learning 0.119s)
             Mean action noise std: 2.32
          Mean value_function loss: 211.6228
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 58.6517
                       Mean reward: 121.07
               Mean episode length: 139.04
    Episode_Reward/reaching_object: 0.6342
     Episode_Reward/lifting_object: 22.8643
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 24.7917
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 2.52s
                      Time elapsed: 00:22:57
                               ETA: 01:02:54

################################################################################
                     [1m Learning iteration 535/2000 [0m                      

                       Computation: 41916 steps/s (collection: 2.235s, learning 0.110s)
             Mean action noise std: 2.32
          Mean value_function loss: 214.9373
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 58.6521
                       Mean reward: 101.26
               Mean episode length: 114.83
    Episode_Reward/reaching_object: 0.6211
     Episode_Reward/lifting_object: 23.0988
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 28.1250
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 2.35s
                      Time elapsed: 00:22:59
                               ETA: 01:02:51

################################################################################
                     [1m Learning iteration 536/2000 [0m                      

                       Computation: 42232 steps/s (collection: 2.168s, learning 0.160s)
             Mean action noise std: 2.32
          Mean value_function loss: 231.4520
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 58.6524
                       Mean reward: 137.50
               Mean episode length: 142.63
    Episode_Reward/reaching_object: 0.6661
     Episode_Reward/lifting_object: 25.3567
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 3.8750
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 23.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 2.33s
                      Time elapsed: 00:23:02
                               ETA: 01:02:48

################################################################################
                     [1m Learning iteration 537/2000 [0m                      

                       Computation: 41762 steps/s (collection: 2.255s, learning 0.099s)
             Mean action noise std: 2.32
          Mean value_function loss: 211.9645
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 58.6529
                       Mean reward: 132.11
               Mean episode length: 148.40
    Episode_Reward/reaching_object: 0.6441
     Episode_Reward/lifting_object: 24.5678
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 26.8333
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 2.35s
                      Time elapsed: 00:23:04
                               ETA: 01:02:44

################################################################################
                     [1m Learning iteration 538/2000 [0m                      

                       Computation: 41781 steps/s (collection: 2.228s, learning 0.125s)
             Mean action noise std: 2.32
          Mean value_function loss: 248.0150
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 58.6533
                       Mean reward: 142.25
               Mean episode length: 140.47
    Episode_Reward/reaching_object: 0.6249
     Episode_Reward/lifting_object: 24.2567
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 25.8333
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 2.35s
                      Time elapsed: 00:23:06
                               ETA: 01:02:41

################################################################################
                     [1m Learning iteration 539/2000 [0m                      

                       Computation: 43383 steps/s (collection: 2.161s, learning 0.105s)
             Mean action noise std: 2.32
          Mean value_function loss: 249.7184
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 58.6544
                       Mean reward: 141.80
               Mean episode length: 135.99
    Episode_Reward/reaching_object: 0.6496
     Episode_Reward/lifting_object: 26.2979
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 24.4167
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 2.27s
                      Time elapsed: 00:23:09
                               ETA: 01:02:38

################################################################################
                     [1m Learning iteration 540/2000 [0m                      

                       Computation: 43448 steps/s (collection: 2.166s, learning 0.097s)
             Mean action noise std: 2.32
          Mean value_function loss: 221.2611
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 58.6560
                       Mean reward: 148.71
               Mean episode length: 139.08
    Episode_Reward/reaching_object: 0.6674
     Episode_Reward/lifting_object: 27.3392
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 4.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 26.4167
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 2.26s
                      Time elapsed: 00:23:11
                               ETA: 01:02:34

################################################################################
                     [1m Learning iteration 541/2000 [0m                      

                       Computation: 43204 steps/s (collection: 2.182s, learning 0.093s)
             Mean action noise std: 2.32
          Mean value_function loss: 261.4847
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.6580
                       Mean reward: 134.42
               Mean episode length: 142.65
    Episode_Reward/reaching_object: 0.6453
     Episode_Reward/lifting_object: 26.0795
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 2.9583
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 27.2083
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 2.28s
                      Time elapsed: 00:23:13
                               ETA: 01:02:31

################################################################################
                     [1m Learning iteration 542/2000 [0m                      

                       Computation: 41686 steps/s (collection: 2.253s, learning 0.105s)
             Mean action noise std: 2.32
          Mean value_function loss: 255.1991
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 58.6593
                       Mean reward: 124.07
               Mean episode length: 129.18
    Episode_Reward/reaching_object: 0.6664
     Episode_Reward/lifting_object: 27.8662
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 28.0417
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 2.36s
                      Time elapsed: 00:23:16
                               ETA: 01:02:28

################################################################################
                     [1m Learning iteration 543/2000 [0m                      

                       Computation: 42876 steps/s (collection: 2.163s, learning 0.130s)
             Mean action noise std: 2.32
          Mean value_function loss: 234.6714
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 58.6610
                       Mean reward: 127.01
               Mean episode length: 136.69
    Episode_Reward/reaching_object: 0.6502
     Episode_Reward/lifting_object: 26.8812
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 27.7500
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 2.29s
                      Time elapsed: 00:23:18
                               ETA: 01:02:25

################################################################################
                     [1m Learning iteration 544/2000 [0m                      

                       Computation: 42390 steps/s (collection: 2.209s, learning 0.111s)
             Mean action noise std: 2.32
          Mean value_function loss: 239.5941
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 58.6633
                       Mean reward: 125.95
               Mean episode length: 126.57
    Episode_Reward/reaching_object: 0.6362
     Episode_Reward/lifting_object: 26.5589
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 27.2917
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 2.32s
                      Time elapsed: 00:23:20
                               ETA: 01:02:21

################################################################################
                     [1m Learning iteration 545/2000 [0m                      

                       Computation: 41403 steps/s (collection: 2.196s, learning 0.179s)
             Mean action noise std: 2.32
          Mean value_function loss: 231.0998
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 58.6659
                       Mean reward: 133.00
               Mean episode length: 129.72
    Episode_Reward/reaching_object: 0.6571
     Episode_Reward/lifting_object: 28.3067
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 25.7500
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 2.37s
                      Time elapsed: 00:23:23
                               ETA: 01:02:18

################################################################################
                     [1m Learning iteration 546/2000 [0m                      

                       Computation: 42198 steps/s (collection: 2.236s, learning 0.094s)
             Mean action noise std: 2.32
          Mean value_function loss: 280.4363
               Mean surrogate loss: 0.0121
                 Mean entropy loss: 58.6679
                       Mean reward: 144.38
               Mean episode length: 135.77
    Episode_Reward/reaching_object: 0.6647
     Episode_Reward/lifting_object: 29.3436
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 25.3750
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 2.33s
                      Time elapsed: 00:23:25
                               ETA: 01:02:15

################################################################################
                     [1m Learning iteration 547/2000 [0m                      

                       Computation: 42591 steps/s (collection: 2.206s, learning 0.102s)
             Mean action noise std: 2.32
          Mean value_function loss: 243.1416
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 58.6682
                       Mean reward: 164.35
               Mean episode length: 137.27
    Episode_Reward/reaching_object: 0.6493
     Episode_Reward/lifting_object: 28.9423
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 24.0417
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 2.31s
                      Time elapsed: 00:23:27
                               ETA: 01:02:12

################################################################################
                     [1m Learning iteration 548/2000 [0m                      

                       Computation: 41334 steps/s (collection: 2.284s, learning 0.095s)
             Mean action noise std: 2.32
          Mean value_function loss: 219.8148
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 58.6683
                       Mean reward: 150.33
               Mean episode length: 133.63
    Episode_Reward/reaching_object: 0.6959
     Episode_Reward/lifting_object: 32.3468
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 26.3750
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 2.38s
                      Time elapsed: 00:23:30
                               ETA: 01:02:09

################################################################################
                     [1m Learning iteration 549/2000 [0m                      

                       Computation: 41672 steps/s (collection: 2.175s, learning 0.184s)
             Mean action noise std: 2.32
          Mean value_function loss: 230.9800
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 58.6684
                       Mean reward: 166.36
               Mean episode length: 140.62
    Episode_Reward/reaching_object: 0.6822
     Episode_Reward/lifting_object: 31.7728
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 26.2083
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 2.36s
                      Time elapsed: 00:23:32
                               ETA: 01:02:06

################################################################################
                     [1m Learning iteration 550/2000 [0m                      

                       Computation: 43215 steps/s (collection: 2.180s, learning 0.095s)
             Mean action noise std: 2.32
          Mean value_function loss: 273.5101
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 58.6694
                       Mean reward: 143.98
               Mean episode length: 137.13
    Episode_Reward/reaching_object: 0.6743
     Episode_Reward/lifting_object: 30.9616
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 25.5417
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 2.27s
                      Time elapsed: 00:23:34
                               ETA: 01:02:02

################################################################################
                     [1m Learning iteration 551/2000 [0m                      

                       Computation: 42103 steps/s (collection: 2.232s, learning 0.103s)
             Mean action noise std: 2.32
          Mean value_function loss: 249.7047
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 58.6700
                       Mean reward: 151.22
               Mean episode length: 130.80
    Episode_Reward/reaching_object: 0.6481
     Episode_Reward/lifting_object: 29.1418
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 27.2500
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 2.33s
                      Time elapsed: 00:23:36
                               ETA: 01:01:59

################################################################################
                     [1m Learning iteration 552/2000 [0m                      

                       Computation: 42109 steps/s (collection: 2.237s, learning 0.097s)
             Mean action noise std: 2.32
          Mean value_function loss: 259.9157
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 58.6704
                       Mean reward: 164.68
               Mean episode length: 140.17
    Episode_Reward/reaching_object: 0.6870
     Episode_Reward/lifting_object: 32.1467
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 24.9167
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 2.33s
                      Time elapsed: 00:23:39
                               ETA: 01:01:56

################################################################################
                     [1m Learning iteration 553/2000 [0m                      

                       Computation: 42864 steps/s (collection: 2.184s, learning 0.109s)
             Mean action noise std: 2.32
          Mean value_function loss: 244.5948
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 58.6706
                       Mean reward: 179.85
               Mean episode length: 136.59
    Episode_Reward/reaching_object: 0.7217
     Episode_Reward/lifting_object: 35.6766
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 24.2500
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 2.29s
                      Time elapsed: 00:23:41
                               ETA: 01:01:53

################################################################################
                     [1m Learning iteration 554/2000 [0m                      

                       Computation: 42956 steps/s (collection: 2.182s, learning 0.107s)
             Mean action noise std: 2.32
          Mean value_function loss: 237.9134
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 58.6705
                       Mean reward: 188.98
               Mean episode length: 152.07
    Episode_Reward/reaching_object: 0.6964
     Episode_Reward/lifting_object: 33.8005
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 3.7083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 26.5833
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 2.29s
                      Time elapsed: 00:23:43
                               ETA: 01:01:49

################################################################################
                     [1m Learning iteration 555/2000 [0m                      

                       Computation: 42232 steps/s (collection: 2.228s, learning 0.100s)
             Mean action noise std: 2.32
          Mean value_function loss: 252.9097
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 58.6696
                       Mean reward: 178.98
               Mean episode length: 140.00
    Episode_Reward/reaching_object: 0.6986
     Episode_Reward/lifting_object: 33.9639
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 26.4583
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 2.33s
                      Time elapsed: 00:23:46
                               ETA: 01:01:46

################################################################################
                     [1m Learning iteration 556/2000 [0m                      

                       Computation: 43801 steps/s (collection: 2.153s, learning 0.091s)
             Mean action noise std: 2.32
          Mean value_function loss: 267.0279
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 58.6683
                       Mean reward: 171.73
               Mean episode length: 138.15
    Episode_Reward/reaching_object: 0.6896
     Episode_Reward/lifting_object: 32.9086
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 25.4167
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 2.24s
                      Time elapsed: 00:23:48
                               ETA: 01:01:43

################################################################################
                     [1m Learning iteration 557/2000 [0m                      

                       Computation: 43947 steps/s (collection: 2.146s, learning 0.091s)
             Mean action noise std: 2.32
          Mean value_function loss: 275.6489
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 58.6677
                       Mean reward: 197.84
               Mean episode length: 154.44
    Episode_Reward/reaching_object: 0.6984
     Episode_Reward/lifting_object: 33.5807
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 25.7500
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 2.24s
                      Time elapsed: 00:23:50
                               ETA: 01:01:39

################################################################################
                     [1m Learning iteration 558/2000 [0m                      

                       Computation: 41720 steps/s (collection: 2.174s, learning 0.183s)
             Mean action noise std: 2.32
          Mean value_function loss: 255.7869
               Mean surrogate loss: 0.0112
                 Mean entropy loss: 58.6676
                       Mean reward: 184.04
               Mean episode length: 149.12
    Episode_Reward/reaching_object: 0.6812
     Episode_Reward/lifting_object: 33.1345
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 24.7083
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 2.36s
                      Time elapsed: 00:23:53
                               ETA: 01:01:36

################################################################################
                     [1m Learning iteration 559/2000 [0m                      

                       Computation: 40661 steps/s (collection: 2.321s, learning 0.097s)
             Mean action noise std: 2.32
          Mean value_function loss: 287.4863
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 58.6678
                       Mean reward: 165.59
               Mean episode length: 140.83
    Episode_Reward/reaching_object: 0.6974
     Episode_Reward/lifting_object: 33.9483
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 26.9583
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 2.42s
                      Time elapsed: 00:23:55
                               ETA: 01:01:33

################################################################################
                     [1m Learning iteration 560/2000 [0m                      

                       Computation: 41633 steps/s (collection: 2.256s, learning 0.105s)
             Mean action noise std: 2.32
          Mean value_function loss: 299.9949
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.6683
                       Mean reward: 167.25
               Mean episode length: 128.01
    Episode_Reward/reaching_object: 0.6743
     Episode_Reward/lifting_object: 32.4722
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 25.3750
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 2.36s
                      Time elapsed: 00:23:57
                               ETA: 01:01:30

################################################################################
                     [1m Learning iteration 561/2000 [0m                      

                       Computation: 38692 steps/s (collection: 2.334s, learning 0.207s)
             Mean action noise std: 2.32
          Mean value_function loss: 327.3261
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 58.6683
                       Mean reward: 156.52
               Mean episode length: 133.13
    Episode_Reward/reaching_object: 0.6769
     Episode_Reward/lifting_object: 32.3852
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 23.5000
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 2.54s
                      Time elapsed: 00:24:00
                               ETA: 01:01:28

################################################################################
                     [1m Learning iteration 562/2000 [0m                      

                       Computation: 41464 steps/s (collection: 2.274s, learning 0.097s)
             Mean action noise std: 2.32
          Mean value_function loss: 355.4157
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 58.6682
                       Mean reward: 197.80
               Mean episode length: 142.86
    Episode_Reward/reaching_object: 0.7231
     Episode_Reward/lifting_object: 35.8970
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 3.7083
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 24.1667
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 2.37s
                      Time elapsed: 00:24:02
                               ETA: 01:01:25

################################################################################
                     [1m Learning iteration 563/2000 [0m                      

                       Computation: 38557 steps/s (collection: 2.419s, learning 0.131s)
             Mean action noise std: 2.32
          Mean value_function loss: 325.0974
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 58.6691
                       Mean reward: 194.68
               Mean episode length: 138.67
    Episode_Reward/reaching_object: 0.6988
     Episode_Reward/lifting_object: 35.0681
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 27.0417
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 2.55s
                      Time elapsed: 00:24:05
                               ETA: 01:01:22

################################################################################
                     [1m Learning iteration 564/2000 [0m                      

                       Computation: 39836 steps/s (collection: 2.325s, learning 0.143s)
             Mean action noise std: 2.32
          Mean value_function loss: 336.6399
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 58.6694
                       Mean reward: 210.17
               Mean episode length: 142.81
    Episode_Reward/reaching_object: 0.7159
     Episode_Reward/lifting_object: 36.4669
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 27.1250
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 2.47s
                      Time elapsed: 00:24:07
                               ETA: 01:01:19

################################################################################
                     [1m Learning iteration 565/2000 [0m                      

                       Computation: 37348 steps/s (collection: 2.476s, learning 0.156s)
             Mean action noise std: 2.32
          Mean value_function loss: 322.8983
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 58.6692
                       Mean reward: 189.12
               Mean episode length: 138.24
    Episode_Reward/reaching_object: 0.7219
     Episode_Reward/lifting_object: 37.8934
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 28.1667
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 2.63s
                      Time elapsed: 00:24:10
                               ETA: 01:01:17

################################################################################
                     [1m Learning iteration 566/2000 [0m                      

                       Computation: 41058 steps/s (collection: 2.302s, learning 0.092s)
             Mean action noise std: 2.32
          Mean value_function loss: 377.6940
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 58.6693
                       Mean reward: 202.56
               Mean episode length: 129.05
    Episode_Reward/reaching_object: 0.6974
     Episode_Reward/lifting_object: 36.8119
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 28.2500
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 2.39s
                      Time elapsed: 00:24:12
                               ETA: 01:01:14

################################################################################
                     [1m Learning iteration 567/2000 [0m                      

                       Computation: 42157 steps/s (collection: 2.234s, learning 0.098s)
             Mean action noise std: 2.32
          Mean value_function loss: 348.7692
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.6685
                       Mean reward: 180.41
               Mean episode length: 126.75
    Episode_Reward/reaching_object: 0.7201
     Episode_Reward/lifting_object: 39.2649
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 28.3750
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 2.33s
                      Time elapsed: 00:24:15
                               ETA: 01:01:11

################################################################################
                     [1m Learning iteration 568/2000 [0m                      

                       Computation: 41044 steps/s (collection: 2.276s, learning 0.119s)
             Mean action noise std: 2.32
          Mean value_function loss: 316.7173
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 58.6673
                       Mean reward: 188.86
               Mean episode length: 129.44
    Episode_Reward/reaching_object: 0.7136
     Episode_Reward/lifting_object: 39.3362
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 25.7083
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 2.40s
                      Time elapsed: 00:24:17
                               ETA: 01:01:08

################################################################################
                     [1m Learning iteration 569/2000 [0m                      

                       Computation: 41641 steps/s (collection: 2.260s, learning 0.101s)
             Mean action noise std: 2.32
          Mean value_function loss: 332.7607
               Mean surrogate loss: 0.0105
                 Mean entropy loss: 58.6671
                       Mean reward: 211.64
               Mean episode length: 132.39
    Episode_Reward/reaching_object: 0.7240
     Episode_Reward/lifting_object: 40.9206
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 25.3333
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 2.36s
                      Time elapsed: 00:24:19
                               ETA: 01:01:05

################################################################################
                     [1m Learning iteration 570/2000 [0m                      

                       Computation: 43456 steps/s (collection: 2.153s, learning 0.109s)
             Mean action noise std: 2.32
          Mean value_function loss: 325.1992
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 58.6672
                       Mean reward: 220.82
               Mean episode length: 138.35
    Episode_Reward/reaching_object: 0.7378
     Episode_Reward/lifting_object: 42.1124
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 25.6250
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 2.26s
                      Time elapsed: 00:24:22
                               ETA: 01:01:01

################################################################################
                     [1m Learning iteration 571/2000 [0m                      

                       Computation: 38308 steps/s (collection: 2.388s, learning 0.179s)
             Mean action noise std: 2.32
          Mean value_function loss: 317.9692
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 58.6674
                       Mean reward: 213.00
               Mean episode length: 137.12
    Episode_Reward/reaching_object: 0.7394
     Episode_Reward/lifting_object: 42.3777
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 25.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 2.57s
                      Time elapsed: 00:24:24
                               ETA: 01:00:59

################################################################################
                     [1m Learning iteration 572/2000 [0m                      

                       Computation: 41683 steps/s (collection: 2.256s, learning 0.102s)
             Mean action noise std: 2.32
          Mean value_function loss: 352.4016
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 58.6678
                       Mean reward: 220.43
               Mean episode length: 135.28
    Episode_Reward/reaching_object: 0.7428
     Episode_Reward/lifting_object: 42.8083
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 25.0417
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 2.36s
                      Time elapsed: 00:24:27
                               ETA: 01:00:56

################################################################################
                     [1m Learning iteration 573/2000 [0m                      

                       Computation: 39485 steps/s (collection: 2.288s, learning 0.201s)
             Mean action noise std: 2.32
          Mean value_function loss: 316.5714
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 58.6676
                       Mean reward: 258.28
               Mean episode length: 152.99
    Episode_Reward/reaching_object: 0.7500
     Episode_Reward/lifting_object: 43.0288
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 25.4583
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 2.49s
                      Time elapsed: 00:24:29
                               ETA: 01:00:53

################################################################################
                     [1m Learning iteration 574/2000 [0m                      

                       Computation: 41621 steps/s (collection: 2.200s, learning 0.162s)
             Mean action noise std: 2.32
          Mean value_function loss: 283.2594
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 58.6670
                       Mean reward: 205.12
               Mean episode length: 131.86
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 44.6347
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 24.4583
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 2.36s
                      Time elapsed: 00:24:31
                               ETA: 01:00:50

################################################################################
                     [1m Learning iteration 575/2000 [0m                      

                       Computation: 40546 steps/s (collection: 2.302s, learning 0.122s)
             Mean action noise std: 2.32
          Mean value_function loss: 315.7829
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 58.6666
                       Mean reward: 218.61
               Mean episode length: 136.85
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 45.3528
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 26.7917
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 2.42s
                      Time elapsed: 00:24:34
                               ETA: 01:00:47

################################################################################
                     [1m Learning iteration 576/2000 [0m                      

                       Computation: 40606 steps/s (collection: 2.300s, learning 0.121s)
             Mean action noise std: 2.32
          Mean value_function loss: 319.5043
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 58.6660
                       Mean reward: 238.70
               Mean episode length: 147.39
    Episode_Reward/reaching_object: 0.7771
     Episode_Reward/lifting_object: 44.6383
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 24.9583
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 2.42s
                      Time elapsed: 00:24:36
                               ETA: 01:00:44

################################################################################
                     [1m Learning iteration 577/2000 [0m                      

                       Computation: 38621 steps/s (collection: 2.350s, learning 0.195s)
             Mean action noise std: 2.32
          Mean value_function loss: 289.4582
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 58.6658
                       Mean reward: 236.85
               Mean episode length: 145.06
    Episode_Reward/reaching_object: 0.7748
     Episode_Reward/lifting_object: 44.2779
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 24.3333
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 2.55s
                      Time elapsed: 00:24:39
                               ETA: 01:00:41

################################################################################
                     [1m Learning iteration 578/2000 [0m                      

                       Computation: 39860 steps/s (collection: 2.354s, learning 0.112s)
             Mean action noise std: 2.32
          Mean value_function loss: 327.1710
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 58.6655
                       Mean reward: 221.96
               Mean episode length: 130.78
    Episode_Reward/reaching_object: 0.7228
     Episode_Reward/lifting_object: 41.2285
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 25.3750
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 2.47s
                      Time elapsed: 00:24:41
                               ETA: 01:00:39

################################################################################
                     [1m Learning iteration 579/2000 [0m                      

                       Computation: 37488 steps/s (collection: 2.450s, learning 0.172s)
             Mean action noise std: 2.32
          Mean value_function loss: 345.1979
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 58.6653
                       Mean reward: 232.50
               Mean episode length: 140.49
    Episode_Reward/reaching_object: 0.7684
     Episode_Reward/lifting_object: 44.2655
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 23.4167
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 2.62s
                      Time elapsed: 00:24:44
                               ETA: 01:00:36

################################################################################
                     [1m Learning iteration 580/2000 [0m                      

                       Computation: 37564 steps/s (collection: 2.476s, learning 0.141s)
             Mean action noise std: 2.32
          Mean value_function loss: 353.0092
               Mean surrogate loss: 0.0133
                 Mean entropy loss: 58.6650
                       Mean reward: 222.43
               Mean episode length: 144.99
    Episode_Reward/reaching_object: 0.7879
     Episode_Reward/lifting_object: 45.4023
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 20.2917
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 2.62s
                      Time elapsed: 00:24:47
                               ETA: 01:00:34

################################################################################
                     [1m Learning iteration 581/2000 [0m                      

                       Computation: 40517 steps/s (collection: 2.308s, learning 0.119s)
             Mean action noise std: 2.32
          Mean value_function loss: 315.6027
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 58.6650
                       Mean reward: 219.46
               Mean episode length: 140.68
    Episode_Reward/reaching_object: 0.8072
     Episode_Reward/lifting_object: 45.8472
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 20.0833
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 2.43s
                      Time elapsed: 00:24:49
                               ETA: 01:00:31

################################################################################
                     [1m Learning iteration 582/2000 [0m                      

                       Computation: 40516 steps/s (collection: 2.326s, learning 0.100s)
             Mean action noise std: 2.32
          Mean value_function loss: 335.4454
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 58.6648
                       Mean reward: 246.93
               Mean episode length: 144.02
    Episode_Reward/reaching_object: 0.8371
     Episode_Reward/lifting_object: 51.2254
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 21.7917
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 2.43s
                      Time elapsed: 00:24:51
                               ETA: 01:00:28

################################################################################
                     [1m Learning iteration 583/2000 [0m                      

                       Computation: 42584 steps/s (collection: 2.208s, learning 0.101s)
             Mean action noise std: 2.32
          Mean value_function loss: 334.5886
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 58.6648
                       Mean reward: 295.73
               Mean episode length: 161.66
    Episode_Reward/reaching_object: 0.8657
     Episode_Reward/lifting_object: 53.7237
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 21.2083
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 2.31s
                      Time elapsed: 00:24:54
                               ETA: 01:00:25

################################################################################
                     [1m Learning iteration 584/2000 [0m                      

                       Computation: 38532 steps/s (collection: 2.353s, learning 0.198s)
             Mean action noise std: 2.32
          Mean value_function loss: 354.6516
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 58.6649
                       Mean reward: 216.32
               Mean episode length: 133.70
    Episode_Reward/reaching_object: 0.8315
     Episode_Reward/lifting_object: 50.1110
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 21.5417
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 2.55s
                      Time elapsed: 00:24:56
                               ETA: 01:00:22

################################################################################
                     [1m Learning iteration 585/2000 [0m                      

                       Computation: 40912 steps/s (collection: 2.309s, learning 0.094s)
             Mean action noise std: 2.32
          Mean value_function loss: 390.2021
               Mean surrogate loss: 0.0121
                 Mean entropy loss: 58.6653
                       Mean reward: 245.88
               Mean episode length: 145.64
    Episode_Reward/reaching_object: 0.8605
     Episode_Reward/lifting_object: 52.7513
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 21.8333
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 2.40s
                      Time elapsed: 00:24:59
                               ETA: 01:00:19

################################################################################
                     [1m Learning iteration 586/2000 [0m                      

                       Computation: 39360 steps/s (collection: 2.372s, learning 0.126s)
             Mean action noise std: 2.32
          Mean value_function loss: 317.4247
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 58.6654
                       Mean reward: 281.74
               Mean episode length: 153.49
    Episode_Reward/reaching_object: 0.8631
     Episode_Reward/lifting_object: 52.2745
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 19.6667
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 2.50s
                      Time elapsed: 00:25:01
                               ETA: 01:00:17

################################################################################
                     [1m Learning iteration 587/2000 [0m                      

                       Computation: 41939 steps/s (collection: 2.227s, learning 0.117s)
             Mean action noise std: 2.32
          Mean value_function loss: 379.3324
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 58.6655
                       Mean reward: 256.02
               Mean episode length: 152.42
    Episode_Reward/reaching_object: 0.8609
     Episode_Reward/lifting_object: 53.5551
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 19.9167
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 2.34s
                      Time elapsed: 00:25:03
                               ETA: 01:00:14

################################################################################
                     [1m Learning iteration 588/2000 [0m                      

                       Computation: 39571 steps/s (collection: 2.299s, learning 0.186s)
             Mean action noise std: 2.32
          Mean value_function loss: 352.0184
               Mean surrogate loss: 0.0110
                 Mean entropy loss: 58.6655
                       Mean reward: 256.57
               Mean episode length: 143.21
    Episode_Reward/reaching_object: 0.8669
     Episode_Reward/lifting_object: 54.8800
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 21.6250
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 2.48s
                      Time elapsed: 00:25:06
                               ETA: 01:00:11

################################################################################
                     [1m Learning iteration 589/2000 [0m                      

                       Computation: 40223 steps/s (collection: 2.334s, learning 0.110s)
             Mean action noise std: 2.32
          Mean value_function loss: 336.4666
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 58.6655
                       Mean reward: 315.47
               Mean episode length: 164.62
    Episode_Reward/reaching_object: 0.8761
     Episode_Reward/lifting_object: 55.5139
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 20.1250
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 2.44s
                      Time elapsed: 00:25:08
                               ETA: 01:00:08

################################################################################
                     [1m Learning iteration 590/2000 [0m                      

                       Computation: 41064 steps/s (collection: 2.287s, learning 0.107s)
             Mean action noise std: 2.32
          Mean value_function loss: 344.7467
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 58.6656
                       Mean reward: 270.19
               Mean episode length: 150.13
    Episode_Reward/reaching_object: 0.8413
     Episode_Reward/lifting_object: 53.1572
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 22.5833
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 2.39s
                      Time elapsed: 00:25:11
                               ETA: 01:00:05

################################################################################
                     [1m Learning iteration 591/2000 [0m                      

                       Computation: 41466 steps/s (collection: 2.273s, learning 0.098s)
             Mean action noise std: 2.32
          Mean value_function loss: 358.6419
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 58.6657
                       Mean reward: 304.66
               Mean episode length: 163.54
    Episode_Reward/reaching_object: 0.8358
     Episode_Reward/lifting_object: 52.1144
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 20.3333
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 2.37s
                      Time elapsed: 00:25:13
                               ETA: 01:00:02

################################################################################
                     [1m Learning iteration 592/2000 [0m                      

                       Computation: 39676 steps/s (collection: 2.377s, learning 0.101s)
             Mean action noise std: 2.32
          Mean value_function loss: 433.7987
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 58.6657
                       Mean reward: 251.31
               Mean episode length: 145.60
    Episode_Reward/reaching_object: 0.8217
     Episode_Reward/lifting_object: 51.5718
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 22.4583
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 2.48s
                      Time elapsed: 00:25:16
                               ETA: 00:59:59

################################################################################
                     [1m Learning iteration 593/2000 [0m                      

                       Computation: 38986 steps/s (collection: 2.353s, learning 0.168s)
             Mean action noise std: 2.32
          Mean value_function loss: 413.6640
               Mean surrogate loss: 0.0419
                 Mean entropy loss: 58.6660
                       Mean reward: 263.17
               Mean episode length: 138.53
    Episode_Reward/reaching_object: 0.8241
     Episode_Reward/lifting_object: 51.6067
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 21.5417
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 2.52s
                      Time elapsed: 00:25:18
                               ETA: 00:59:57

################################################################################
                     [1m Learning iteration 594/2000 [0m                      

                       Computation: 40590 steps/s (collection: 2.316s, learning 0.106s)
             Mean action noise std: 2.32
          Mean value_function loss: 374.7829
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 58.6661
                       Mean reward: 299.06
               Mean episode length: 155.16
    Episode_Reward/reaching_object: 0.8514
     Episode_Reward/lifting_object: 55.4632
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 21.1667
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 2.42s
                      Time elapsed: 00:25:21
                               ETA: 00:59:54

################################################################################
                     [1m Learning iteration 595/2000 [0m                      

                       Computation: 39894 steps/s (collection: 2.349s, learning 0.116s)
             Mean action noise std: 2.32
          Mean value_function loss: 358.3781
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 58.6657
                       Mean reward: 265.63
               Mean episode length: 142.78
    Episode_Reward/reaching_object: 0.8447
     Episode_Reward/lifting_object: 54.6706
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 22.5417
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 2.46s
                      Time elapsed: 00:25:23
                               ETA: 00:59:51

################################################################################
                     [1m Learning iteration 596/2000 [0m                      

                       Computation: 40772 steps/s (collection: 2.294s, learning 0.117s)
             Mean action noise std: 2.32
          Mean value_function loss: 358.3866
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.6647
                       Mean reward: 271.37
               Mean episode length: 151.12
    Episode_Reward/reaching_object: 0.8449
     Episode_Reward/lifting_object: 55.0402
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 21.8750
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 2.41s
                      Time elapsed: 00:25:25
                               ETA: 00:59:48

################################################################################
                     [1m Learning iteration 597/2000 [0m                      

                       Computation: 41469 steps/s (collection: 2.259s, learning 0.111s)
             Mean action noise std: 2.32
          Mean value_function loss: 383.7623
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 58.6638
                       Mean reward: 268.16
               Mean episode length: 146.24
    Episode_Reward/reaching_object: 0.8267
     Episode_Reward/lifting_object: 53.6104
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 20.3750
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 2.37s
                      Time elapsed: 00:25:28
                               ETA: 00:59:45

################################################################################
                     [1m Learning iteration 598/2000 [0m                      

                       Computation: 41966 steps/s (collection: 2.202s, learning 0.140s)
             Mean action noise std: 2.32
          Mean value_function loss: 398.7673
               Mean surrogate loss: 0.0115
                 Mean entropy loss: 58.6634
                       Mean reward: 256.51
               Mean episode length: 151.95
    Episode_Reward/reaching_object: 0.8421
     Episode_Reward/lifting_object: 53.5209
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 21.3750
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 2.34s
                      Time elapsed: 00:25:30
                               ETA: 00:59:42

################################################################################
                     [1m Learning iteration 599/2000 [0m                      

                       Computation: 40884 steps/s (collection: 2.304s, learning 0.100s)
             Mean action noise std: 2.32
          Mean value_function loss: 388.9890
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 58.6634
                       Mean reward: 289.32
               Mean episode length: 153.27
    Episode_Reward/reaching_object: 0.8469
     Episode_Reward/lifting_object: 54.6855
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 20.7083
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 2.40s
                      Time elapsed: 00:25:33
                               ETA: 00:59:39

################################################################################
                     [1m Learning iteration 600/2000 [0m                      

                       Computation: 38147 steps/s (collection: 2.331s, learning 0.246s)
             Mean action noise std: 2.32
          Mean value_function loss: 401.9870
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 58.6635
                       Mean reward: 299.18
               Mean episode length: 160.58
    Episode_Reward/reaching_object: 0.8993
     Episode_Reward/lifting_object: 62.0452
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 20.0833
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 2.58s
                      Time elapsed: 00:25:35
                               ETA: 00:59:37

################################################################################
                     [1m Learning iteration 601/2000 [0m                      

                       Computation: 38986 steps/s (collection: 2.412s, learning 0.110s)
             Mean action noise std: 2.32
          Mean value_function loss: 381.8604
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 58.6636
                       Mean reward: 266.57
               Mean episode length: 152.68
    Episode_Reward/reaching_object: 0.8593
     Episode_Reward/lifting_object: 56.9913
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 20.7083
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 2.52s
                      Time elapsed: 00:25:38
                               ETA: 00:59:34

################################################################################
                     [1m Learning iteration 602/2000 [0m                      

                       Computation: 38382 steps/s (collection: 2.388s, learning 0.173s)
             Mean action noise std: 2.32
          Mean value_function loss: 356.0494
               Mean surrogate loss: 0.0095
                 Mean entropy loss: 58.6636
                       Mean reward: 361.89
               Mean episode length: 164.66
    Episode_Reward/reaching_object: 0.8576
     Episode_Reward/lifting_object: 58.9819
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.2917
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 2.56s
                      Time elapsed: 00:25:40
                               ETA: 00:59:32

################################################################################
                     [1m Learning iteration 603/2000 [0m                      

                       Computation: 40415 steps/s (collection: 2.325s, learning 0.107s)
             Mean action noise std: 2.32
          Mean value_function loss: 341.2627
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 58.6637
                       Mean reward: 295.94
               Mean episode length: 158.96
    Episode_Reward/reaching_object: 0.8835
     Episode_Reward/lifting_object: 59.7353
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 20.7917
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 2.43s
                      Time elapsed: 00:25:43
                               ETA: 00:59:29

################################################################################
                     [1m Learning iteration 604/2000 [0m                      

                       Computation: 41738 steps/s (collection: 2.233s, learning 0.123s)
             Mean action noise std: 2.32
          Mean value_function loss: 358.8707
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 58.6637
                       Mean reward: 246.71
               Mean episode length: 144.92
    Episode_Reward/reaching_object: 0.8810
     Episode_Reward/lifting_object: 58.4489
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 21.5417
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 2.36s
                      Time elapsed: 00:25:45
                               ETA: 00:59:26

################################################################################
                     [1m Learning iteration 605/2000 [0m                      

                       Computation: 43318 steps/s (collection: 2.167s, learning 0.102s)
             Mean action noise std: 2.32
          Mean value_function loss: 397.7583
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 58.6638
                       Mean reward: 259.58
               Mean episode length: 148.71
    Episode_Reward/reaching_object: 0.8367
     Episode_Reward/lifting_object: 53.6933
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 20.5000
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 2.27s
                      Time elapsed: 00:25:47
                               ETA: 00:59:23

################################################################################
                     [1m Learning iteration 606/2000 [0m                      

                       Computation: 41891 steps/s (collection: 2.196s, learning 0.151s)
             Mean action noise std: 2.32
          Mean value_function loss: 369.6118
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 58.6638
                       Mean reward: 256.04
               Mean episode length: 145.11
    Episode_Reward/reaching_object: 0.8588
     Episode_Reward/lifting_object: 54.8518
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 22.7917
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 2.35s
                      Time elapsed: 00:25:50
                               ETA: 00:59:19

################################################################################
                     [1m Learning iteration 607/2000 [0m                      

                       Computation: 37507 steps/s (collection: 2.504s, learning 0.117s)
             Mean action noise std: 2.32
          Mean value_function loss: 407.2692
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 58.6638
                       Mean reward: 244.01
               Mean episode length: 146.03
    Episode_Reward/reaching_object: 0.8541
     Episode_Reward/lifting_object: 54.8434
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 22.5833
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 2.62s
                      Time elapsed: 00:25:52
                               ETA: 00:59:17

################################################################################
                     [1m Learning iteration 608/2000 [0m                      

                       Computation: 43071 steps/s (collection: 2.184s, learning 0.098s)
             Mean action noise std: 2.32
          Mean value_function loss: 432.2900
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 58.6640
                       Mean reward: 220.53
               Mean episode length: 143.93
    Episode_Reward/reaching_object: 0.8291
     Episode_Reward/lifting_object: 50.3140
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 22.7083
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 2.28s
                      Time elapsed: 00:25:55
                               ETA: 00:59:14

################################################################################
                     [1m Learning iteration 609/2000 [0m                      

                       Computation: 37894 steps/s (collection: 2.461s, learning 0.133s)
             Mean action noise std: 2.32
          Mean value_function loss: 396.6287
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 58.6650
                       Mean reward: 211.19
               Mean episode length: 137.60
    Episode_Reward/reaching_object: 0.7957
     Episode_Reward/lifting_object: 48.2493
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 21.3750
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 2.59s
                      Time elapsed: 00:25:57
                               ETA: 00:59:11

################################################################################
                     [1m Learning iteration 610/2000 [0m                      

                       Computation: 42611 steps/s (collection: 2.212s, learning 0.095s)
             Mean action noise std: 2.32
          Mean value_function loss: 435.9093
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 58.6656
                       Mean reward: 220.50
               Mean episode length: 150.14
    Episode_Reward/reaching_object: 0.7950
     Episode_Reward/lifting_object: 45.4873
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 23.5833
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 2.31s
                      Time elapsed: 00:25:59
                               ETA: 00:59:08

################################################################################
                     [1m Learning iteration 611/2000 [0m                      

                       Computation: 40905 steps/s (collection: 2.236s, learning 0.168s)
             Mean action noise std: 2.32
          Mean value_function loss: 440.3623
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 58.6651
                       Mean reward: 247.92
               Mean episode length: 147.89
    Episode_Reward/reaching_object: 0.7901
     Episode_Reward/lifting_object: 45.8642
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0686
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 23.5417
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 2.40s
                      Time elapsed: 00:26:02
                               ETA: 00:59:05

################################################################################
                     [1m Learning iteration 612/2000 [0m                      

                       Computation: 40505 steps/s (collection: 2.300s, learning 0.127s)
             Mean action noise std: 2.32
          Mean value_function loss: 423.1754
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 58.6652
                       Mean reward: 224.93
               Mean episode length: 142.82
    Episode_Reward/reaching_object: 0.7890
     Episode_Reward/lifting_object: 45.1253
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 22.9583
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 2.43s
                      Time elapsed: 00:26:04
                               ETA: 00:59:03

################################################################################
                     [1m Learning iteration 613/2000 [0m                      

                       Computation: 40250 steps/s (collection: 2.344s, learning 0.099s)
             Mean action noise std: 2.32
          Mean value_function loss: 395.8930
               Mean surrogate loss: 0.0185
                 Mean entropy loss: 58.6652
                       Mean reward: 224.80
               Mean episode length: 131.63
    Episode_Reward/reaching_object: 0.7483
     Episode_Reward/lifting_object: 42.8849
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 22.2500
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 2.44s
                      Time elapsed: 00:26:07
                               ETA: 00:59:00

################################################################################
                     [1m Learning iteration 614/2000 [0m                      

                       Computation: 42656 steps/s (collection: 2.199s, learning 0.106s)
             Mean action noise std: 2.32
          Mean value_function loss: 411.8211
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 58.6652
                       Mean reward: 242.87
               Mean episode length: 149.95
    Episode_Reward/reaching_object: 0.7999
     Episode_Reward/lifting_object: 48.3353
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 22.4583
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 2.30s
                      Time elapsed: 00:26:09
                               ETA: 00:58:57

################################################################################
                     [1m Learning iteration 615/2000 [0m                      

                       Computation: 42670 steps/s (collection: 2.199s, learning 0.105s)
             Mean action noise std: 2.32
          Mean value_function loss: 395.1256
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 58.6653
                       Mean reward: 244.08
               Mean episode length: 139.32
    Episode_Reward/reaching_object: 0.7758
     Episode_Reward/lifting_object: 46.8630
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 22.8333
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 2.30s
                      Time elapsed: 00:26:11
                               ETA: 00:58:54

################################################################################
                     [1m Learning iteration 616/2000 [0m                      

                       Computation: 41133 steps/s (collection: 2.274s, learning 0.116s)
             Mean action noise std: 2.32
          Mean value_function loss: 428.3750
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 58.6651
                       Mean reward: 254.25
               Mean episode length: 142.48
    Episode_Reward/reaching_object: 0.8383
     Episode_Reward/lifting_object: 53.6747
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 23.2917
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 2.39s
                      Time elapsed: 00:26:14
                               ETA: 00:58:51

################################################################################
                     [1m Learning iteration 617/2000 [0m                      

                       Computation: 41497 steps/s (collection: 2.269s, learning 0.100s)
             Mean action noise std: 2.32
          Mean value_function loss: 445.6221
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 58.6652
                       Mean reward: 254.93
               Mean episode length: 147.66
    Episode_Reward/reaching_object: 0.8334
     Episode_Reward/lifting_object: 52.7158
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 22.2917
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 2.37s
                      Time elapsed: 00:26:16
                               ETA: 00:58:48

################################################################################
                     [1m Learning iteration 618/2000 [0m                      

                       Computation: 39239 steps/s (collection: 2.401s, learning 0.104s)
             Mean action noise std: 2.32
          Mean value_function loss: 425.2279
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 58.6657
                       Mean reward: 261.70
               Mean episode length: 146.38
    Episode_Reward/reaching_object: 0.8215
     Episode_Reward/lifting_object: 51.5722
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 23.2917
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 2.51s
                      Time elapsed: 00:26:19
                               ETA: 00:58:45

################################################################################
                     [1m Learning iteration 619/2000 [0m                      

                       Computation: 43359 steps/s (collection: 2.174s, learning 0.093s)
             Mean action noise std: 2.32
          Mean value_function loss: 436.7440
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 58.6658
                       Mean reward: 240.16
               Mean episode length: 134.05
    Episode_Reward/reaching_object: 0.7852
     Episode_Reward/lifting_object: 50.7832
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 22.4583
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 2.27s
                      Time elapsed: 00:26:21
                               ETA: 00:58:42

################################################################################
                     [1m Learning iteration 620/2000 [0m                      

                       Computation: 43040 steps/s (collection: 2.163s, learning 0.121s)
             Mean action noise std: 2.32
          Mean value_function loss: 377.8710
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 58.6656
                       Mean reward: 267.29
               Mean episode length: 138.22
    Episode_Reward/reaching_object: 0.7940
     Episode_Reward/lifting_object: 53.1924
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 22.7083
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 2.28s
                      Time elapsed: 00:26:23
                               ETA: 00:58:39

################################################################################
                     [1m Learning iteration 621/2000 [0m                      

                       Computation: 38243 steps/s (collection: 2.404s, learning 0.167s)
             Mean action noise std: 2.32
          Mean value_function loss: 411.9893
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 58.6655
                       Mean reward: 332.69
               Mean episode length: 159.98
    Episode_Reward/reaching_object: 0.8711
     Episode_Reward/lifting_object: 60.4092
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 20.3333
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 2.57s
                      Time elapsed: 00:26:26
                               ETA: 00:58:36

################################################################################
                     [1m Learning iteration 622/2000 [0m                      

                       Computation: 36416 steps/s (collection: 2.580s, learning 0.120s)
             Mean action noise std: 2.32
          Mean value_function loss: 417.6692
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 58.6653
                       Mean reward: 267.74
               Mean episode length: 143.96
    Episode_Reward/reaching_object: 0.8430
     Episode_Reward/lifting_object: 57.5982
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 21.1667
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 2.70s
                      Time elapsed: 00:26:28
                               ETA: 00:58:34

################################################################################
                     [1m Learning iteration 623/2000 [0m                      

                       Computation: 41477 steps/s (collection: 2.241s, learning 0.129s)
             Mean action noise std: 2.32
          Mean value_function loss: 433.7128
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.6638
                       Mean reward: 289.47
               Mean episode length: 144.68
    Episode_Reward/reaching_object: 0.8389
     Episode_Reward/lifting_object: 58.3781
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 20.3750
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 2.37s
                      Time elapsed: 00:26:31
                               ETA: 00:58:31

################################################################################
                     [1m Learning iteration 624/2000 [0m                      

                       Computation: 40240 steps/s (collection: 2.286s, learning 0.157s)
             Mean action noise std: 2.32
          Mean value_function loss: 418.3928
               Mean surrogate loss: 0.0114
                 Mean entropy loss: 58.6633
                       Mean reward: 261.61
               Mean episode length: 145.85
    Episode_Reward/reaching_object: 0.8570
     Episode_Reward/lifting_object: 58.8493
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 20.8750
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 2.44s
                      Time elapsed: 00:26:33
                               ETA: 00:58:28

################################################################################
                     [1m Learning iteration 625/2000 [0m                      

                       Computation: 43482 steps/s (collection: 2.162s, learning 0.099s)
             Mean action noise std: 2.32
          Mean value_function loss: 394.6220
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 58.6633
                       Mean reward: 319.38
               Mean episode length: 148.21
    Episode_Reward/reaching_object: 0.8921
     Episode_Reward/lifting_object: 62.9227
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 19.6250
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 2.26s
                      Time elapsed: 00:26:35
                               ETA: 00:58:25

################################################################################
                     [1m Learning iteration 626/2000 [0m                      

                       Computation: 43223 steps/s (collection: 2.173s, learning 0.102s)
             Mean action noise std: 2.32
          Mean value_function loss: 402.7347
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 58.6633
                       Mean reward: 335.53
               Mean episode length: 157.57
    Episode_Reward/reaching_object: 0.8785
     Episode_Reward/lifting_object: 63.5010
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 20.5417
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 2.27s
                      Time elapsed: 00:26:38
                               ETA: 00:58:22

################################################################################
                     [1m Learning iteration 627/2000 [0m                      

                       Computation: 42057 steps/s (collection: 2.240s, learning 0.098s)
             Mean action noise std: 2.32
          Mean value_function loss: 421.3199
               Mean surrogate loss: 0.0098
                 Mean entropy loss: 58.6632
                       Mean reward: 314.41
               Mean episode length: 153.00
    Episode_Reward/reaching_object: 0.8668
     Episode_Reward/lifting_object: 60.7926
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 21.2917
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 2.34s
                      Time elapsed: 00:26:40
                               ETA: 00:58:19

################################################################################
                     [1m Learning iteration 628/2000 [0m                      

                       Computation: 44513 steps/s (collection: 2.110s, learning 0.098s)
             Mean action noise std: 2.32
          Mean value_function loss: 399.2118
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 58.6632
                       Mean reward: 369.71
               Mean episode length: 161.94
    Episode_Reward/reaching_object: 0.9118
     Episode_Reward/lifting_object: 67.8231
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 19.0833
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 2.21s
                      Time elapsed: 00:26:42
                               ETA: 00:58:16

################################################################################
                     [1m Learning iteration 629/2000 [0m                      

                       Computation: 40861 steps/s (collection: 2.284s, learning 0.122s)
             Mean action noise std: 2.32
          Mean value_function loss: 416.0331
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 58.6632
                       Mean reward: 324.54
               Mean episode length: 152.97
    Episode_Reward/reaching_object: 0.8810
     Episode_Reward/lifting_object: 64.0505
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 21.2917
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 2.41s
                      Time elapsed: 00:26:45
                               ETA: 00:58:13

################################################################################
                     [1m Learning iteration 630/2000 [0m                      

                       Computation: 38606 steps/s (collection: 2.398s, learning 0.149s)
             Mean action noise std: 2.32
          Mean value_function loss: 460.3647
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 58.6624
                       Mean reward: 308.75
               Mean episode length: 151.21
    Episode_Reward/reaching_object: 0.8850
     Episode_Reward/lifting_object: 65.3548
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 21.5833
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 2.55s
                      Time elapsed: 00:26:47
                               ETA: 00:58:10

################################################################################
                     [1m Learning iteration 631/2000 [0m                      

                       Computation: 41597 steps/s (collection: 2.268s, learning 0.095s)
             Mean action noise std: 2.32
          Mean value_function loss: 450.2229
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 58.6608
                       Mean reward: 314.98
               Mean episode length: 146.08
    Episode_Reward/reaching_object: 0.8439
     Episode_Reward/lifting_object: 62.7496
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 22.5417
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 2.36s
                      Time elapsed: 00:26:50
                               ETA: 00:58:07

################################################################################
                     [1m Learning iteration 632/2000 [0m                      

                       Computation: 39618 steps/s (collection: 2.306s, learning 0.176s)
             Mean action noise std: 2.32
          Mean value_function loss: 468.7881
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 58.6593
                       Mean reward: 346.87
               Mean episode length: 150.95
    Episode_Reward/reaching_object: 0.8586
     Episode_Reward/lifting_object: 64.0092
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.7917
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 2.48s
                      Time elapsed: 00:26:52
                               ETA: 00:58:05

################################################################################
                     [1m Learning iteration 633/2000 [0m                      

                       Computation: 41548 steps/s (collection: 2.267s, learning 0.099s)
             Mean action noise std: 2.32
          Mean value_function loss: 457.7085
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 58.6586
                       Mean reward: 332.06
               Mean episode length: 147.98
    Episode_Reward/reaching_object: 0.8586
     Episode_Reward/lifting_object: 64.0738
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 22.6250
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 2.37s
                      Time elapsed: 00:26:54
                               ETA: 00:58:02

################################################################################
                     [1m Learning iteration 634/2000 [0m                      

                       Computation: 42131 steps/s (collection: 2.187s, learning 0.146s)
             Mean action noise std: 2.32
          Mean value_function loss: 488.3127
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 58.6569
                       Mean reward: 343.22
               Mean episode length: 157.16
    Episode_Reward/reaching_object: 0.8726
     Episode_Reward/lifting_object: 65.7488
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 21.7917
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 2.33s
                      Time elapsed: 00:26:57
                               ETA: 00:57:59

################################################################################
                     [1m Learning iteration 635/2000 [0m                      

                       Computation: 42643 steps/s (collection: 2.204s, learning 0.101s)
             Mean action noise std: 2.32
          Mean value_function loss: 589.3336
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 58.6547
                       Mean reward: 272.09
               Mean episode length: 129.21
    Episode_Reward/reaching_object: 0.8396
     Episode_Reward/lifting_object: 64.0515
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 20.9167
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 2.31s
                      Time elapsed: 00:26:59
                               ETA: 00:57:56

################################################################################
                     [1m Learning iteration 636/2000 [0m                      

                       Computation: 42369 steps/s (collection: 2.134s, learning 0.186s)
             Mean action noise std: 2.32
          Mean value_function loss: 486.0525
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 58.6540
                       Mean reward: 353.88
               Mean episode length: 153.62
    Episode_Reward/reaching_object: 0.8689
     Episode_Reward/lifting_object: 67.0610
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 20.8750
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 2.32s
                      Time elapsed: 00:27:01
                               ETA: 00:57:53

################################################################################
                     [1m Learning iteration 637/2000 [0m                      

                       Computation: 43716 steps/s (collection: 2.149s, learning 0.100s)
             Mean action noise std: 2.32
          Mean value_function loss: 521.1378
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 58.6539
                       Mean reward: 342.40
               Mean episode length: 150.24
    Episode_Reward/reaching_object: 0.8702
     Episode_Reward/lifting_object: 68.3612
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 23.2500
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 2.25s
                      Time elapsed: 00:27:04
                               ETA: 00:57:49

################################################################################
                     [1m Learning iteration 638/2000 [0m                      

                       Computation: 44071 steps/s (collection: 2.138s, learning 0.092s)
             Mean action noise std: 2.32
          Mean value_function loss: 524.0737
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 58.6544
                       Mean reward: 350.00
               Mean episode length: 141.09
    Episode_Reward/reaching_object: 0.8540
     Episode_Reward/lifting_object: 68.2205
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 25.8750
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 2.23s
                      Time elapsed: 00:27:06
                               ETA: 00:57:46

################################################################################
                     [1m Learning iteration 639/2000 [0m                      

                       Computation: 43992 steps/s (collection: 2.142s, learning 0.093s)
             Mean action noise std: 2.32
          Mean value_function loss: 547.9968
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 58.6550
                       Mean reward: 392.94
               Mean episode length: 149.20
    Episode_Reward/reaching_object: 0.8802
     Episode_Reward/lifting_object: 71.3837
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 5.5000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 22.9167
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 2.23s
                      Time elapsed: 00:27:08
                               ETA: 00:57:43

################################################################################
                     [1m Learning iteration 640/2000 [0m                      

                       Computation: 42145 steps/s (collection: 2.167s, learning 0.166s)
             Mean action noise std: 2.32
          Mean value_function loss: 522.2608
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 58.6554
                       Mean reward: 340.80
               Mean episode length: 139.79
    Episode_Reward/reaching_object: 0.8391
     Episode_Reward/lifting_object: 67.9784
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 23.0417
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 2.33s
                      Time elapsed: 00:27:10
                               ETA: 00:57:40

################################################################################
                     [1m Learning iteration 641/2000 [0m                      

                       Computation: 42289 steps/s (collection: 2.150s, learning 0.174s)
             Mean action noise std: 2.32
          Mean value_function loss: 513.3202
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 58.6544
                       Mean reward: 334.82
               Mean episode length: 140.79
    Episode_Reward/reaching_object: 0.8514
     Episode_Reward/lifting_object: 68.6927
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 20.5000
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 2.32s
                      Time elapsed: 00:27:13
                               ETA: 00:57:37

################################################################################
                     [1m Learning iteration 642/2000 [0m                      

                       Computation: 42082 steps/s (collection: 2.249s, learning 0.087s)
             Mean action noise std: 2.32
          Mean value_function loss: 487.0509
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 58.6535
                       Mean reward: 366.94
               Mean episode length: 150.14
    Episode_Reward/reaching_object: 0.8588
     Episode_Reward/lifting_object: 69.5818
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 19.3333
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 2.34s
                      Time elapsed: 00:27:15
                               ETA: 00:57:34

################################################################################
                     [1m Learning iteration 643/2000 [0m                      

                       Computation: 39035 steps/s (collection: 2.274s, learning 0.245s)
             Mean action noise std: 2.32
          Mean value_function loss: 471.7726
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 58.6536
                       Mean reward: 391.29
               Mean episode length: 162.93
    Episode_Reward/reaching_object: 0.8885
     Episode_Reward/lifting_object: 71.9912
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 19.0833
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 2.52s
                      Time elapsed: 00:27:18
                               ETA: 00:57:31

################################################################################
                     [1m Learning iteration 644/2000 [0m                      

                       Computation: 41388 steps/s (collection: 2.264s, learning 0.111s)
             Mean action noise std: 2.32
          Mean value_function loss: 508.2643
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 58.6538
                       Mean reward: 422.71
               Mean episode length: 163.37
    Episode_Reward/reaching_object: 0.9180
     Episode_Reward/lifting_object: 77.1679
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.6667
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 2.38s
                      Time elapsed: 00:27:20
                               ETA: 00:57:28

################################################################################
                     [1m Learning iteration 645/2000 [0m                      

                       Computation: 43691 steps/s (collection: 2.149s, learning 0.101s)
             Mean action noise std: 2.32
          Mean value_function loss: 464.0159
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 58.6535
                       Mean reward: 373.48
               Mean episode length: 152.54
    Episode_Reward/reaching_object: 0.9482
     Episode_Reward/lifting_object: 76.4757
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 17.0417
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 2.25s
                      Time elapsed: 00:27:22
                               ETA: 00:57:25

################################################################################
                     [1m Learning iteration 646/2000 [0m                      

                       Computation: 44999 steps/s (collection: 2.079s, learning 0.106s)
             Mean action noise std: 2.32
          Mean value_function loss: 491.7622
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 58.6516
                       Mean reward: 387.27
               Mean episode length: 148.02
    Episode_Reward/reaching_object: 0.9492
     Episode_Reward/lifting_object: 78.6745
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 18.0417
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 2.18s
                      Time elapsed: 00:27:24
                               ETA: 00:57:22

################################################################################
                     [1m Learning iteration 647/2000 [0m                      

                       Computation: 41059 steps/s (collection: 2.215s, learning 0.179s)
             Mean action noise std: 2.32
          Mean value_function loss: 469.4320
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 58.6512
                       Mean reward: 440.81
               Mean episode length: 158.99
    Episode_Reward/reaching_object: 0.9905
     Episode_Reward/lifting_object: 84.4927
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 18.7500
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 2.39s
                      Time elapsed: 00:27:27
                               ETA: 00:57:19

################################################################################
                     [1m Learning iteration 648/2000 [0m                      

                       Computation: 43438 steps/s (collection: 2.175s, learning 0.089s)
             Mean action noise std: 2.32
          Mean value_function loss: 477.8289
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 58.6510
                       Mean reward: 425.64
               Mean episode length: 158.31
    Episode_Reward/reaching_object: 0.9850
     Episode_Reward/lifting_object: 84.1927
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 18.2083
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 2.26s
                      Time elapsed: 00:27:29
                               ETA: 00:57:16

################################################################################
                     [1m Learning iteration 649/2000 [0m                      

                       Computation: 42478 steps/s (collection: 2.155s, learning 0.160s)
             Mean action noise std: 2.32
          Mean value_function loss: 518.5825
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 58.6512
                       Mean reward: 423.89
               Mean episode length: 163.30
    Episode_Reward/reaching_object: 1.0096
     Episode_Reward/lifting_object: 87.0581
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.3333
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 2.31s
                      Time elapsed: 00:27:31
                               ETA: 00:57:13

################################################################################
                     [1m Learning iteration 650/2000 [0m                      

                       Computation: 44621 steps/s (collection: 2.082s, learning 0.121s)
             Mean action noise std: 2.32
          Mean value_function loss: 515.3668
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 58.6519
                       Mean reward: 428.88
               Mean episode length: 164.41
    Episode_Reward/reaching_object: 0.9985
     Episode_Reward/lifting_object: 85.0760
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 17.7917
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 2.20s
                      Time elapsed: 00:27:34
                               ETA: 00:57:10

################################################################################
                     [1m Learning iteration 651/2000 [0m                      

                       Computation: 45527 steps/s (collection: 2.058s, learning 0.102s)
             Mean action noise std: 2.32
          Mean value_function loss: 471.6624
               Mean surrogate loss: 0.0095
                 Mean entropy loss: 58.6523
                       Mean reward: 390.18
               Mean episode length: 149.49
    Episode_Reward/reaching_object: 0.9621
     Episode_Reward/lifting_object: 82.0600
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.7917
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 2.16s
                      Time elapsed: 00:27:36
                               ETA: 00:57:06

################################################################################
                     [1m Learning iteration 652/2000 [0m                      

                       Computation: 45859 steps/s (collection: 2.047s, learning 0.097s)
             Mean action noise std: 2.32
          Mean value_function loss: 465.3906
               Mean surrogate loss: 0.0102
                 Mean entropy loss: 58.6523
                       Mean reward: 430.09
               Mean episode length: 155.11
    Episode_Reward/reaching_object: 0.9848
     Episode_Reward/lifting_object: 84.6778
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 2.14s
                      Time elapsed: 00:27:38
                               ETA: 00:57:03

################################################################################
                     [1m Learning iteration 653/2000 [0m                      

                       Computation: 45182 steps/s (collection: 2.063s, learning 0.113s)
             Mean action noise std: 2.32
          Mean value_function loss: 487.8611
               Mean surrogate loss: 0.0105
                 Mean entropy loss: 58.6523
                       Mean reward: 485.48
               Mean episode length: 181.66
    Episode_Reward/reaching_object: 0.9943
     Episode_Reward/lifting_object: 84.8358
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.7917
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 2.18s
                      Time elapsed: 00:27:40
                               ETA: 00:57:00

################################################################################
                     [1m Learning iteration 654/2000 [0m                      

                       Computation: 45803 steps/s (collection: 2.051s, learning 0.095s)
             Mean action noise std: 2.32
          Mean value_function loss: 471.4317
               Mean surrogate loss: 0.0103
                 Mean entropy loss: 58.6523
                       Mean reward: 429.15
               Mean episode length: 160.03
    Episode_Reward/reaching_object: 0.9671
     Episode_Reward/lifting_object: 81.9323
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.0833
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 2.15s
                      Time elapsed: 00:27:42
                               ETA: 00:56:56

################################################################################
                     [1m Learning iteration 655/2000 [0m                      

                       Computation: 45332 steps/s (collection: 2.045s, learning 0.124s)
             Mean action noise std: 2.32
          Mean value_function loss: 445.5770
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 58.6523
                       Mean reward: 436.93
               Mean episode length: 162.53
    Episode_Reward/reaching_object: 1.0528
     Episode_Reward/lifting_object: 90.8540
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 2.17s
                      Time elapsed: 00:27:44
                               ETA: 00:56:53

################################################################################
                     [1m Learning iteration 656/2000 [0m                      

                       Computation: 45944 steps/s (collection: 2.037s, learning 0.103s)
             Mean action noise std: 2.32
          Mean value_function loss: 447.7064
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 58.6523
                       Mean reward: 542.80
               Mean episode length: 184.56
    Episode_Reward/reaching_object: 1.0702
     Episode_Reward/lifting_object: 94.1151
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 2.14s
                      Time elapsed: 00:27:47
                               ETA: 00:56:50

################################################################################
                     [1m Learning iteration 657/2000 [0m                      

                       Computation: 46290 steps/s (collection: 2.037s, learning 0.087s)
             Mean action noise std: 2.32
          Mean value_function loss: 469.4057
               Mean surrogate loss: 0.0104
                 Mean entropy loss: 58.6523
                       Mean reward: 479.55
               Mean episode length: 172.32
    Episode_Reward/reaching_object: 1.0695
     Episode_Reward/lifting_object: 93.2382
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 2.12s
                      Time elapsed: 00:27:49
                               ETA: 00:56:46

################################################################################
                     [1m Learning iteration 658/2000 [0m                      

                       Computation: 45594 steps/s (collection: 2.066s, learning 0.091s)
             Mean action noise std: 2.32
          Mean value_function loss: 463.8612
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 58.6523
                       Mean reward: 437.19
               Mean episode length: 157.48
    Episode_Reward/reaching_object: 1.0557
     Episode_Reward/lifting_object: 91.6581
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 15.2917
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 2.16s
                      Time elapsed: 00:27:51
                               ETA: 00:56:43

################################################################################
                     [1m Learning iteration 659/2000 [0m                      

                       Computation: 44420 steps/s (collection: 2.123s, learning 0.091s)
             Mean action noise std: 2.32
          Mean value_function loss: 464.5952
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 58.6523
                       Mean reward: 422.23
               Mean episode length: 160.27
    Episode_Reward/reaching_object: 1.0364
     Episode_Reward/lifting_object: 89.8356
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 14.7083
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 2.21s
                      Time elapsed: 00:27:53
                               ETA: 00:56:40

################################################################################
                     [1m Learning iteration 660/2000 [0m                      

                       Computation: 44209 steps/s (collection: 2.116s, learning 0.108s)
             Mean action noise std: 2.32
          Mean value_function loss: 448.9244
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.6522
                       Mean reward: 417.64
               Mean episode length: 159.90
    Episode_Reward/reaching_object: 1.0266
     Episode_Reward/lifting_object: 86.6209
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.7500
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 2.22s
                      Time elapsed: 00:27:55
                               ETA: 00:56:37

################################################################################
                     [1m Learning iteration 661/2000 [0m                      

                       Computation: 44907 steps/s (collection: 2.086s, learning 0.103s)
             Mean action noise std: 2.32
          Mean value_function loss: 460.3286
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.6530
                       Mean reward: 466.61
               Mean episode length: 169.69
    Episode_Reward/reaching_object: 1.0767
     Episode_Reward/lifting_object: 94.9700
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 2.19s
                      Time elapsed: 00:27:57
                               ETA: 00:56:33

################################################################################
                     [1m Learning iteration 662/2000 [0m                      

                       Computation: 45704 steps/s (collection: 2.048s, learning 0.103s)
             Mean action noise std: 2.32
          Mean value_function loss: 498.9488
               Mean surrogate loss: 0.0180
                 Mean entropy loss: 58.6565
                       Mean reward: 454.04
               Mean episode length: 163.31
    Episode_Reward/reaching_object: 0.9988
     Episode_Reward/lifting_object: 84.7641
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.0833
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 2.15s
                      Time elapsed: 00:28:00
                               ETA: 00:56:30

################################################################################
                     [1m Learning iteration 663/2000 [0m                      

                       Computation: 44816 steps/s (collection: 2.081s, learning 0.112s)
             Mean action noise std: 2.33
          Mean value_function loss: 482.0323
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 58.6587
                       Mean reward: 419.35
               Mean episode length: 159.25
    Episode_Reward/reaching_object: 1.0060
     Episode_Reward/lifting_object: 86.3935
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.9167
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 2.19s
                      Time elapsed: 00:28:02
                               ETA: 00:56:27

################################################################################
                     [1m Learning iteration 664/2000 [0m                      

                       Computation: 45733 steps/s (collection: 2.063s, learning 0.086s)
             Mean action noise std: 2.33
          Mean value_function loss: 464.2862
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 58.6601
                       Mean reward: 445.97
               Mean episode length: 158.96
    Episode_Reward/reaching_object: 0.9858
     Episode_Reward/lifting_object: 84.3695
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.9583
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 2.15s
                      Time elapsed: 00:28:04
                               ETA: 00:56:24

################################################################################
                     [1m Learning iteration 665/2000 [0m                      

                       Computation: 46053 steps/s (collection: 2.046s, learning 0.089s)
             Mean action noise std: 2.33
          Mean value_function loss: 459.7908
               Mean surrogate loss: 0.0186
                 Mean entropy loss: 58.6604
                       Mean reward: 400.90
               Mean episode length: 158.31
    Episode_Reward/reaching_object: 0.9871
     Episode_Reward/lifting_object: 84.9984
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 15.2917
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 2.13s
                      Time elapsed: 00:28:06
                               ETA: 00:56:20

################################################################################
                     [1m Learning iteration 666/2000 [0m                      

                       Computation: 26925 steps/s (collection: 3.551s, learning 0.100s)
             Mean action noise std: 2.33
          Mean value_function loss: 450.4028
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 58.6605
                       Mean reward: 461.00
               Mean episode length: 170.53
    Episode_Reward/reaching_object: 1.0473
     Episode_Reward/lifting_object: 92.1365
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 3.65s
                      Time elapsed: 00:28:10
                               ETA: 00:56:20

################################################################################
                     [1m Learning iteration 667/2000 [0m                      

                       Computation: 14259 steps/s (collection: 6.772s, learning 0.122s)
             Mean action noise std: 2.33
          Mean value_function loss: 449.6610
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 58.6605
                       Mean reward: 447.15
               Mean episode length: 167.36
    Episode_Reward/reaching_object: 1.0935
     Episode_Reward/lifting_object: 98.2377
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 6.89s
                      Time elapsed: 00:28:17
                               ETA: 00:56:26

################################################################################
                     [1m Learning iteration 668/2000 [0m                      

                       Computation: 13625 steps/s (collection: 7.024s, learning 0.191s)
             Mean action noise std: 2.33
          Mean value_function loss: 444.8531
               Mean surrogate loss: 0.0136
                 Mean entropy loss: 58.6604
                       Mean reward: 513.49
               Mean episode length: 173.05
    Episode_Reward/reaching_object: 1.0808
     Episode_Reward/lifting_object: 97.7343
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 7.21s
                      Time elapsed: 00:28:24
                               ETA: 00:56:33

################################################################################
                     [1m Learning iteration 669/2000 [0m                      

                       Computation: 14035 steps/s (collection: 6.855s, learning 0.149s)
             Mean action noise std: 2.33
          Mean value_function loss: 407.5090
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 58.6603
                       Mean reward: 462.62
               Mean episode length: 157.13
    Episode_Reward/reaching_object: 1.0697
     Episode_Reward/lifting_object: 97.1350
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 7.00s
                      Time elapsed: 00:28:31
                               ETA: 00:56:39

################################################################################
                     [1m Learning iteration 670/2000 [0m                      

                       Computation: 13820 steps/s (collection: 6.964s, learning 0.149s)
             Mean action noise std: 2.33
          Mean value_function loss: 458.6894
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 58.6601
                       Mean reward: 460.44
               Mean episode length: 170.17
    Episode_Reward/reaching_object: 1.0916
     Episode_Reward/lifting_object: 98.6672
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 7.11s
                      Time elapsed: 00:28:38
                               ETA: 00:56:46

################################################################################
                     [1m Learning iteration 671/2000 [0m                      

                       Computation: 13795 steps/s (collection: 7.010s, learning 0.116s)
             Mean action noise std: 2.33
          Mean value_function loss: 451.3436
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 58.6601
                       Mean reward: 465.32
               Mean episode length: 163.05
    Episode_Reward/reaching_object: 1.0541
     Episode_Reward/lifting_object: 96.4171
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.2917
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 7.13s
                      Time elapsed: 00:28:45
                               ETA: 00:56:52

################################################################################
                     [1m Learning iteration 672/2000 [0m                      

                       Computation: 13870 steps/s (collection: 6.973s, learning 0.115s)
             Mean action noise std: 2.33
          Mean value_function loss: 457.1144
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 58.6601
                       Mean reward: 498.51
               Mean episode length: 168.16
    Episode_Reward/reaching_object: 1.0689
     Episode_Reward/lifting_object: 96.9470
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 7.09s
                      Time elapsed: 00:28:52
                               ETA: 00:56:59

################################################################################
                     [1m Learning iteration 673/2000 [0m                      

                       Computation: 13582 steps/s (collection: 7.099s, learning 0.139s)
             Mean action noise std: 2.33
          Mean value_function loss: 442.1082
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 58.6601
                       Mean reward: 457.04
               Mean episode length: 166.69
    Episode_Reward/reaching_object: 1.0373
     Episode_Reward/lifting_object: 93.2711
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.2917
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 7.24s
                      Time elapsed: 00:28:59
                               ETA: 00:57:05

################################################################################
                     [1m Learning iteration 674/2000 [0m                      

                       Computation: 13746 steps/s (collection: 7.029s, learning 0.123s)
             Mean action noise std: 2.33
          Mean value_function loss: 485.8071
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 58.6601
                       Mean reward: 449.93
               Mean episode length: 155.92
    Episode_Reward/reaching_object: 1.0351
     Episode_Reward/lifting_object: 93.4079
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.8750
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 7.15s
                      Time elapsed: 00:29:07
                               ETA: 00:57:12

################################################################################
                     [1m Learning iteration 675/2000 [0m                      

                       Computation: 19292 steps/s (collection: 4.977s, learning 0.118s)
             Mean action noise std: 2.33
          Mean value_function loss: 463.6642
               Mean surrogate loss: 0.0096
                 Mean entropy loss: 58.6601
                       Mean reward: 473.63
               Mean episode length: 160.87
    Episode_Reward/reaching_object: 1.0329
     Episode_Reward/lifting_object: 93.3912
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.7083
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 5.10s
                      Time elapsed: 00:29:12
                               ETA: 00:57:14

################################################################################
                     [1m Learning iteration 676/2000 [0m                      

                       Computation: 44423 steps/s (collection: 2.095s, learning 0.118s)
             Mean action noise std: 2.33
          Mean value_function loss: 499.9309
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 58.6601
                       Mean reward: 461.68
               Mean episode length: 163.73
    Episode_Reward/reaching_object: 1.0163
     Episode_Reward/lifting_object: 91.6046
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 18.0417
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 2.21s
                      Time elapsed: 00:29:14
                               ETA: 00:57:11

################################################################################
                     [1m Learning iteration 677/2000 [0m                      

                       Computation: 40492 steps/s (collection: 2.289s, learning 0.139s)
             Mean action noise std: 2.33
          Mean value_function loss: 478.9101
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 58.6605
                       Mean reward: 501.19
               Mean episode length: 173.57
    Episode_Reward/reaching_object: 0.9869
     Episode_Reward/lifting_object: 88.5415
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 17.5417
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 2.43s
                      Time elapsed: 00:29:16
                               ETA: 00:57:08

################################################################################
                     [1m Learning iteration 678/2000 [0m                      

                       Computation: 45731 steps/s (collection: 2.049s, learning 0.101s)
             Mean action noise std: 2.33
          Mean value_function loss: 485.7458
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 58.6612
                       Mean reward: 417.72
               Mean episode length: 148.26
    Episode_Reward/reaching_object: 1.0015
     Episode_Reward/lifting_object: 90.6541
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 2.15s
                      Time elapsed: 00:29:18
                               ETA: 00:57:04

################################################################################
                     [1m Learning iteration 679/2000 [0m                      

                       Computation: 45891 steps/s (collection: 2.049s, learning 0.093s)
             Mean action noise std: 2.33
          Mean value_function loss: 444.3514
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 58.6615
                       Mean reward: 492.41
               Mean episode length: 174.22
    Episode_Reward/reaching_object: 1.0038
     Episode_Reward/lifting_object: 89.5413
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 2.14s
                      Time elapsed: 00:29:21
                               ETA: 00:57:01

################################################################################
                     [1m Learning iteration 680/2000 [0m                      

                       Computation: 40374 steps/s (collection: 2.234s, learning 0.200s)
             Mean action noise std: 2.33
          Mean value_function loss: 442.3758
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 58.6616
                       Mean reward: 447.13
               Mean episode length: 157.78
    Episode_Reward/reaching_object: 1.0156
     Episode_Reward/lifting_object: 91.4188
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.7083
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 2.43s
                      Time elapsed: 00:29:23
                               ETA: 00:56:58

################################################################################
                     [1m Learning iteration 681/2000 [0m                      

                       Computation: 38383 steps/s (collection: 2.432s, learning 0.129s)
             Mean action noise std: 2.33
          Mean value_function loss: 442.6088
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 58.6619
                       Mean reward: 482.99
               Mean episode length: 169.50
    Episode_Reward/reaching_object: 1.0662
     Episode_Reward/lifting_object: 95.7082
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 2.56s
                      Time elapsed: 00:29:26
                               ETA: 00:56:55

################################################################################
                     [1m Learning iteration 682/2000 [0m                      

                       Computation: 38438 steps/s (collection: 2.400s, learning 0.158s)
             Mean action noise std: 2.33
          Mean value_function loss: 430.4044
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 58.6620
                       Mean reward: 459.31
               Mean episode length: 165.87
    Episode_Reward/reaching_object: 1.0800
     Episode_Reward/lifting_object: 97.0261
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 2.56s
                      Time elapsed: 00:29:28
                               ETA: 00:56:53

################################################################################
                     [1m Learning iteration 683/2000 [0m                      

                       Computation: 41217 steps/s (collection: 2.203s, learning 0.182s)
             Mean action noise std: 2.33
          Mean value_function loss: 441.1441
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.6615
                       Mean reward: 516.26
               Mean episode length: 176.23
    Episode_Reward/reaching_object: 1.1165
     Episode_Reward/lifting_object: 101.4884
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 2.39s
                      Time elapsed: 00:29:31
                               ETA: 00:56:50

################################################################################
                     [1m Learning iteration 684/2000 [0m                      

                       Computation: 39799 steps/s (collection: 2.289s, learning 0.181s)
             Mean action noise std: 2.33
          Mean value_function loss: 513.2027
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.6615
                       Mean reward: 510.80
               Mean episode length: 174.10
    Episode_Reward/reaching_object: 1.0899
     Episode_Reward/lifting_object: 98.5931
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 2.47s
                      Time elapsed: 00:29:33
                               ETA: 00:56:47

################################################################################
                     [1m Learning iteration 685/2000 [0m                      

                       Computation: 42091 steps/s (collection: 2.228s, learning 0.108s)
             Mean action noise std: 2.33
          Mean value_function loss: 502.3971
               Mean surrogate loss: 0.0142
                 Mean entropy loss: 58.6631
                       Mean reward: 588.55
               Mean episode length: 193.22
    Episode_Reward/reaching_object: 1.1999
     Episode_Reward/lifting_object: 111.4086
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.0417
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 2.34s
                      Time elapsed: 00:29:35
                               ETA: 00:56:44

################################################################################
                     [1m Learning iteration 686/2000 [0m                      

                       Computation: 44510 steps/s (collection: 2.092s, learning 0.117s)
             Mean action noise std: 2.33
          Mean value_function loss: 430.4295
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 58.6634
                       Mean reward: 470.72
               Mean episode length: 167.17
    Episode_Reward/reaching_object: 1.1165
     Episode_Reward/lifting_object: 100.7226
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.6667
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 2.21s
                      Time elapsed: 00:29:38
                               ETA: 00:56:40

################################################################################
                     [1m Learning iteration 687/2000 [0m                      

                       Computation: 44986 steps/s (collection: 2.047s, learning 0.139s)
             Mean action noise std: 2.33
          Mean value_function loss: 476.8905
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 58.6611
                       Mean reward: 463.24
               Mean episode length: 160.60
    Episode_Reward/reaching_object: 1.0967
     Episode_Reward/lifting_object: 99.6647
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 2.19s
                      Time elapsed: 00:29:40
                               ETA: 00:56:37

################################################################################
                     [1m Learning iteration 688/2000 [0m                      

                       Computation: 44639 steps/s (collection: 2.090s, learning 0.112s)
             Mean action noise std: 2.33
          Mean value_function loss: 447.0787
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 58.6599
                       Mean reward: 474.86
               Mean episode length: 168.05
    Episode_Reward/reaching_object: 1.1297
     Episode_Reward/lifting_object: 102.1095
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 2.20s
                      Time elapsed: 00:29:42
                               ETA: 00:56:34

################################################################################
                     [1m Learning iteration 689/2000 [0m                      

                       Computation: 43825 steps/s (collection: 2.110s, learning 0.133s)
             Mean action noise std: 2.33
          Mean value_function loss: 441.1953
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 58.6602
                       Mean reward: 557.76
               Mean episode length: 185.83
    Episode_Reward/reaching_object: 1.1263
     Episode_Reward/lifting_object: 102.3902
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 2.24s
                      Time elapsed: 00:29:44
                               ETA: 00:56:30

################################################################################
                     [1m Learning iteration 690/2000 [0m                      

                       Computation: 45832 steps/s (collection: 2.050s, learning 0.095s)
             Mean action noise std: 2.33
          Mean value_function loss: 447.2850
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 58.6606
                       Mean reward: 428.18
               Mean episode length: 160.69
    Episode_Reward/reaching_object: 1.0508
     Episode_Reward/lifting_object: 93.0662
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 2.14s
                      Time elapsed: 00:29:46
                               ETA: 00:56:27

################################################################################
                     [1m Learning iteration 691/2000 [0m                      

                       Computation: 45223 steps/s (collection: 2.061s, learning 0.113s)
             Mean action noise std: 2.33
          Mean value_function loss: 451.6283
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 58.6608
                       Mean reward: 477.36
               Mean episode length: 171.14
    Episode_Reward/reaching_object: 1.0934
     Episode_Reward/lifting_object: 97.4124
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 2.17s
                      Time elapsed: 00:29:49
                               ETA: 00:56:24

################################################################################
                     [1m Learning iteration 692/2000 [0m                      

                       Computation: 42731 steps/s (collection: 2.183s, learning 0.117s)
             Mean action noise std: 2.33
          Mean value_function loss: 484.8087
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 58.6608
                       Mean reward: 527.98
               Mean episode length: 179.93
    Episode_Reward/reaching_object: 1.1274
     Episode_Reward/lifting_object: 101.8436
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 2.30s
                      Time elapsed: 00:29:51
                               ETA: 00:56:21

################################################################################
                     [1m Learning iteration 693/2000 [0m                      

                       Computation: 43463 steps/s (collection: 2.152s, learning 0.110s)
             Mean action noise std: 2.33
          Mean value_function loss: 425.6688
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 58.6608
                       Mean reward: 481.16
               Mean episode length: 172.27
    Episode_Reward/reaching_object: 1.1227
     Episode_Reward/lifting_object: 101.6251
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 2.26s
                      Time elapsed: 00:29:53
                               ETA: 00:56:17

################################################################################
                     [1m Learning iteration 694/2000 [0m                      

                       Computation: 41312 steps/s (collection: 2.220s, learning 0.159s)
             Mean action noise std: 2.33
          Mean value_function loss: 432.4987
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 58.6608
                       Mean reward: 594.56
               Mean episode length: 195.98
    Episode_Reward/reaching_object: 1.1453
     Episode_Reward/lifting_object: 106.0989
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 2.38s
                      Time elapsed: 00:29:55
                               ETA: 00:56:14

################################################################################
                     [1m Learning iteration 695/2000 [0m                      

                       Computation: 46269 steps/s (collection: 2.022s, learning 0.103s)
             Mean action noise std: 2.33
          Mean value_function loss: 447.4564
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 58.6608
                       Mean reward: 590.03
               Mean episode length: 190.35
    Episode_Reward/reaching_object: 1.1457
     Episode_Reward/lifting_object: 106.8542
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 2.12s
                      Time elapsed: 00:29:58
                               ETA: 00:56:11

################################################################################
                     [1m Learning iteration 696/2000 [0m                      

                       Computation: 44265 steps/s (collection: 2.110s, learning 0.111s)
             Mean action noise std: 2.33
          Mean value_function loss: 421.0191
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 58.6605
                       Mean reward: 568.10
               Mean episode length: 179.01
    Episode_Reward/reaching_object: 1.1291
     Episode_Reward/lifting_object: 105.1539
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 2.22s
                      Time elapsed: 00:30:00
                               ETA: 00:56:08

################################################################################
                     [1m Learning iteration 697/2000 [0m                      

                       Computation: 45417 steps/s (collection: 2.071s, learning 0.094s)
             Mean action noise std: 2.33
          Mean value_function loss: 430.7011
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 58.6603
                       Mean reward: 570.79
               Mean episode length: 184.71
    Episode_Reward/reaching_object: 1.1440
     Episode_Reward/lifting_object: 106.3528
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.6667
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 2.16s
                      Time elapsed: 00:30:02
                               ETA: 00:56:04

################################################################################
                     [1m Learning iteration 698/2000 [0m                      

                       Computation: 40914 steps/s (collection: 2.309s, learning 0.094s)
             Mean action noise std: 2.33
          Mean value_function loss: 437.1118
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 58.6603
                       Mean reward: 561.53
               Mean episode length: 182.45
    Episode_Reward/reaching_object: 1.0790
     Episode_Reward/lifting_object: 100.0833
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.7083
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 2.40s
                      Time elapsed: 00:30:04
                               ETA: 00:56:01

################################################################################
                     [1m Learning iteration 699/2000 [0m                      

                       Computation: 46008 steps/s (collection: 2.049s, learning 0.088s)
             Mean action noise std: 2.33
          Mean value_function loss: 459.0065
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 58.6602
                       Mean reward: 498.60
               Mean episode length: 167.45
    Episode_Reward/reaching_object: 1.1010
     Episode_Reward/lifting_object: 102.2181
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 2.14s
                      Time elapsed: 00:30:07
                               ETA: 00:55:58

################################################################################
                     [1m Learning iteration 700/2000 [0m                      

                       Computation: 46549 steps/s (collection: 2.022s, learning 0.090s)
             Mean action noise std: 2.33
          Mean value_function loss: 507.4706
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 58.6602
                       Mean reward: 532.84
               Mean episode length: 171.06
    Episode_Reward/reaching_object: 1.0608
     Episode_Reward/lifting_object: 99.2395
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.6667
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 2.11s
                      Time elapsed: 00:30:09
                               ETA: 00:55:55

################################################################################
                     [1m Learning iteration 701/2000 [0m                      

                       Computation: 45414 steps/s (collection: 2.075s, learning 0.090s)
             Mean action noise std: 2.33
          Mean value_function loss: 474.0247
               Mean surrogate loss: 0.0096
                 Mean entropy loss: 58.6603
                       Mean reward: 444.12
               Mean episode length: 155.79
    Episode_Reward/reaching_object: 1.0447
     Episode_Reward/lifting_object: 96.4089
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.4167
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 2.16s
                      Time elapsed: 00:30:11
                               ETA: 00:55:51

################################################################################
                     [1m Learning iteration 702/2000 [0m                      

                       Computation: 45998 steps/s (collection: 2.044s, learning 0.094s)
             Mean action noise std: 2.33
          Mean value_function loss: 483.1448
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 58.6603
                       Mean reward: 498.49
               Mean episode length: 165.39
    Episode_Reward/reaching_object: 1.0496
     Episode_Reward/lifting_object: 98.0087
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.7917
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 2.14s
                      Time elapsed: 00:30:13
                               ETA: 00:55:48

################################################################################
                     [1m Learning iteration 703/2000 [0m                      

                       Computation: 44493 steps/s (collection: 2.107s, learning 0.102s)
             Mean action noise std: 2.33
          Mean value_function loss: 453.0802
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 58.6604
                       Mean reward: 558.23
               Mean episode length: 174.76
    Episode_Reward/reaching_object: 1.0320
     Episode_Reward/lifting_object: 96.4957
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 2.21s
                      Time elapsed: 00:30:15
                               ETA: 00:55:44

################################################################################
                     [1m Learning iteration 704/2000 [0m                      

                       Computation: 46094 steps/s (collection: 2.044s, learning 0.089s)
             Mean action noise std: 2.33
          Mean value_function loss: 460.0874
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 58.6607
                       Mean reward: 479.91
               Mean episode length: 164.90
    Episode_Reward/reaching_object: 1.0468
     Episode_Reward/lifting_object: 97.3325
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.7500
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 2.13s
                      Time elapsed: 00:30:17
                               ETA: 00:55:41

################################################################################
                     [1m Learning iteration 705/2000 [0m                      

                       Computation: 45100 steps/s (collection: 2.092s, learning 0.088s)
             Mean action noise std: 2.33
          Mean value_function loss: 455.2089
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 58.6610
                       Mean reward: 495.31
               Mean episode length: 164.84
    Episode_Reward/reaching_object: 1.0872
     Episode_Reward/lifting_object: 103.0832
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 2.18s
                      Time elapsed: 00:30:19
                               ETA: 00:55:38

################################################################################
                     [1m Learning iteration 706/2000 [0m                      

                       Computation: 46763 steps/s (collection: 2.017s, learning 0.086s)
             Mean action noise std: 2.33
          Mean value_function loss: 482.3448
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 58.6613
                       Mean reward: 491.30
               Mean episode length: 165.08
    Episode_Reward/reaching_object: 1.0642
     Episode_Reward/lifting_object: 99.2606
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.9583
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 2.10s
                      Time elapsed: 00:30:22
                               ETA: 00:55:34

################################################################################
                     [1m Learning iteration 707/2000 [0m                      

                       Computation: 39960 steps/s (collection: 2.304s, learning 0.156s)
             Mean action noise std: 2.33
          Mean value_function loss: 459.6851
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 58.6614
                       Mean reward: 494.18
               Mean episode length: 164.21
    Episode_Reward/reaching_object: 1.0685
     Episode_Reward/lifting_object: 99.1748
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 2.46s
                      Time elapsed: 00:30:24
                               ETA: 00:55:32

################################################################################
                     [1m Learning iteration 708/2000 [0m                      

                       Computation: 44830 steps/s (collection: 2.087s, learning 0.106s)
             Mean action noise std: 2.33
          Mean value_function loss: 443.3518
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 58.6616
                       Mean reward: 551.89
               Mean episode length: 174.31
    Episode_Reward/reaching_object: 1.1440
     Episode_Reward/lifting_object: 108.2305
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 2.19s
                      Time elapsed: 00:30:26
                               ETA: 00:55:28

################################################################################
                     [1m Learning iteration 709/2000 [0m                      

                       Computation: 45665 steps/s (collection: 2.060s, learning 0.093s)
             Mean action noise std: 2.33
          Mean value_function loss: 459.6069
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 58.6618
                       Mean reward: 598.69
               Mean episode length: 187.95
    Episode_Reward/reaching_object: 1.1246
     Episode_Reward/lifting_object: 105.6131
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 2.15s
                      Time elapsed: 00:30:28
                               ETA: 00:55:25

################################################################################
                     [1m Learning iteration 710/2000 [0m                      

                       Computation: 41319 steps/s (collection: 2.215s, learning 0.165s)
             Mean action noise std: 2.33
          Mean value_function loss: 447.7635
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 58.6619
                       Mean reward: 591.53
               Mean episode length: 185.92
    Episode_Reward/reaching_object: 1.1637
     Episode_Reward/lifting_object: 109.0325
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 2.38s
                      Time elapsed: 00:30:31
                               ETA: 00:55:22

################################################################################
                     [1m Learning iteration 711/2000 [0m                      

                       Computation: 35354 steps/s (collection: 2.586s, learning 0.195s)
             Mean action noise std: 2.33
          Mean value_function loss: 465.8188
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 58.6621
                       Mean reward: 571.60
               Mean episode length: 182.24
    Episode_Reward/reaching_object: 1.1382
     Episode_Reward/lifting_object: 106.3964
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 2.78s
                      Time elapsed: 00:30:34
                               ETA: 00:55:20

################################################################################
                     [1m Learning iteration 712/2000 [0m                      

                       Computation: 36173 steps/s (collection: 2.556s, learning 0.161s)
             Mean action noise std: 2.33
          Mean value_function loss: 452.1337
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 58.6623
                       Mean reward: 528.79
               Mean episode length: 174.87
    Episode_Reward/reaching_object: 1.0963
     Episode_Reward/lifting_object: 102.1452
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 2.72s
                      Time elapsed: 00:30:36
                               ETA: 00:55:17

################################################################################
                     [1m Learning iteration 713/2000 [0m                      

                       Computation: 39259 steps/s (collection: 2.319s, learning 0.185s)
             Mean action noise std: 2.33
          Mean value_function loss: 443.7568
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 58.6623
                       Mean reward: 493.56
               Mean episode length: 163.48
    Episode_Reward/reaching_object: 1.0742
     Episode_Reward/lifting_object: 99.3834
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 2.50s
                      Time elapsed: 00:30:39
                               ETA: 00:55:15

################################################################################
                     [1m Learning iteration 714/2000 [0m                      

                       Computation: 45507 steps/s (collection: 2.074s, learning 0.086s)
             Mean action noise std: 2.33
          Mean value_function loss: 482.0619
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 58.6620
                       Mean reward: 528.55
               Mean episode length: 178.68
    Episode_Reward/reaching_object: 1.1535
     Episode_Reward/lifting_object: 109.5057
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 2.16s
                      Time elapsed: 00:30:41
                               ETA: 00:55:11

################################################################################
                     [1m Learning iteration 715/2000 [0m                      

                       Computation: 40499 steps/s (collection: 2.311s, learning 0.116s)
             Mean action noise std: 2.33
          Mean value_function loss: 439.8421
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 58.6625
                       Mean reward: 507.46
               Mean episode length: 173.70
    Episode_Reward/reaching_object: 1.1227
     Episode_Reward/lifting_object: 104.1222
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 2.43s
                      Time elapsed: 00:30:43
                               ETA: 00:55:09

################################################################################
                     [1m Learning iteration 716/2000 [0m                      

                       Computation: 42391 steps/s (collection: 2.210s, learning 0.109s)
             Mean action noise std: 2.33
          Mean value_function loss: 452.7758
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 58.6628
                       Mean reward: 500.23
               Mean episode length: 170.69
    Episode_Reward/reaching_object: 1.1034
     Episode_Reward/lifting_object: 101.9341
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 2.32s
                      Time elapsed: 00:30:46
                               ETA: 00:55:06

################################################################################
                     [1m Learning iteration 717/2000 [0m                      

                       Computation: 39904 steps/s (collection: 2.351s, learning 0.112s)
             Mean action noise std: 2.33
          Mean value_function loss: 480.7614
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 58.6627
                       Mean reward: 488.53
               Mean episode length: 165.91
    Episode_Reward/reaching_object: 1.0985
     Episode_Reward/lifting_object: 101.2384
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 2.46s
                      Time elapsed: 00:30:48
                               ETA: 00:55:03

################################################################################
                     [1m Learning iteration 718/2000 [0m                      

                       Computation: 43387 steps/s (collection: 2.158s, learning 0.108s)
             Mean action noise std: 2.33
          Mean value_function loss: 426.2749
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 58.6633
                       Mean reward: 501.37
               Mean episode length: 169.30
    Episode_Reward/reaching_object: 1.0890
     Episode_Reward/lifting_object: 101.3808
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 2.27s
                      Time elapsed: 00:30:50
                               ETA: 00:55:00

################################################################################
                     [1m Learning iteration 719/2000 [0m                      

                       Computation: 44974 steps/s (collection: 2.075s, learning 0.111s)
             Mean action noise std: 2.33
          Mean value_function loss: 506.3041
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 58.6640
                       Mean reward: 516.92
               Mean episode length: 172.39
    Episode_Reward/reaching_object: 1.1157
     Episode_Reward/lifting_object: 104.7456
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.0833
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 2.19s
                      Time elapsed: 00:30:53
                               ETA: 00:54:56

################################################################################
                     [1m Learning iteration 720/2000 [0m                      

                       Computation: 44681 steps/s (collection: 2.107s, learning 0.094s)
             Mean action noise std: 2.33
          Mean value_function loss: 490.8101
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 58.6646
                       Mean reward: 529.25
               Mean episode length: 170.82
    Episode_Reward/reaching_object: 1.1161
     Episode_Reward/lifting_object: 105.5133
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.0417
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 2.20s
                      Time elapsed: 00:30:55
                               ETA: 00:54:53

################################################################################
                     [1m Learning iteration 721/2000 [0m                      

                       Computation: 43379 steps/s (collection: 2.168s, learning 0.098s)
             Mean action noise std: 2.33
          Mean value_function loss: 453.4199
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 58.6658
                       Mean reward: 537.90
               Mean episode length: 177.20
    Episode_Reward/reaching_object: 1.1265
     Episode_Reward/lifting_object: 107.9438
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 2.27s
                      Time elapsed: 00:30:57
                               ETA: 00:54:50

################################################################################
                     [1m Learning iteration 722/2000 [0m                      

                       Computation: 45090 steps/s (collection: 2.088s, learning 0.092s)
             Mean action noise std: 2.33
          Mean value_function loss: 431.5716
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 58.6683
                       Mean reward: 571.00
               Mean episode length: 181.80
    Episode_Reward/reaching_object: 1.1063
     Episode_Reward/lifting_object: 105.5681
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.9583
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 2.18s
                      Time elapsed: 00:30:59
                               ETA: 00:54:47

################################################################################
                     [1m Learning iteration 723/2000 [0m                      

                       Computation: 45243 steps/s (collection: 2.083s, learning 0.090s)
             Mean action noise std: 2.33
          Mean value_function loss: 421.7607
               Mean surrogate loss: 0.0101
                 Mean entropy loss: 58.6691
                       Mean reward: 575.77
               Mean episode length: 187.66
    Episode_Reward/reaching_object: 1.1316
     Episode_Reward/lifting_object: 107.1973
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 2.17s
                      Time elapsed: 00:31:01
                               ETA: 00:54:43

################################################################################
                     [1m Learning iteration 724/2000 [0m                      

                       Computation: 44616 steps/s (collection: 2.103s, learning 0.100s)
             Mean action noise std: 2.33
          Mean value_function loss: 429.4039
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 58.6693
                       Mean reward: 587.98
               Mean episode length: 186.95
    Episode_Reward/reaching_object: 1.1170
     Episode_Reward/lifting_object: 106.0385
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 2.20s
                      Time elapsed: 00:31:04
                               ETA: 00:54:40

################################################################################
                     [1m Learning iteration 725/2000 [0m                      

                       Computation: 46063 steps/s (collection: 2.038s, learning 0.096s)
             Mean action noise std: 2.33
          Mean value_function loss: 413.8015
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 58.6693
                       Mean reward: 593.63
               Mean episode length: 187.22
    Episode_Reward/reaching_object: 1.1382
     Episode_Reward/lifting_object: 108.4260
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 2.13s
                      Time elapsed: 00:31:06
                               ETA: 00:54:37

################################################################################
                     [1m Learning iteration 726/2000 [0m                      

                       Computation: 41037 steps/s (collection: 2.305s, learning 0.091s)
             Mean action noise std: 2.33
          Mean value_function loss: 411.0607
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 58.6694
                       Mean reward: 583.91
               Mean episode length: 181.50
    Episode_Reward/reaching_object: 1.1702
     Episode_Reward/lifting_object: 112.7005
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 2.40s
                      Time elapsed: 00:31:08
                               ETA: 00:54:34

################################################################################
                     [1m Learning iteration 727/2000 [0m                      

                       Computation: 42976 steps/s (collection: 2.172s, learning 0.116s)
             Mean action noise std: 2.33
          Mean value_function loss: 436.6695
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 58.6695
                       Mean reward: 598.72
               Mean episode length: 183.89
    Episode_Reward/reaching_object: 1.1498
     Episode_Reward/lifting_object: 110.0581
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 2.29s
                      Time elapsed: 00:31:10
                               ETA: 00:54:31

################################################################################
                     [1m Learning iteration 728/2000 [0m                      

                       Computation: 38643 steps/s (collection: 2.416s, learning 0.128s)
             Mean action noise std: 2.33
          Mean value_function loss: 439.3633
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 58.6695
                       Mean reward: 603.22
               Mean episode length: 189.33
    Episode_Reward/reaching_object: 1.2041
     Episode_Reward/lifting_object: 115.3661
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 2.54s
                      Time elapsed: 00:31:13
                               ETA: 00:54:28

################################################################################
                     [1m Learning iteration 729/2000 [0m                      

                       Computation: 45606 steps/s (collection: 2.038s, learning 0.118s)
             Mean action noise std: 2.33
          Mean value_function loss: 457.2577
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 58.6695
                       Mean reward: 600.30
               Mean episode length: 187.73
    Episode_Reward/reaching_object: 1.1740
     Episode_Reward/lifting_object: 113.7354
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.7083
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 2.16s
                      Time elapsed: 00:31:15
                               ETA: 00:54:25

################################################################################
                     [1m Learning iteration 730/2000 [0m                      

                       Computation: 44271 steps/s (collection: 2.093s, learning 0.128s)
             Mean action noise std: 2.33
          Mean value_function loss: 464.4049
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 58.6696
                       Mean reward: 510.91
               Mean episode length: 168.51
    Episode_Reward/reaching_object: 1.1510
     Episode_Reward/lifting_object: 109.0993
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 2.22s
                      Time elapsed: 00:31:17
                               ETA: 00:54:22

################################################################################
                     [1m Learning iteration 731/2000 [0m                      

                       Computation: 43518 steps/s (collection: 2.091s, learning 0.168s)
             Mean action noise std: 2.33
          Mean value_function loss: 425.1353
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 58.6693
                       Mean reward: 568.65
               Mean episode length: 182.68
    Episode_Reward/reaching_object: 1.2000
     Episode_Reward/lifting_object: 114.8192
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 2.26s
                      Time elapsed: 00:31:20
                               ETA: 00:54:19

################################################################################
                     [1m Learning iteration 732/2000 [0m                      

                       Computation: 44445 steps/s (collection: 2.094s, learning 0.118s)
             Mean action noise std: 2.33
          Mean value_function loss: 423.8601
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 58.6692
                       Mean reward: 564.28
               Mean episode length: 182.02
    Episode_Reward/reaching_object: 1.1345
     Episode_Reward/lifting_object: 108.1186
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.6667
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 2.21s
                      Time elapsed: 00:31:22
                               ETA: 00:54:16

################################################################################
                     [1m Learning iteration 733/2000 [0m                      

                       Computation: 45185 steps/s (collection: 2.081s, learning 0.094s)
             Mean action noise std: 2.33
          Mean value_function loss: 477.2352
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 58.6693
                       Mean reward: 561.30
               Mean episode length: 179.02
    Episode_Reward/reaching_object: 1.1953
     Episode_Reward/lifting_object: 117.1269
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 2.18s
                      Time elapsed: 00:31:24
                               ETA: 00:54:12

################################################################################
                     [1m Learning iteration 734/2000 [0m                      

                       Computation: 43628 steps/s (collection: 2.137s, learning 0.116s)
             Mean action noise std: 2.33
          Mean value_function loss: 514.8063
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.6693
                       Mean reward: 551.31
               Mean episode length: 177.44
    Episode_Reward/reaching_object: 1.0940
     Episode_Reward/lifting_object: 104.7554
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.3333
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 2.25s
                      Time elapsed: 00:31:26
                               ETA: 00:54:09

################################################################################
                     [1m Learning iteration 735/2000 [0m                      

                       Computation: 42799 steps/s (collection: 2.160s, learning 0.137s)
             Mean action noise std: 2.33
          Mean value_function loss: 557.1267
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.6689
                       Mean reward: 412.91
               Mean episode length: 145.65
    Episode_Reward/reaching_object: 1.0778
     Episode_Reward/lifting_object: 102.6189
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 17.6667
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 2.30s
                      Time elapsed: 00:31:29
                               ETA: 00:54:06

################################################################################
                     [1m Learning iteration 736/2000 [0m                      

                       Computation: 40672 steps/s (collection: 2.287s, learning 0.130s)
             Mean action noise std: 2.33
          Mean value_function loss: 566.8406
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 58.6681
                       Mean reward: 496.04
               Mean episode length: 161.27
    Episode_Reward/reaching_object: 1.0450
     Episode_Reward/lifting_object: 99.7658
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.2917
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 2.42s
                      Time elapsed: 00:31:31
                               ETA: 00:54:03

################################################################################
                     [1m Learning iteration 737/2000 [0m                      

                       Computation: 43947 steps/s (collection: 2.128s, learning 0.109s)
             Mean action noise std: 2.33
          Mean value_function loss: 586.3421
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 58.6679
                       Mean reward: 488.62
               Mean episode length: 160.95
    Episode_Reward/reaching_object: 1.0403
     Episode_Reward/lifting_object: 98.4216
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.3750
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 2.24s
                      Time elapsed: 00:31:33
                               ETA: 00:54:00

################################################################################
                     [1m Learning iteration 738/2000 [0m                      

                       Computation: 44749 steps/s (collection: 2.084s, learning 0.113s)
             Mean action noise std: 2.33
          Mean value_function loss: 583.8185
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 58.6688
                       Mean reward: 534.72
               Mean episode length: 174.12
    Episode_Reward/reaching_object: 1.0799
     Episode_Reward/lifting_object: 101.7374
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 2.20s
                      Time elapsed: 00:31:35
                               ETA: 00:53:57

################################################################################
                     [1m Learning iteration 739/2000 [0m                      

                       Computation: 42282 steps/s (collection: 2.188s, learning 0.137s)
             Mean action noise std: 2.33
          Mean value_function loss: 542.6821
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 58.6700
                       Mean reward: 579.49
               Mean episode length: 185.15
    Episode_Reward/reaching_object: 1.0613
     Episode_Reward/lifting_object: 98.8504
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 2.32s
                      Time elapsed: 00:31:38
                               ETA: 00:53:54

################################################################################
                     [1m Learning iteration 740/2000 [0m                      

                       Computation: 45167 steps/s (collection: 2.083s, learning 0.094s)
             Mean action noise std: 2.33
          Mean value_function loss: 500.6547
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 58.6702
                       Mean reward: 507.00
               Mean episode length: 169.01
    Episode_Reward/reaching_object: 1.0254
     Episode_Reward/lifting_object: 95.4998
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 2.18s
                      Time elapsed: 00:31:40
                               ETA: 00:53:51

################################################################################
                     [1m Learning iteration 741/2000 [0m                      

                       Computation: 43811 steps/s (collection: 2.132s, learning 0.112s)
             Mean action noise std: 2.33
          Mean value_function loss: 482.0685
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 58.6702
                       Mean reward: 487.53
               Mean episode length: 162.97
    Episode_Reward/reaching_object: 1.1088
     Episode_Reward/lifting_object: 105.0123
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 2.24s
                      Time elapsed: 00:31:42
                               ETA: 00:53:48

################################################################################
                     [1m Learning iteration 742/2000 [0m                      

                       Computation: 42158 steps/s (collection: 2.187s, learning 0.145s)
             Mean action noise std: 2.33
          Mean value_function loss: 444.5473
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 58.6707
                       Mean reward: 592.62
               Mean episode length: 182.32
    Episode_Reward/reaching_object: 1.1690
     Episode_Reward/lifting_object: 112.8212
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 2.33s
                      Time elapsed: 00:31:44
                               ETA: 00:53:45

################################################################################
                     [1m Learning iteration 743/2000 [0m                      

                       Computation: 43557 steps/s (collection: 2.125s, learning 0.132s)
             Mean action noise std: 2.33
          Mean value_function loss: 483.3017
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 58.6738
                       Mean reward: 531.28
               Mean episode length: 173.67
    Episode_Reward/reaching_object: 1.1450
     Episode_Reward/lifting_object: 109.5172
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 2.26s
                      Time elapsed: 00:31:47
                               ETA: 00:53:42

################################################################################
                     [1m Learning iteration 744/2000 [0m                      

                       Computation: 44557 steps/s (collection: 2.093s, learning 0.113s)
             Mean action noise std: 2.33
          Mean value_function loss: 472.4092
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 58.6747
                       Mean reward: 609.00
               Mean episode length: 189.92
    Episode_Reward/reaching_object: 1.2223
     Episode_Reward/lifting_object: 119.2883
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 2.21s
                      Time elapsed: 00:31:49
                               ETA: 00:53:39

################################################################################
                     [1m Learning iteration 745/2000 [0m                      

                       Computation: 42637 steps/s (collection: 2.212s, learning 0.094s)
             Mean action noise std: 2.33
          Mean value_function loss: 532.0050
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.6761
                       Mean reward: 573.70
               Mean episode length: 178.38
    Episode_Reward/reaching_object: 1.1684
     Episode_Reward/lifting_object: 113.8075
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.9583
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 2.31s
                      Time elapsed: 00:31:51
                               ETA: 00:53:36

################################################################################
                     [1m Learning iteration 746/2000 [0m                      

                       Computation: 43333 steps/s (collection: 2.177s, learning 0.092s)
             Mean action noise std: 2.33
          Mean value_function loss: 489.5403
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 58.6798
                       Mean reward: 544.01
               Mean episode length: 170.07
    Episode_Reward/reaching_object: 1.1809
     Episode_Reward/lifting_object: 114.8990
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 2.27s
                      Time elapsed: 00:31:53
                               ETA: 00:53:33

################################################################################
                     [1m Learning iteration 747/2000 [0m                      

                       Computation: 44487 steps/s (collection: 2.074s, learning 0.135s)
             Mean action noise std: 2.33
          Mean value_function loss: 475.3683
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 58.6818
                       Mean reward: 602.75
               Mean episode length: 187.35
    Episode_Reward/reaching_object: 1.1280
     Episode_Reward/lifting_object: 109.5996
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.1667
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 2.21s
                      Time elapsed: 00:31:56
                               ETA: 00:53:29

################################################################################
                     [1m Learning iteration 748/2000 [0m                      

                       Computation: 44776 steps/s (collection: 2.088s, learning 0.107s)
             Mean action noise std: 2.33
          Mean value_function loss: 508.0547
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.6830
                       Mean reward: 632.75
               Mean episode length: 193.65
    Episode_Reward/reaching_object: 1.1893
     Episode_Reward/lifting_object: 116.0548
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.1667
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 2.20s
                      Time elapsed: 00:31:58
                               ETA: 00:53:26

################################################################################
                     [1m Learning iteration 749/2000 [0m                      

                       Computation: 44490 steps/s (collection: 2.112s, learning 0.097s)
             Mean action noise std: 2.33
          Mean value_function loss: 485.4564
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 58.6853
                       Mean reward: 548.65
               Mean episode length: 173.94
    Episode_Reward/reaching_object: 1.1386
     Episode_Reward/lifting_object: 110.0367
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 2.21s
                      Time elapsed: 00:32:00
                               ETA: 00:53:23

################################################################################
                     [1m Learning iteration 750/2000 [0m                      

                       Computation: 44056 steps/s (collection: 2.130s, learning 0.102s)
             Mean action noise std: 2.33
          Mean value_function loss: 475.1446
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 58.6862
                       Mean reward: 619.46
               Mean episode length: 188.60
    Episode_Reward/reaching_object: 1.2105
     Episode_Reward/lifting_object: 118.4993
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 2.23s
                      Time elapsed: 00:32:02
                               ETA: 00:53:20

################################################################################
                     [1m Learning iteration 751/2000 [0m                      

                       Computation: 42519 steps/s (collection: 2.200s, learning 0.112s)
             Mean action noise std: 2.33
          Mean value_function loss: 448.7569
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 58.6881
                       Mean reward: 516.54
               Mean episode length: 164.71
    Episode_Reward/reaching_object: 1.1597
     Episode_Reward/lifting_object: 112.7238
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 2.31s
                      Time elapsed: 00:32:05
                               ETA: 00:53:17

################################################################################
                     [1m Learning iteration 752/2000 [0m                      

                       Computation: 45164 steps/s (collection: 2.086s, learning 0.091s)
             Mean action noise std: 2.33
          Mean value_function loss: 441.6850
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.6895
                       Mean reward: 590.28
               Mean episode length: 186.09
    Episode_Reward/reaching_object: 1.1513
     Episode_Reward/lifting_object: 110.6492
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 2.18s
                      Time elapsed: 00:32:07
                               ETA: 00:53:14

################################################################################
                     [1m Learning iteration 753/2000 [0m                      

                       Computation: 43603 steps/s (collection: 2.159s, learning 0.096s)
             Mean action noise std: 2.33
          Mean value_function loss: 453.1673
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 58.6919
                       Mean reward: 577.89
               Mean episode length: 176.41
    Episode_Reward/reaching_object: 1.1543
     Episode_Reward/lifting_object: 111.5510
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 2.25s
                      Time elapsed: 00:32:09
                               ETA: 00:53:11

################################################################################
                     [1m Learning iteration 754/2000 [0m                      

                       Computation: 40449 steps/s (collection: 2.250s, learning 0.181s)
             Mean action noise std: 2.33
          Mean value_function loss: 405.8871
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 58.6968
                       Mean reward: 609.19
               Mean episode length: 188.83
    Episode_Reward/reaching_object: 1.2273
     Episode_Reward/lifting_object: 119.9081
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 2.43s
                      Time elapsed: 00:32:11
                               ETA: 00:53:08

################################################################################
                     [1m Learning iteration 755/2000 [0m                      

                       Computation: 40301 steps/s (collection: 2.338s, learning 0.101s)
             Mean action noise std: 2.33
          Mean value_function loss: 411.1083
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 58.6982
                       Mean reward: 634.96
               Mean episode length: 188.31
    Episode_Reward/reaching_object: 1.2455
     Episode_Reward/lifting_object: 123.9114
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 2.44s
                      Time elapsed: 00:32:14
                               ETA: 00:53:05

################################################################################
                     [1m Learning iteration 756/2000 [0m                      

                       Computation: 42690 steps/s (collection: 2.214s, learning 0.089s)
             Mean action noise std: 2.33
          Mean value_function loss: 459.2595
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 58.6991
                       Mean reward: 635.35
               Mean episode length: 193.22
    Episode_Reward/reaching_object: 1.2557
     Episode_Reward/lifting_object: 124.7012
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 2.30s
                      Time elapsed: 00:32:16
                               ETA: 00:53:02

################################################################################
                     [1m Learning iteration 757/2000 [0m                      

                       Computation: 45933 steps/s (collection: 2.053s, learning 0.087s)
             Mean action noise std: 2.33
          Mean value_function loss: 465.0510
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 58.6998
                       Mean reward: 531.20
               Mean episode length: 170.20
    Episode_Reward/reaching_object: 1.2061
     Episode_Reward/lifting_object: 119.6027
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 2.14s
                      Time elapsed: 00:32:18
                               ETA: 00:52:59

################################################################################
                     [1m Learning iteration 758/2000 [0m                      

                       Computation: 45168 steps/s (collection: 2.043s, learning 0.133s)
             Mean action noise std: 2.33
          Mean value_function loss: 503.6871
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 58.7009
                       Mean reward: 526.37
               Mean episode length: 166.56
    Episode_Reward/reaching_object: 1.2282
     Episode_Reward/lifting_object: 121.3527
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 2.18s
                      Time elapsed: 00:32:21
                               ETA: 00:52:56

################################################################################
                     [1m Learning iteration 759/2000 [0m                      

                       Computation: 43513 steps/s (collection: 2.144s, learning 0.115s)
             Mean action noise std: 2.33
          Mean value_function loss: 499.9591
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 58.7017
                       Mean reward: 559.82
               Mean episode length: 177.02
    Episode_Reward/reaching_object: 1.1890
     Episode_Reward/lifting_object: 117.3158
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.1667
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 2.26s
                      Time elapsed: 00:32:23
                               ETA: 00:52:53

################################################################################
                     [1m Learning iteration 760/2000 [0m                      

                       Computation: 45415 steps/s (collection: 2.051s, learning 0.114s)
             Mean action noise std: 2.33
          Mean value_function loss: 440.8654
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 58.7025
                       Mean reward: 580.02
               Mean episode length: 177.86
    Episode_Reward/reaching_object: 1.2118
     Episode_Reward/lifting_object: 118.2277
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.3750
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 2.16s
                      Time elapsed: 00:32:25
                               ETA: 00:52:50

################################################################################
                     [1m Learning iteration 761/2000 [0m                      

                       Computation: 45945 steps/s (collection: 2.052s, learning 0.088s)
             Mean action noise std: 2.33
          Mean value_function loss: 493.2211
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 58.7051
                       Mean reward: 555.15
               Mean episode length: 174.40
    Episode_Reward/reaching_object: 1.1467
     Episode_Reward/lifting_object: 111.4058
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.9583
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 2.14s
                      Time elapsed: 00:32:27
                               ETA: 00:52:46

################################################################################
                     [1m Learning iteration 762/2000 [0m                      

                       Computation: 45752 steps/s (collection: 2.053s, learning 0.096s)
             Mean action noise std: 2.33
          Mean value_function loss: 480.5157
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 58.7082
                       Mean reward: 600.65
               Mean episode length: 184.49
    Episode_Reward/reaching_object: 1.1860
     Episode_Reward/lifting_object: 117.1211
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 2.15s
                      Time elapsed: 00:32:29
                               ETA: 00:52:43

################################################################################
                     [1m Learning iteration 763/2000 [0m                      

                       Computation: 44530 steps/s (collection: 2.109s, learning 0.099s)
             Mean action noise std: 2.33
          Mean value_function loss: 441.3174
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 58.7106
                       Mean reward: 530.62
               Mean episode length: 162.45
    Episode_Reward/reaching_object: 1.1419
     Episode_Reward/lifting_object: 111.5405
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 2.21s
                      Time elapsed: 00:32:31
                               ETA: 00:52:40

################################################################################
                     [1m Learning iteration 764/2000 [0m                      

                       Computation: 44648 steps/s (collection: 2.077s, learning 0.125s)
             Mean action noise std: 2.33
          Mean value_function loss: 422.8927
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 58.7111
                       Mean reward: 630.92
               Mean episode length: 191.03
    Episode_Reward/reaching_object: 1.1629
     Episode_Reward/lifting_object: 113.4309
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 2.20s
                      Time elapsed: 00:32:34
                               ETA: 00:52:37

################################################################################
                     [1m Learning iteration 765/2000 [0m                      

                       Computation: 42699 steps/s (collection: 2.192s, learning 0.110s)
             Mean action noise std: 2.33
          Mean value_function loss: 412.4687
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 58.7118
                       Mean reward: 601.42
               Mean episode length: 184.61
    Episode_Reward/reaching_object: 1.2264
     Episode_Reward/lifting_object: 121.2852
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 2.30s
                      Time elapsed: 00:32:36
                               ETA: 00:52:34

################################################################################
                     [1m Learning iteration 766/2000 [0m                      

                       Computation: 42519 steps/s (collection: 2.204s, learning 0.108s)
             Mean action noise std: 2.33
          Mean value_function loss: 415.0055
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.7132
                       Mean reward: 597.62
               Mean episode length: 181.46
    Episode_Reward/reaching_object: 1.2200
     Episode_Reward/lifting_object: 121.3646
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 2.31s
                      Time elapsed: 00:32:38
                               ETA: 00:52:31

################################################################################
                     [1m Learning iteration 767/2000 [0m                      

                       Computation: 45010 steps/s (collection: 2.068s, learning 0.116s)
             Mean action noise std: 2.33
          Mean value_function loss: 435.5031
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 58.7147
                       Mean reward: 639.15
               Mean episode length: 194.86
    Episode_Reward/reaching_object: 1.2633
     Episode_Reward/lifting_object: 124.9488
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 2.18s
                      Time elapsed: 00:32:40
                               ETA: 00:52:28

################################################################################
                     [1m Learning iteration 768/2000 [0m                      

                       Computation: 42684 steps/s (collection: 2.181s, learning 0.122s)
             Mean action noise std: 2.33
          Mean value_function loss: 500.9279
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 58.7154
                       Mean reward: 604.69
               Mean episode length: 181.46
    Episode_Reward/reaching_object: 1.2597
     Episode_Reward/lifting_object: 125.7634
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 2.30s
                      Time elapsed: 00:32:43
                               ETA: 00:52:25

################################################################################
                     [1m Learning iteration 769/2000 [0m                      

                       Computation: 44512 steps/s (collection: 2.108s, learning 0.101s)
             Mean action noise std: 2.33
          Mean value_function loss: 475.3094
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 58.7156
                       Mean reward: 632.01
               Mean episode length: 189.33
    Episode_Reward/reaching_object: 1.2670
     Episode_Reward/lifting_object: 125.5309
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 2.21s
                      Time elapsed: 00:32:45
                               ETA: 00:52:22

################################################################################
                     [1m Learning iteration 770/2000 [0m                      

                       Computation: 45880 steps/s (collection: 2.054s, learning 0.089s)
             Mean action noise std: 2.33
          Mean value_function loss: 530.4439
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 58.7154
                       Mean reward: 605.10
               Mean episode length: 184.35
    Episode_Reward/reaching_object: 1.2269
     Episode_Reward/lifting_object: 119.8782
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.0833
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 2.14s
                      Time elapsed: 00:32:47
                               ETA: 00:52:19

################################################################################
                     [1m Learning iteration 771/2000 [0m                      

                       Computation: 45843 steps/s (collection: 2.037s, learning 0.108s)
             Mean action noise std: 2.33
          Mean value_function loss: 481.3413
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 58.7157
                       Mean reward: 564.89
               Mean episode length: 172.63
    Episode_Reward/reaching_object: 1.2145
     Episode_Reward/lifting_object: 117.7143
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 2.14s
                      Time elapsed: 00:32:49
                               ETA: 00:52:15

################################################################################
                     [1m Learning iteration 772/2000 [0m                      

                       Computation: 43173 steps/s (collection: 2.151s, learning 0.126s)
             Mean action noise std: 2.33
          Mean value_function loss: 485.9227
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 58.7157
                       Mean reward: 615.07
               Mean episode length: 188.09
    Episode_Reward/reaching_object: 1.2059
     Episode_Reward/lifting_object: 116.5937
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 2.28s
                      Time elapsed: 00:32:52
                               ETA: 00:52:12

################################################################################
                     [1m Learning iteration 773/2000 [0m                      

                       Computation: 43256 steps/s (collection: 2.107s, learning 0.166s)
             Mean action noise std: 2.33
          Mean value_function loss: 533.4229
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.7148
                       Mean reward: 535.67
               Mean episode length: 169.61
    Episode_Reward/reaching_object: 1.1882
     Episode_Reward/lifting_object: 114.8152
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 2.27s
                      Time elapsed: 00:32:54
                               ETA: 00:52:09

################################################################################
                     [1m Learning iteration 774/2000 [0m                      

                       Computation: 45219 steps/s (collection: 2.078s, learning 0.096s)
             Mean action noise std: 2.33
          Mean value_function loss: 558.8763
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 58.7147
                       Mean reward: 555.75
               Mean episode length: 175.07
    Episode_Reward/reaching_object: 1.1117
     Episode_Reward/lifting_object: 105.6406
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 2.17s
                      Time elapsed: 00:32:56
                               ETA: 00:52:06

################################################################################
                     [1m Learning iteration 775/2000 [0m                      

                       Computation: 44039 steps/s (collection: 2.144s, learning 0.089s)
             Mean action noise std: 2.33
          Mean value_function loss: 491.6209
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 58.7148
                       Mean reward: 478.64
               Mean episode length: 160.72
    Episode_Reward/reaching_object: 1.0581
     Episode_Reward/lifting_object: 98.8334
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 2.23s
                      Time elapsed: 00:32:58
                               ETA: 00:52:03

################################################################################
                     [1m Learning iteration 776/2000 [0m                      

                       Computation: 45198 steps/s (collection: 2.071s, learning 0.104s)
             Mean action noise std: 2.33
          Mean value_function loss: 504.8783
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 58.7149
                       Mean reward: 463.65
               Mean episode length: 155.99
    Episode_Reward/reaching_object: 1.0832
     Episode_Reward/lifting_object: 101.2113
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.1667
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 2.17s
                      Time elapsed: 00:33:00
                               ETA: 00:52:00

################################################################################
                     [1m Learning iteration 777/2000 [0m                      

                       Computation: 44102 steps/s (collection: 2.109s, learning 0.120s)
             Mean action noise std: 2.33
          Mean value_function loss: 444.3515
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 58.7149
                       Mean reward: 561.35
               Mean episode length: 174.67
    Episode_Reward/reaching_object: 1.1231
     Episode_Reward/lifting_object: 107.4922
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 2.23s
                      Time elapsed: 00:33:03
                               ETA: 00:51:57

################################################################################
                     [1m Learning iteration 778/2000 [0m                      

                       Computation: 42668 steps/s (collection: 2.102s, learning 0.202s)
             Mean action noise std: 2.33
          Mean value_function loss: 414.7862
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 58.7152
                       Mean reward: 583.03
               Mean episode length: 179.01
    Episode_Reward/reaching_object: 1.1780
     Episode_Reward/lifting_object: 114.1722
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 2.30s
                      Time elapsed: 00:33:05
                               ETA: 00:51:54

################################################################################
                     [1m Learning iteration 779/2000 [0m                      

                       Computation: 42646 steps/s (collection: 2.154s, learning 0.152s)
             Mean action noise std: 2.33
          Mean value_function loss: 377.4032
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 58.7164
                       Mean reward: 626.22
               Mean episode length: 192.94
    Episode_Reward/reaching_object: 1.2136
     Episode_Reward/lifting_object: 117.9031
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 2.31s
                      Time elapsed: 00:33:07
                               ETA: 00:51:51

################################################################################
                     [1m Learning iteration 780/2000 [0m                      

                       Computation: 41865 steps/s (collection: 2.216s, learning 0.133s)
             Mean action noise std: 2.33
          Mean value_function loss: 363.1458
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 58.7171
                       Mean reward: 674.68
               Mean episode length: 200.65
    Episode_Reward/reaching_object: 1.2661
     Episode_Reward/lifting_object: 124.4602
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 2.35s
                      Time elapsed: 00:33:10
                               ETA: 00:51:48

################################################################################
                     [1m Learning iteration 781/2000 [0m                      

                       Computation: 43711 steps/s (collection: 2.102s, learning 0.147s)
             Mean action noise std: 2.33
          Mean value_function loss: 404.6215
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 58.7177
                       Mean reward: 627.02
               Mean episode length: 192.33
    Episode_Reward/reaching_object: 1.2595
     Episode_Reward/lifting_object: 122.9603
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 2.25s
                      Time elapsed: 00:33:12
                               ETA: 00:51:45

################################################################################
                     [1m Learning iteration 782/2000 [0m                      

                       Computation: 40755 steps/s (collection: 2.270s, learning 0.143s)
             Mean action noise std: 2.33
          Mean value_function loss: 424.3005
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 58.7190
                       Mean reward: 626.18
               Mean episode length: 188.60
    Episode_Reward/reaching_object: 1.2389
     Episode_Reward/lifting_object: 121.8341
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 2.41s
                      Time elapsed: 00:33:14
                               ETA: 00:51:42

################################################################################
                     [1m Learning iteration 783/2000 [0m                      

                       Computation: 43077 steps/s (collection: 2.188s, learning 0.094s)
             Mean action noise std: 2.34
          Mean value_function loss: 462.6259
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 58.7200
                       Mean reward: 653.53
               Mean episode length: 195.04
    Episode_Reward/reaching_object: 1.2995
     Episode_Reward/lifting_object: 129.0997
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 2.28s
                      Time elapsed: 00:33:17
                               ETA: 00:51:39

################################################################################
                     [1m Learning iteration 784/2000 [0m                      

                       Computation: 44866 steps/s (collection: 2.099s, learning 0.092s)
             Mean action noise std: 2.34
          Mean value_function loss: 483.4625
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 58.7213
                       Mean reward: 624.23
               Mean episode length: 188.96
    Episode_Reward/reaching_object: 1.2440
     Episode_Reward/lifting_object: 122.2848
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 2.19s
                      Time elapsed: 00:33:19
                               ETA: 00:51:36

################################################################################
                     [1m Learning iteration 785/2000 [0m                      

                       Computation: 43554 steps/s (collection: 2.133s, learning 0.124s)
             Mean action noise std: 2.34
          Mean value_function loss: 455.8686
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 58.7223
                       Mean reward: 639.32
               Mean episode length: 194.81
    Episode_Reward/reaching_object: 1.2430
     Episode_Reward/lifting_object: 122.7906
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 2.26s
                      Time elapsed: 00:33:21
                               ETA: 00:51:33

################################################################################
                     [1m Learning iteration 786/2000 [0m                      

                       Computation: 40595 steps/s (collection: 2.294s, learning 0.127s)
             Mean action noise std: 2.34
          Mean value_function loss: 432.3544
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 58.7245
                       Mean reward: 580.52
               Mean episode length: 173.75
    Episode_Reward/reaching_object: 1.2052
     Episode_Reward/lifting_object: 118.8634
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 2.42s
                      Time elapsed: 00:33:23
                               ETA: 00:51:31

################################################################################
                     [1m Learning iteration 787/2000 [0m                      

                       Computation: 45470 steps/s (collection: 2.055s, learning 0.107s)
             Mean action noise std: 2.34
          Mean value_function loss: 399.8070
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.7271
                       Mean reward: 565.81
               Mean episode length: 175.32
    Episode_Reward/reaching_object: 1.2254
     Episode_Reward/lifting_object: 121.1634
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 2.16s
                      Time elapsed: 00:33:26
                               ETA: 00:51:28

################################################################################
                     [1m Learning iteration 788/2000 [0m                      

                       Computation: 42574 steps/s (collection: 2.155s, learning 0.154s)
             Mean action noise std: 2.34
          Mean value_function loss: 409.0410
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 58.7293
                       Mean reward: 580.43
               Mean episode length: 178.59
    Episode_Reward/reaching_object: 1.2806
     Episode_Reward/lifting_object: 128.2747
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 2.31s
                      Time elapsed: 00:33:28
                               ETA: 00:51:25

################################################################################
                     [1m Learning iteration 789/2000 [0m                      

                       Computation: 40707 steps/s (collection: 2.303s, learning 0.112s)
             Mean action noise std: 2.34
          Mean value_function loss: 422.6263
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 58.7332
                       Mean reward: 613.46
               Mean episode length: 184.54
    Episode_Reward/reaching_object: 1.2434
     Episode_Reward/lifting_object: 123.5346
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 2.41s
                      Time elapsed: 00:33:30
                               ETA: 00:51:22

################################################################################
                     [1m Learning iteration 790/2000 [0m                      

                       Computation: 45251 steps/s (collection: 2.078s, learning 0.095s)
             Mean action noise std: 2.34
          Mean value_function loss: 386.6930
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 58.7357
                       Mean reward: 590.47
               Mean episode length: 182.49
    Episode_Reward/reaching_object: 1.1661
     Episode_Reward/lifting_object: 113.4735
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 2.17s
                      Time elapsed: 00:33:32
                               ETA: 00:51:19

################################################################################
                     [1m Learning iteration 791/2000 [0m                      

                       Computation: 42865 steps/s (collection: 2.127s, learning 0.166s)
             Mean action noise std: 2.34
          Mean value_function loss: 400.7875
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 58.7372
                       Mean reward: 636.12
               Mean episode length: 190.85
    Episode_Reward/reaching_object: 1.2666
     Episode_Reward/lifting_object: 126.6511
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 2.29s
                      Time elapsed: 00:33:35
                               ETA: 00:51:16

################################################################################
                     [1m Learning iteration 792/2000 [0m                      

                       Computation: 43317 steps/s (collection: 2.180s, learning 0.090s)
             Mean action noise std: 2.34
          Mean value_function loss: 438.3754
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 58.7380
                       Mean reward: 612.33
               Mean episode length: 185.55
    Episode_Reward/reaching_object: 1.2665
     Episode_Reward/lifting_object: 127.6130
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 2.27s
                      Time elapsed: 00:33:37
                               ETA: 00:51:13

################################################################################
                     [1m Learning iteration 793/2000 [0m                      

                       Computation: 43238 steps/s (collection: 2.139s, learning 0.134s)
             Mean action noise std: 2.34
          Mean value_function loss: 409.5510
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.7389
                       Mean reward: 636.43
               Mean episode length: 195.15
    Episode_Reward/reaching_object: 1.3202
     Episode_Reward/lifting_object: 133.7540
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 2.27s
                      Time elapsed: 00:33:39
                               ETA: 00:51:10

################################################################################
                     [1m Learning iteration 794/2000 [0m                      

                       Computation: 41963 steps/s (collection: 2.243s, learning 0.100s)
             Mean action noise std: 2.34
          Mean value_function loss: 434.3835
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 58.7403
                       Mean reward: 684.97
               Mean episode length: 200.76
    Episode_Reward/reaching_object: 1.2992
     Episode_Reward/lifting_object: 132.3503
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 2.34s
                      Time elapsed: 00:33:42
                               ETA: 00:51:07

################################################################################
                     [1m Learning iteration 795/2000 [0m                      

                       Computation: 45965 steps/s (collection: 2.040s, learning 0.099s)
             Mean action noise std: 2.34
          Mean value_function loss: 497.8498
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.7421
                       Mean reward: 670.48
               Mean episode length: 193.45
    Episode_Reward/reaching_object: 1.2822
     Episode_Reward/lifting_object: 131.1947
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 2.14s
                      Time elapsed: 00:33:44
                               ETA: 00:51:04

################################################################################
                     [1m Learning iteration 796/2000 [0m                      

                       Computation: 43121 steps/s (collection: 2.107s, learning 0.173s)
             Mean action noise std: 2.34
          Mean value_function loss: 462.8060
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 58.7460
                       Mean reward: 609.43
               Mean episode length: 181.16
    Episode_Reward/reaching_object: 1.2464
     Episode_Reward/lifting_object: 125.9680
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 2.28s
                      Time elapsed: 00:33:46
                               ETA: 00:51:01

################################################################################
                     [1m Learning iteration 797/2000 [0m                      

                       Computation: 44820 steps/s (collection: 2.090s, learning 0.104s)
             Mean action noise std: 2.34
          Mean value_function loss: 472.3803
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 58.7490
                       Mean reward: 642.52
               Mean episode length: 190.83
    Episode_Reward/reaching_object: 1.1965
     Episode_Reward/lifting_object: 119.3363
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 2.19s
                      Time elapsed: 00:33:48
                               ETA: 00:50:58

################################################################################
                     [1m Learning iteration 798/2000 [0m                      

                       Computation: 43835 steps/s (collection: 2.111s, learning 0.132s)
             Mean action noise std: 2.34
          Mean value_function loss: 397.3183
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.7513
                       Mean reward: 629.06
               Mean episode length: 188.46
    Episode_Reward/reaching_object: 1.2376
     Episode_Reward/lifting_object: 124.3179
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 2.24s
                      Time elapsed: 00:33:50
                               ETA: 00:50:55

################################################################################
                     [1m Learning iteration 799/2000 [0m                      

                       Computation: 45624 steps/s (collection: 2.060s, learning 0.095s)
             Mean action noise std: 2.34
          Mean value_function loss: 406.9262
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 58.7568
                       Mean reward: 640.20
               Mean episode length: 189.87
    Episode_Reward/reaching_object: 1.2713
     Episode_Reward/lifting_object: 129.6280
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 2.15s
                      Time elapsed: 00:33:53
                               ETA: 00:50:52

################################################################################
                     [1m Learning iteration 800/2000 [0m                      

                       Computation: 44080 steps/s (collection: 2.125s, learning 0.106s)
             Mean action noise std: 2.34
          Mean value_function loss: 449.3563
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 58.7576
                       Mean reward: 598.95
               Mean episode length: 180.14
    Episode_Reward/reaching_object: 1.2296
     Episode_Reward/lifting_object: 123.4185
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 2.23s
                      Time elapsed: 00:33:55
                               ETA: 00:50:49

################################################################################
                     [1m Learning iteration 801/2000 [0m                      

                       Computation: 38880 steps/s (collection: 2.353s, learning 0.176s)
             Mean action noise std: 2.34
          Mean value_function loss: 503.9093
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.7576
                       Mean reward: 612.17
               Mean episode length: 180.39
    Episode_Reward/reaching_object: 1.2270
     Episode_Reward/lifting_object: 123.0544
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 2.53s
                      Time elapsed: 00:33:57
                               ETA: 00:50:46

################################################################################
                     [1m Learning iteration 802/2000 [0m                      

                       Computation: 43803 steps/s (collection: 2.108s, learning 0.137s)
             Mean action noise std: 2.34
          Mean value_function loss: 526.6619
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 58.7590
                       Mean reward: 617.75
               Mean episode length: 183.92
    Episode_Reward/reaching_object: 1.2305
     Episode_Reward/lifting_object: 124.5250
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 2.24s
                      Time elapsed: 00:34:00
                               ETA: 00:50:43

################################################################################
                     [1m Learning iteration 803/2000 [0m                      

                       Computation: 44274 steps/s (collection: 2.117s, learning 0.103s)
             Mean action noise std: 2.34
          Mean value_function loss: 476.5726
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 58.7629
                       Mean reward: 584.47
               Mean episode length: 175.93
    Episode_Reward/reaching_object: 1.2401
     Episode_Reward/lifting_object: 125.0725
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 2.22s
                      Time elapsed: 00:34:02
                               ETA: 00:50:40

################################################################################
                     [1m Learning iteration 804/2000 [0m                      

                       Computation: 44482 steps/s (collection: 2.094s, learning 0.116s)
             Mean action noise std: 2.34
          Mean value_function loss: 450.3874
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 58.7655
                       Mean reward: 619.99
               Mean episode length: 185.58
    Episode_Reward/reaching_object: 1.2568
     Episode_Reward/lifting_object: 126.7879
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 2.21s
                      Time elapsed: 00:34:04
                               ETA: 00:50:37

################################################################################
                     [1m Learning iteration 805/2000 [0m                      

                       Computation: 44816 steps/s (collection: 2.106s, learning 0.088s)
             Mean action noise std: 2.34
          Mean value_function loss: 492.4615
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 58.7686
                       Mean reward: 653.97
               Mean episode length: 192.93
    Episode_Reward/reaching_object: 1.2519
     Episode_Reward/lifting_object: 126.7092
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 2.19s
                      Time elapsed: 00:34:06
                               ETA: 00:50:34

################################################################################
                     [1m Learning iteration 806/2000 [0m                      

                       Computation: 45616 steps/s (collection: 2.047s, learning 0.108s)
             Mean action noise std: 2.34
          Mean value_function loss: 432.2602
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 58.7723
                       Mean reward: 643.43
               Mean episode length: 192.59
    Episode_Reward/reaching_object: 1.2584
     Episode_Reward/lifting_object: 127.7986
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 2.15s
                      Time elapsed: 00:34:08
                               ETA: 00:50:31

################################################################################
                     [1m Learning iteration 807/2000 [0m                      

                       Computation: 42590 steps/s (collection: 2.198s, learning 0.110s)
             Mean action noise std: 2.34
          Mean value_function loss: 407.1593
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 58.7747
                       Mean reward: 636.98
               Mean episode length: 187.18
    Episode_Reward/reaching_object: 1.2796
     Episode_Reward/lifting_object: 129.4111
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 2.31s
                      Time elapsed: 00:34:11
                               ETA: 00:50:28

################################################################################
                     [1m Learning iteration 808/2000 [0m                      

                       Computation: 42897 steps/s (collection: 2.152s, learning 0.140s)
             Mean action noise std: 2.34
          Mean value_function loss: 401.8001
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 58.7771
                       Mean reward: 612.92
               Mean episode length: 182.97
    Episode_Reward/reaching_object: 1.2464
     Episode_Reward/lifting_object: 125.6682
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 2.29s
                      Time elapsed: 00:34:13
                               ETA: 00:50:25

################################################################################
                     [1m Learning iteration 809/2000 [0m                      

                       Computation: 41860 steps/s (collection: 2.232s, learning 0.117s)
             Mean action noise std: 2.34
          Mean value_function loss: 352.1671
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 58.7827
                       Mean reward: 677.56
               Mean episode length: 198.71
    Episode_Reward/reaching_object: 1.2963
     Episode_Reward/lifting_object: 130.5976
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 2.35s
                      Time elapsed: 00:34:15
                               ETA: 00:50:22

################################################################################
                     [1m Learning iteration 810/2000 [0m                      

                       Computation: 45102 steps/s (collection: 2.075s, learning 0.105s)
             Mean action noise std: 2.34
          Mean value_function loss: 374.1862
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 58.7856
                       Mean reward: 697.90
               Mean episode length: 202.87
    Episode_Reward/reaching_object: 1.2996
     Episode_Reward/lifting_object: 130.7378
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 2.18s
                      Time elapsed: 00:34:18
                               ETA: 00:50:19

################################################################################
                     [1m Learning iteration 811/2000 [0m                      

                       Computation: 44337 steps/s (collection: 2.040s, learning 0.177s)
             Mean action noise std: 2.34
          Mean value_function loss: 338.3666
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 58.7867
                       Mean reward: 737.73
               Mean episode length: 213.15
    Episode_Reward/reaching_object: 1.3759
     Episode_Reward/lifting_object: 139.5865
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 2.22s
                      Time elapsed: 00:34:20
                               ETA: 00:50:16

################################################################################
                     [1m Learning iteration 812/2000 [0m                      

                       Computation: 42260 steps/s (collection: 2.206s, learning 0.120s)
             Mean action noise std: 2.34
          Mean value_function loss: 354.0312
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 58.7872
                       Mean reward: 683.51
               Mean episode length: 198.09
    Episode_Reward/reaching_object: 1.3889
     Episode_Reward/lifting_object: 141.8780
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 2.33s
                      Time elapsed: 00:34:22
                               ETA: 00:50:13

################################################################################
                     [1m Learning iteration 813/2000 [0m                      

                       Computation: 43454 steps/s (collection: 2.141s, learning 0.122s)
             Mean action noise std: 2.34
          Mean value_function loss: 394.9477
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 58.7877
                       Mean reward: 687.99
               Mean episode length: 198.85
    Episode_Reward/reaching_object: 1.3663
     Episode_Reward/lifting_object: 139.4690
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 2.26s
                      Time elapsed: 00:34:24
                               ETA: 00:50:11

################################################################################
                     [1m Learning iteration 814/2000 [0m                      

                       Computation: 41739 steps/s (collection: 2.233s, learning 0.123s)
             Mean action noise std: 2.34
          Mean value_function loss: 360.2779
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 58.7899
                       Mean reward: 665.79
               Mean episode length: 197.05
    Episode_Reward/reaching_object: 1.3396
     Episode_Reward/lifting_object: 135.9864
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 2.36s
                      Time elapsed: 00:34:27
                               ETA: 00:50:08

################################################################################
                     [1m Learning iteration 815/2000 [0m                      

                       Computation: 42788 steps/s (collection: 2.153s, learning 0.144s)
             Mean action noise std: 2.34
          Mean value_function loss: 361.2549
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 58.7918
                       Mean reward: 692.01
               Mean episode length: 198.91
    Episode_Reward/reaching_object: 1.3791
     Episode_Reward/lifting_object: 140.2636
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 2.30s
                      Time elapsed: 00:34:29
                               ETA: 00:50:05

################################################################################
                     [1m Learning iteration 816/2000 [0m                      

                       Computation: 38349 steps/s (collection: 2.412s, learning 0.152s)
             Mean action noise std: 2.34
          Mean value_function loss: 346.6174
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 58.7928
                       Mean reward: 748.53
               Mean episode length: 213.54
    Episode_Reward/reaching_object: 1.4268
     Episode_Reward/lifting_object: 145.9983
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 2.56s
                      Time elapsed: 00:34:32
                               ETA: 00:50:02

################################################################################
                     [1m Learning iteration 817/2000 [0m                      

                       Computation: 45225 steps/s (collection: 2.078s, learning 0.096s)
             Mean action noise std: 2.34
          Mean value_function loss: 389.5545
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.7937
                       Mean reward: 728.18
               Mean episode length: 204.65
    Episode_Reward/reaching_object: 1.3461
     Episode_Reward/lifting_object: 137.4259
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 2.17s
                      Time elapsed: 00:34:34
                               ETA: 00:49:59

################################################################################
                     [1m Learning iteration 818/2000 [0m                      

                       Computation: 43089 steps/s (collection: 2.100s, learning 0.181s)
             Mean action noise std: 2.35
          Mean value_function loss: 398.2142
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 58.7982
                       Mean reward: 655.33
               Mean episode length: 189.18
    Episode_Reward/reaching_object: 1.3230
     Episode_Reward/lifting_object: 135.0589
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 2.28s
                      Time elapsed: 00:34:36
                               ETA: 00:49:56

################################################################################
                     [1m Learning iteration 819/2000 [0m                      

                       Computation: 44791 steps/s (collection: 2.092s, learning 0.103s)
             Mean action noise std: 2.35
          Mean value_function loss: 422.4032
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 58.8042
                       Mean reward: 630.45
               Mean episode length: 185.39
    Episode_Reward/reaching_object: 1.2993
     Episode_Reward/lifting_object: 131.5118
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 2.19s
                      Time elapsed: 00:34:38
                               ETA: 00:49:53

################################################################################
                     [1m Learning iteration 820/2000 [0m                      

                       Computation: 45779 steps/s (collection: 2.055s, learning 0.092s)
             Mean action noise std: 2.35
          Mean value_function loss: 392.1846
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.8078
                       Mean reward: 652.62
               Mean episode length: 191.55
    Episode_Reward/reaching_object: 1.2799
     Episode_Reward/lifting_object: 129.0030
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 2.15s
                      Time elapsed: 00:34:40
                               ETA: 00:49:50

################################################################################
                     [1m Learning iteration 821/2000 [0m                      

                       Computation: 43613 steps/s (collection: 2.141s, learning 0.113s)
             Mean action noise std: 2.35
          Mean value_function loss: 430.6120
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 58.8118
                       Mean reward: 687.41
               Mean episode length: 196.63
    Episode_Reward/reaching_object: 1.3243
     Episode_Reward/lifting_object: 134.6910
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 2.25s
                      Time elapsed: 00:34:43
                               ETA: 00:49:47

################################################################################
                     [1m Learning iteration 822/2000 [0m                      

                       Computation: 45850 steps/s (collection: 2.052s, learning 0.092s)
             Mean action noise std: 2.35
          Mean value_function loss: 389.4249
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 58.8152
                       Mean reward: 663.41
               Mean episode length: 193.55
    Episode_Reward/reaching_object: 1.3456
     Episode_Reward/lifting_object: 137.3099
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 2.14s
                      Time elapsed: 00:34:45
                               ETA: 00:49:44

################################################################################
                     [1m Learning iteration 823/2000 [0m                      

                       Computation: 40656 steps/s (collection: 2.318s, learning 0.100s)
             Mean action noise std: 2.35
          Mean value_function loss: 329.0911
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 58.8208
                       Mean reward: 740.40
               Mean episode length: 209.89
    Episode_Reward/reaching_object: 1.3859
     Episode_Reward/lifting_object: 143.1405
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 2.42s
                      Time elapsed: 00:34:47
                               ETA: 00:49:42

################################################################################
                     [1m Learning iteration 824/2000 [0m                      

                       Computation: 46048 steps/s (collection: 2.039s, learning 0.096s)
             Mean action noise std: 2.35
          Mean value_function loss: 368.0195
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 58.8228
                       Mean reward: 713.75
               Mean episode length: 205.89
    Episode_Reward/reaching_object: 1.3507
     Episode_Reward/lifting_object: 138.2264
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 2.13s
                      Time elapsed: 00:34:49
                               ETA: 00:49:38

################################################################################
                     [1m Learning iteration 825/2000 [0m                      

                       Computation: 40732 steps/s (collection: 2.258s, learning 0.156s)
             Mean action noise std: 2.35
          Mean value_function loss: 309.1799
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.8243
                       Mean reward: 673.25
               Mean episode length: 191.78
    Episode_Reward/reaching_object: 1.3531
     Episode_Reward/lifting_object: 138.5575
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 2.41s
                      Time elapsed: 00:34:52
                               ETA: 00:49:36

################################################################################
                     [1m Learning iteration 826/2000 [0m                      

                       Computation: 41765 steps/s (collection: 2.209s, learning 0.145s)
             Mean action noise std: 2.35
          Mean value_function loss: 345.8680
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 58.8288
                       Mean reward: 713.69
               Mean episode length: 202.12
    Episode_Reward/reaching_object: 1.3593
     Episode_Reward/lifting_object: 139.4057
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 2.35s
                      Time elapsed: 00:34:54
                               ETA: 00:49:33

################################################################################
                     [1m Learning iteration 827/2000 [0m                      

                       Computation: 40313 steps/s (collection: 2.320s, learning 0.118s)
             Mean action noise std: 2.35
          Mean value_function loss: 379.4597
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 58.8380
                       Mean reward: 636.63
               Mean episode length: 187.94
    Episode_Reward/reaching_object: 1.3554
     Episode_Reward/lifting_object: 138.9608
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 2.44s
                      Time elapsed: 00:34:57
                               ETA: 00:49:30

################################################################################
                     [1m Learning iteration 828/2000 [0m                      

                       Computation: 45215 steps/s (collection: 2.083s, learning 0.092s)
             Mean action noise std: 2.35
          Mean value_function loss: 369.8990
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 58.8468
                       Mean reward: 679.79
               Mean episode length: 194.23
    Episode_Reward/reaching_object: 1.3584
     Episode_Reward/lifting_object: 139.7997
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 2.17s
                      Time elapsed: 00:34:59
                               ETA: 00:49:27

################################################################################
                     [1m Learning iteration 829/2000 [0m                      

                       Computation: 45136 steps/s (collection: 2.073s, learning 0.105s)
             Mean action noise std: 2.35
          Mean value_function loss: 338.3453
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 58.8533
                       Mean reward: 735.87
               Mean episode length: 208.11
    Episode_Reward/reaching_object: 1.3715
     Episode_Reward/lifting_object: 141.3369
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 2.18s
                      Time elapsed: 00:35:01
                               ETA: 00:49:24

################################################################################
                     [1m Learning iteration 830/2000 [0m                      

                       Computation: 43333 steps/s (collection: 2.138s, learning 0.130s)
             Mean action noise std: 2.35
          Mean value_function loss: 317.6991
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 58.8584
                       Mean reward: 718.64
               Mean episode length: 205.24
    Episode_Reward/reaching_object: 1.3741
     Episode_Reward/lifting_object: 141.6224
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 2.27s
                      Time elapsed: 00:35:03
                               ETA: 00:49:21

################################################################################
                     [1m Learning iteration 831/2000 [0m                      

                       Computation: 43227 steps/s (collection: 2.119s, learning 0.155s)
             Mean action noise std: 2.35
          Mean value_function loss: 351.7652
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 58.8597
                       Mean reward: 712.19
               Mean episode length: 203.16
    Episode_Reward/reaching_object: 1.4250
     Episode_Reward/lifting_object: 147.5760
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 2.27s
                      Time elapsed: 00:35:05
                               ETA: 00:49:18

################################################################################
                     [1m Learning iteration 832/2000 [0m                      

                       Computation: 45314 steps/s (collection: 2.065s, learning 0.104s)
             Mean action noise std: 2.35
          Mean value_function loss: 367.3047
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 58.8612
                       Mean reward: 766.25
               Mean episode length: 216.36
    Episode_Reward/reaching_object: 1.4186
     Episode_Reward/lifting_object: 146.2029
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 2.17s
                      Time elapsed: 00:35:08
                               ETA: 00:49:15

################################################################################
                     [1m Learning iteration 833/2000 [0m                      

                       Computation: 44736 steps/s (collection: 2.096s, learning 0.101s)
             Mean action noise std: 2.35
          Mean value_function loss: 378.4116
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 58.8618
                       Mean reward: 730.93
               Mean episode length: 206.00
    Episode_Reward/reaching_object: 1.3760
     Episode_Reward/lifting_object: 140.7375
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 2.20s
                      Time elapsed: 00:35:10
                               ETA: 00:49:12

################################################################################
                     [1m Learning iteration 834/2000 [0m                      

                       Computation: 46132 steps/s (collection: 2.035s, learning 0.096s)
             Mean action noise std: 2.35
          Mean value_function loss: 397.1651
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 58.8632
                       Mean reward: 678.63
               Mean episode length: 194.63
    Episode_Reward/reaching_object: 1.3271
     Episode_Reward/lifting_object: 135.7557
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 2.13s
                      Time elapsed: 00:35:12
                               ETA: 00:49:09

################################################################################
                     [1m Learning iteration 835/2000 [0m                      

                       Computation: 45165 steps/s (collection: 2.085s, learning 0.091s)
             Mean action noise std: 2.35
          Mean value_function loss: 358.1695
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.8668
                       Mean reward: 693.87
               Mean episode length: 195.30
    Episode_Reward/reaching_object: 1.3785
     Episode_Reward/lifting_object: 142.5164
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 2.18s
                      Time elapsed: 00:35:14
                               ETA: 00:49:06

################################################################################
                     [1m Learning iteration 836/2000 [0m                      

                       Computation: 42532 steps/s (collection: 2.176s, learning 0.135s)
             Mean action noise std: 2.35
          Mean value_function loss: 391.3956
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 58.8728
                       Mean reward: 713.63
               Mean episode length: 202.04
    Episode_Reward/reaching_object: 1.3379
     Episode_Reward/lifting_object: 136.1613
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 2.31s
                      Time elapsed: 00:35:16
                               ETA: 00:49:03

################################################################################
                     [1m Learning iteration 837/2000 [0m                      

                       Computation: 44601 steps/s (collection: 2.116s, learning 0.088s)
             Mean action noise std: 2.35
          Mean value_function loss: 347.6319
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 58.8766
                       Mean reward: 711.10
               Mean episode length: 201.27
    Episode_Reward/reaching_object: 1.3870
     Episode_Reward/lifting_object: 143.3782
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 2.20s
                      Time elapsed: 00:35:19
                               ETA: 00:49:00

################################################################################
                     [1m Learning iteration 838/2000 [0m                      

                       Computation: 41557 steps/s (collection: 2.246s, learning 0.120s)
             Mean action noise std: 2.35
          Mean value_function loss: 378.2760
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 58.8802
                       Mean reward: 716.93
               Mean episode length: 202.26
    Episode_Reward/reaching_object: 1.3522
     Episode_Reward/lifting_object: 138.1811
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 2.37s
                      Time elapsed: 00:35:21
                               ETA: 00:48:58

################################################################################
                     [1m Learning iteration 839/2000 [0m                      

                       Computation: 41983 steps/s (collection: 2.221s, learning 0.120s)
             Mean action noise std: 2.35
          Mean value_function loss: 420.7681
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 58.8853
                       Mean reward: 660.74
               Mean episode length: 189.19
    Episode_Reward/reaching_object: 1.3677
     Episode_Reward/lifting_object: 140.5361
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 2.34s
                      Time elapsed: 00:35:23
                               ETA: 00:48:55

################################################################################
                     [1m Learning iteration 840/2000 [0m                      

                       Computation: 41159 steps/s (collection: 2.137s, learning 0.252s)
             Mean action noise std: 2.35
          Mean value_function loss: 404.4373
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.8888
                       Mean reward: 709.65
               Mean episode length: 204.13
    Episode_Reward/reaching_object: 1.3300
     Episode_Reward/lifting_object: 135.8636
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 2.39s
                      Time elapsed: 00:35:26
                               ETA: 00:48:52

################################################################################
                     [1m Learning iteration 841/2000 [0m                      

                       Computation: 42002 steps/s (collection: 2.241s, learning 0.099s)
             Mean action noise std: 2.36
          Mean value_function loss: 388.9516
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 58.8953
                       Mean reward: 686.82
               Mean episode length: 195.67
    Episode_Reward/reaching_object: 1.3193
     Episode_Reward/lifting_object: 135.2894
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 2.34s
                      Time elapsed: 00:35:28
                               ETA: 00:48:49

################################################################################
                     [1m Learning iteration 842/2000 [0m                      

                       Computation: 45939 steps/s (collection: 2.034s, learning 0.106s)
             Mean action noise std: 2.36
          Mean value_function loss: 423.1113
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 58.9020
                       Mean reward: 733.52
               Mean episode length: 208.82
    Episode_Reward/reaching_object: 1.3466
     Episode_Reward/lifting_object: 137.4094
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 2.14s
                      Time elapsed: 00:35:30
                               ETA: 00:48:46

################################################################################
                     [1m Learning iteration 843/2000 [0m                      

                       Computation: 45902 steps/s (collection: 2.043s, learning 0.099s)
             Mean action noise std: 2.36
          Mean value_function loss: 458.5630
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.9047
                       Mean reward: 721.24
               Mean episode length: 205.20
    Episode_Reward/reaching_object: 1.3375
     Episode_Reward/lifting_object: 136.4861
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 2.14s
                      Time elapsed: 00:35:32
                               ETA: 00:48:43

################################################################################
                     [1m Learning iteration 844/2000 [0m                      

                       Computation: 45914 steps/s (collection: 2.037s, learning 0.104s)
             Mean action noise std: 2.36
          Mean value_function loss: 500.8959
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.9081
                       Mean reward: 670.18
               Mean episode length: 193.19
    Episode_Reward/reaching_object: 1.3208
     Episode_Reward/lifting_object: 133.6898
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 2.14s
                      Time elapsed: 00:35:34
                               ETA: 00:48:40

################################################################################
                     [1m Learning iteration 845/2000 [0m                      

                       Computation: 45582 steps/s (collection: 2.067s, learning 0.090s)
             Mean action noise std: 2.36
          Mean value_function loss: 478.2374
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 58.9122
                       Mean reward: 607.58
               Mean episode length: 180.35
    Episode_Reward/reaching_object: 1.2977
     Episode_Reward/lifting_object: 129.0651
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 2.16s
                      Time elapsed: 00:35:37
                               ETA: 00:48:37

################################################################################
                     [1m Learning iteration 846/2000 [0m                      

                       Computation: 45373 steps/s (collection: 2.069s, learning 0.097s)
             Mean action noise std: 2.36
          Mean value_function loss: 483.4262
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 58.9157
                       Mean reward: 698.22
               Mean episode length: 199.22
    Episode_Reward/reaching_object: 1.2761
     Episode_Reward/lifting_object: 127.8952
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 2.17s
                      Time elapsed: 00:35:39
                               ETA: 00:48:34

################################################################################
                     [1m Learning iteration 847/2000 [0m                      

                       Computation: 43352 steps/s (collection: 2.098s, learning 0.169s)
             Mean action noise std: 2.36
          Mean value_function loss: 431.8229
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 58.9168
                       Mean reward: 682.90
               Mean episode length: 195.99
    Episode_Reward/reaching_object: 1.3317
     Episode_Reward/lifting_object: 133.8564
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 2.27s
                      Time elapsed: 00:35:41
                               ETA: 00:48:31

################################################################################
                     [1m Learning iteration 848/2000 [0m                      

                       Computation: 42679 steps/s (collection: 2.217s, learning 0.086s)
             Mean action noise std: 2.36
          Mean value_function loss: 413.7964
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 58.9186
                       Mean reward: 664.93
               Mean episode length: 193.65
    Episode_Reward/reaching_object: 1.2963
     Episode_Reward/lifting_object: 130.5336
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 2.30s
                      Time elapsed: 00:35:43
                               ETA: 00:48:28

################################################################################
                     [1m Learning iteration 849/2000 [0m                      

                       Computation: 45702 steps/s (collection: 2.041s, learning 0.110s)
             Mean action noise std: 2.36
          Mean value_function loss: 377.0838
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 58.9209
                       Mean reward: 690.38
               Mean episode length: 196.13
    Episode_Reward/reaching_object: 1.3565
     Episode_Reward/lifting_object: 137.5660
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 2.15s
                      Time elapsed: 00:35:46
                               ETA: 00:48:25

################################################################################
                     [1m Learning iteration 850/2000 [0m                      

                       Computation: 39006 steps/s (collection: 2.382s, learning 0.138s)
             Mean action noise std: 2.36
          Mean value_function loss: 476.9009
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 58.9248
                       Mean reward: 659.03
               Mean episode length: 192.67
    Episode_Reward/reaching_object: 1.3058
     Episode_Reward/lifting_object: 131.3141
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 2.52s
                      Time elapsed: 00:35:48
                               ETA: 00:48:23

################################################################################
                     [1m Learning iteration 851/2000 [0m                      

                       Computation: 43741 steps/s (collection: 2.137s, learning 0.110s)
             Mean action noise std: 2.36
          Mean value_function loss: 430.7023
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 58.9273
                       Mean reward: 604.03
               Mean episode length: 176.38
    Episode_Reward/reaching_object: 1.2890
     Episode_Reward/lifting_object: 129.3683
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 2.25s
                      Time elapsed: 00:35:50
                               ETA: 00:48:20

################################################################################
                     [1m Learning iteration 852/2000 [0m                      

                       Computation: 42346 steps/s (collection: 2.177s, learning 0.145s)
             Mean action noise std: 2.36
          Mean value_function loss: 372.5752
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 58.9297
                       Mean reward: 708.53
               Mean episode length: 200.87
    Episode_Reward/reaching_object: 1.3858
     Episode_Reward/lifting_object: 141.1048
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 2.32s
                      Time elapsed: 00:35:53
                               ETA: 00:48:17

################################################################################
                     [1m Learning iteration 853/2000 [0m                      

                       Computation: 42684 steps/s (collection: 2.168s, learning 0.135s)
             Mean action noise std: 2.36
          Mean value_function loss: 433.1557
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 58.9320
                       Mean reward: 669.47
               Mean episode length: 193.37
    Episode_Reward/reaching_object: 1.2895
     Episode_Reward/lifting_object: 129.9903
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 2.30s
                      Time elapsed: 00:35:55
                               ETA: 00:48:14

################################################################################
                     [1m Learning iteration 854/2000 [0m                      

                       Computation: 42062 steps/s (collection: 2.199s, learning 0.138s)
             Mean action noise std: 2.36
          Mean value_function loss: 399.0524
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 58.9361
                       Mean reward: 664.00
               Mean episode length: 190.18
    Episode_Reward/reaching_object: 1.3602
     Episode_Reward/lifting_object: 137.6396
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 2.34s
                      Time elapsed: 00:35:57
                               ETA: 00:48:12

################################################################################
                     [1m Learning iteration 855/2000 [0m                      

                       Computation: 41734 steps/s (collection: 2.220s, learning 0.136s)
             Mean action noise std: 2.36
          Mean value_function loss: 393.1822
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 58.9411
                       Mean reward: 651.57
               Mean episode length: 188.79
    Episode_Reward/reaching_object: 1.3638
     Episode_Reward/lifting_object: 138.0269
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 2.36s
                      Time elapsed: 00:36:00
                               ETA: 00:48:09

################################################################################
                     [1m Learning iteration 856/2000 [0m                      

                       Computation: 42507 steps/s (collection: 2.179s, learning 0.133s)
             Mean action noise std: 2.36
          Mean value_function loss: 404.5674
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 58.9438
                       Mean reward: 719.18
               Mean episode length: 201.56
    Episode_Reward/reaching_object: 1.3248
     Episode_Reward/lifting_object: 133.8293
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 2.31s
                      Time elapsed: 00:36:02
                               ETA: 00:48:06

################################################################################
                     [1m Learning iteration 857/2000 [0m                      

                       Computation: 42766 steps/s (collection: 2.185s, learning 0.114s)
             Mean action noise std: 2.36
          Mean value_function loss: 400.6070
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 58.9473
                       Mean reward: 680.60
               Mean episode length: 194.27
    Episode_Reward/reaching_object: 1.3735
     Episode_Reward/lifting_object: 140.1486
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 2.30s
                      Time elapsed: 00:36:04
                               ETA: 00:48:03

################################################################################
                     [1m Learning iteration 858/2000 [0m                      

                       Computation: 42076 steps/s (collection: 2.171s, learning 0.165s)
             Mean action noise std: 2.36
          Mean value_function loss: 410.4376
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 58.9508
                       Mean reward: 688.83
               Mean episode length: 197.16
    Episode_Reward/reaching_object: 1.3459
     Episode_Reward/lifting_object: 137.0440
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 2.34s
                      Time elapsed: 00:36:07
                               ETA: 00:48:00

################################################################################
                     [1m Learning iteration 859/2000 [0m                      

                       Computation: 43927 steps/s (collection: 2.119s, learning 0.119s)
             Mean action noise std: 2.36
          Mean value_function loss: 361.9987
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 58.9527
                       Mean reward: 729.88
               Mean episode length: 207.74
    Episode_Reward/reaching_object: 1.4125
     Episode_Reward/lifting_object: 143.9750
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 2.24s
                      Time elapsed: 00:36:09
                               ETA: 00:47:58

################################################################################
                     [1m Learning iteration 860/2000 [0m                      

                       Computation: 43912 steps/s (collection: 2.144s, learning 0.095s)
             Mean action noise std: 2.36
          Mean value_function loss: 392.3387
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 58.9592
                       Mean reward: 709.47
               Mean episode length: 203.85
    Episode_Reward/reaching_object: 1.3828
     Episode_Reward/lifting_object: 140.3958
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 2.24s
                      Time elapsed: 00:36:11
                               ETA: 00:47:55

################################################################################
                     [1m Learning iteration 861/2000 [0m                      

                       Computation: 41384 steps/s (collection: 2.256s, learning 0.119s)
             Mean action noise std: 2.36
          Mean value_function loss: 364.8357
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 58.9668
                       Mean reward: 712.83
               Mean episode length: 203.92
    Episode_Reward/reaching_object: 1.3610
     Episode_Reward/lifting_object: 137.3406
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 2.38s
                      Time elapsed: 00:36:13
                               ETA: 00:47:52

################################################################################
                     [1m Learning iteration 862/2000 [0m                      

                       Computation: 43744 steps/s (collection: 2.103s, learning 0.145s)
             Mean action noise std: 2.36
          Mean value_function loss: 378.7648
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 58.9732
                       Mean reward: 745.59
               Mean episode length: 212.86
    Episode_Reward/reaching_object: 1.4213
     Episode_Reward/lifting_object: 145.1887
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 2.25s
                      Time elapsed: 00:36:16
                               ETA: 00:47:49

################################################################################
                     [1m Learning iteration 863/2000 [0m                      

                       Computation: 44511 steps/s (collection: 2.080s, learning 0.128s)
             Mean action noise std: 2.36
          Mean value_function loss: 366.7287
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 58.9766
                       Mean reward: 708.35
               Mean episode length: 202.22
    Episode_Reward/reaching_object: 1.3670
     Episode_Reward/lifting_object: 139.5711
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 2.21s
                      Time elapsed: 00:36:18
                               ETA: 00:47:46

################################################################################
                     [1m Learning iteration 864/2000 [0m                      

                       Computation: 45247 steps/s (collection: 2.077s, learning 0.095s)
             Mean action noise std: 2.36
          Mean value_function loss: 352.4328
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 58.9780
                       Mean reward: 716.82
               Mean episode length: 205.40
    Episode_Reward/reaching_object: 1.4434
     Episode_Reward/lifting_object: 147.3060
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 2.17s
                      Time elapsed: 00:36:20
                               ETA: 00:47:43

################################################################################
                     [1m Learning iteration 865/2000 [0m                      

                       Computation: 42586 steps/s (collection: 2.213s, learning 0.096s)
             Mean action noise std: 2.36
          Mean value_function loss: 364.4429
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.9796
                       Mean reward: 751.51
               Mean episode length: 213.23
    Episode_Reward/reaching_object: 1.3922
     Episode_Reward/lifting_object: 141.1595
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 2.31s
                      Time elapsed: 00:36:22
                               ETA: 00:47:40

################################################################################
                     [1m Learning iteration 866/2000 [0m                      

                       Computation: 44069 steps/s (collection: 2.102s, learning 0.129s)
             Mean action noise std: 2.37
          Mean value_function loss: 340.6352
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.9834
                       Mean reward: 677.93
               Mean episode length: 194.67
    Episode_Reward/reaching_object: 1.4274
     Episode_Reward/lifting_object: 144.9100
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 2.23s
                      Time elapsed: 00:36:25
                               ETA: 00:47:37

################################################################################
                     [1m Learning iteration 867/2000 [0m                      

                       Computation: 45226 steps/s (collection: 2.076s, learning 0.098s)
             Mean action noise std: 2.37
          Mean value_function loss: 394.5083
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 58.9871
                       Mean reward: 728.62
               Mean episode length: 208.15
    Episode_Reward/reaching_object: 1.4494
     Episode_Reward/lifting_object: 146.7165
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 2.17s
                      Time elapsed: 00:36:27
                               ETA: 00:47:35

################################################################################
                     [1m Learning iteration 868/2000 [0m                      

                       Computation: 42851 steps/s (collection: 2.139s, learning 0.155s)
             Mean action noise std: 2.37
          Mean value_function loss: 322.1416
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 58.9901
                       Mean reward: 700.12
               Mean episode length: 197.26
    Episode_Reward/reaching_object: 1.4484
     Episode_Reward/lifting_object: 147.2589
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 2.29s
                      Time elapsed: 00:36:29
                               ETA: 00:47:32

################################################################################
                     [1m Learning iteration 869/2000 [0m                      

                       Computation: 44934 steps/s (collection: 2.081s, learning 0.107s)
             Mean action noise std: 2.37
          Mean value_function loss: 364.0755
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 58.9923
                       Mean reward: 766.66
               Mean episode length: 214.40
    Episode_Reward/reaching_object: 1.4090
     Episode_Reward/lifting_object: 143.0992
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 2.19s
                      Time elapsed: 00:36:31
                               ETA: 00:47:29

################################################################################
                     [1m Learning iteration 870/2000 [0m                      

                       Computation: 44044 steps/s (collection: 2.097s, learning 0.135s)
             Mean action noise std: 2.37
          Mean value_function loss: 345.3236
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 58.9935
                       Mean reward: 741.11
               Mean episode length: 208.15
    Episode_Reward/reaching_object: 1.4599
     Episode_Reward/lifting_object: 147.9992
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 2.23s
                      Time elapsed: 00:36:33
                               ETA: 00:47:26

################################################################################
                     [1m Learning iteration 871/2000 [0m                      

                       Computation: 44049 steps/s (collection: 2.084s, learning 0.148s)
             Mean action noise std: 2.37
          Mean value_function loss: 375.4828
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 58.9941
                       Mean reward: 796.56
               Mean episode length: 221.24
    Episode_Reward/reaching_object: 1.4019
     Episode_Reward/lifting_object: 142.5867
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 2.23s
                      Time elapsed: 00:36:36
                               ETA: 00:47:23

################################################################################
                     [1m Learning iteration 872/2000 [0m                      

                       Computation: 44656 steps/s (collection: 2.093s, learning 0.108s)
             Mean action noise std: 2.37
          Mean value_function loss: 391.7383
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 58.9948
                       Mean reward: 726.26
               Mean episode length: 204.57
    Episode_Reward/reaching_object: 1.4628
     Episode_Reward/lifting_object: 149.7293
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 2.20s
                      Time elapsed: 00:36:38
                               ETA: 00:47:20

################################################################################
                     [1m Learning iteration 873/2000 [0m                      

                       Computation: 45527 steps/s (collection: 2.064s, learning 0.096s)
             Mean action noise std: 2.37
          Mean value_function loss: 373.7065
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 58.9961
                       Mean reward: 740.43
               Mean episode length: 208.44
    Episode_Reward/reaching_object: 1.3925
     Episode_Reward/lifting_object: 141.5494
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 2.16s
                      Time elapsed: 00:36:40
                               ETA: 00:47:17

################################################################################
                     [1m Learning iteration 874/2000 [0m                      

                       Computation: 44237 steps/s (collection: 2.112s, learning 0.110s)
             Mean action noise std: 2.37
          Mean value_function loss: 403.5014
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.9996
                       Mean reward: 721.61
               Mean episode length: 202.30
    Episode_Reward/reaching_object: 1.4101
     Episode_Reward/lifting_object: 143.1115
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 2.22s
                      Time elapsed: 00:36:42
                               ETA: 00:47:14

################################################################################
                     [1m Learning iteration 875/2000 [0m                      

                       Computation: 42803 steps/s (collection: 2.177s, learning 0.120s)
             Mean action noise std: 2.37
          Mean value_function loss: 355.6336
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 59.0052
                       Mean reward: 762.38
               Mean episode length: 211.21
    Episode_Reward/reaching_object: 1.4485
     Episode_Reward/lifting_object: 148.9123
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 2.30s
                      Time elapsed: 00:36:45
                               ETA: 00:47:11

################################################################################
                     [1m Learning iteration 876/2000 [0m                      

                       Computation: 43415 steps/s (collection: 2.131s, learning 0.133s)
             Mean action noise std: 2.37
          Mean value_function loss: 386.3788
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 59.0090
                       Mean reward: 738.75
               Mean episode length: 207.73
    Episode_Reward/reaching_object: 1.4000
     Episode_Reward/lifting_object: 142.8152
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 2.26s
                      Time elapsed: 00:36:47
                               ETA: 00:47:09

################################################################################
                     [1m Learning iteration 877/2000 [0m                      

                       Computation: 42408 steps/s (collection: 2.213s, learning 0.105s)
             Mean action noise std: 2.37
          Mean value_function loss: 354.1647
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 59.0136
                       Mean reward: 753.43
               Mean episode length: 212.08
    Episode_Reward/reaching_object: 1.3925
     Episode_Reward/lifting_object: 141.7431
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 2.32s
                      Time elapsed: 00:36:49
                               ETA: 00:47:06

################################################################################
                     [1m Learning iteration 878/2000 [0m                      

                       Computation: 41846 steps/s (collection: 2.254s, learning 0.096s)
             Mean action noise std: 2.37
          Mean value_function loss: 367.3577
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 59.0239
                       Mean reward: 725.01
               Mean episode length: 205.37
    Episode_Reward/reaching_object: 1.4176
     Episode_Reward/lifting_object: 144.7015
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 2.35s
                      Time elapsed: 00:36:51
                               ETA: 00:47:03

################################################################################
                     [1m Learning iteration 879/2000 [0m                      

                       Computation: 43957 steps/s (collection: 2.138s, learning 0.098s)
             Mean action noise std: 2.37
          Mean value_function loss: 306.9910
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 59.0306
                       Mean reward: 789.81
               Mean episode length: 218.77
    Episode_Reward/reaching_object: 1.4338
     Episode_Reward/lifting_object: 146.9126
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 2.24s
                      Time elapsed: 00:36:54
                               ETA: 00:47:00

################################################################################
                     [1m Learning iteration 880/2000 [0m                      

                       Computation: 42548 steps/s (collection: 2.211s, learning 0.100s)
             Mean action noise std: 2.37
          Mean value_function loss: 354.2635
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 59.0354
                       Mean reward: 773.36
               Mean episode length: 214.53
    Episode_Reward/reaching_object: 1.4259
     Episode_Reward/lifting_object: 146.2569
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 2.31s
                      Time elapsed: 00:36:56
                               ETA: 00:46:57

################################################################################
                     [1m Learning iteration 881/2000 [0m                      

                       Computation: 42878 steps/s (collection: 2.164s, learning 0.129s)
             Mean action noise std: 2.37
          Mean value_function loss: 361.7071
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 59.0409
                       Mean reward: 750.37
               Mean episode length: 207.53
    Episode_Reward/reaching_object: 1.4491
     Episode_Reward/lifting_object: 147.6832
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 2.29s
                      Time elapsed: 00:36:58
                               ETA: 00:46:55

################################################################################
                     [1m Learning iteration 882/2000 [0m                      

                       Computation: 42408 steps/s (collection: 2.204s, learning 0.114s)
             Mean action noise std: 2.37
          Mean value_function loss: 327.6975
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 59.0443
                       Mean reward: 791.67
               Mean episode length: 220.64
    Episode_Reward/reaching_object: 1.4758
     Episode_Reward/lifting_object: 151.3791
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 2.32s
                      Time elapsed: 00:37:01
                               ETA: 00:46:52

################################################################################
                     [1m Learning iteration 883/2000 [0m                      

                       Computation: 44476 steps/s (collection: 2.110s, learning 0.101s)
             Mean action noise std: 2.37
          Mean value_function loss: 314.5211
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 59.0491
                       Mean reward: 788.70
               Mean episode length: 218.42
    Episode_Reward/reaching_object: 1.4807
     Episode_Reward/lifting_object: 151.6305
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 2.21s
                      Time elapsed: 00:37:03
                               ETA: 00:46:49

################################################################################
                     [1m Learning iteration 884/2000 [0m                      

                       Computation: 44702 steps/s (collection: 2.097s, learning 0.103s)
             Mean action noise std: 2.37
          Mean value_function loss: 356.3486
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 59.0513
                       Mean reward: 734.34
               Mean episode length: 205.83
    Episode_Reward/reaching_object: 1.4374
     Episode_Reward/lifting_object: 147.1421
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 2.20s
                      Time elapsed: 00:37:05
                               ETA: 00:46:46

################################################################################
                     [1m Learning iteration 885/2000 [0m                      

                       Computation: 44475 steps/s (collection: 2.114s, learning 0.097s)
             Mean action noise std: 2.37
          Mean value_function loss: 385.0814
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 59.0523
                       Mean reward: 726.10
               Mean episode length: 203.20
    Episode_Reward/reaching_object: 1.4268
     Episode_Reward/lifting_object: 146.8866
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 2.21s
                      Time elapsed: 00:37:07
                               ETA: 00:46:43

################################################################################
                     [1m Learning iteration 886/2000 [0m                      

                       Computation: 42129 steps/s (collection: 2.171s, learning 0.163s)
             Mean action noise std: 2.37
          Mean value_function loss: 360.5542
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 59.0534
                       Mean reward: 665.69
               Mean episode length: 189.99
    Episode_Reward/reaching_object: 1.3930
     Episode_Reward/lifting_object: 142.0806
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 2.33s
                      Time elapsed: 00:37:10
                               ETA: 00:46:40

################################################################################
                     [1m Learning iteration 887/2000 [0m                      

                       Computation: 40620 steps/s (collection: 2.316s, learning 0.104s)
             Mean action noise std: 2.37
          Mean value_function loss: 361.2410
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 59.0560
                       Mean reward: 691.78
               Mean episode length: 194.46
    Episode_Reward/reaching_object: 1.4068
     Episode_Reward/lifting_object: 145.0256
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 2.42s
                      Time elapsed: 00:37:12
                               ETA: 00:46:38

################################################################################
                     [1m Learning iteration 888/2000 [0m                      

                       Computation: 44541 steps/s (collection: 2.106s, learning 0.101s)
             Mean action noise std: 2.37
          Mean value_function loss: 354.3872
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 59.0582
                       Mean reward: 738.56
               Mean episode length: 207.34
    Episode_Reward/reaching_object: 1.4175
     Episode_Reward/lifting_object: 145.7533
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 2.21s
                      Time elapsed: 00:37:14
                               ETA: 00:46:35

################################################################################
                     [1m Learning iteration 889/2000 [0m                      

                       Computation: 42609 steps/s (collection: 2.199s, learning 0.109s)
             Mean action noise std: 2.37
          Mean value_function loss: 383.2669
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 59.0614
                       Mean reward: 706.94
               Mean episode length: 198.61
    Episode_Reward/reaching_object: 1.4115
     Episode_Reward/lifting_object: 145.2769
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 2.31s
                      Time elapsed: 00:37:17
                               ETA: 00:46:32

################################################################################
                     [1m Learning iteration 890/2000 [0m                      

                       Computation: 41847 steps/s (collection: 2.238s, learning 0.111s)
             Mean action noise std: 2.37
          Mean value_function loss: 336.0597
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 59.0687
                       Mean reward: 725.91
               Mean episode length: 203.72
    Episode_Reward/reaching_object: 1.3768
     Episode_Reward/lifting_object: 140.7789
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 2.35s
                      Time elapsed: 00:37:19
                               ETA: 00:46:29

################################################################################
                     [1m Learning iteration 891/2000 [0m                      

                       Computation: 43163 steps/s (collection: 2.161s, learning 0.117s)
             Mean action noise std: 2.38
          Mean value_function loss: 372.9291
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 59.0762
                       Mean reward: 747.83
               Mean episode length: 210.17
    Episode_Reward/reaching_object: 1.3904
     Episode_Reward/lifting_object: 141.9813
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 2.28s
                      Time elapsed: 00:37:21
                               ETA: 00:46:27

################################################################################
                     [1m Learning iteration 892/2000 [0m                      

                       Computation: 43612 steps/s (collection: 2.148s, learning 0.106s)
             Mean action noise std: 2.38
          Mean value_function loss: 347.4186
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 59.0871
                       Mean reward: 717.42
               Mean episode length: 202.28
    Episode_Reward/reaching_object: 1.4184
     Episode_Reward/lifting_object: 145.7366
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 2.25s
                      Time elapsed: 00:37:23
                               ETA: 00:46:24

################################################################################
                     [1m Learning iteration 893/2000 [0m                      

                       Computation: 44454 steps/s (collection: 2.083s, learning 0.129s)
             Mean action noise std: 2.38
          Mean value_function loss: 368.8183
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 59.0919
                       Mean reward: 793.09
               Mean episode length: 218.55
    Episode_Reward/reaching_object: 1.4275
     Episode_Reward/lifting_object: 147.3148
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 2.21s
                      Time elapsed: 00:37:26
                               ETA: 00:46:21

################################################################################
                     [1m Learning iteration 894/2000 [0m                      

                       Computation: 44987 steps/s (collection: 2.066s, learning 0.119s)
             Mean action noise std: 2.38
          Mean value_function loss: 336.5782
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 59.0944
                       Mean reward: 734.86
               Mean episode length: 203.51
    Episode_Reward/reaching_object: 1.4047
     Episode_Reward/lifting_object: 144.1511
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 2.19s
                      Time elapsed: 00:37:28
                               ETA: 00:46:18

################################################################################
                     [1m Learning iteration 895/2000 [0m                      

                       Computation: 43272 steps/s (collection: 2.129s, learning 0.143s)
             Mean action noise std: 2.38
          Mean value_function loss: 374.5879
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 59.0958
                       Mean reward: 757.71
               Mean episode length: 208.71
    Episode_Reward/reaching_object: 1.4294
     Episode_Reward/lifting_object: 147.0002
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 2.27s
                      Time elapsed: 00:37:30
                               ETA: 00:46:15

################################################################################
                     [1m Learning iteration 896/2000 [0m                      

                       Computation: 44070 steps/s (collection: 2.123s, learning 0.108s)
             Mean action noise std: 2.38
          Mean value_function loss: 370.8304
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.0995
                       Mean reward: 758.96
               Mean episode length: 210.92
    Episode_Reward/reaching_object: 1.4250
     Episode_Reward/lifting_object: 146.3589
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 2.23s
                      Time elapsed: 00:37:32
                               ETA: 00:46:12

################################################################################
                     [1m Learning iteration 897/2000 [0m                      

                       Computation: 44228 steps/s (collection: 2.111s, learning 0.112s)
             Mean action noise std: 2.38
          Mean value_function loss: 344.2605
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 59.1057
                       Mean reward: 733.36
               Mean episode length: 207.51
    Episode_Reward/reaching_object: 1.4619
     Episode_Reward/lifting_object: 149.9582
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 2.22s
                      Time elapsed: 00:37:35
                               ETA: 00:46:09

################################################################################
                     [1m Learning iteration 898/2000 [0m                      

                       Computation: 43874 steps/s (collection: 2.102s, learning 0.139s)
             Mean action noise std: 2.38
          Mean value_function loss: 333.5409
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 59.1122
                       Mean reward: 770.85
               Mean episode length: 213.09
    Episode_Reward/reaching_object: 1.4717
     Episode_Reward/lifting_object: 151.9991
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 2.24s
                      Time elapsed: 00:37:37
                               ETA: 00:46:06

################################################################################
                     [1m Learning iteration 899/2000 [0m                      

                       Computation: 44002 steps/s (collection: 2.129s, learning 0.106s)
             Mean action noise std: 2.38
          Mean value_function loss: 343.3277
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 59.1189
                       Mean reward: 757.62
               Mean episode length: 212.25
    Episode_Reward/reaching_object: 1.4580
     Episode_Reward/lifting_object: 150.2109
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 2.23s
                      Time elapsed: 00:37:39
                               ETA: 00:46:04

################################################################################
                     [1m Learning iteration 900/2000 [0m                      

                       Computation: 41641 steps/s (collection: 2.266s, learning 0.095s)
             Mean action noise std: 2.38
          Mean value_function loss: 293.3336
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 59.1235
                       Mean reward: 743.63
               Mean episode length: 208.25
    Episode_Reward/reaching_object: 1.4168
     Episode_Reward/lifting_object: 145.3297
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 2.36s
                      Time elapsed: 00:37:41
                               ETA: 00:46:01

################################################################################
                     [1m Learning iteration 901/2000 [0m                      

                       Computation: 44536 steps/s (collection: 2.117s, learning 0.091s)
             Mean action noise std: 2.38
          Mean value_function loss: 266.4947
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 59.1317
                       Mean reward: 767.55
               Mean episode length: 212.40
    Episode_Reward/reaching_object: 1.4713
     Episode_Reward/lifting_object: 151.4882
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 2.21s
                      Time elapsed: 00:37:44
                               ETA: 00:45:58

################################################################################
                     [1m Learning iteration 902/2000 [0m                      

                       Computation: 40681 steps/s (collection: 2.323s, learning 0.094s)
             Mean action noise std: 2.38
          Mean value_function loss: 322.8498
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 59.1371
                       Mean reward: 793.05
               Mean episode length: 221.38
    Episode_Reward/reaching_object: 1.4726
     Episode_Reward/lifting_object: 151.3898
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 2.42s
                      Time elapsed: 00:37:46
                               ETA: 00:45:55

################################################################################
                     [1m Learning iteration 903/2000 [0m                      

                       Computation: 44863 steps/s (collection: 2.093s, learning 0.099s)
             Mean action noise std: 2.38
          Mean value_function loss: 314.8888
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 59.1424
                       Mean reward: 767.50
               Mean episode length: 212.72
    Episode_Reward/reaching_object: 1.4853
     Episode_Reward/lifting_object: 153.5534
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 2.19s
                      Time elapsed: 00:37:48
                               ETA: 00:45:53

################################################################################
                     [1m Learning iteration 904/2000 [0m                      

                       Computation: 41917 steps/s (collection: 2.180s, learning 0.166s)
             Mean action noise std: 2.38
          Mean value_function loss: 360.1322
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 59.1506
                       Mean reward: 741.43
               Mean episode length: 204.85
    Episode_Reward/reaching_object: 1.4729
     Episode_Reward/lifting_object: 152.0604
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 2.35s
                      Time elapsed: 00:37:51
                               ETA: 00:45:50

################################################################################
                     [1m Learning iteration 905/2000 [0m                      

                       Computation: 43091 steps/s (collection: 2.172s, learning 0.110s)
             Mean action noise std: 2.38
          Mean value_function loss: 372.1684
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 59.1599
                       Mean reward: 715.82
               Mean episode length: 199.66
    Episode_Reward/reaching_object: 1.4487
     Episode_Reward/lifting_object: 149.1043
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 2.28s
                      Time elapsed: 00:37:53
                               ETA: 00:45:47

################################################################################
                     [1m Learning iteration 906/2000 [0m                      

                       Computation: 44149 steps/s (collection: 2.130s, learning 0.097s)
             Mean action noise std: 2.39
          Mean value_function loss: 348.8450
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 59.1694
                       Mean reward: 701.73
               Mean episode length: 196.86
    Episode_Reward/reaching_object: 1.4364
     Episode_Reward/lifting_object: 147.6403
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 2.23s
                      Time elapsed: 00:37:55
                               ETA: 00:45:44

################################################################################
                     [1m Learning iteration 907/2000 [0m                      

                       Computation: 44190 steps/s (collection: 2.113s, learning 0.112s)
             Mean action noise std: 2.39
          Mean value_function loss: 341.7171
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 59.1769
                       Mean reward: 757.85
               Mean episode length: 210.49
    Episode_Reward/reaching_object: 1.4542
     Episode_Reward/lifting_object: 150.4247
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 2.22s
                      Time elapsed: 00:37:57
                               ETA: 00:45:41

################################################################################
                     [1m Learning iteration 908/2000 [0m                      

                       Computation: 44739 steps/s (collection: 2.082s, learning 0.115s)
             Mean action noise std: 2.39
          Mean value_function loss: 317.6237
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 59.1868
                       Mean reward: 741.39
               Mean episode length: 205.21
    Episode_Reward/reaching_object: 1.4490
     Episode_Reward/lifting_object: 149.5013
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 2.20s
                      Time elapsed: 00:37:59
                               ETA: 00:45:38

################################################################################
                     [1m Learning iteration 909/2000 [0m                      

                       Computation: 43387 steps/s (collection: 2.170s, learning 0.096s)
             Mean action noise std: 2.39
          Mean value_function loss: 367.6862
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 59.1969
                       Mean reward: 768.09
               Mean episode length: 211.68
    Episode_Reward/reaching_object: 1.4375
     Episode_Reward/lifting_object: 148.4183
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 2.27s
                      Time elapsed: 00:38:02
                               ETA: 00:45:36

################################################################################
                     [1m Learning iteration 910/2000 [0m                      

                       Computation: 44219 steps/s (collection: 2.098s, learning 0.125s)
             Mean action noise std: 2.39
          Mean value_function loss: 346.4160
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 59.2046
                       Mean reward: 713.78
               Mean episode length: 199.35
    Episode_Reward/reaching_object: 1.4438
     Episode_Reward/lifting_object: 149.9739
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 2.22s
                      Time elapsed: 00:38:04
                               ETA: 00:45:33

################################################################################
                     [1m Learning iteration 911/2000 [0m                      

                       Computation: 43954 steps/s (collection: 2.134s, learning 0.103s)
             Mean action noise std: 2.39
          Mean value_function loss: 367.3855
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 59.2142
                       Mean reward: 708.63
               Mean episode length: 200.63
    Episode_Reward/reaching_object: 1.4072
     Episode_Reward/lifting_object: 144.7089
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 2.24s
                      Time elapsed: 00:38:06
                               ETA: 00:45:30

################################################################################
                     [1m Learning iteration 912/2000 [0m                      

                       Computation: 45206 steps/s (collection: 2.063s, learning 0.111s)
             Mean action noise std: 2.39
          Mean value_function loss: 399.9468
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 59.2228
                       Mean reward: 729.26
               Mean episode length: 203.58
    Episode_Reward/reaching_object: 1.4312
     Episode_Reward/lifting_object: 148.1344
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 2.17s
                      Time elapsed: 00:38:08
                               ETA: 00:45:27

################################################################################
                     [1m Learning iteration 913/2000 [0m                      

                       Computation: 43885 steps/s (collection: 2.133s, learning 0.107s)
             Mean action noise std: 2.39
          Mean value_function loss: 376.3964
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.2301
                       Mean reward: 758.64
               Mean episode length: 208.71
    Episode_Reward/reaching_object: 1.4657
     Episode_Reward/lifting_object: 152.4662
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 2.24s
                      Time elapsed: 00:38:11
                               ETA: 00:45:24

################################################################################
                     [1m Learning iteration 914/2000 [0m                      

                       Computation: 43467 steps/s (collection: 2.160s, learning 0.101s)
             Mean action noise std: 2.39
          Mean value_function loss: 301.5313
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.2402
                       Mean reward: 698.18
               Mean episode length: 197.92
    Episode_Reward/reaching_object: 1.4163
     Episode_Reward/lifting_object: 146.5078
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 2.26s
                      Time elapsed: 00:38:13
                               ETA: 00:45:21

################################################################################
                     [1m Learning iteration 915/2000 [0m                      

                       Computation: 44560 steps/s (collection: 2.101s, learning 0.106s)
             Mean action noise std: 2.39
          Mean value_function loss: 353.5904
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 59.2531
                       Mean reward: 766.89
               Mean episode length: 213.03
    Episode_Reward/reaching_object: 1.4618
     Episode_Reward/lifting_object: 151.6538
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 2.21s
                      Time elapsed: 00:38:15
                               ETA: 00:45:19

################################################################################
                     [1m Learning iteration 916/2000 [0m                      

                       Computation: 45092 steps/s (collection: 2.083s, learning 0.097s)
             Mean action noise std: 2.40
          Mean value_function loss: 323.2211
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 59.2631
                       Mean reward: 801.04
               Mean episode length: 218.96
    Episode_Reward/reaching_object: 1.5012
     Episode_Reward/lifting_object: 156.1688
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 2.18s
                      Time elapsed: 00:38:17
                               ETA: 00:45:16

################################################################################
                     [1m Learning iteration 917/2000 [0m                      

                       Computation: 44194 steps/s (collection: 2.128s, learning 0.097s)
             Mean action noise std: 2.40
          Mean value_function loss: 297.9358
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 59.2644
                       Mean reward: 776.52
               Mean episode length: 212.34
    Episode_Reward/reaching_object: 1.4602
     Episode_Reward/lifting_object: 151.4469
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 2.22s
                      Time elapsed: 00:38:19
                               ETA: 00:45:13

################################################################################
                     [1m Learning iteration 918/2000 [0m                      

                       Computation: 43856 steps/s (collection: 2.126s, learning 0.115s)
             Mean action noise std: 2.40
          Mean value_function loss: 284.6556
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 59.2651
                       Mean reward: 762.17
               Mean episode length: 211.01
    Episode_Reward/reaching_object: 1.4708
     Episode_Reward/lifting_object: 152.3661
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 2.24s
                      Time elapsed: 00:38:22
                               ETA: 00:45:10

################################################################################
                     [1m Learning iteration 919/2000 [0m                      

                       Computation: 42205 steps/s (collection: 2.178s, learning 0.151s)
             Mean action noise std: 2.40
          Mean value_function loss: 292.8847
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 59.2658
                       Mean reward: 736.77
               Mean episode length: 203.95
    Episode_Reward/reaching_object: 1.4976
     Episode_Reward/lifting_object: 156.3315
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 2.33s
                      Time elapsed: 00:38:24
                               ETA: 00:45:07

################################################################################
                     [1m Learning iteration 920/2000 [0m                      

                       Computation: 44288 steps/s (collection: 2.105s, learning 0.115s)
             Mean action noise std: 2.40
          Mean value_function loss: 289.9945
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 59.2672
                       Mean reward: 779.89
               Mean episode length: 215.02
    Episode_Reward/reaching_object: 1.4949
     Episode_Reward/lifting_object: 155.4073
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 2.22s
                      Time elapsed: 00:38:26
                               ETA: 00:45:05

################################################################################
                     [1m Learning iteration 921/2000 [0m                      

                       Computation: 42734 steps/s (collection: 2.141s, learning 0.160s)
             Mean action noise std: 2.40
          Mean value_function loss: 300.2258
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 59.2683
                       Mean reward: 778.60
               Mean episode length: 215.53
    Episode_Reward/reaching_object: 1.4562
     Episode_Reward/lifting_object: 151.6446
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 2.30s
                      Time elapsed: 00:38:29
                               ETA: 00:45:02

################################################################################
                     [1m Learning iteration 922/2000 [0m                      

                       Computation: 44673 steps/s (collection: 2.106s, learning 0.095s)
             Mean action noise std: 2.40
          Mean value_function loss: 327.6427
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 59.2693
                       Mean reward: 774.71
               Mean episode length: 215.08
    Episode_Reward/reaching_object: 1.4794
     Episode_Reward/lifting_object: 153.6270
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 2.20s
                      Time elapsed: 00:38:31
                               ETA: 00:44:59

################################################################################
                     [1m Learning iteration 923/2000 [0m                      

                       Computation: 44743 steps/s (collection: 2.076s, learning 0.121s)
             Mean action noise std: 2.40
          Mean value_function loss: 305.6392
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 59.2713
                       Mean reward: 786.92
               Mean episode length: 217.88
    Episode_Reward/reaching_object: 1.4966
     Episode_Reward/lifting_object: 155.1417
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 2.20s
                      Time elapsed: 00:38:33
                               ETA: 00:44:56

################################################################################
                     [1m Learning iteration 924/2000 [0m                      

                       Computation: 43553 steps/s (collection: 2.139s, learning 0.118s)
             Mean action noise std: 2.40
          Mean value_function loss: 355.5042
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 59.2750
                       Mean reward: 753.50
               Mean episode length: 209.37
    Episode_Reward/reaching_object: 1.4161
     Episode_Reward/lifting_object: 146.7024
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 2.26s
                      Time elapsed: 00:38:35
                               ETA: 00:44:53

################################################################################
                     [1m Learning iteration 925/2000 [0m                      

                       Computation: 44833 steps/s (collection: 2.100s, learning 0.092s)
             Mean action noise std: 2.40
          Mean value_function loss: 362.4563
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 59.2789
                       Mean reward: 690.31
               Mean episode length: 195.80
    Episode_Reward/reaching_object: 1.4583
     Episode_Reward/lifting_object: 150.6503
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 2.19s
                      Time elapsed: 00:38:37
                               ETA: 00:44:50

################################################################################
                     [1m Learning iteration 926/2000 [0m                      

                       Computation: 44360 steps/s (collection: 2.117s, learning 0.100s)
             Mean action noise std: 2.40
          Mean value_function loss: 330.3952
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 59.2868
                       Mean reward: 762.52
               Mean episode length: 211.11
    Episode_Reward/reaching_object: 1.4501
     Episode_Reward/lifting_object: 150.7046
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 2.22s
                      Time elapsed: 00:38:40
                               ETA: 00:44:48

################################################################################
                     [1m Learning iteration 927/2000 [0m                      

                       Computation: 43112 steps/s (collection: 2.122s, learning 0.159s)
             Mean action noise std: 2.40
          Mean value_function loss: 337.0929
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 59.3003
                       Mean reward: 792.15
               Mean episode length: 218.68
    Episode_Reward/reaching_object: 1.4438
     Episode_Reward/lifting_object: 149.7116
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 2.28s
                      Time elapsed: 00:38:42
                               ETA: 00:44:45

################################################################################
                     [1m Learning iteration 928/2000 [0m                      

                       Computation: 44650 steps/s (collection: 2.097s, learning 0.105s)
             Mean action noise std: 2.40
          Mean value_function loss: 356.1459
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 59.3130
                       Mean reward: 696.03
               Mean episode length: 194.22
    Episode_Reward/reaching_object: 1.4341
     Episode_Reward/lifting_object: 148.1246
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 2.20s
                      Time elapsed: 00:38:44
                               ETA: 00:44:42

################################################################################
                     [1m Learning iteration 929/2000 [0m                      

                       Computation: 44280 steps/s (collection: 2.125s, learning 0.096s)
             Mean action noise std: 2.40
          Mean value_function loss: 322.0317
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 59.3300
                       Mean reward: 730.49
               Mean episode length: 203.05
    Episode_Reward/reaching_object: 1.4455
     Episode_Reward/lifting_object: 150.1243
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 2.22s
                      Time elapsed: 00:38:46
                               ETA: 00:44:39

################################################################################
                     [1m Learning iteration 930/2000 [0m                      

                       Computation: 44272 steps/s (collection: 2.095s, learning 0.126s)
             Mean action noise std: 2.40
          Mean value_function loss: 338.9295
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 59.3483
                       Mean reward: 709.65
               Mean episode length: 198.99
    Episode_Reward/reaching_object: 1.4518
     Episode_Reward/lifting_object: 151.2332
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 2.22s
                      Time elapsed: 00:38:49
                               ETA: 00:44:36

################################################################################
                     [1m Learning iteration 931/2000 [0m                      

                       Computation: 40624 steps/s (collection: 2.227s, learning 0.193s)
             Mean action noise std: 2.41
          Mean value_function loss: 366.1919
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 59.3621
                       Mean reward: 731.19
               Mean episode length: 204.80
    Episode_Reward/reaching_object: 1.3969
     Episode_Reward/lifting_object: 144.7675
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 2.42s
                      Time elapsed: 00:38:51
                               ETA: 00:44:34

################################################################################
                     [1m Learning iteration 932/2000 [0m                      

                       Computation: 41377 steps/s (collection: 2.274s, learning 0.102s)
             Mean action noise std: 2.41
          Mean value_function loss: 341.6577
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 59.3799
                       Mean reward: 797.31
               Mean episode length: 217.79
    Episode_Reward/reaching_object: 1.4227
     Episode_Reward/lifting_object: 147.6794
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 2.38s
                      Time elapsed: 00:38:53
                               ETA: 00:44:31

################################################################################
                     [1m Learning iteration 933/2000 [0m                      

                       Computation: 44486 steps/s (collection: 2.112s, learning 0.098s)
             Mean action noise std: 2.41
          Mean value_function loss: 363.5762
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 59.3853
                       Mean reward: 758.99
               Mean episode length: 208.84
    Episode_Reward/reaching_object: 1.4420
     Episode_Reward/lifting_object: 149.2871
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 2.21s
                      Time elapsed: 00:38:56
                               ETA: 00:44:28

################################################################################
                     [1m Learning iteration 934/2000 [0m                      

                       Computation: 45774 steps/s (collection: 2.054s, learning 0.094s)
             Mean action noise std: 2.41
          Mean value_function loss: 343.2224
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 59.3892
                       Mean reward: 779.34
               Mean episode length: 214.38
    Episode_Reward/reaching_object: 1.4852
     Episode_Reward/lifting_object: 155.1474
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 2.15s
                      Time elapsed: 00:38:58
                               ETA: 00:44:25

################################################################################
                     [1m Learning iteration 935/2000 [0m                      

                       Computation: 44044 steps/s (collection: 2.088s, learning 0.144s)
             Mean action noise std: 2.41
          Mean value_function loss: 341.8250
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 59.3973
                       Mean reward: 788.98
               Mean episode length: 216.61
    Episode_Reward/reaching_object: 1.4638
     Episode_Reward/lifting_object: 152.0523
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 2.23s
                      Time elapsed: 00:39:00
                               ETA: 00:44:23

################################################################################
                     [1m Learning iteration 936/2000 [0m                      

                       Computation: 44310 steps/s (collection: 2.101s, learning 0.117s)
             Mean action noise std: 2.41
          Mean value_function loss: 380.5877
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 59.4083
                       Mean reward: 696.16
               Mean episode length: 195.45
    Episode_Reward/reaching_object: 1.4154
     Episode_Reward/lifting_object: 146.0329
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 2.22s
                      Time elapsed: 00:39:02
                               ETA: 00:44:20

################################################################################
                     [1m Learning iteration 937/2000 [0m                      

                       Computation: 43884 steps/s (collection: 2.117s, learning 0.123s)
             Mean action noise std: 2.41
          Mean value_function loss: 376.0900
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 59.4109
                       Mean reward: 728.70
               Mean episode length: 201.95
    Episode_Reward/reaching_object: 1.4281
     Episode_Reward/lifting_object: 148.3805
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 2.24s
                      Time elapsed: 00:39:04
                               ETA: 00:44:17

################################################################################
                     [1m Learning iteration 938/2000 [0m                      

                       Computation: 44831 steps/s (collection: 2.101s, learning 0.092s)
             Mean action noise std: 2.41
          Mean value_function loss: 307.7111
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 59.4199
                       Mean reward: 786.27
               Mean episode length: 215.39
    Episode_Reward/reaching_object: 1.4877
     Episode_Reward/lifting_object: 155.1682
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 2.19s
                      Time elapsed: 00:39:07
                               ETA: 00:44:14

################################################################################
                     [1m Learning iteration 939/2000 [0m                      

                       Computation: 45044 steps/s (collection: 2.072s, learning 0.111s)
             Mean action noise std: 2.41
          Mean value_function loss: 316.5339
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 59.4363
                       Mean reward: 773.85
               Mean episode length: 213.94
    Episode_Reward/reaching_object: 1.4519
     Episode_Reward/lifting_object: 151.3593
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 2.18s
                      Time elapsed: 00:39:09
                               ETA: 00:44:11

################################################################################
                     [1m Learning iteration 940/2000 [0m                      

                       Computation: 45469 steps/s (collection: 2.062s, learning 0.100s)
             Mean action noise std: 2.42
          Mean value_function loss: 346.0239
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 59.4563
                       Mean reward: 740.61
               Mean episode length: 206.08
    Episode_Reward/reaching_object: 1.4324
     Episode_Reward/lifting_object: 148.2556
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 2.16s
                      Time elapsed: 00:39:11
                               ETA: 00:44:08

################################################################################
                     [1m Learning iteration 941/2000 [0m                      

                       Computation: 44572 steps/s (collection: 2.096s, learning 0.109s)
             Mean action noise std: 2.42
          Mean value_function loss: 333.9981
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 59.4686
                       Mean reward: 750.47
               Mean episode length: 208.04
    Episode_Reward/reaching_object: 1.4825
     Episode_Reward/lifting_object: 154.6251
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 2.21s
                      Time elapsed: 00:39:13
                               ETA: 00:44:05

################################################################################
                     [1m Learning iteration 942/2000 [0m                      

                       Computation: 43464 steps/s (collection: 2.160s, learning 0.102s)
             Mean action noise std: 2.42
          Mean value_function loss: 304.0963
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 59.4779
                       Mean reward: 752.79
               Mean episode length: 208.81
    Episode_Reward/reaching_object: 1.4625
     Episode_Reward/lifting_object: 152.0490
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 2.26s
                      Time elapsed: 00:39:15
                               ETA: 00:44:03

################################################################################
                     [1m Learning iteration 943/2000 [0m                      

                       Computation: 45145 steps/s (collection: 2.070s, learning 0.107s)
             Mean action noise std: 2.42
          Mean value_function loss: 330.8714
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 59.4950
                       Mean reward: 801.41
               Mean episode length: 218.00
    Episode_Reward/reaching_object: 1.4714
     Episode_Reward/lifting_object: 152.9362
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 2.18s
                      Time elapsed: 00:39:18
                               ETA: 00:44:00

################################################################################
                     [1m Learning iteration 944/2000 [0m                      

                       Computation: 44897 steps/s (collection: 2.084s, learning 0.106s)
             Mean action noise std: 2.42
          Mean value_function loss: 285.1532
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 59.5071
                       Mean reward: 765.82
               Mean episode length: 212.90
    Episode_Reward/reaching_object: 1.5203
     Episode_Reward/lifting_object: 157.9726
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 2.19s
                      Time elapsed: 00:39:20
                               ETA: 00:43:57

################################################################################
                     [1m Learning iteration 945/2000 [0m                      

                       Computation: 44991 steps/s (collection: 2.072s, learning 0.113s)
             Mean action noise std: 2.42
          Mean value_function loss: 270.4190
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 59.5108
                       Mean reward: 762.47
               Mean episode length: 210.84
    Episode_Reward/reaching_object: 1.4613
     Episode_Reward/lifting_object: 151.6922
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 2.18s
                      Time elapsed: 00:39:22
                               ETA: 00:43:54

################################################################################
                     [1m Learning iteration 946/2000 [0m                      

                       Computation: 41952 steps/s (collection: 2.158s, learning 0.185s)
             Mean action noise std: 2.42
          Mean value_function loss: 290.4564
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 59.5126
                       Mean reward: 780.08
               Mean episode length: 215.48
    Episode_Reward/reaching_object: 1.4865
     Episode_Reward/lifting_object: 153.8049
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 2.34s
                      Time elapsed: 00:39:24
                               ETA: 00:43:51

################################################################################
                     [1m Learning iteration 947/2000 [0m                      

                       Computation: 39769 steps/s (collection: 2.367s, learning 0.105s)
             Mean action noise std: 2.42
          Mean value_function loss: 320.1134
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 59.5215
                       Mean reward: 757.48
               Mean episode length: 207.23
    Episode_Reward/reaching_object: 1.5406
     Episode_Reward/lifting_object: 160.4795
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 2.47s
                      Time elapsed: 00:39:27
                               ETA: 00:43:49

################################################################################
                     [1m Learning iteration 948/2000 [0m                      

                       Computation: 37451 steps/s (collection: 2.524s, learning 0.100s)
             Mean action noise std: 2.42
          Mean value_function loss: 289.4976
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 59.5328
                       Mean reward: 756.62
               Mean episode length: 210.57
    Episode_Reward/reaching_object: 1.5073
     Episode_Reward/lifting_object: 155.2812
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 2.62s
                      Time elapsed: 00:39:29
                               ETA: 00:43:47

################################################################################
                     [1m Learning iteration 949/2000 [0m                      

                       Computation: 41024 steps/s (collection: 2.267s, learning 0.129s)
             Mean action noise std: 2.42
          Mean value_function loss: 331.0311
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 59.5436
                       Mean reward: 711.58
               Mean episode length: 199.77
    Episode_Reward/reaching_object: 1.4288
     Episode_Reward/lifting_object: 146.8593
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 2.40s
                      Time elapsed: 00:39:32
                               ETA: 00:43:44

################################################################################
                     [1m Learning iteration 950/2000 [0m                      

                       Computation: 37187 steps/s (collection: 2.453s, learning 0.190s)
             Mean action noise std: 2.43
          Mean value_function loss: 362.8254
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 59.5514
                       Mean reward: 756.92
               Mean episode length: 208.96
    Episode_Reward/reaching_object: 1.4506
     Episode_Reward/lifting_object: 149.9718
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 2.64s
                      Time elapsed: 00:39:34
                               ETA: 00:43:42

################################################################################
                     [1m Learning iteration 951/2000 [0m                      

                       Computation: 36025 steps/s (collection: 2.516s, learning 0.213s)
             Mean action noise std: 2.43
          Mean value_function loss: 308.1632
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 59.5651
                       Mean reward: 761.38
               Mean episode length: 211.16
    Episode_Reward/reaching_object: 1.4738
     Episode_Reward/lifting_object: 151.7528
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 2.73s
                      Time elapsed: 00:39:37
                               ETA: 00:43:39

################################################################################
                     [1m Learning iteration 952/2000 [0m                      

                       Computation: 42153 steps/s (collection: 2.174s, learning 0.158s)
             Mean action noise std: 2.43
          Mean value_function loss: 297.1980
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 59.5791
                       Mean reward: 741.27
               Mean episode length: 205.24
    Episode_Reward/reaching_object: 1.4944
     Episode_Reward/lifting_object: 154.8731
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 2.33s
                      Time elapsed: 00:39:39
                               ETA: 00:43:37

################################################################################
                     [1m Learning iteration 953/2000 [0m                      

                       Computation: 40911 steps/s (collection: 2.263s, learning 0.140s)
             Mean action noise std: 2.43
          Mean value_function loss: 305.8228
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 59.5907
                       Mean reward: 808.51
               Mean episode length: 222.11
    Episode_Reward/reaching_object: 1.4745
     Episode_Reward/lifting_object: 152.4808
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 2.40s
                      Time elapsed: 00:39:42
                               ETA: 00:43:34

################################################################################
                     [1m Learning iteration 954/2000 [0m                      

                       Computation: 41194 steps/s (collection: 2.216s, learning 0.170s)
             Mean action noise std: 2.43
          Mean value_function loss: 292.0391
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 59.6021
                       Mean reward: 792.07
               Mean episode length: 218.22
    Episode_Reward/reaching_object: 1.5236
     Episode_Reward/lifting_object: 157.7220
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 2.39s
                      Time elapsed: 00:39:44
                               ETA: 00:43:32

################################################################################
                     [1m Learning iteration 955/2000 [0m                      

                       Computation: 39555 steps/s (collection: 2.269s, learning 0.216s)
             Mean action noise std: 2.43
          Mean value_function loss: 300.0463
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 59.6104
                       Mean reward: 716.90
               Mean episode length: 202.38
    Episode_Reward/reaching_object: 1.4936
     Episode_Reward/lifting_object: 153.8738
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 2.49s
                      Time elapsed: 00:39:47
                               ETA: 00:43:29

################################################################################
                     [1m Learning iteration 956/2000 [0m                      

                       Computation: 38772 steps/s (collection: 2.350s, learning 0.186s)
             Mean action noise std: 2.43
          Mean value_function loss: 302.6885
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 59.6163
                       Mean reward: 792.48
               Mean episode length: 216.38
    Episode_Reward/reaching_object: 1.4722
     Episode_Reward/lifting_object: 152.3772
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 2.54s
                      Time elapsed: 00:39:49
                               ETA: 00:43:27

################################################################################
                     [1m Learning iteration 957/2000 [0m                      

                       Computation: 38827 steps/s (collection: 2.427s, learning 0.105s)
             Mean action noise std: 2.43
          Mean value_function loss: 361.2369
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 59.6257
                       Mean reward: 794.37
               Mean episode length: 217.57
    Episode_Reward/reaching_object: 1.5014
     Episode_Reward/lifting_object: 155.3544
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 2.53s
                      Time elapsed: 00:39:52
                               ETA: 00:43:24

################################################################################
                     [1m Learning iteration 958/2000 [0m                      

                       Computation: 39365 steps/s (collection: 2.338s, learning 0.160s)
             Mean action noise std: 2.43
          Mean value_function loss: 342.4982
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 59.6316
                       Mean reward: 745.15
               Mean episode length: 205.34
    Episode_Reward/reaching_object: 1.4624
     Episode_Reward/lifting_object: 150.9276
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 2.50s
                      Time elapsed: 00:39:54
                               ETA: 00:43:22

################################################################################
                     [1m Learning iteration 959/2000 [0m                      

                       Computation: 39090 steps/s (collection: 2.316s, learning 0.199s)
             Mean action noise std: 2.43
          Mean value_function loss: 358.8386
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 59.6337
                       Mean reward: 772.15
               Mean episode length: 215.55
    Episode_Reward/reaching_object: 1.4564
     Episode_Reward/lifting_object: 149.9388
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 2.51s
                      Time elapsed: 00:39:57
                               ETA: 00:43:19

################################################################################
                     [1m Learning iteration 960/2000 [0m                      

                       Computation: 39282 steps/s (collection: 2.339s, learning 0.163s)
             Mean action noise std: 2.43
          Mean value_function loss: 320.6624
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 59.6360
                       Mean reward: 750.03
               Mean episode length: 208.32
    Episode_Reward/reaching_object: 1.4311
     Episode_Reward/lifting_object: 147.8190
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 2.50s
                      Time elapsed: 00:39:59
                               ETA: 00:43:17

################################################################################
                     [1m Learning iteration 961/2000 [0m                      

                       Computation: 39666 steps/s (collection: 2.318s, learning 0.160s)
             Mean action noise std: 2.43
          Mean value_function loss: 349.5821
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 59.6395
                       Mean reward: 744.89
               Mean episode length: 204.89
    Episode_Reward/reaching_object: 1.4251
     Episode_Reward/lifting_object: 146.2874
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 2.48s
                      Time elapsed: 00:40:02
                               ETA: 00:43:14

################################################################################
                     [1m Learning iteration 962/2000 [0m                      

                       Computation: 37719 steps/s (collection: 2.471s, learning 0.135s)
             Mean action noise std: 2.44
          Mean value_function loss: 370.0271
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 59.6454
                       Mean reward: 773.61
               Mean episode length: 212.62
    Episode_Reward/reaching_object: 1.4606
     Episode_Reward/lifting_object: 150.4112
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 2.61s
                      Time elapsed: 00:40:04
                               ETA: 00:43:12

################################################################################
                     [1m Learning iteration 963/2000 [0m                      

                       Computation: 41168 steps/s (collection: 2.209s, learning 0.179s)
             Mean action noise std: 2.44
          Mean value_function loss: 385.8984
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 59.6531
                       Mean reward: 733.66
               Mean episode length: 202.84
    Episode_Reward/reaching_object: 1.4276
     Episode_Reward/lifting_object: 147.2705
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 2.39s
                      Time elapsed: 00:40:07
                               ETA: 00:43:09

################################################################################
                     [1m Learning iteration 964/2000 [0m                      

                       Computation: 40070 steps/s (collection: 2.221s, learning 0.232s)
             Mean action noise std: 2.44
          Mean value_function loss: 403.9830
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 59.6593
                       Mean reward: 746.89
               Mean episode length: 206.97
    Episode_Reward/reaching_object: 1.4037
     Episode_Reward/lifting_object: 143.7080
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 2.45s
                      Time elapsed: 00:40:09
                               ETA: 00:43:07

################################################################################
                     [1m Learning iteration 965/2000 [0m                      

                       Computation: 38269 steps/s (collection: 2.412s, learning 0.157s)
             Mean action noise std: 2.44
          Mean value_function loss: 397.4832
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 59.6619
                       Mean reward: 727.70
               Mean episode length: 202.75
    Episode_Reward/reaching_object: 1.3916
     Episode_Reward/lifting_object: 142.4889
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 2.57s
                      Time elapsed: 00:40:12
                               ETA: 00:43:04

################################################################################
                     [1m Learning iteration 966/2000 [0m                      

                       Computation: 40610 steps/s (collection: 2.197s, learning 0.224s)
             Mean action noise std: 2.44
          Mean value_function loss: 368.1241
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 59.6636
                       Mean reward: 673.91
               Mean episode length: 191.24
    Episode_Reward/reaching_object: 1.3856
     Episode_Reward/lifting_object: 141.9124
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 2.42s
                      Time elapsed: 00:40:14
                               ETA: 00:43:02

################################################################################
                     [1m Learning iteration 967/2000 [0m                      

                       Computation: 39129 steps/s (collection: 2.354s, learning 0.159s)
             Mean action noise std: 2.44
          Mean value_function loss: 387.6748
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 59.6648
                       Mean reward: 781.76
               Mean episode length: 214.72
    Episode_Reward/reaching_object: 1.3993
     Episode_Reward/lifting_object: 143.4508
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 2.51s
                      Time elapsed: 00:40:17
                               ETA: 00:42:59

################################################################################
                     [1m Learning iteration 968/2000 [0m                      

                       Computation: 41326 steps/s (collection: 2.190s, learning 0.189s)
             Mean action noise std: 2.44
          Mean value_function loss: 329.1552
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 59.6660
                       Mean reward: 758.10
               Mean episode length: 207.82
    Episode_Reward/reaching_object: 1.4305
     Episode_Reward/lifting_object: 148.1880
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 2.38s
                      Time elapsed: 00:40:19
                               ETA: 00:42:56

################################################################################
                     [1m Learning iteration 969/2000 [0m                      

                       Computation: 38774 steps/s (collection: 2.402s, learning 0.134s)
             Mean action noise std: 2.44
          Mean value_function loss: 313.5873
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 59.6669
                       Mean reward: 797.89
               Mean episode length: 218.43
    Episode_Reward/reaching_object: 1.4944
     Episode_Reward/lifting_object: 155.8120
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 2.54s
                      Time elapsed: 00:40:22
                               ETA: 00:42:54

################################################################################
                     [1m Learning iteration 970/2000 [0m                      

                       Computation: 39356 steps/s (collection: 2.327s, learning 0.171s)
             Mean action noise std: 2.44
          Mean value_function loss: 314.1249
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 59.6708
                       Mean reward: 752.22
               Mean episode length: 207.57
    Episode_Reward/reaching_object: 1.4658
     Episode_Reward/lifting_object: 152.7299
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 2.50s
                      Time elapsed: 00:40:24
                               ETA: 00:42:52

################################################################################
                     [1m Learning iteration 971/2000 [0m                      

                       Computation: 40011 steps/s (collection: 2.295s, learning 0.162s)
             Mean action noise std: 2.44
          Mean value_function loss: 287.3247
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 59.6804
                       Mean reward: 791.29
               Mean episode length: 214.95
    Episode_Reward/reaching_object: 1.5274
     Episode_Reward/lifting_object: 159.3204
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 2.46s
                      Time elapsed: 00:40:27
                               ETA: 00:42:49

################################################################################
                     [1m Learning iteration 972/2000 [0m                      

                       Computation: 40652 steps/s (collection: 2.289s, learning 0.129s)
             Mean action noise std: 2.44
          Mean value_function loss: 349.1666
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.6896
                       Mean reward: 763.65
               Mean episode length: 208.66
    Episode_Reward/reaching_object: 1.4464
     Episode_Reward/lifting_object: 150.1724
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 2.42s
                      Time elapsed: 00:40:29
                               ETA: 00:42:46

################################################################################
                     [1m Learning iteration 973/2000 [0m                      

                       Computation: 42819 steps/s (collection: 2.199s, learning 0.097s)
             Mean action noise std: 2.44
          Mean value_function loss: 370.0343
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 59.7018
                       Mean reward: 757.34
               Mean episode length: 209.96
    Episode_Reward/reaching_object: 1.4635
     Episode_Reward/lifting_object: 150.8459
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 2.30s
                      Time elapsed: 00:40:31
                               ETA: 00:42:44

################################################################################
                     [1m Learning iteration 974/2000 [0m                      

                       Computation: 44888 steps/s (collection: 2.099s, learning 0.091s)
             Mean action noise std: 2.44
          Mean value_function loss: 358.5491
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 59.7128
                       Mean reward: 759.88
               Mean episode length: 211.76
    Episode_Reward/reaching_object: 1.4750
     Episode_Reward/lifting_object: 151.8753
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 2.19s
                      Time elapsed: 00:40:34
                               ETA: 00:42:41

################################################################################
                     [1m Learning iteration 975/2000 [0m                      

                       Computation: 44897 steps/s (collection: 2.081s, learning 0.109s)
             Mean action noise std: 2.44
          Mean value_function loss: 319.8985
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 59.7182
                       Mean reward: 775.95
               Mean episode length: 212.07
    Episode_Reward/reaching_object: 1.5093
     Episode_Reward/lifting_object: 155.9734
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 2.19s
                      Time elapsed: 00:40:36
                               ETA: 00:42:38

################################################################################
                     [1m Learning iteration 976/2000 [0m                      

                       Computation: 44667 steps/s (collection: 2.098s, learning 0.103s)
             Mean action noise std: 2.44
          Mean value_function loss: 321.0258
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 59.7214
                       Mean reward: 765.44
               Mean episode length: 210.44
    Episode_Reward/reaching_object: 1.4625
     Episode_Reward/lifting_object: 151.1934
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 2.20s
                      Time elapsed: 00:40:38
                               ETA: 00:42:35

################################################################################
                     [1m Learning iteration 977/2000 [0m                      

                       Computation: 43956 steps/s (collection: 2.131s, learning 0.106s)
             Mean action noise std: 2.44
          Mean value_function loss: 334.4009
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 59.7252
                       Mean reward: 807.14
               Mean episode length: 219.66
    Episode_Reward/reaching_object: 1.4548
     Episode_Reward/lifting_object: 150.5639
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 2.24s
                      Time elapsed: 00:40:40
                               ETA: 00:42:32

################################################################################
                     [1m Learning iteration 978/2000 [0m                      

                       Computation: 43733 steps/s (collection: 2.143s, learning 0.105s)
             Mean action noise std: 2.45
          Mean value_function loss: 304.2456
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 59.7380
                       Mean reward: 738.91
               Mean episode length: 208.13
    Episode_Reward/reaching_object: 1.4958
     Episode_Reward/lifting_object: 155.1831
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 2.25s
                      Time elapsed: 00:40:42
                               ETA: 00:42:30

################################################################################
                     [1m Learning iteration 979/2000 [0m                      

                       Computation: 44162 steps/s (collection: 2.135s, learning 0.090s)
             Mean action noise std: 2.45
          Mean value_function loss: 327.9746
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 59.7453
                       Mean reward: 816.03
               Mean episode length: 223.88
    Episode_Reward/reaching_object: 1.4801
     Episode_Reward/lifting_object: 153.0477
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 2.23s
                      Time elapsed: 00:40:45
                               ETA: 00:42:27

################################################################################
                     [1m Learning iteration 980/2000 [0m                      

                       Computation: 43105 steps/s (collection: 2.170s, learning 0.111s)
             Mean action noise std: 2.45
          Mean value_function loss: 315.1843
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 59.7503
                       Mean reward: 773.29
               Mean episode length: 212.01
    Episode_Reward/reaching_object: 1.4627
     Episode_Reward/lifting_object: 150.5071
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 2.28s
                      Time elapsed: 00:40:47
                               ETA: 00:42:24

################################################################################
                     [1m Learning iteration 981/2000 [0m                      

                       Computation: 42827 steps/s (collection: 2.155s, learning 0.140s)
             Mean action noise std: 2.45
          Mean value_function loss: 348.7498
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 59.7538
                       Mean reward: 775.71
               Mean episode length: 213.48
    Episode_Reward/reaching_object: 1.4814
     Episode_Reward/lifting_object: 152.7092
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 2.30s
                      Time elapsed: 00:40:49
                               ETA: 00:42:22

################################################################################
                     [1m Learning iteration 982/2000 [0m                      

                       Computation: 42676 steps/s (collection: 2.180s, learning 0.123s)
             Mean action noise std: 2.45
          Mean value_function loss: 326.4357
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 59.7578
                       Mean reward: 768.01
               Mean episode length: 211.99
    Episode_Reward/reaching_object: 1.4781
     Episode_Reward/lifting_object: 152.4249
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 2.30s
                      Time elapsed: 00:40:52
                               ETA: 00:42:19

################################################################################
                     [1m Learning iteration 983/2000 [0m                      

                       Computation: 40980 steps/s (collection: 2.262s, learning 0.137s)
             Mean action noise std: 2.45
          Mean value_function loss: 294.7112
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 59.7645
                       Mean reward: 797.82
               Mean episode length: 220.50
    Episode_Reward/reaching_object: 1.4837
     Episode_Reward/lifting_object: 152.9607
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 2.40s
                      Time elapsed: 00:40:54
                               ETA: 00:42:16

################################################################################
                     [1m Learning iteration 984/2000 [0m                      

                       Computation: 41638 steps/s (collection: 2.198s, learning 0.163s)
             Mean action noise std: 2.45
          Mean value_function loss: 316.9183
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 59.7764
                       Mean reward: 778.74
               Mean episode length: 211.05
    Episode_Reward/reaching_object: 1.4855
     Episode_Reward/lifting_object: 154.6620
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 2.36s
                      Time elapsed: 00:40:56
                               ETA: 00:42:14

################################################################################
                     [1m Learning iteration 985/2000 [0m                      

                       Computation: 37545 steps/s (collection: 2.472s, learning 0.147s)
             Mean action noise std: 2.45
          Mean value_function loss: 327.3007
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 59.7860
                       Mean reward: 784.49
               Mean episode length: 213.85
    Episode_Reward/reaching_object: 1.5117
     Episode_Reward/lifting_object: 156.5585
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 2.62s
                      Time elapsed: 00:40:59
                               ETA: 00:42:11

################################################################################
                     [1m Learning iteration 986/2000 [0m                      

                       Computation: 33271 steps/s (collection: 2.753s, learning 0.201s)
             Mean action noise std: 2.45
          Mean value_function loss: 327.1628
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 59.7907
                       Mean reward: 768.45
               Mean episode length: 212.99
    Episode_Reward/reaching_object: 1.4988
     Episode_Reward/lifting_object: 155.9231
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 2.95s
                      Time elapsed: 00:41:02
                               ETA: 00:42:09

################################################################################
                     [1m Learning iteration 987/2000 [0m                      

                       Computation: 36816 steps/s (collection: 2.528s, learning 0.142s)
             Mean action noise std: 2.45
          Mean value_function loss: 353.5712
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 59.7961
                       Mean reward: 789.84
               Mean episode length: 216.66
    Episode_Reward/reaching_object: 1.4447
     Episode_Reward/lifting_object: 149.9310
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 2.67s
                      Time elapsed: 00:41:05
                               ETA: 00:42:07

################################################################################
                     [1m Learning iteration 988/2000 [0m                      

                       Computation: 41690 steps/s (collection: 2.241s, learning 0.117s)
             Mean action noise std: 2.45
          Mean value_function loss: 324.0266
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 59.8022
                       Mean reward: 751.37
               Mean episode length: 206.10
    Episode_Reward/reaching_object: 1.4412
     Episode_Reward/lifting_object: 149.6641
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 2.36s
                      Time elapsed: 00:41:07
                               ETA: 00:42:04

################################################################################
                     [1m Learning iteration 989/2000 [0m                      

                       Computation: 36377 steps/s (collection: 2.573s, learning 0.130s)
             Mean action noise std: 2.45
          Mean value_function loss: 338.4841
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 59.8059
                       Mean reward: 772.68
               Mean episode length: 211.07
    Episode_Reward/reaching_object: 1.4852
     Episode_Reward/lifting_object: 154.2741
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 2.70s
                      Time elapsed: 00:41:10
                               ETA: 00:42:02

################################################################################
                     [1m Learning iteration 990/2000 [0m                      

                       Computation: 41145 steps/s (collection: 2.289s, learning 0.101s)
             Mean action noise std: 2.45
          Mean value_function loss: 280.3065
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 59.8153
                       Mean reward: 829.89
               Mean episode length: 223.36
    Episode_Reward/reaching_object: 1.4980
     Episode_Reward/lifting_object: 155.9790
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 2.39s
                      Time elapsed: 00:41:12
                               ETA: 00:41:59

################################################################################
                     [1m Learning iteration 991/2000 [0m                      

                       Computation: 42142 steps/s (collection: 2.224s, learning 0.109s)
             Mean action noise std: 2.46
          Mean value_function loss: 302.1658
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 59.8239
                       Mean reward: 751.88
               Mean episode length: 204.79
    Episode_Reward/reaching_object: 1.4909
     Episode_Reward/lifting_object: 154.9544
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 2.33s
                      Time elapsed: 00:41:14
                               ETA: 00:41:57

################################################################################
                     [1m Learning iteration 992/2000 [0m                      

                       Computation: 42392 steps/s (collection: 2.216s, learning 0.103s)
             Mean action noise std: 2.46
          Mean value_function loss: 286.8274
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 59.8264
                       Mean reward: 800.98
               Mean episode length: 217.97
    Episode_Reward/reaching_object: 1.5035
     Episode_Reward/lifting_object: 156.3500
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 2.32s
                      Time elapsed: 00:41:17
                               ETA: 00:41:54

################################################################################
                     [1m Learning iteration 993/2000 [0m                      

                       Computation: 40885 steps/s (collection: 2.298s, learning 0.107s)
             Mean action noise std: 2.46
          Mean value_function loss: 288.8384
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 59.8276
                       Mean reward: 777.61
               Mean episode length: 213.65
    Episode_Reward/reaching_object: 1.5170
     Episode_Reward/lifting_object: 157.6810
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 2.40s
                      Time elapsed: 00:41:19
                               ETA: 00:41:51

################################################################################
                     [1m Learning iteration 994/2000 [0m                      

                       Computation: 38884 steps/s (collection: 2.409s, learning 0.120s)
             Mean action noise std: 2.46
          Mean value_function loss: 324.7722
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 59.8296
                       Mean reward: 777.57
               Mean episode length: 213.65
    Episode_Reward/reaching_object: 1.5011
     Episode_Reward/lifting_object: 156.2368
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 2.53s
                      Time elapsed: 00:41:22
                               ETA: 00:41:49

################################################################################
                     [1m Learning iteration 995/2000 [0m                      

                       Computation: 43226 steps/s (collection: 2.171s, learning 0.103s)
             Mean action noise std: 2.46
          Mean value_function loss: 307.6856
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 59.8313
                       Mean reward: 798.08
               Mean episode length: 220.65
    Episode_Reward/reaching_object: 1.4921
     Episode_Reward/lifting_object: 154.5349
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 2.27s
                      Time elapsed: 00:41:24
                               ETA: 00:41:46

################################################################################
                     [1m Learning iteration 996/2000 [0m                      

                       Computation: 44580 steps/s (collection: 2.092s, learning 0.113s)
             Mean action noise std: 2.46
          Mean value_function loss: 297.8026
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 59.8326
                       Mean reward: 784.11
               Mean episode length: 215.71
    Episode_Reward/reaching_object: 1.4964
     Episode_Reward/lifting_object: 154.9572
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 2.21s
                      Time elapsed: 00:41:26
                               ETA: 00:41:44

################################################################################
                     [1m Learning iteration 997/2000 [0m                      

                       Computation: 43741 steps/s (collection: 2.137s, learning 0.110s)
             Mean action noise std: 2.46
          Mean value_function loss: 372.1638
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 59.8333
                       Mean reward: 764.48
               Mean episode length: 210.15
    Episode_Reward/reaching_object: 1.4924
     Episode_Reward/lifting_object: 154.8122
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 2.25s
                      Time elapsed: 00:41:28
                               ETA: 00:41:41

################################################################################
                     [1m Learning iteration 998/2000 [0m                      

                       Computation: 43906 steps/s (collection: 2.119s, learning 0.120s)
             Mean action noise std: 2.46
          Mean value_function loss: 427.7774
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 59.8340
                       Mean reward: 739.34
               Mean episode length: 205.60
    Episode_Reward/reaching_object: 1.4473
     Episode_Reward/lifting_object: 149.2833
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 2.24s
                      Time elapsed: 00:41:31
                               ETA: 00:41:38

################################################################################
                     [1m Learning iteration 999/2000 [0m                      

                       Computation: 42597 steps/s (collection: 2.206s, learning 0.102s)
             Mean action noise std: 2.46
          Mean value_function loss: 478.0196
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 59.8350
                       Mean reward: 743.13
               Mean episode length: 205.30
    Episode_Reward/reaching_object: 1.4290
     Episode_Reward/lifting_object: 145.8614
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 2.31s
                      Time elapsed: 00:41:33
                               ETA: 00:41:35

################################################################################
                     [1m Learning iteration 1000/2000 [0m                     

                       Computation: 13426 steps/s (collection: 7.204s, learning 0.118s)
             Mean action noise std: 2.46
          Mean value_function loss: 467.5330
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 59.8360
                       Mean reward: 740.83
               Mean episode length: 205.98
    Episode_Reward/reaching_object: 1.4022
     Episode_Reward/lifting_object: 143.6495
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 7.32s
                      Time elapsed: 00:41:40
                               ETA: 00:41:38

################################################################################
                     [1m Learning iteration 1001/2000 [0m                     

                       Computation: 14040 steps/s (collection: 6.883s, learning 0.118s)
             Mean action noise std: 2.46
          Mean value_function loss: 458.3506
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 59.8383
                       Mean reward: 696.78
               Mean episode length: 196.60
    Episode_Reward/reaching_object: 1.3839
     Episode_Reward/lifting_object: 140.1495
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 7.00s
                      Time elapsed: 00:41:47
                               ETA: 00:41:40

################################################################################
                     [1m Learning iteration 1002/2000 [0m                     

                       Computation: 14214 steps/s (collection: 6.797s, learning 0.119s)
             Mean action noise std: 2.46
          Mean value_function loss: 372.7273
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 59.8467
                       Mean reward: 681.45
               Mean episode length: 192.08
    Episode_Reward/reaching_object: 1.3838
     Episode_Reward/lifting_object: 140.4877
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 6.92s
                      Time elapsed: 00:41:54
                               ETA: 00:41:42

################################################################################
                     [1m Learning iteration 1003/2000 [0m                     

                       Computation: 14612 steps/s (collection: 6.611s, learning 0.117s)
             Mean action noise std: 2.46
          Mean value_function loss: 305.5615
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 59.8601
                       Mean reward: 758.50
               Mean episode length: 209.23
    Episode_Reward/reaching_object: 1.4249
     Episode_Reward/lifting_object: 147.1952
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 6.73s
                      Time elapsed: 00:42:01
                               ETA: 00:41:43

################################################################################
                     [1m Learning iteration 1004/2000 [0m                     

                       Computation: 14749 steps/s (collection: 6.543s, learning 0.123s)
             Mean action noise std: 2.46
          Mean value_function loss: 293.0725
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 59.8846
                       Mean reward: 796.71
               Mean episode length: 219.53
    Episode_Reward/reaching_object: 1.4458
     Episode_Reward/lifting_object: 147.8360
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 6.67s
                      Time elapsed: 00:42:07
                               ETA: 00:41:45

################################################################################
                     [1m Learning iteration 1005/2000 [0m                     

                       Computation: 14473 steps/s (collection: 6.673s, learning 0.119s)
             Mean action noise std: 2.46
          Mean value_function loss: 280.2278
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 59.9078
                       Mean reward: 792.63
               Mean episode length: 220.37
    Episode_Reward/reaching_object: 1.4974
     Episode_Reward/lifting_object: 154.0254
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 6.79s
                      Time elapsed: 00:42:14
                               ETA: 00:41:47

################################################################################
                     [1m Learning iteration 1006/2000 [0m                     

                       Computation: 14727 steps/s (collection: 6.553s, learning 0.122s)
             Mean action noise std: 2.47
          Mean value_function loss: 270.3412
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 59.9204
                       Mean reward: 795.36
               Mean episode length: 221.06
    Episode_Reward/reaching_object: 1.4705
     Episode_Reward/lifting_object: 152.1952
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 6.67s
                      Time elapsed: 00:42:21
                               ETA: 00:41:48

################################################################################
                     [1m Learning iteration 1007/2000 [0m                     

                       Computation: 14628 steps/s (collection: 6.601s, learning 0.119s)
             Mean action noise std: 2.47
          Mean value_function loss: 295.4086
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.9329
                       Mean reward: 775.65
               Mean episode length: 211.05
    Episode_Reward/reaching_object: 1.5173
     Episode_Reward/lifting_object: 158.4467
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 6.72s
                      Time elapsed: 00:42:28
                               ETA: 00:41:50

################################################################################
                     [1m Learning iteration 1008/2000 [0m                     

                       Computation: 17646 steps/s (collection: 5.473s, learning 0.098s)
             Mean action noise std: 2.47
          Mean value_function loss: 309.5265
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 59.9467
                       Mean reward: 840.47
               Mean episode length: 225.83
    Episode_Reward/reaching_object: 1.4982
     Episode_Reward/lifting_object: 157.5631
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 5.57s
                      Time elapsed: 00:42:33
                               ETA: 00:41:50

################################################################################
                     [1m Learning iteration 1009/2000 [0m                     

                       Computation: 46830 steps/s (collection: 1.990s, learning 0.109s)
             Mean action noise std: 2.47
          Mean value_function loss: 296.0292
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 59.9610
                       Mean reward: 814.99
               Mean episode length: 220.89
    Episode_Reward/reaching_object: 1.5212
     Episode_Reward/lifting_object: 159.4698
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 2.10s
                      Time elapsed: 00:42:35
                               ETA: 00:41:47

################################################################################
                     [1m Learning iteration 1010/2000 [0m                     

                       Computation: 47927 steps/s (collection: 1.944s, learning 0.107s)
             Mean action noise std: 2.47
          Mean value_function loss: 300.0624
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 59.9714
                       Mean reward: 789.97
               Mean episode length: 217.13
    Episode_Reward/reaching_object: 1.5372
     Episode_Reward/lifting_object: 161.5332
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 2.05s
                      Time elapsed: 00:42:37
                               ETA: 00:41:44

################################################################################
                     [1m Learning iteration 1011/2000 [0m                     

                       Computation: 47050 steps/s (collection: 1.993s, learning 0.096s)
             Mean action noise std: 2.47
          Mean value_function loss: 264.9055
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 59.9908
                       Mean reward: 772.42
               Mean episode length: 211.32
    Episode_Reward/reaching_object: 1.5186
     Episode_Reward/lifting_object: 159.6641
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 2.09s
                      Time elapsed: 00:42:39
                               ETA: 00:41:41

################################################################################
                     [1m Learning iteration 1012/2000 [0m                     

                       Computation: 47402 steps/s (collection: 1.989s, learning 0.085s)
             Mean action noise std: 2.47
          Mean value_function loss: 318.2352
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 60.0054
                       Mean reward: 816.67
               Mean episode length: 222.97
    Episode_Reward/reaching_object: 1.5036
     Episode_Reward/lifting_object: 156.8860
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 2.07s
                      Time elapsed: 00:42:42
                               ETA: 00:41:38

################################################################################
                     [1m Learning iteration 1013/2000 [0m                     

                       Computation: 47743 steps/s (collection: 1.968s, learning 0.091s)
             Mean action noise std: 2.47
          Mean value_function loss: 259.7772
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 60.0077
                       Mean reward: 788.80
               Mean episode length: 215.70
    Episode_Reward/reaching_object: 1.5003
     Episode_Reward/lifting_object: 156.6760
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 2.06s
                      Time elapsed: 00:42:44
                               ETA: 00:41:35

################################################################################
                     [1m Learning iteration 1014/2000 [0m                     

                       Computation: 48211 steps/s (collection: 1.953s, learning 0.086s)
             Mean action noise std: 2.48
          Mean value_function loss: 271.2208
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 60.0145
                       Mean reward: 815.83
               Mean episode length: 220.40
    Episode_Reward/reaching_object: 1.5002
     Episode_Reward/lifting_object: 157.4077
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 2.04s
                      Time elapsed: 00:42:46
                               ETA: 00:41:32

################################################################################
                     [1m Learning iteration 1015/2000 [0m                     

                       Computation: 48235 steps/s (collection: 1.950s, learning 0.088s)
             Mean action noise std: 2.48
          Mean value_function loss: 220.1476
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 60.0251
                       Mean reward: 817.40
               Mean episode length: 222.79
    Episode_Reward/reaching_object: 1.4944
     Episode_Reward/lifting_object: 155.5688
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 2.04s
                      Time elapsed: 00:42:48
                               ETA: 00:41:29

################################################################################
                     [1m Learning iteration 1016/2000 [0m                     

                       Computation: 47230 steps/s (collection: 1.990s, learning 0.091s)
             Mean action noise std: 2.48
          Mean value_function loss: 276.2301
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 60.0417
                       Mean reward: 784.57
               Mean episode length: 213.59
    Episode_Reward/reaching_object: 1.4734
     Episode_Reward/lifting_object: 153.5749
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 2.08s
                      Time elapsed: 00:42:50
                               ETA: 00:41:26

################################################################################
                     [1m Learning iteration 1017/2000 [0m                     

                       Computation: 47320 steps/s (collection: 1.991s, learning 0.086s)
             Mean action noise std: 2.48
          Mean value_function loss: 294.3784
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 60.0666
                       Mean reward: 819.49
               Mean episode length: 223.13
    Episode_Reward/reaching_object: 1.5242
     Episode_Reward/lifting_object: 159.5509
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 2.08s
                      Time elapsed: 00:42:52
                               ETA: 00:41:23

################################################################################
                     [1m Learning iteration 1018/2000 [0m                     

                       Computation: 48498 steps/s (collection: 1.932s, learning 0.095s)
             Mean action noise std: 2.48
          Mean value_function loss: 259.0362
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 60.0931
                       Mean reward: 814.98
               Mean episode length: 222.34
    Episode_Reward/reaching_object: 1.5160
     Episode_Reward/lifting_object: 158.1233
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 2.03s
                      Time elapsed: 00:42:54
                               ETA: 00:41:20

################################################################################
                     [1m Learning iteration 1019/2000 [0m                     

                       Computation: 48388 steps/s (collection: 1.937s, learning 0.095s)
             Mean action noise std: 2.48
          Mean value_function loss: 293.4927
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 60.1030
                       Mean reward: 782.25
               Mean episode length: 214.13
    Episode_Reward/reaching_object: 1.5526
     Episode_Reward/lifting_object: 163.9289
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 2.03s
                      Time elapsed: 00:42:56
                               ETA: 00:41:17

################################################################################
                     [1m Learning iteration 1020/2000 [0m                     

                       Computation: 47224 steps/s (collection: 1.984s, learning 0.098s)
             Mean action noise std: 2.49
          Mean value_function loss: 269.7735
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 60.1083
                       Mean reward: 816.78
               Mean episode length: 223.29
    Episode_Reward/reaching_object: 1.5448
     Episode_Reward/lifting_object: 161.9897
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 2.08s
                      Time elapsed: 00:42:58
                               ETA: 00:41:14

################################################################################
                     [1m Learning iteration 1021/2000 [0m                     

                       Computation: 47965 steps/s (collection: 1.947s, learning 0.102s)
             Mean action noise std: 2.49
          Mean value_function loss: 340.4859
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 60.1207
                       Mean reward: 774.90
               Mean episode length: 212.86
    Episode_Reward/reaching_object: 1.4943
     Episode_Reward/lifting_object: 156.2970
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 2.05s
                      Time elapsed: 00:43:00
                               ETA: 00:41:11

################################################################################
                     [1m Learning iteration 1022/2000 [0m                     

                       Computation: 47748 steps/s (collection: 1.947s, learning 0.112s)
             Mean action noise std: 2.49
          Mean value_function loss: 292.8738
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 60.1362
                       Mean reward: 760.57
               Mean episode length: 208.16
    Episode_Reward/reaching_object: 1.5016
     Episode_Reward/lifting_object: 157.2360
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 2.06s
                      Time elapsed: 00:43:02
                               ETA: 00:41:08

################################################################################
                     [1m Learning iteration 1023/2000 [0m                     

                       Computation: 47548 steps/s (collection: 1.961s, learning 0.106s)
             Mean action noise std: 2.49
          Mean value_function loss: 252.4304
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 60.1453
                       Mean reward: 797.09
               Mean episode length: 220.12
    Episode_Reward/reaching_object: 1.5139
     Episode_Reward/lifting_object: 156.9627
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 2.07s
                      Time elapsed: 00:43:04
                               ETA: 00:41:06

################################################################################
                     [1m Learning iteration 1024/2000 [0m                     

                       Computation: 48051 steps/s (collection: 1.954s, learning 0.092s)
             Mean action noise std: 2.49
          Mean value_function loss: 218.6437
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 60.1533
                       Mean reward: 849.88
               Mean episode length: 230.38
    Episode_Reward/reaching_object: 1.5309
     Episode_Reward/lifting_object: 160.3076
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 2.05s
                      Time elapsed: 00:43:06
                               ETA: 00:41:03

################################################################################
                     [1m Learning iteration 1025/2000 [0m                     

                       Computation: 46801 steps/s (collection: 2.013s, learning 0.088s)
             Mean action noise std: 2.49
          Mean value_function loss: 240.1397
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 60.1745
                       Mean reward: 802.33
               Mean episode length: 220.16
    Episode_Reward/reaching_object: 1.5180
     Episode_Reward/lifting_object: 158.1278
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 2.10s
                      Time elapsed: 00:43:08
                               ETA: 00:41:00

################################################################################
                     [1m Learning iteration 1026/2000 [0m                     

                       Computation: 46590 steps/s (collection: 2.019s, learning 0.091s)
             Mean action noise std: 2.50
          Mean value_function loss: 256.5515
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 60.2016
                       Mean reward: 823.04
               Mean episode length: 223.39
    Episode_Reward/reaching_object: 1.5121
     Episode_Reward/lifting_object: 158.0712
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 2.11s
                      Time elapsed: 00:43:10
                               ETA: 00:40:57

################################################################################
                     [1m Learning iteration 1027/2000 [0m                     

                       Computation: 46789 steps/s (collection: 2.013s, learning 0.088s)
             Mean action noise std: 2.50
          Mean value_function loss: 229.4563
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 60.2279
                       Mean reward: 838.53
               Mean episode length: 226.11
    Episode_Reward/reaching_object: 1.5515
     Episode_Reward/lifting_object: 162.5581
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 2.10s
                      Time elapsed: 00:43:13
                               ETA: 00:40:54

################################################################################
                     [1m Learning iteration 1028/2000 [0m                     

                       Computation: 46328 steps/s (collection: 2.031s, learning 0.091s)
             Mean action noise std: 2.50
          Mean value_function loss: 229.9985
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 60.2528
                       Mean reward: 789.19
               Mean episode length: 216.36
    Episode_Reward/reaching_object: 1.5652
     Episode_Reward/lifting_object: 163.8289
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 2.12s
                      Time elapsed: 00:43:15
                               ETA: 00:40:51

################################################################################
                     [1m Learning iteration 1029/2000 [0m                     

                       Computation: 47930 steps/s (collection: 1.966s, learning 0.085s)
             Mean action noise std: 2.50
          Mean value_function loss: 265.3743
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 60.2667
                       Mean reward: 808.90
               Mean episode length: 218.77
    Episode_Reward/reaching_object: 1.5301
     Episode_Reward/lifting_object: 160.2715
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 2.05s
                      Time elapsed: 00:43:17
                               ETA: 00:40:48

################################################################################
                     [1m Learning iteration 1030/2000 [0m                     

                       Computation: 47860 steps/s (collection: 1.968s, learning 0.086s)
             Mean action noise std: 2.50
          Mean value_function loss: 280.6266
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 60.2802
                       Mean reward: 816.99
               Mean episode length: 222.78
    Episode_Reward/reaching_object: 1.5567
     Episode_Reward/lifting_object: 162.5292
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 2.05s
                      Time elapsed: 00:43:19
                               ETA: 00:40:45

################################################################################
                     [1m Learning iteration 1031/2000 [0m                     

                       Computation: 47738 steps/s (collection: 1.974s, learning 0.086s)
             Mean action noise std: 2.51
          Mean value_function loss: 295.8827
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 60.2995
                       Mean reward: 768.52
               Mean episode length: 211.07
    Episode_Reward/reaching_object: 1.5220
     Episode_Reward/lifting_object: 158.6843
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 2.06s
                      Time elapsed: 00:43:21
                               ETA: 00:40:42

################################################################################
                     [1m Learning iteration 1032/2000 [0m                     

                       Computation: 47150 steps/s (collection: 1.986s, learning 0.099s)
             Mean action noise std: 2.51
          Mean value_function loss: 267.5163
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 60.3140
                       Mean reward: 763.39
               Mean episode length: 210.17
    Episode_Reward/reaching_object: 1.5287
     Episode_Reward/lifting_object: 160.0082
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 2.08s
                      Time elapsed: 00:43:23
                               ETA: 00:40:39

################################################################################
                     [1m Learning iteration 1033/2000 [0m                     

                       Computation: 47904 steps/s (collection: 1.954s, learning 0.099s)
             Mean action noise std: 2.51
          Mean value_function loss: 291.5319
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 60.3255
                       Mean reward: 771.37
               Mean episode length: 211.36
    Episode_Reward/reaching_object: 1.4979
     Episode_Reward/lifting_object: 156.8833
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 2.05s
                      Time elapsed: 00:43:25
                               ETA: 00:40:36

################################################################################
                     [1m Learning iteration 1034/2000 [0m                     

                       Computation: 48002 steps/s (collection: 1.951s, learning 0.097s)
             Mean action noise std: 2.51
          Mean value_function loss: 284.7918
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 60.3448
                       Mean reward: 742.95
               Mean episode length: 204.13
    Episode_Reward/reaching_object: 1.4523
     Episode_Reward/lifting_object: 151.3925
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 2.05s
                      Time elapsed: 00:43:27
                               ETA: 00:40:33

################################################################################
                     [1m Learning iteration 1035/2000 [0m                     

                       Computation: 47067 steps/s (collection: 1.973s, learning 0.116s)
             Mean action noise std: 2.51
          Mean value_function loss: 217.1138
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 60.3605
                       Mean reward: 812.18
               Mean episode length: 223.40
    Episode_Reward/reaching_object: 1.5035
     Episode_Reward/lifting_object: 157.7575
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 2.09s
                      Time elapsed: 00:43:29
                               ETA: 00:40:30

################################################################################
                     [1m Learning iteration 1036/2000 [0m                     

                       Computation: 47512 steps/s (collection: 1.961s, learning 0.108s)
             Mean action noise std: 2.51
          Mean value_function loss: 264.6113
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 60.3691
                       Mean reward: 790.00
               Mean episode length: 215.68
    Episode_Reward/reaching_object: 1.4441
     Episode_Reward/lifting_object: 151.4104
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 2.07s
                      Time elapsed: 00:43:31
                               ETA: 00:40:27

################################################################################
                     [1m Learning iteration 1037/2000 [0m                     

                       Computation: 47590 steps/s (collection: 1.957s, learning 0.109s)
             Mean action noise std: 2.51
          Mean value_function loss: 246.4896
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 60.3725
                       Mean reward: 819.62
               Mean episode length: 222.50
    Episode_Reward/reaching_object: 1.5117
     Episode_Reward/lifting_object: 158.6603
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 2.07s
                      Time elapsed: 00:43:33
                               ETA: 00:40:24

################################################################################
                     [1m Learning iteration 1038/2000 [0m                     

                       Computation: 47976 steps/s (collection: 1.945s, learning 0.104s)
             Mean action noise std: 2.51
          Mean value_function loss: 267.9204
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 60.3745
                       Mean reward: 776.88
               Mean episode length: 213.33
    Episode_Reward/reaching_object: 1.5091
     Episode_Reward/lifting_object: 157.8204
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 2.05s
                      Time elapsed: 00:43:35
                               ETA: 00:40:21

################################################################################
                     [1m Learning iteration 1039/2000 [0m                     

                       Computation: 48166 steps/s (collection: 1.943s, learning 0.098s)
             Mean action noise std: 2.51
          Mean value_function loss: 281.2865
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 60.3791
                       Mean reward: 812.14
               Mean episode length: 223.03
    Episode_Reward/reaching_object: 1.5150
     Episode_Reward/lifting_object: 158.3327
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 2.04s
                      Time elapsed: 00:43:37
                               ETA: 00:40:18

################################################################################
                     [1m Learning iteration 1040/2000 [0m                     

                       Computation: 47043 steps/s (collection: 1.994s, learning 0.096s)
             Mean action noise std: 2.51
          Mean value_function loss: 301.0288
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 60.3880
                       Mean reward: 803.86
               Mean episode length: 217.53
    Episode_Reward/reaching_object: 1.5152
     Episode_Reward/lifting_object: 158.7768
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 2.09s
                      Time elapsed: 00:43:39
                               ETA: 00:40:16

################################################################################
                     [1m Learning iteration 1041/2000 [0m                     

                       Computation: 46816 steps/s (collection: 2.002s, learning 0.097s)
             Mean action noise std: 2.52
          Mean value_function loss: 261.9410
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 60.3973
                       Mean reward: 758.81
               Mean episode length: 210.45
    Episode_Reward/reaching_object: 1.5069
     Episode_Reward/lifting_object: 157.7946
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 2.10s
                      Time elapsed: 00:43:41
                               ETA: 00:40:13

################################################################################
                     [1m Learning iteration 1042/2000 [0m                     

                       Computation: 46826 steps/s (collection: 1.999s, learning 0.100s)
             Mean action noise std: 2.52
          Mean value_function loss: 295.8212
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 60.4071
                       Mean reward: 840.75
               Mean episode length: 228.54
    Episode_Reward/reaching_object: 1.5191
     Episode_Reward/lifting_object: 159.1152
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 2.10s
                      Time elapsed: 00:43:44
                               ETA: 00:40:10

################################################################################
                     [1m Learning iteration 1043/2000 [0m                     

                       Computation: 46670 steps/s (collection: 2.003s, learning 0.103s)
             Mean action noise std: 2.52
          Mean value_function loss: 266.6460
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 60.4259
                       Mean reward: 797.84
               Mean episode length: 217.95
    Episode_Reward/reaching_object: 1.5165
     Episode_Reward/lifting_object: 158.4699
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 2.11s
                      Time elapsed: 00:43:46
                               ETA: 00:40:07

################################################################################
                     [1m Learning iteration 1044/2000 [0m                     

                       Computation: 47389 steps/s (collection: 1.981s, learning 0.093s)
             Mean action noise std: 2.52
          Mean value_function loss: 259.9520
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 60.4408
                       Mean reward: 725.09
               Mean episode length: 200.26
    Episode_Reward/reaching_object: 1.4889
     Episode_Reward/lifting_object: 155.5560
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 2.07s
                      Time elapsed: 00:43:48
                               ETA: 00:40:04

################################################################################
                     [1m Learning iteration 1045/2000 [0m                     

                       Computation: 47645 steps/s (collection: 1.973s, learning 0.091s)
             Mean action noise std: 2.52
          Mean value_function loss: 263.7924
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 60.4492
                       Mean reward: 791.61
               Mean episode length: 216.23
    Episode_Reward/reaching_object: 1.5174
     Episode_Reward/lifting_object: 158.7683
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 2.06s
                      Time elapsed: 00:43:50
                               ETA: 00:40:01

################################################################################
                     [1m Learning iteration 1046/2000 [0m                     

                       Computation: 47479 steps/s (collection: 1.981s, learning 0.089s)
             Mean action noise std: 2.52
          Mean value_function loss: 278.8753
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 60.4552
                       Mean reward: 796.67
               Mean episode length: 217.23
    Episode_Reward/reaching_object: 1.4633
     Episode_Reward/lifting_object: 152.2066
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 2.07s
                      Time elapsed: 00:43:52
                               ETA: 00:39:58

################################################################################
                     [1m Learning iteration 1047/2000 [0m                     

                       Computation: 47248 steps/s (collection: 1.988s, learning 0.093s)
             Mean action noise std: 2.52
          Mean value_function loss: 306.0601
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 60.4645
                       Mean reward: 787.68
               Mean episode length: 213.85
    Episode_Reward/reaching_object: 1.4747
     Episode_Reward/lifting_object: 153.4923
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 2.08s
                      Time elapsed: 00:43:54
                               ETA: 00:39:55

################################################################################
                     [1m Learning iteration 1048/2000 [0m                     

                       Computation: 48257 steps/s (collection: 1.949s, learning 0.089s)
             Mean action noise std: 2.52
          Mean value_function loss: 256.2026
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 60.4766
                       Mean reward: 791.23
               Mean episode length: 217.33
    Episode_Reward/reaching_object: 1.5121
     Episode_Reward/lifting_object: 157.7296
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 2.04s
                      Time elapsed: 00:43:56
                               ETA: 00:39:52

################################################################################
                     [1m Learning iteration 1049/2000 [0m                     

                       Computation: 47305 steps/s (collection: 1.989s, learning 0.090s)
             Mean action noise std: 2.53
          Mean value_function loss: 269.8350
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 60.4856
                       Mean reward: 823.06
               Mean episode length: 222.99
    Episode_Reward/reaching_object: 1.5016
     Episode_Reward/lifting_object: 157.2395
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 2.08s
                      Time elapsed: 00:43:58
                               ETA: 00:39:49

################################################################################
                     [1m Learning iteration 1050/2000 [0m                     

                       Computation: 47593 steps/s (collection: 1.948s, learning 0.118s)
             Mean action noise std: 2.53
          Mean value_function loss: 233.0379
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 60.4922
                       Mean reward: 811.98
               Mean episode length: 219.68
    Episode_Reward/reaching_object: 1.5470
     Episode_Reward/lifting_object: 161.5761
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 2.07s
                      Time elapsed: 00:44:00
                               ETA: 00:39:46

################################################################################
                     [1m Learning iteration 1051/2000 [0m                     

                       Computation: 47157 steps/s (collection: 1.984s, learning 0.101s)
             Mean action noise std: 2.53
          Mean value_function loss: 256.9082
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 60.5029
                       Mean reward: 812.03
               Mean episode length: 220.58
    Episode_Reward/reaching_object: 1.5663
     Episode_Reward/lifting_object: 164.4029
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 2.08s
                      Time elapsed: 00:44:02
                               ETA: 00:39:44

################################################################################
                     [1m Learning iteration 1052/2000 [0m                     

                       Computation: 47513 steps/s (collection: 1.958s, learning 0.111s)
             Mean action noise std: 2.53
          Mean value_function loss: 263.6578
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 60.5196
                       Mean reward: 852.99
               Mean episode length: 230.11
    Episode_Reward/reaching_object: 1.5427
     Episode_Reward/lifting_object: 160.2534
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 2.07s
                      Time elapsed: 00:44:04
                               ETA: 00:39:41

################################################################################
                     [1m Learning iteration 1053/2000 [0m                     

                       Computation: 47390 steps/s (collection: 1.979s, learning 0.096s)
             Mean action noise std: 2.53
          Mean value_function loss: 251.3569
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 60.5311
                       Mean reward: 826.15
               Mean episode length: 224.04
    Episode_Reward/reaching_object: 1.5535
     Episode_Reward/lifting_object: 162.0897
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 2.07s
                      Time elapsed: 00:44:06
                               ETA: 00:39:38

################################################################################
                     [1m Learning iteration 1054/2000 [0m                     

                       Computation: 47467 steps/s (collection: 1.973s, learning 0.098s)
             Mean action noise std: 2.53
          Mean value_function loss: 292.9522
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 60.5345
                       Mean reward: 832.21
               Mean episode length: 223.81
    Episode_Reward/reaching_object: 1.5207
     Episode_Reward/lifting_object: 158.5732
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 2.07s
                      Time elapsed: 00:44:08
                               ETA: 00:39:35

################################################################################
                     [1m Learning iteration 1055/2000 [0m                     

                       Computation: 46892 steps/s (collection: 2.009s, learning 0.087s)
             Mean action noise std: 2.53
          Mean value_function loss: 277.0719
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 60.5426
                       Mean reward: 776.12
               Mean episode length: 211.96
    Episode_Reward/reaching_object: 1.5212
     Episode_Reward/lifting_object: 158.1771
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 2.10s
                      Time elapsed: 00:44:11
                               ETA: 00:39:32

################################################################################
                     [1m Learning iteration 1056/2000 [0m                     

                       Computation: 46787 steps/s (collection: 1.984s, learning 0.117s)
             Mean action noise std: 2.53
          Mean value_function loss: 291.6690
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 60.5584
                       Mean reward: 848.38
               Mean episode length: 228.91
    Episode_Reward/reaching_object: 1.4984
     Episode_Reward/lifting_object: 157.2682
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 2.10s
                      Time elapsed: 00:44:13
                               ETA: 00:39:29

################################################################################
                     [1m Learning iteration 1057/2000 [0m                     

                       Computation: 47148 steps/s (collection: 1.997s, learning 0.088s)
             Mean action noise std: 2.54
          Mean value_function loss: 353.9376
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 60.5738
                       Mean reward: 767.15
               Mean episode length: 213.21
    Episode_Reward/reaching_object: 1.4553
     Episode_Reward/lifting_object: 151.0396
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 2.08s
                      Time elapsed: 00:44:15
                               ETA: 00:39:26

################################################################################
                     [1m Learning iteration 1058/2000 [0m                     

                       Computation: 47939 steps/s (collection: 1.961s, learning 0.090s)
             Mean action noise std: 2.54
          Mean value_function loss: 294.5196
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 60.5895
                       Mean reward: 790.01
               Mean episode length: 214.55
    Episode_Reward/reaching_object: 1.4803
     Episode_Reward/lifting_object: 155.5039
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 2.05s
                      Time elapsed: 00:44:17
                               ETA: 00:39:23

################################################################################
                     [1m Learning iteration 1059/2000 [0m                     

                       Computation: 46861 steps/s (collection: 2.012s, learning 0.086s)
             Mean action noise std: 2.54
          Mean value_function loss: 317.2204
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 60.6041
                       Mean reward: 795.28
               Mean episode length: 215.72
    Episode_Reward/reaching_object: 1.4627
     Episode_Reward/lifting_object: 153.0511
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 2.10s
                      Time elapsed: 00:44:19
                               ETA: 00:39:20

################################################################################
                     [1m Learning iteration 1060/2000 [0m                     

                       Computation: 47730 steps/s (collection: 1.975s, learning 0.085s)
             Mean action noise std: 2.54
          Mean value_function loss: 310.4108
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 60.6233
                       Mean reward: 774.61
               Mean episode length: 211.52
    Episode_Reward/reaching_object: 1.4520
     Episode_Reward/lifting_object: 152.9390
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 2.06s
                      Time elapsed: 00:44:21
                               ETA: 00:39:17

################################################################################
                     [1m Learning iteration 1061/2000 [0m                     

                       Computation: 47834 steps/s (collection: 1.968s, learning 0.087s)
             Mean action noise std: 2.54
          Mean value_function loss: 293.2143
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 60.6376
                       Mean reward: 801.03
               Mean episode length: 217.56
    Episode_Reward/reaching_object: 1.4831
     Episode_Reward/lifting_object: 156.1173
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 2.06s
                      Time elapsed: 00:44:23
                               ETA: 00:39:15

################################################################################
                     [1m Learning iteration 1062/2000 [0m                     

                       Computation: 44907 steps/s (collection: 2.096s, learning 0.093s)
             Mean action noise std: 2.54
          Mean value_function loss: 309.1045
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 60.6538
                       Mean reward: 813.86
               Mean episode length: 221.59
    Episode_Reward/reaching_object: 1.5007
     Episode_Reward/lifting_object: 158.0111
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 2.19s
                      Time elapsed: 00:44:25
                               ETA: 00:39:12

################################################################################
                     [1m Learning iteration 1063/2000 [0m                     

                       Computation: 46012 steps/s (collection: 2.049s, learning 0.087s)
             Mean action noise std: 2.55
          Mean value_function loss: 315.1683
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 60.6730
                       Mean reward: 798.73
               Mean episode length: 216.96
    Episode_Reward/reaching_object: 1.4961
     Episode_Reward/lifting_object: 158.0792
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 2.14s
                      Time elapsed: 00:44:27
                               ETA: 00:39:09

################################################################################
                     [1m Learning iteration 1064/2000 [0m                     

                       Computation: 46459 steps/s (collection: 2.007s, learning 0.109s)
             Mean action noise std: 2.55
          Mean value_function loss: 296.8735
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 60.6858
                       Mean reward: 790.05
               Mean episode length: 216.91
    Episode_Reward/reaching_object: 1.4307
     Episode_Reward/lifting_object: 150.3292
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 2.12s
                      Time elapsed: 00:44:29
                               ETA: 00:39:06

################################################################################
                     [1m Learning iteration 1065/2000 [0m                     

                       Computation: 46197 steps/s (collection: 2.009s, learning 0.119s)
             Mean action noise std: 2.55
          Mean value_function loss: 309.5386
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 60.7063
                       Mean reward: 751.69
               Mean episode length: 204.97
    Episode_Reward/reaching_object: 1.4586
     Episode_Reward/lifting_object: 154.4338
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 2.13s
                      Time elapsed: 00:44:32
                               ETA: 00:39:03

################################################################################
                     [1m Learning iteration 1066/2000 [0m                     

                       Computation: 47007 steps/s (collection: 1.981s, learning 0.110s)
             Mean action noise std: 2.55
          Mean value_function loss: 312.2489
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 60.7264
                       Mean reward: 805.02
               Mean episode length: 219.39
    Episode_Reward/reaching_object: 1.4887
     Episode_Reward/lifting_object: 157.6129
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 2.09s
                      Time elapsed: 00:44:34
                               ETA: 00:39:00

################################################################################
                     [1m Learning iteration 1067/2000 [0m                     

                       Computation: 47631 steps/s (collection: 1.968s, learning 0.096s)
             Mean action noise std: 2.55
          Mean value_function loss: 315.7868
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 60.7484
                       Mean reward: 830.87
               Mean episode length: 224.64
    Episode_Reward/reaching_object: 1.4658
     Episode_Reward/lifting_object: 155.3834
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 2.06s
                      Time elapsed: 00:44:36
                               ETA: 00:38:57

################################################################################
                     [1m Learning iteration 1068/2000 [0m                     

                       Computation: 48209 steps/s (collection: 1.951s, learning 0.088s)
             Mean action noise std: 2.55
          Mean value_function loss: 288.6561
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 60.7606
                       Mean reward: 758.71
               Mean episode length: 210.40
    Episode_Reward/reaching_object: 1.4477
     Episode_Reward/lifting_object: 152.6815
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 2.04s
                      Time elapsed: 00:44:38
                               ETA: 00:38:55

################################################################################
                     [1m Learning iteration 1069/2000 [0m                     

                       Computation: 47918 steps/s (collection: 1.962s, learning 0.090s)
             Mean action noise std: 2.56
          Mean value_function loss: 318.8012
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 60.7798
                       Mean reward: 781.41
               Mean episode length: 212.40
    Episode_Reward/reaching_object: 1.4689
     Episode_Reward/lifting_object: 155.5512
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 2.05s
                      Time elapsed: 00:44:40
                               ETA: 00:38:52

################################################################################
                     [1m Learning iteration 1070/2000 [0m                     

                       Computation: 46963 steps/s (collection: 1.998s, learning 0.096s)
             Mean action noise std: 2.56
          Mean value_function loss: 343.0790
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 60.7988
                       Mean reward: 801.66
               Mean episode length: 217.67
    Episode_Reward/reaching_object: 1.4565
     Episode_Reward/lifting_object: 154.8275
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 2.09s
                      Time elapsed: 00:44:42
                               ETA: 00:38:49

################################################################################
                     [1m Learning iteration 1071/2000 [0m                     

                       Computation: 46708 steps/s (collection: 2.002s, learning 0.103s)
             Mean action noise std: 2.56
          Mean value_function loss: 355.7022
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 60.8118
                       Mean reward: 781.75
               Mean episode length: 213.06
    Episode_Reward/reaching_object: 1.4640
     Episode_Reward/lifting_object: 155.4543
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 2.10s
                      Time elapsed: 00:44:44
                               ETA: 00:38:46

################################################################################
                     [1m Learning iteration 1072/2000 [0m                     

                       Computation: 46867 steps/s (collection: 1.999s, learning 0.098s)
             Mean action noise std: 2.56
          Mean value_function loss: 306.7314
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 60.8273
                       Mean reward: 820.33
               Mean episode length: 223.90
    Episode_Reward/reaching_object: 1.4649
     Episode_Reward/lifting_object: 155.1369
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 2.10s
                      Time elapsed: 00:44:46
                               ETA: 00:38:43

################################################################################
                     [1m Learning iteration 1073/2000 [0m                     

                       Computation: 47492 steps/s (collection: 1.982s, learning 0.088s)
             Mean action noise std: 2.56
          Mean value_function loss: 268.8937
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 60.8391
                       Mean reward: 804.95
               Mean episode length: 218.10
    Episode_Reward/reaching_object: 1.4995
     Episode_Reward/lifting_object: 159.3541
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 2.07s
                      Time elapsed: 00:44:48
                               ETA: 00:38:40

################################################################################
                     [1m Learning iteration 1074/2000 [0m                     

                       Computation: 47880 steps/s (collection: 1.965s, learning 0.088s)
             Mean action noise std: 2.56
          Mean value_function loss: 309.8785
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 60.8464
                       Mean reward: 792.75
               Mean episode length: 217.12
    Episode_Reward/reaching_object: 1.4677
     Episode_Reward/lifting_object: 155.6719
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 2.05s
                      Time elapsed: 00:44:50
                               ETA: 00:38:37

################################################################################
                     [1m Learning iteration 1075/2000 [0m                     

                       Computation: 47379 steps/s (collection: 1.987s, learning 0.088s)
             Mean action noise std: 2.56
          Mean value_function loss: 268.0552
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 60.8493
                       Mean reward: 807.84
               Mean episode length: 217.89
    Episode_Reward/reaching_object: 1.4461
     Episode_Reward/lifting_object: 153.0485
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 2.07s
                      Time elapsed: 00:44:52
                               ETA: 00:38:34

################################################################################
                     [1m Learning iteration 1076/2000 [0m                     

                       Computation: 46730 steps/s (collection: 2.015s, learning 0.089s)
             Mean action noise std: 2.56
          Mean value_function loss: 306.4987
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 60.8559
                       Mean reward: 726.47
               Mean episode length: 200.47
    Episode_Reward/reaching_object: 1.4388
     Episode_Reward/lifting_object: 153.1114
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 2.10s
                      Time elapsed: 00:44:54
                               ETA: 00:38:32

################################################################################
                     [1m Learning iteration 1077/2000 [0m                     

                       Computation: 46675 steps/s (collection: 2.010s, learning 0.097s)
             Mean action noise std: 2.57
          Mean value_function loss: 305.3646
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 60.8732
                       Mean reward: 781.06
               Mean episode length: 213.15
    Episode_Reward/reaching_object: 1.4836
     Episode_Reward/lifting_object: 157.5483
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 2.11s
                      Time elapsed: 00:44:57
                               ETA: 00:38:29

################################################################################
                     [1m Learning iteration 1078/2000 [0m                     

                       Computation: 45872 steps/s (collection: 2.044s, learning 0.099s)
             Mean action noise std: 2.57
          Mean value_function loss: 252.1141
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 60.8918
                       Mean reward: 815.90
               Mean episode length: 221.55
    Episode_Reward/reaching_object: 1.5016
     Episode_Reward/lifting_object: 159.3767
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 2.14s
                      Time elapsed: 00:44:59
                               ETA: 00:38:26

################################################################################
                     [1m Learning iteration 1079/2000 [0m                     

                       Computation: 44584 steps/s (collection: 2.115s, learning 0.090s)
             Mean action noise std: 2.57
          Mean value_function loss: 270.5775
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 60.8965
                       Mean reward: 852.23
               Mean episode length: 230.00
    Episode_Reward/reaching_object: 1.4940
     Episode_Reward/lifting_object: 158.9928
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 2.20s
                      Time elapsed: 00:45:01
                               ETA: 00:38:23

################################################################################
                     [1m Learning iteration 1080/2000 [0m                     

                       Computation: 45670 steps/s (collection: 2.044s, learning 0.108s)
             Mean action noise std: 2.57
          Mean value_function loss: 301.8145
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 60.9024
                       Mean reward: 750.05
               Mean episode length: 208.27
    Episode_Reward/reaching_object: 1.4778
     Episode_Reward/lifting_object: 156.2181
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 2.15s
                      Time elapsed: 00:45:03
                               ETA: 00:38:20

################################################################################
                     [1m Learning iteration 1081/2000 [0m                     

                       Computation: 46291 steps/s (collection: 2.016s, learning 0.108s)
             Mean action noise std: 2.57
          Mean value_function loss: 296.1958
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 60.9122
                       Mean reward: 803.82
               Mean episode length: 217.77
    Episode_Reward/reaching_object: 1.4700
     Episode_Reward/lifting_object: 155.8672
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 2.12s
                      Time elapsed: 00:45:05
                               ETA: 00:38:18

################################################################################
                     [1m Learning iteration 1082/2000 [0m                     

                       Computation: 45910 steps/s (collection: 2.053s, learning 0.089s)
             Mean action noise std: 2.57
          Mean value_function loss: 303.2633
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 60.9316
                       Mean reward: 786.92
               Mean episode length: 216.00
    Episode_Reward/reaching_object: 1.4668
     Episode_Reward/lifting_object: 155.5575
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 2.14s
                      Time elapsed: 00:45:07
                               ETA: 00:38:15

################################################################################
                     [1m Learning iteration 1083/2000 [0m                     

                       Computation: 46445 steps/s (collection: 2.019s, learning 0.098s)
             Mean action noise std: 2.58
          Mean value_function loss: 265.7643
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 60.9533
                       Mean reward: 785.40
               Mean episode length: 213.12
    Episode_Reward/reaching_object: 1.5142
     Episode_Reward/lifting_object: 161.6615
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 2.12s
                      Time elapsed: 00:45:09
                               ETA: 00:38:12

################################################################################
                     [1m Learning iteration 1084/2000 [0m                     

                       Computation: 46320 steps/s (collection: 2.029s, learning 0.093s)
             Mean action noise std: 2.58
          Mean value_function loss: 244.9387
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 60.9702
                       Mean reward: 798.49
               Mean episode length: 215.97
    Episode_Reward/reaching_object: 1.5178
     Episode_Reward/lifting_object: 162.0094
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 2.12s
                      Time elapsed: 00:45:12
                               ETA: 00:38:09

################################################################################
                     [1m Learning iteration 1085/2000 [0m                     

                       Computation: 45341 steps/s (collection: 2.064s, learning 0.104s)
             Mean action noise std: 2.58
          Mean value_function loss: 287.5589
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 60.9841
                       Mean reward: 781.19
               Mean episode length: 217.21
    Episode_Reward/reaching_object: 1.4856
     Episode_Reward/lifting_object: 157.1640
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 2.17s
                      Time elapsed: 00:45:14
                               ETA: 00:38:06

################################################################################
                     [1m Learning iteration 1086/2000 [0m                     

                       Computation: 46096 steps/s (collection: 2.030s, learning 0.103s)
             Mean action noise std: 2.58
          Mean value_function loss: 256.0071
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 61.0026
                       Mean reward: 782.01
               Mean episode length: 215.21
    Episode_Reward/reaching_object: 1.5108
     Episode_Reward/lifting_object: 160.8288
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 2.13s
                      Time elapsed: 00:45:16
                               ETA: 00:38:04

################################################################################
                     [1m Learning iteration 1087/2000 [0m                     

                       Computation: 47682 steps/s (collection: 1.976s, learning 0.086s)
             Mean action noise std: 2.58
          Mean value_function loss: 246.6853
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 61.0148
                       Mean reward: 791.58
               Mean episode length: 217.30
    Episode_Reward/reaching_object: 1.4961
     Episode_Reward/lifting_object: 157.9872
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 2.06s
                      Time elapsed: 00:45:18
                               ETA: 00:38:01

################################################################################
                     [1m Learning iteration 1088/2000 [0m                     

                       Computation: 46843 steps/s (collection: 2.009s, learning 0.090s)
             Mean action noise std: 2.58
          Mean value_function loss: 234.7327
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 61.0320
                       Mean reward: 796.38
               Mean episode length: 215.41
    Episode_Reward/reaching_object: 1.4984
     Episode_Reward/lifting_object: 159.9658
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 2.10s
                      Time elapsed: 00:45:20
                               ETA: 00:37:58

################################################################################
                     [1m Learning iteration 1089/2000 [0m                     

                       Computation: 47642 steps/s (collection: 1.978s, learning 0.085s)
             Mean action noise std: 2.58
          Mean value_function loss: 250.2187
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 61.0430
                       Mean reward: 789.77
               Mean episode length: 214.40
    Episode_Reward/reaching_object: 1.5057
     Episode_Reward/lifting_object: 160.1393
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 2.06s
                      Time elapsed: 00:45:22
                               ETA: 00:37:55

################################################################################
                     [1m Learning iteration 1090/2000 [0m                     

                       Computation: 47711 steps/s (collection: 1.976s, learning 0.085s)
             Mean action noise std: 2.59
          Mean value_function loss: 224.1991
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 61.0574
                       Mean reward: 808.76
               Mean episode length: 220.85
    Episode_Reward/reaching_object: 1.5088
     Episode_Reward/lifting_object: 160.0298
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 2.06s
                      Time elapsed: 00:45:24
                               ETA: 00:37:52

################################################################################
                     [1m Learning iteration 1091/2000 [0m                     

                       Computation: 47848 steps/s (collection: 1.959s, learning 0.095s)
             Mean action noise std: 2.59
          Mean value_function loss: 275.3068
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 61.0709
                       Mean reward: 783.42
               Mean episode length: 215.79
    Episode_Reward/reaching_object: 1.4898
     Episode_Reward/lifting_object: 157.7272
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 2.05s
                      Time elapsed: 00:45:26
                               ETA: 00:37:49

################################################################################
                     [1m Learning iteration 1092/2000 [0m                     

                       Computation: 47251 steps/s (collection: 1.993s, learning 0.088s)
             Mean action noise std: 2.59
          Mean value_function loss: 288.0981
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 61.0837
                       Mean reward: 819.83
               Mean episode length: 223.03
    Episode_Reward/reaching_object: 1.5017
     Episode_Reward/lifting_object: 159.5782
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 2.08s
                      Time elapsed: 00:45:28
                               ETA: 00:37:46

################################################################################
                     [1m Learning iteration 1093/2000 [0m                     

                       Computation: 47317 steps/s (collection: 1.990s, learning 0.088s)
             Mean action noise std: 2.59
          Mean value_function loss: 285.7799
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 61.0939
                       Mean reward: 779.63
               Mean episode length: 214.38
    Episode_Reward/reaching_object: 1.4765
     Episode_Reward/lifting_object: 156.3861
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 2.08s
                      Time elapsed: 00:45:30
                               ETA: 00:37:44

################################################################################
                     [1m Learning iteration 1094/2000 [0m                     

                       Computation: 47067 steps/s (collection: 1.992s, learning 0.097s)
             Mean action noise std: 2.59
          Mean value_function loss: 262.8236
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 61.1101
                       Mean reward: 810.76
               Mean episode length: 223.46
    Episode_Reward/reaching_object: 1.4909
     Episode_Reward/lifting_object: 158.1901
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 2.09s
                      Time elapsed: 00:45:32
                               ETA: 00:37:41

################################################################################
                     [1m Learning iteration 1095/2000 [0m                     

                       Computation: 46558 steps/s (collection: 1.993s, learning 0.119s)
             Mean action noise std: 2.59
          Mean value_function loss: 275.8566
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 61.1238
                       Mean reward: 790.89
               Mean episode length: 216.10
    Episode_Reward/reaching_object: 1.4866
     Episode_Reward/lifting_object: 158.1549
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 2.11s
                      Time elapsed: 00:45:35
                               ETA: 00:37:38

################################################################################
                     [1m Learning iteration 1096/2000 [0m                     

                       Computation: 46527 steps/s (collection: 2.022s, learning 0.091s)
             Mean action noise std: 2.59
          Mean value_function loss: 250.1869
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 61.1396
                       Mean reward: 811.38
               Mean episode length: 219.17
    Episode_Reward/reaching_object: 1.4442
     Episode_Reward/lifting_object: 153.6273
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 2.11s
                      Time elapsed: 00:45:37
                               ETA: 00:37:35

################################################################################
                     [1m Learning iteration 1097/2000 [0m                     

                       Computation: 46691 steps/s (collection: 2.005s, learning 0.101s)
             Mean action noise std: 2.60
          Mean value_function loss: 257.9371
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 61.1563
                       Mean reward: 795.41
               Mean episode length: 216.66
    Episode_Reward/reaching_object: 1.5014
     Episode_Reward/lifting_object: 159.8415
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 2.11s
                      Time elapsed: 00:45:39
                               ETA: 00:37:32

################################################################################
                     [1m Learning iteration 1098/2000 [0m                     

                       Computation: 47211 steps/s (collection: 1.989s, learning 0.094s)
             Mean action noise std: 2.60
          Mean value_function loss: 266.8126
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 61.1779
                       Mean reward: 819.69
               Mean episode length: 221.39
    Episode_Reward/reaching_object: 1.5021
     Episode_Reward/lifting_object: 160.0186
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 2.08s
                      Time elapsed: 00:45:41
                               ETA: 00:37:29

################################################################################
                     [1m Learning iteration 1099/2000 [0m                     

                       Computation: 47149 steps/s (collection: 1.993s, learning 0.091s)
             Mean action noise std: 2.60
          Mean value_function loss: 241.4734
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 61.1928
                       Mean reward: 782.80
               Mean episode length: 213.94
    Episode_Reward/reaching_object: 1.4500
     Episode_Reward/lifting_object: 154.4221
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 2.08s
                      Time elapsed: 00:45:43
                               ETA: 00:37:27

################################################################################
                     [1m Learning iteration 1100/2000 [0m                     

                       Computation: 46326 steps/s (collection: 2.029s, learning 0.093s)
             Mean action noise std: 2.60
          Mean value_function loss: 222.7642
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 61.2028
                       Mean reward: 799.44
               Mean episode length: 218.63
    Episode_Reward/reaching_object: 1.5279
     Episode_Reward/lifting_object: 162.7093
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 2.12s
                      Time elapsed: 00:45:45
                               ETA: 00:37:24

################################################################################
                     [1m Learning iteration 1101/2000 [0m                     

                       Computation: 46067 steps/s (collection: 2.044s, learning 0.090s)
             Mean action noise std: 2.60
          Mean value_function loss: 204.4568
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 61.2107
                       Mean reward: 843.13
               Mean episode length: 227.35
    Episode_Reward/reaching_object: 1.5619
     Episode_Reward/lifting_object: 168.0036
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 2.13s
                      Time elapsed: 00:45:47
                               ETA: 00:37:21

################################################################################
                     [1m Learning iteration 1102/2000 [0m                     

                       Computation: 47570 steps/s (collection: 1.977s, learning 0.089s)
             Mean action noise std: 2.60
          Mean value_function loss: 249.0277
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 61.2239
                       Mean reward: 799.77
               Mean episode length: 220.24
    Episode_Reward/reaching_object: 1.5409
     Episode_Reward/lifting_object: 163.8294
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 2.07s
                      Time elapsed: 00:45:49
                               ETA: 00:37:18

################################################################################
                     [1m Learning iteration 1103/2000 [0m                     

                       Computation: 47402 steps/s (collection: 1.988s, learning 0.086s)
             Mean action noise std: 2.61
          Mean value_function loss: 234.4991
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 61.2375
                       Mean reward: 827.27
               Mean episode length: 223.75
    Episode_Reward/reaching_object: 1.5248
     Episode_Reward/lifting_object: 163.0922
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 2.07s
                      Time elapsed: 00:45:51
                               ETA: 00:37:15

################################################################################
                     [1m Learning iteration 1104/2000 [0m                     

                       Computation: 46811 steps/s (collection: 2.014s, learning 0.086s)
             Mean action noise std: 2.61
          Mean value_function loss: 248.8267
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 61.2528
                       Mean reward: 819.49
               Mean episode length: 223.97
    Episode_Reward/reaching_object: 1.5011
     Episode_Reward/lifting_object: 159.9769
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 2.10s
                      Time elapsed: 00:45:53
                               ETA: 00:37:13

################################################################################
                     [1m Learning iteration 1105/2000 [0m                     

                       Computation: 46035 steps/s (collection: 2.039s, learning 0.096s)
             Mean action noise std: 2.61
          Mean value_function loss: 274.5229
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 61.2745
                       Mean reward: 775.46
               Mean episode length: 210.72
    Episode_Reward/reaching_object: 1.4812
     Episode_Reward/lifting_object: 157.6828
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 2.14s
                      Time elapsed: 00:45:56
                               ETA: 00:37:10

################################################################################
                     [1m Learning iteration 1106/2000 [0m                     

                       Computation: 45968 steps/s (collection: 2.048s, learning 0.090s)
             Mean action noise std: 2.61
          Mean value_function loss: 248.0480
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 61.2831
                       Mean reward: 809.70
               Mean episode length: 221.50
    Episode_Reward/reaching_object: 1.5279
     Episode_Reward/lifting_object: 162.2917
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 2.14s
                      Time elapsed: 00:45:58
                               ETA: 00:37:07

################################################################################
                     [1m Learning iteration 1107/2000 [0m                     

                       Computation: 47406 steps/s (collection: 1.982s, learning 0.092s)
             Mean action noise std: 2.61
          Mean value_function loss: 304.0395
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 61.2944
                       Mean reward: 812.44
               Mean episode length: 220.50
    Episode_Reward/reaching_object: 1.4397
     Episode_Reward/lifting_object: 152.9828
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 2.07s
                      Time elapsed: 00:46:00
                               ETA: 00:37:04

################################################################################
                     [1m Learning iteration 1108/2000 [0m                     

                       Computation: 45954 steps/s (collection: 2.031s, learning 0.109s)
             Mean action noise std: 2.62
          Mean value_function loss: 277.1985
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 61.3138
                       Mean reward: 805.32
               Mean episode length: 221.11
    Episode_Reward/reaching_object: 1.4770
     Episode_Reward/lifting_object: 157.3480
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 2.14s
                      Time elapsed: 00:46:02
                               ETA: 00:37:01

################################################################################
                     [1m Learning iteration 1109/2000 [0m                     

                       Computation: 46891 steps/s (collection: 2.004s, learning 0.093s)
             Mean action noise std: 2.62
          Mean value_function loss: 304.9109
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 61.3429
                       Mean reward: 800.19
               Mean episode length: 218.02
    Episode_Reward/reaching_object: 1.4614
     Episode_Reward/lifting_object: 155.0117
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 2.10s
                      Time elapsed: 00:46:04
                               ETA: 00:36:59

################################################################################
                     [1m Learning iteration 1110/2000 [0m                     

                       Computation: 47045 steps/s (collection: 2.000s, learning 0.090s)
             Mean action noise std: 2.62
          Mean value_function loss: 271.7838
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 61.3601
                       Mean reward: 820.72
               Mean episode length: 223.10
    Episode_Reward/reaching_object: 1.4755
     Episode_Reward/lifting_object: 158.2642
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 2.09s
                      Time elapsed: 00:46:06
                               ETA: 00:36:56

################################################################################
                     [1m Learning iteration 1111/2000 [0m                     

                       Computation: 46378 steps/s (collection: 2.031s, learning 0.089s)
             Mean action noise std: 2.62
          Mean value_function loss: 315.7729
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 61.3754
                       Mean reward: 765.01
               Mean episode length: 210.48
    Episode_Reward/reaching_object: 1.4380
     Episode_Reward/lifting_object: 152.6850
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 2.12s
                      Time elapsed: 00:46:08
                               ETA: 00:36:53

################################################################################
                     [1m Learning iteration 1112/2000 [0m                     

                       Computation: 47020 steps/s (collection: 1.998s, learning 0.093s)
             Mean action noise std: 2.62
          Mean value_function loss: 260.8820
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 61.3849
                       Mean reward: 827.97
               Mean episode length: 222.91
    Episode_Reward/reaching_object: 1.4541
     Episode_Reward/lifting_object: 155.8043
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 2.09s
                      Time elapsed: 00:46:10
                               ETA: 00:36:50

################################################################################
                     [1m Learning iteration 1113/2000 [0m                     

                       Computation: 46966 steps/s (collection: 2.002s, learning 0.091s)
             Mean action noise std: 2.62
          Mean value_function loss: 264.2278
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 61.3903
                       Mean reward: 826.58
               Mean episode length: 224.65
    Episode_Reward/reaching_object: 1.4786
     Episode_Reward/lifting_object: 158.0263
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 2.09s
                      Time elapsed: 00:46:12
                               ETA: 00:36:47

################################################################################
                     [1m Learning iteration 1114/2000 [0m                     

                       Computation: 46652 steps/s (collection: 2.019s, learning 0.088s)
             Mean action noise std: 2.62
          Mean value_function loss: 285.7130
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 61.4064
                       Mean reward: 823.32
               Mean episode length: 222.40
    Episode_Reward/reaching_object: 1.4689
     Episode_Reward/lifting_object: 157.6723
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 2.11s
                      Time elapsed: 00:46:14
                               ETA: 00:36:45

################################################################################
                     [1m Learning iteration 1115/2000 [0m                     

                       Computation: 46390 steps/s (collection: 2.025s, learning 0.094s)
             Mean action noise std: 2.63
          Mean value_function loss: 293.2147
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 61.4249
                       Mean reward: 767.90
               Mean episode length: 208.47
    Episode_Reward/reaching_object: 1.4623
     Episode_Reward/lifting_object: 156.8149
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 2.12s
                      Time elapsed: 00:46:17
                               ETA: 00:36:42

################################################################################
                     [1m Learning iteration 1116/2000 [0m                     

                       Computation: 46226 steps/s (collection: 2.040s, learning 0.087s)
             Mean action noise std: 2.63
          Mean value_function loss: 307.2394
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 61.4339
                       Mean reward: 791.30
               Mean episode length: 217.38
    Episode_Reward/reaching_object: 1.4590
     Episode_Reward/lifting_object: 155.5509
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 2.13s
                      Time elapsed: 00:46:19
                               ETA: 00:36:39

################################################################################
                     [1m Learning iteration 1117/2000 [0m                     

                       Computation: 47384 steps/s (collection: 1.982s, learning 0.093s)
             Mean action noise std: 2.63
          Mean value_function loss: 292.1126
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 61.4582
                       Mean reward: 792.69
               Mean episode length: 215.58
    Episode_Reward/reaching_object: 1.4686
     Episode_Reward/lifting_object: 156.9671
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 2.07s
                      Time elapsed: 00:46:21
                               ETA: 00:36:36

################################################################################
                     [1m Learning iteration 1118/2000 [0m                     

                       Computation: 47170 steps/s (collection: 1.993s, learning 0.091s)
             Mean action noise std: 2.63
          Mean value_function loss: 337.8514
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 61.4859
                       Mean reward: 733.49
               Mean episode length: 202.93
    Episode_Reward/reaching_object: 1.4284
     Episode_Reward/lifting_object: 152.6666
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 2.08s
                      Time elapsed: 00:46:23
                               ETA: 00:36:33

################################################################################
                     [1m Learning iteration 1119/2000 [0m                     

                       Computation: 46455 steps/s (collection: 2.017s, learning 0.099s)
             Mean action noise std: 2.63
          Mean value_function loss: 340.1125
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 61.5028
                       Mean reward: 752.50
               Mean episode length: 207.79
    Episode_Reward/reaching_object: 1.4534
     Episode_Reward/lifting_object: 155.6027
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 2.12s
                      Time elapsed: 00:46:25
                               ETA: 00:36:31

################################################################################
                     [1m Learning iteration 1120/2000 [0m                     

                       Computation: 47560 steps/s (collection: 1.978s, learning 0.089s)
             Mean action noise std: 2.64
          Mean value_function loss: 347.0347
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 61.5069
                       Mean reward: 836.52
               Mean episode length: 226.76
    Episode_Reward/reaching_object: 1.4060
     Episode_Reward/lifting_object: 149.8603
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 2.07s
                      Time elapsed: 00:46:27
                               ETA: 00:36:28

################################################################################
                     [1m Learning iteration 1121/2000 [0m                     

                       Computation: 47280 steps/s (collection: 1.993s, learning 0.087s)
             Mean action noise std: 2.64
          Mean value_function loss: 339.1666
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 61.5206
                       Mean reward: 768.65
               Mean episode length: 211.01
    Episode_Reward/reaching_object: 1.4202
     Episode_Reward/lifting_object: 151.7735
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 2.08s
                      Time elapsed: 00:46:29
                               ETA: 00:36:25

################################################################################
                     [1m Learning iteration 1122/2000 [0m                     

                       Computation: 46447 steps/s (collection: 2.016s, learning 0.101s)
             Mean action noise std: 2.64
          Mean value_function loss: 295.8735
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 61.5358
                       Mean reward: 762.23
               Mean episode length: 206.76
    Episode_Reward/reaching_object: 1.4075
     Episode_Reward/lifting_object: 150.5165
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 2.12s
                      Time elapsed: 00:46:31
                               ETA: 00:36:22

################################################################################
                     [1m Learning iteration 1123/2000 [0m                     

                       Computation: 46513 steps/s (collection: 1.998s, learning 0.115s)
             Mean action noise std: 2.64
          Mean value_function loss: 291.3902
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 61.5454
                       Mean reward: 765.88
               Mean episode length: 208.24
    Episode_Reward/reaching_object: 1.4465
     Episode_Reward/lifting_object: 155.6361
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 2.11s
                      Time elapsed: 00:46:33
                               ETA: 00:36:19

################################################################################
                     [1m Learning iteration 1124/2000 [0m                     

                       Computation: 46776 steps/s (collection: 1.998s, learning 0.103s)
             Mean action noise std: 2.64
          Mean value_function loss: 293.2977
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 61.5514
                       Mean reward: 793.35
               Mean episode length: 215.69
    Episode_Reward/reaching_object: 1.4430
     Episode_Reward/lifting_object: 155.0435
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 2.10s
                      Time elapsed: 00:46:35
                               ETA: 00:36:17

################################################################################
                     [1m Learning iteration 1125/2000 [0m                     

                       Computation: 46929 steps/s (collection: 1.998s, learning 0.097s)
             Mean action noise std: 2.64
          Mean value_function loss: 298.3114
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 61.5553
                       Mean reward: 814.15
               Mean episode length: 220.62
    Episode_Reward/reaching_object: 1.4625
     Episode_Reward/lifting_object: 157.7416
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 2.09s
                      Time elapsed: 00:46:38
                               ETA: 00:36:14

################################################################################
                     [1m Learning iteration 1126/2000 [0m                     

                       Computation: 44849 steps/s (collection: 2.094s, learning 0.098s)
             Mean action noise std: 2.64
          Mean value_function loss: 335.4642
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 61.5566
                       Mean reward: 804.60
               Mean episode length: 220.52
    Episode_Reward/reaching_object: 1.4378
     Episode_Reward/lifting_object: 154.5366
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 2.19s
                      Time elapsed: 00:46:40
                               ETA: 00:36:11

################################################################################
                     [1m Learning iteration 1127/2000 [0m                     

                       Computation: 46053 steps/s (collection: 2.038s, learning 0.097s)
             Mean action noise std: 2.64
          Mean value_function loss: 360.8289
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 61.5577
                       Mean reward: 737.00
               Mean episode length: 200.33
    Episode_Reward/reaching_object: 1.3815
     Episode_Reward/lifting_object: 147.5703
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 2.13s
                      Time elapsed: 00:46:42
                               ETA: 00:36:08

################################################################################
                     [1m Learning iteration 1128/2000 [0m                     

                       Computation: 45413 steps/s (collection: 2.075s, learning 0.090s)
             Mean action noise std: 2.64
          Mean value_function loss: 348.5174
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 61.5607
                       Mean reward: 761.91
               Mean episode length: 208.07
    Episode_Reward/reaching_object: 1.4125
     Episode_Reward/lifting_object: 152.0511
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 2.16s
                      Time elapsed: 00:46:44
                               ETA: 00:36:06

################################################################################
                     [1m Learning iteration 1129/2000 [0m                     

                       Computation: 47445 steps/s (collection: 1.982s, learning 0.090s)
             Mean action noise std: 2.64
          Mean value_function loss: 309.8731
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 61.5692
                       Mean reward: 790.60
               Mean episode length: 216.45
    Episode_Reward/reaching_object: 1.4342
     Episode_Reward/lifting_object: 154.3955
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 2.07s
                      Time elapsed: 00:46:46
                               ETA: 00:36:03

################################################################################
                     [1m Learning iteration 1130/2000 [0m                     

                       Computation: 44921 steps/s (collection: 2.077s, learning 0.111s)
             Mean action noise std: 2.64
          Mean value_function loss: 308.4668
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 61.5809
                       Mean reward: 769.57
               Mean episode length: 209.86
    Episode_Reward/reaching_object: 1.4280
     Episode_Reward/lifting_object: 153.9038
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 2.19s
                      Time elapsed: 00:46:48
                               ETA: 00:36:00

################################################################################
                     [1m Learning iteration 1131/2000 [0m                     

                       Computation: 42592 steps/s (collection: 2.216s, learning 0.092s)
             Mean action noise std: 2.65
          Mean value_function loss: 253.3925
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 61.5930
                       Mean reward: 755.17
               Mean episode length: 206.79
    Episode_Reward/reaching_object: 1.4570
     Episode_Reward/lifting_object: 156.7288
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 2.31s
                      Time elapsed: 00:46:51
                               ETA: 00:35:58

################################################################################
                     [1m Learning iteration 1132/2000 [0m                     

                       Computation: 45649 steps/s (collection: 2.054s, learning 0.100s)
             Mean action noise std: 2.65
          Mean value_function loss: 262.6101
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 61.6056
                       Mean reward: 772.31
               Mean episode length: 211.20
    Episode_Reward/reaching_object: 1.4651
     Episode_Reward/lifting_object: 157.8179
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 2.15s
                      Time elapsed: 00:46:53
                               ETA: 00:35:55

################################################################################
                     [1m Learning iteration 1133/2000 [0m                     

                       Computation: 46827 steps/s (collection: 2.008s, learning 0.092s)
             Mean action noise std: 2.65
          Mean value_function loss: 244.1404
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 61.6189
                       Mean reward: 792.98
               Mean episode length: 214.40
    Episode_Reward/reaching_object: 1.4787
     Episode_Reward/lifting_object: 159.6514
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 2.10s
                      Time elapsed: 00:46:55
                               ETA: 00:35:52

################################################################################
                     [1m Learning iteration 1134/2000 [0m                     

                       Computation: 44172 steps/s (collection: 2.118s, learning 0.108s)
             Mean action noise std: 2.65
          Mean value_function loss: 272.8218
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 61.6330
                       Mean reward: 774.77
               Mean episode length: 209.87
    Episode_Reward/reaching_object: 1.5052
     Episode_Reward/lifting_object: 162.1235
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 2.23s
                      Time elapsed: 00:46:57
                               ETA: 00:35:49

################################################################################
                     [1m Learning iteration 1135/2000 [0m                     

                       Computation: 46096 steps/s (collection: 2.036s, learning 0.097s)
             Mean action noise std: 2.65
          Mean value_function loss: 281.2067
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 61.6440
                       Mean reward: 778.96
               Mean episode length: 212.73
    Episode_Reward/reaching_object: 1.4304
     Episode_Reward/lifting_object: 153.9967
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 2.13s
                      Time elapsed: 00:46:59
                               ETA: 00:35:47

################################################################################
                     [1m Learning iteration 1136/2000 [0m                     

                       Computation: 44270 steps/s (collection: 2.085s, learning 0.136s)
             Mean action noise std: 2.65
          Mean value_function loss: 288.1667
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 61.6497
                       Mean reward: 863.12
               Mean episode length: 232.79
    Episode_Reward/reaching_object: 1.4686
     Episode_Reward/lifting_object: 158.1168
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 2.22s
                      Time elapsed: 00:47:01
                               ETA: 00:35:44

################################################################################
                     [1m Learning iteration 1137/2000 [0m                     

                       Computation: 38326 steps/s (collection: 2.397s, learning 0.168s)
             Mean action noise std: 2.65
          Mean value_function loss: 245.9796
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 61.6630
                       Mean reward: 802.36
               Mean episode length: 217.12
    Episode_Reward/reaching_object: 1.4817
     Episode_Reward/lifting_object: 159.9319
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 2.56s
                      Time elapsed: 00:47:04
                               ETA: 00:35:41

################################################################################
                     [1m Learning iteration 1138/2000 [0m                     

                       Computation: 43344 steps/s (collection: 2.115s, learning 0.153s)
             Mean action noise std: 2.66
          Mean value_function loss: 289.4835
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 61.6747
                       Mean reward: 817.96
               Mean episode length: 220.79
    Episode_Reward/reaching_object: 1.4822
     Episode_Reward/lifting_object: 160.1880
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 2.27s
                      Time elapsed: 00:47:06
                               ETA: 00:35:39

################################################################################
                     [1m Learning iteration 1139/2000 [0m                     

                       Computation: 45043 steps/s (collection: 2.087s, learning 0.095s)
             Mean action noise std: 2.66
          Mean value_function loss: 267.7502
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 61.6840
                       Mean reward: 770.78
               Mean episode length: 211.10
    Episode_Reward/reaching_object: 1.4645
     Episode_Reward/lifting_object: 157.1497
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 2.18s
                      Time elapsed: 00:47:08
                               ETA: 00:35:36

################################################################################
                     [1m Learning iteration 1140/2000 [0m                     

                       Computation: 43253 steps/s (collection: 2.179s, learning 0.094s)
             Mean action noise std: 2.66
          Mean value_function loss: 256.0681
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 61.6909
                       Mean reward: 756.56
               Mean episode length: 205.49
    Episode_Reward/reaching_object: 1.4733
     Episode_Reward/lifting_object: 159.8496
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 2.27s
                      Time elapsed: 00:47:11
                               ETA: 00:35:33

################################################################################
                     [1m Learning iteration 1141/2000 [0m                     

                       Computation: 44257 steps/s (collection: 2.096s, learning 0.126s)
             Mean action noise std: 2.66
          Mean value_function loss: 217.8474
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 61.6989
                       Mean reward: 815.99
               Mean episode length: 221.26
    Episode_Reward/reaching_object: 1.5383
     Episode_Reward/lifting_object: 166.5021
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 2.22s
                      Time elapsed: 00:47:13
                               ETA: 00:35:31

################################################################################
                     [1m Learning iteration 1142/2000 [0m                     

                       Computation: 42087 steps/s (collection: 2.236s, learning 0.100s)
             Mean action noise std: 2.66
          Mean value_function loss: 233.7664
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 61.7145
                       Mean reward: 766.70
               Mean episode length: 208.42
    Episode_Reward/reaching_object: 1.5057
     Episode_Reward/lifting_object: 163.1452
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 2.34s
                      Time elapsed: 00:47:15
                               ETA: 00:35:28

################################################################################
                     [1m Learning iteration 1143/2000 [0m                     

                       Computation: 43937 steps/s (collection: 2.086s, learning 0.151s)
             Mean action noise std: 2.66
          Mean value_function loss: 225.9203
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 61.7306
                       Mean reward: 835.35
               Mean episode length: 226.24
    Episode_Reward/reaching_object: 1.5102
     Episode_Reward/lifting_object: 163.0703
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 2.24s
                      Time elapsed: 00:47:18
                               ETA: 00:35:26

################################################################################
                     [1m Learning iteration 1144/2000 [0m                     

                       Computation: 45783 steps/s (collection: 2.050s, learning 0.098s)
             Mean action noise std: 2.66
          Mean value_function loss: 292.9892
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 61.7434
                       Mean reward: 748.25
               Mean episode length: 205.31
    Episode_Reward/reaching_object: 1.4757
     Episode_Reward/lifting_object: 159.0390
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 2.15s
                      Time elapsed: 00:47:20
                               ETA: 00:35:23

################################################################################
                     [1m Learning iteration 1145/2000 [0m                     

                       Computation: 43732 steps/s (collection: 2.143s, learning 0.105s)
             Mean action noise std: 2.67
          Mean value_function loss: 240.4206
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 61.7628
                       Mean reward: 799.14
               Mean episode length: 216.04
    Episode_Reward/reaching_object: 1.4494
     Episode_Reward/lifting_object: 156.4397
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 2.25s
                      Time elapsed: 00:47:22
                               ETA: 00:35:20

################################################################################
                     [1m Learning iteration 1146/2000 [0m                     

                       Computation: 43672 steps/s (collection: 2.147s, learning 0.104s)
             Mean action noise std: 2.67
          Mean value_function loss: 244.9339
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 61.7751
                       Mean reward: 769.28
               Mean episode length: 209.29
    Episode_Reward/reaching_object: 1.4931
     Episode_Reward/lifting_object: 161.3236
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 2.25s
                      Time elapsed: 00:47:24
                               ETA: 00:35:18

################################################################################
                     [1m Learning iteration 1147/2000 [0m                     

                       Computation: 42867 steps/s (collection: 2.170s, learning 0.123s)
             Mean action noise std: 2.67
          Mean value_function loss: 236.6286
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 61.7898
                       Mean reward: 818.06
               Mean episode length: 219.27
    Episode_Reward/reaching_object: 1.5131
     Episode_Reward/lifting_object: 163.4428
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 2.29s
                      Time elapsed: 00:47:26
                               ETA: 00:35:15

################################################################################
                     [1m Learning iteration 1148/2000 [0m                     

                       Computation: 43578 steps/s (collection: 2.156s, learning 0.100s)
             Mean action noise std: 2.67
          Mean value_function loss: 283.5525
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 61.8065
                       Mean reward: 840.25
               Mean episode length: 226.77
    Episode_Reward/reaching_object: 1.5068
     Episode_Reward/lifting_object: 163.5141
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 2.26s
                      Time elapsed: 00:47:29
                               ETA: 00:35:12

################################################################################
                     [1m Learning iteration 1149/2000 [0m                     

                       Computation: 44225 steps/s (collection: 2.085s, learning 0.138s)
             Mean action noise std: 2.67
          Mean value_function loss: 249.2188
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 61.8170
                       Mean reward: 838.76
               Mean episode length: 227.85
    Episode_Reward/reaching_object: 1.5122
     Episode_Reward/lifting_object: 163.2177
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 2.22s
                      Time elapsed: 00:47:31
                               ETA: 00:35:10

################################################################################
                     [1m Learning iteration 1150/2000 [0m                     

                       Computation: 43813 steps/s (collection: 2.085s, learning 0.159s)
             Mean action noise std: 2.67
          Mean value_function loss: 263.3032
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 61.8400
                       Mean reward: 817.50
               Mean episode length: 222.20
    Episode_Reward/reaching_object: 1.5042
     Episode_Reward/lifting_object: 162.6662
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 2.24s
                      Time elapsed: 00:47:33
                               ETA: 00:35:07

################################################################################
                     [1m Learning iteration 1151/2000 [0m                     

                       Computation: 42932 steps/s (collection: 2.166s, learning 0.124s)
             Mean action noise std: 2.68
          Mean value_function loss: 283.4339
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 61.8629
                       Mean reward: 772.99
               Mean episode length: 211.08
    Episode_Reward/reaching_object: 1.4794
     Episode_Reward/lifting_object: 159.5370
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 2.29s
                      Time elapsed: 00:47:36
                               ETA: 00:35:04

################################################################################
                     [1m Learning iteration 1152/2000 [0m                     

                       Computation: 43426 steps/s (collection: 2.124s, learning 0.140s)
             Mean action noise std: 2.68
          Mean value_function loss: 292.8859
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 61.8875
                       Mean reward: 828.05
               Mean episode length: 224.87
    Episode_Reward/reaching_object: 1.4600
     Episode_Reward/lifting_object: 157.8079
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 2.26s
                      Time elapsed: 00:47:38
                               ETA: 00:35:02

################################################################################
                     [1m Learning iteration 1153/2000 [0m                     

                       Computation: 44924 steps/s (collection: 2.097s, learning 0.091s)
             Mean action noise std: 2.68
          Mean value_function loss: 285.3503
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 61.9027
                       Mean reward: 826.69
               Mean episode length: 223.66
    Episode_Reward/reaching_object: 1.4752
     Episode_Reward/lifting_object: 159.6684
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 2.19s
                      Time elapsed: 00:47:40
                               ETA: 00:34:59

################################################################################
                     [1m Learning iteration 1154/2000 [0m                     

                       Computation: 44167 steps/s (collection: 2.116s, learning 0.110s)
             Mean action noise std: 2.68
          Mean value_function loss: 263.0955
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 61.9282
                       Mean reward: 781.55
               Mean episode length: 211.82
    Episode_Reward/reaching_object: 1.4738
     Episode_Reward/lifting_object: 160.1881
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 2.23s
                      Time elapsed: 00:47:42
                               ETA: 00:34:56

################################################################################
                     [1m Learning iteration 1155/2000 [0m                     

                       Computation: 45204 steps/s (collection: 2.056s, learning 0.119s)
             Mean action noise std: 2.68
          Mean value_function loss: 272.1311
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 61.9410
                       Mean reward: 783.04
               Mean episode length: 215.46
    Episode_Reward/reaching_object: 1.4667
     Episode_Reward/lifting_object: 158.6705
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 2.17s
                      Time elapsed: 00:47:44
                               ETA: 00:34:54

################################################################################
                     [1m Learning iteration 1156/2000 [0m                     

                       Computation: 43956 steps/s (collection: 2.098s, learning 0.138s)
             Mean action noise std: 2.69
          Mean value_function loss: 313.7458
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 61.9568
                       Mean reward: 805.28
               Mean episode length: 217.23
    Episode_Reward/reaching_object: 1.4102
     Episode_Reward/lifting_object: 152.1749
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 2.24s
                      Time elapsed: 00:47:47
                               ETA: 00:34:51

################################################################################
                     [1m Learning iteration 1157/2000 [0m                     

                       Computation: 44518 steps/s (collection: 2.118s, learning 0.091s)
             Mean action noise std: 2.69
          Mean value_function loss: 264.3829
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 61.9753
                       Mean reward: 848.32
               Mean episode length: 228.86
    Episode_Reward/reaching_object: 1.4771
     Episode_Reward/lifting_object: 160.4939
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 2.21s
                      Time elapsed: 00:47:49
                               ETA: 00:34:48

################################################################################
                     [1m Learning iteration 1158/2000 [0m                     

                       Computation: 46269 steps/s (collection: 2.039s, learning 0.086s)
             Mean action noise std: 2.69
          Mean value_function loss: 270.2218
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 61.9934
                       Mean reward: 824.38
               Mean episode length: 223.95
    Episode_Reward/reaching_object: 1.4748
     Episode_Reward/lifting_object: 160.0942
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 2.12s
                      Time elapsed: 00:47:51
                               ETA: 00:34:46

################################################################################
                     [1m Learning iteration 1159/2000 [0m                     

                       Computation: 46916 steps/s (collection: 1.998s, learning 0.097s)
             Mean action noise std: 2.69
          Mean value_function loss: 261.0507
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 62.0057
                       Mean reward: 813.88
               Mean episode length: 221.75
    Episode_Reward/reaching_object: 1.4750
     Episode_Reward/lifting_object: 159.7560
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 2.10s
                      Time elapsed: 00:47:53
                               ETA: 00:34:43

################################################################################
                     [1m Learning iteration 1160/2000 [0m                     

                       Computation: 44966 steps/s (collection: 2.056s, learning 0.130s)
             Mean action noise std: 2.69
          Mean value_function loss: 290.2438
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 62.0252
                       Mean reward: 772.46
               Mean episode length: 209.87
    Episode_Reward/reaching_object: 1.4566
     Episode_Reward/lifting_object: 158.5637
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 2.19s
                      Time elapsed: 00:47:55
                               ETA: 00:34:40

################################################################################
                     [1m Learning iteration 1161/2000 [0m                     

                       Computation: 46181 steps/s (collection: 2.011s, learning 0.118s)
             Mean action noise std: 2.69
          Mean value_function loss: 286.8823
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 62.0318
                       Mean reward: 773.02
               Mean episode length: 210.73
    Episode_Reward/reaching_object: 1.4558
     Episode_Reward/lifting_object: 157.9514
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 2.13s
                      Time elapsed: 00:47:57
                               ETA: 00:34:37

################################################################################
                     [1m Learning iteration 1162/2000 [0m                     

                       Computation: 47020 steps/s (collection: 1.996s, learning 0.095s)
             Mean action noise std: 2.69
          Mean value_function loss: 227.5462
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.0377
                       Mean reward: 837.32
               Mean episode length: 230.11
    Episode_Reward/reaching_object: 1.5209
     Episode_Reward/lifting_object: 165.3580
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 2.09s
                      Time elapsed: 00:47:59
                               ETA: 00:34:35

################################################################################
                     [1m Learning iteration 1163/2000 [0m                     

                       Computation: 45642 steps/s (collection: 2.055s, learning 0.099s)
             Mean action noise std: 2.70
          Mean value_function loss: 210.5643
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 62.0475
                       Mean reward: 825.00
               Mean episode length: 223.77
    Episode_Reward/reaching_object: 1.4829
     Episode_Reward/lifting_object: 161.2195
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 2.15s
                      Time elapsed: 00:48:02
                               ETA: 00:34:32

################################################################################
                     [1m Learning iteration 1164/2000 [0m                     

                       Computation: 43839 steps/s (collection: 2.150s, learning 0.093s)
             Mean action noise std: 2.70
          Mean value_function loss: 224.3363
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 62.0607
                       Mean reward: 833.66
               Mean episode length: 225.93
    Episode_Reward/reaching_object: 1.4653
     Episode_Reward/lifting_object: 159.6999
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 2.24s
                      Time elapsed: 00:48:04
                               ETA: 00:34:29

################################################################################
                     [1m Learning iteration 1165/2000 [0m                     

                       Computation: 44845 steps/s (collection: 2.098s, learning 0.095s)
             Mean action noise std: 2.70
          Mean value_function loss: 211.2800
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 62.0701
                       Mean reward: 792.65
               Mean episode length: 216.69
    Episode_Reward/reaching_object: 1.4856
     Episode_Reward/lifting_object: 161.1449
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 2.19s
                      Time elapsed: 00:48:06
                               ETA: 00:34:27

################################################################################
                     [1m Learning iteration 1166/2000 [0m                     

                       Computation: 43867 steps/s (collection: 2.089s, learning 0.152s)
             Mean action noise std: 2.70
          Mean value_function loss: 241.1780
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.0785
                       Mean reward: 797.19
               Mean episode length: 218.89
    Episode_Reward/reaching_object: 1.4933
     Episode_Reward/lifting_object: 162.3111
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 2.24s
                      Time elapsed: 00:48:08
                               ETA: 00:34:24

################################################################################
                     [1m Learning iteration 1167/2000 [0m                     

                       Computation: 43295 steps/s (collection: 2.165s, learning 0.106s)
             Mean action noise std: 2.70
          Mean value_function loss: 176.9322
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 62.0928
                       Mean reward: 835.73
               Mean episode length: 226.21
    Episode_Reward/reaching_object: 1.5203
     Episode_Reward/lifting_object: 164.4409
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0694
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 2.27s
                      Time elapsed: 00:48:11
                               ETA: 00:34:21

################################################################################
                     [1m Learning iteration 1168/2000 [0m                     

                       Computation: 44012 steps/s (collection: 2.145s, learning 0.089s)
             Mean action noise std: 2.70
          Mean value_function loss: 227.6818
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 62.1056
                       Mean reward: 839.39
               Mean episode length: 227.37
    Episode_Reward/reaching_object: 1.4907
     Episode_Reward/lifting_object: 161.8305
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 2.23s
                      Time elapsed: 00:48:13
                               ETA: 00:34:19

################################################################################
                     [1m Learning iteration 1169/2000 [0m                     

                       Computation: 43489 steps/s (collection: 2.171s, learning 0.090s)
             Mean action noise std: 2.71
          Mean value_function loss: 211.6337
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 62.1250
                       Mean reward: 834.56
               Mean episode length: 226.96
    Episode_Reward/reaching_object: 1.5221
     Episode_Reward/lifting_object: 165.6406
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 2.26s
                      Time elapsed: 00:48:15
                               ETA: 00:34:16

################################################################################
                     [1m Learning iteration 1170/2000 [0m                     

                       Computation: 44709 steps/s (collection: 2.096s, learning 0.102s)
             Mean action noise std: 2.71
          Mean value_function loss: 224.7277
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 62.1526
                       Mean reward: 810.88
               Mean episode length: 221.20
    Episode_Reward/reaching_object: 1.5265
     Episode_Reward/lifting_object: 166.1290
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 2.20s
                      Time elapsed: 00:48:17
                               ETA: 00:34:13

################################################################################
                     [1m Learning iteration 1171/2000 [0m                     

                       Computation: 43762 steps/s (collection: 2.152s, learning 0.095s)
             Mean action noise std: 2.71
          Mean value_function loss: 220.6884
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 62.1757
                       Mean reward: 827.88
               Mean episode length: 225.61
    Episode_Reward/reaching_object: 1.5363
     Episode_Reward/lifting_object: 167.6830
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 2.25s
                      Time elapsed: 00:48:19
                               ETA: 00:34:11

################################################################################
                     [1m Learning iteration 1172/2000 [0m                     

                       Computation: 44233 steps/s (collection: 2.125s, learning 0.097s)
             Mean action noise std: 2.71
          Mean value_function loss: 199.5757
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 62.1848
                       Mean reward: 858.60
               Mean episode length: 231.50
    Episode_Reward/reaching_object: 1.5380
     Episode_Reward/lifting_object: 167.1129
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 2.22s
                      Time elapsed: 00:48:22
                               ETA: 00:34:08

################################################################################
                     [1m Learning iteration 1173/2000 [0m                     

                       Computation: 42936 steps/s (collection: 2.185s, learning 0.104s)
             Mean action noise std: 2.71
          Mean value_function loss: 240.6687
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 62.1935
                       Mean reward: 825.85
               Mean episode length: 225.71
    Episode_Reward/reaching_object: 1.4860
     Episode_Reward/lifting_object: 160.5191
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 2.29s
                      Time elapsed: 00:48:24
                               ETA: 00:34:06

################################################################################
                     [1m Learning iteration 1174/2000 [0m                     

                       Computation: 41090 steps/s (collection: 2.277s, learning 0.115s)
             Mean action noise std: 2.71
          Mean value_function loss: 237.2149
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 62.2082
                       Mean reward: 823.96
               Mean episode length: 224.72
    Episode_Reward/reaching_object: 1.4900
     Episode_Reward/lifting_object: 161.2330
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0692
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 2.39s
                      Time elapsed: 00:48:26
                               ETA: 00:34:03

################################################################################
                     [1m Learning iteration 1175/2000 [0m                     

                       Computation: 41874 steps/s (collection: 2.239s, learning 0.109s)
             Mean action noise std: 2.72
          Mean value_function loss: 284.2412
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 62.2246
                       Mean reward: 811.56
               Mean episode length: 221.15
    Episode_Reward/reaching_object: 1.5006
     Episode_Reward/lifting_object: 162.6613
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 2.35s
                      Time elapsed: 00:48:29
                               ETA: 00:34:00

################################################################################
                     [1m Learning iteration 1176/2000 [0m                     

                       Computation: 44941 steps/s (collection: 2.092s, learning 0.095s)
             Mean action noise std: 2.72
          Mean value_function loss: 251.2823
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 62.2330
                       Mean reward: 744.36
               Mean episode length: 206.64
    Episode_Reward/reaching_object: 1.4641
     Episode_Reward/lifting_object: 158.4007
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 2.19s
                      Time elapsed: 00:48:31
                               ETA: 00:33:58

################################################################################
                     [1m Learning iteration 1177/2000 [0m                     

                       Computation: 45090 steps/s (collection: 2.078s, learning 0.102s)
             Mean action noise std: 2.72
          Mean value_function loss: 286.6070
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 62.2381
                       Mean reward: 768.00
               Mean episode length: 210.57
    Episode_Reward/reaching_object: 1.4522
     Episode_Reward/lifting_object: 157.3069
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 2.18s
                      Time elapsed: 00:48:33
                               ETA: 00:33:55

################################################################################
                     [1m Learning iteration 1178/2000 [0m                     

                       Computation: 45319 steps/s (collection: 2.067s, learning 0.103s)
             Mean action noise std: 2.72
          Mean value_function loss: 258.3317
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 62.2505
                       Mean reward: 818.80
               Mean episode length: 223.38
    Episode_Reward/reaching_object: 1.4712
     Episode_Reward/lifting_object: 158.2911
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 2.17s
                      Time elapsed: 00:48:35
                               ETA: 00:33:52

################################################################################
                     [1m Learning iteration 1179/2000 [0m                     

                       Computation: 46038 steps/s (collection: 2.039s, learning 0.097s)
             Mean action noise std: 2.72
          Mean value_function loss: 206.8021
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 62.2659
                       Mean reward: 840.23
               Mean episode length: 228.65
    Episode_Reward/reaching_object: 1.5129
     Episode_Reward/lifting_object: 163.5035
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0695
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 2.14s
                      Time elapsed: 00:48:37
                               ETA: 00:33:50

################################################################################
                     [1m Learning iteration 1180/2000 [0m                     

                       Computation: 44431 steps/s (collection: 2.124s, learning 0.089s)
             Mean action noise std: 2.72
          Mean value_function loss: 209.6088
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 62.2780
                       Mean reward: 840.11
               Mean episode length: 226.55
    Episode_Reward/reaching_object: 1.5103
     Episode_Reward/lifting_object: 163.5198
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0701
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 2.21s
                      Time elapsed: 00:48:40
                               ETA: 00:33:47

################################################################################
                     [1m Learning iteration 1181/2000 [0m                     

                       Computation: 44996 steps/s (collection: 2.072s, learning 0.113s)
             Mean action noise std: 2.72
          Mean value_function loss: 246.0489
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 62.2978
                       Mean reward: 808.43
               Mean episode length: 220.77
    Episode_Reward/reaching_object: 1.5108
     Episode_Reward/lifting_object: 163.6246
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 2.18s
                      Time elapsed: 00:48:42
                               ETA: 00:33:44

################################################################################
                     [1m Learning iteration 1182/2000 [0m                     

                       Computation: 44234 steps/s (collection: 2.115s, learning 0.108s)
             Mean action noise std: 2.73
          Mean value_function loss: 273.5271
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 62.3187
                       Mean reward: 812.78
               Mean episode length: 220.15
    Episode_Reward/reaching_object: 1.4858
     Episode_Reward/lifting_object: 161.0803
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 2.22s
                      Time elapsed: 00:48:44
                               ETA: 00:33:42

################################################################################
                     [1m Learning iteration 1183/2000 [0m                     

                       Computation: 43125 steps/s (collection: 2.120s, learning 0.159s)
             Mean action noise std: 2.73
          Mean value_function loss: 266.0767
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 62.3381
                       Mean reward: 841.02
               Mean episode length: 226.17
    Episode_Reward/reaching_object: 1.4930
     Episode_Reward/lifting_object: 162.3837
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 2.28s
                      Time elapsed: 00:48:46
                               ETA: 00:33:39

################################################################################
                     [1m Learning iteration 1184/2000 [0m                     

                       Computation: 44414 steps/s (collection: 2.115s, learning 0.099s)
             Mean action noise std: 2.73
          Mean value_function loss: 261.7556
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 62.3485
                       Mean reward: 795.86
               Mean episode length: 215.24
    Episode_Reward/reaching_object: 1.4745
     Episode_Reward/lifting_object: 160.4541
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 2.21s
                      Time elapsed: 00:48:49
                               ETA: 00:33:36

################################################################################
                     [1m Learning iteration 1185/2000 [0m                     

                       Computation: 43872 steps/s (collection: 2.131s, learning 0.110s)
             Mean action noise std: 2.73
          Mean value_function loss: 239.5506
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 62.3565
                       Mean reward: 856.19
               Mean episode length: 229.64
    Episode_Reward/reaching_object: 1.4323
     Episode_Reward/lifting_object: 156.1588
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 2.24s
                      Time elapsed: 00:48:51
                               ETA: 00:33:34

################################################################################
                     [1m Learning iteration 1186/2000 [0m                     

                       Computation: 45432 steps/s (collection: 2.060s, learning 0.104s)
             Mean action noise std: 2.73
          Mean value_function loss: 323.9787
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 62.3657
                       Mean reward: 798.16
               Mean episode length: 216.54
    Episode_Reward/reaching_object: 1.4503
     Episode_Reward/lifting_object: 157.6049
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 2.16s
                      Time elapsed: 00:48:53
                               ETA: 00:33:31

################################################################################
                     [1m Learning iteration 1187/2000 [0m                     

                       Computation: 45764 steps/s (collection: 2.057s, learning 0.091s)
             Mean action noise std: 2.73
          Mean value_function loss: 267.5907
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 62.3751
                       Mean reward: 820.71
               Mean episode length: 223.92
    Episode_Reward/reaching_object: 1.4434
     Episode_Reward/lifting_object: 155.9117
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 2.15s
                      Time elapsed: 00:48:55
                               ETA: 00:33:28

################################################################################
                     [1m Learning iteration 1188/2000 [0m                     

                       Computation: 42840 steps/s (collection: 2.137s, learning 0.158s)
             Mean action noise std: 2.73
          Mean value_function loss: 289.9350
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 62.3851
                       Mean reward: 780.35
               Mean episode length: 212.54
    Episode_Reward/reaching_object: 1.4251
     Episode_Reward/lifting_object: 154.9362
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 2.29s
                      Time elapsed: 00:48:57
                               ETA: 00:33:26

################################################################################
                     [1m Learning iteration 1189/2000 [0m                     

                       Computation: 44438 steps/s (collection: 2.118s, learning 0.094s)
             Mean action noise std: 2.74
          Mean value_function loss: 262.0101
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 62.3982
                       Mean reward: 806.07
               Mean episode length: 220.76
    Episode_Reward/reaching_object: 1.4835
     Episode_Reward/lifting_object: 161.2735
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 2.21s
                      Time elapsed: 00:49:00
                               ETA: 00:33:23

################################################################################
                     [1m Learning iteration 1190/2000 [0m                     

                       Computation: 45567 steps/s (collection: 2.064s, learning 0.094s)
             Mean action noise std: 2.74
          Mean value_function loss: 282.5155
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 62.4123
                       Mean reward: 809.00
               Mean episode length: 218.78
    Episode_Reward/reaching_object: 1.4322
     Episode_Reward/lifting_object: 156.4855
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 2.16s
                      Time elapsed: 00:49:02
                               ETA: 00:33:21

################################################################################
                     [1m Learning iteration 1191/2000 [0m                     

                       Computation: 44467 steps/s (collection: 2.094s, learning 0.117s)
             Mean action noise std: 2.74
          Mean value_function loss: 228.7743
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 62.4243
                       Mean reward: 844.08
               Mean episode length: 226.86
    Episode_Reward/reaching_object: 1.4780
     Episode_Reward/lifting_object: 161.3743
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 2.21s
                      Time elapsed: 00:49:04
                               ETA: 00:33:18

################################################################################
                     [1m Learning iteration 1192/2000 [0m                     

                       Computation: 43708 steps/s (collection: 2.161s, learning 0.089s)
             Mean action noise std: 2.74
          Mean value_function loss: 236.1246
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.4404
                       Mean reward: 827.11
               Mean episode length: 222.99
    Episode_Reward/reaching_object: 1.4939
     Episode_Reward/lifting_object: 163.6165
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 2.25s
                      Time elapsed: 00:49:06
                               ETA: 00:33:15

################################################################################
                     [1m Learning iteration 1193/2000 [0m                     

                       Computation: 45465 steps/s (collection: 2.060s, learning 0.103s)
             Mean action noise std: 2.74
          Mean value_function loss: 229.9484
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 62.4529
                       Mean reward: 805.54
               Mean episode length: 218.62
    Episode_Reward/reaching_object: 1.4741
     Episode_Reward/lifting_object: 161.2813
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 2.16s
                      Time elapsed: 00:49:08
                               ETA: 00:33:13

################################################################################
                     [1m Learning iteration 1194/2000 [0m                     

                       Computation: 43855 steps/s (collection: 2.119s, learning 0.123s)
             Mean action noise std: 2.74
          Mean value_function loss: 243.5500
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.4627
                       Mean reward: 807.13
               Mean episode length: 218.94
    Episode_Reward/reaching_object: 1.5097
     Episode_Reward/lifting_object: 165.1162
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 2.24s
                      Time elapsed: 00:49:11
                               ETA: 00:33:10

################################################################################
                     [1m Learning iteration 1195/2000 [0m                     

                       Computation: 41857 steps/s (collection: 2.193s, learning 0.155s)
             Mean action noise std: 2.74
          Mean value_function loss: 265.6026
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 62.4781
                       Mean reward: 796.26
               Mean episode length: 214.71
    Episode_Reward/reaching_object: 1.4474
     Episode_Reward/lifting_object: 157.8030
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 2.35s
                      Time elapsed: 00:49:13
                               ETA: 00:33:07

################################################################################
                     [1m Learning iteration 1196/2000 [0m                     

                       Computation: 44644 steps/s (collection: 2.102s, learning 0.099s)
             Mean action noise std: 2.75
          Mean value_function loss: 244.4648
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 62.4888
                       Mean reward: 794.12
               Mean episode length: 216.51
    Episode_Reward/reaching_object: 1.4648
     Episode_Reward/lifting_object: 159.4492
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 2.20s
                      Time elapsed: 00:49:15
                               ETA: 00:33:05

################################################################################
                     [1m Learning iteration 1197/2000 [0m                     

                       Computation: 42545 steps/s (collection: 2.168s, learning 0.143s)
             Mean action noise std: 2.75
          Mean value_function loss: 281.5688
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 62.4993
                       Mean reward: 826.02
               Mean episode length: 222.87
    Episode_Reward/reaching_object: 1.4791
     Episode_Reward/lifting_object: 160.9809
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 2.31s
                      Time elapsed: 00:49:17
                               ETA: 00:33:02

################################################################################
                     [1m Learning iteration 1198/2000 [0m                     

                       Computation: 42930 steps/s (collection: 2.137s, learning 0.153s)
             Mean action noise std: 2.75
          Mean value_function loss: 253.0951
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 62.5173
                       Mean reward: 771.70
               Mean episode length: 211.19
    Episode_Reward/reaching_object: 1.4746
     Episode_Reward/lifting_object: 160.4644
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 2.29s
                      Time elapsed: 00:49:20
                               ETA: 00:33:00

################################################################################
                     [1m Learning iteration 1199/2000 [0m                     

                       Computation: 42384 steps/s (collection: 2.200s, learning 0.119s)
             Mean action noise std: 2.75
          Mean value_function loss: 235.1864
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 62.5339
                       Mean reward: 820.60
               Mean episode length: 222.23
    Episode_Reward/reaching_object: 1.4995
     Episode_Reward/lifting_object: 163.1096
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 2.32s
                      Time elapsed: 00:49:22
                               ETA: 00:32:57

################################################################################
                     [1m Learning iteration 1200/2000 [0m                     

                       Computation: 36614 steps/s (collection: 2.586s, learning 0.099s)
             Mean action noise std: 2.75
          Mean value_function loss: 213.5379
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 62.5430
                       Mean reward: 819.32
               Mean episode length: 220.90
    Episode_Reward/reaching_object: 1.5167
     Episode_Reward/lifting_object: 165.4182
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 2.68s
                      Time elapsed: 00:49:25
                               ETA: 00:32:55

################################################################################
                     [1m Learning iteration 1201/2000 [0m                     

                       Computation: 36933 steps/s (collection: 2.389s, learning 0.273s)
             Mean action noise std: 2.75
          Mean value_function loss: 275.9435
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 62.5546
                       Mean reward: 794.64
               Mean episode length: 217.91
    Episode_Reward/reaching_object: 1.4730
     Episode_Reward/lifting_object: 159.9748
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 2.66s
                      Time elapsed: 00:49:27
                               ETA: 00:32:52

################################################################################
                     [1m Learning iteration 1202/2000 [0m                     

                       Computation: 37279 steps/s (collection: 2.494s, learning 0.143s)
             Mean action noise std: 2.76
          Mean value_function loss: 285.3740
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 62.5669
                       Mean reward: 804.77
               Mean episode length: 219.20
    Episode_Reward/reaching_object: 1.4823
     Episode_Reward/lifting_object: 161.4095
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 2.64s
                      Time elapsed: 00:49:30
                               ETA: 00:32:50

################################################################################
                     [1m Learning iteration 1203/2000 [0m                     

                       Computation: 44823 steps/s (collection: 2.102s, learning 0.092s)
             Mean action noise std: 2.76
          Mean value_function loss: 277.8902
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 62.5790
                       Mean reward: 801.45
               Mean episode length: 219.99
    Episode_Reward/reaching_object: 1.4394
     Episode_Reward/lifting_object: 155.5229
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 2.19s
                      Time elapsed: 00:49:32
                               ETA: 00:32:47

################################################################################
                     [1m Learning iteration 1204/2000 [0m                     

                       Computation: 42490 steps/s (collection: 2.107s, learning 0.207s)
             Mean action noise std: 2.76
          Mean value_function loss: 224.2263
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 62.5985
                       Mean reward: 846.13
               Mean episode length: 230.65
    Episode_Reward/reaching_object: 1.5020
     Episode_Reward/lifting_object: 162.1945
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0686
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 2.31s
                      Time elapsed: 00:49:35
                               ETA: 00:32:45

################################################################################
                     [1m Learning iteration 1205/2000 [0m                     

                       Computation: 40659 steps/s (collection: 2.221s, learning 0.197s)
             Mean action noise std: 2.76
          Mean value_function loss: 235.5852
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 62.6096
                       Mean reward: 755.35
               Mean episode length: 209.30
    Episode_Reward/reaching_object: 1.4645
     Episode_Reward/lifting_object: 159.6215
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 2.42s
                      Time elapsed: 00:49:37
                               ETA: 00:32:42

################################################################################
                     [1m Learning iteration 1206/2000 [0m                     

                       Computation: 40625 steps/s (collection: 2.250s, learning 0.170s)
             Mean action noise std: 2.76
          Mean value_function loss: 226.6452
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 62.6213
                       Mean reward: 796.62
               Mean episode length: 217.55
    Episode_Reward/reaching_object: 1.4887
     Episode_Reward/lifting_object: 161.1367
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 2.42s
                      Time elapsed: 00:49:39
                               ETA: 00:32:40

################################################################################
                     [1m Learning iteration 1207/2000 [0m                     

                       Computation: 42899 steps/s (collection: 2.170s, learning 0.121s)
             Mean action noise std: 2.76
          Mean value_function loss: 227.5394
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 62.6371
                       Mean reward: 835.66
               Mean episode length: 228.64
    Episode_Reward/reaching_object: 1.5079
     Episode_Reward/lifting_object: 163.7781
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0695
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 2.29s
                      Time elapsed: 00:49:42
                               ETA: 00:32:37

################################################################################
                     [1m Learning iteration 1208/2000 [0m                     

                       Computation: 39660 steps/s (collection: 2.380s, learning 0.099s)
             Mean action noise std: 2.77
          Mean value_function loss: 242.0338
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 62.6509
                       Mean reward: 783.23
               Mean episode length: 213.84
    Episode_Reward/reaching_object: 1.4866
     Episode_Reward/lifting_object: 161.5304
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0686
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 2.48s
                      Time elapsed: 00:49:44
                               ETA: 00:32:35

################################################################################
                     [1m Learning iteration 1209/2000 [0m                     

                       Computation: 41969 steps/s (collection: 2.196s, learning 0.146s)
             Mean action noise std: 2.77
          Mean value_function loss: 197.4105
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 62.6625
                       Mean reward: 841.26
               Mean episode length: 226.01
    Episode_Reward/reaching_object: 1.5136
     Episode_Reward/lifting_object: 164.3784
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 2.34s
                      Time elapsed: 00:49:46
                               ETA: 00:32:32

################################################################################
                     [1m Learning iteration 1210/2000 [0m                     

                       Computation: 44773 steps/s (collection: 2.058s, learning 0.138s)
             Mean action noise std: 2.77
          Mean value_function loss: 229.6740
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 62.6775
                       Mean reward: 789.43
               Mean episode length: 216.78
    Episode_Reward/reaching_object: 1.4625
     Episode_Reward/lifting_object: 157.8772
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 2.20s
                      Time elapsed: 00:49:49
                               ETA: 00:32:30

################################################################################
                     [1m Learning iteration 1211/2000 [0m                     

                       Computation: 42089 steps/s (collection: 2.187s, learning 0.149s)
             Mean action noise std: 2.77
          Mean value_function loss: 238.2135
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 62.6901
                       Mean reward: 789.75
               Mean episode length: 217.59
    Episode_Reward/reaching_object: 1.5011
     Episode_Reward/lifting_object: 162.2104
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 2.34s
                      Time elapsed: 00:49:51
                               ETA: 00:32:27

################################################################################
                     [1m Learning iteration 1212/2000 [0m                     

                       Computation: 45750 steps/s (collection: 2.053s, learning 0.096s)
             Mean action noise std: 2.77
          Mean value_function loss: 204.1486
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.7206
                       Mean reward: 837.98
               Mean episode length: 226.94
    Episode_Reward/reaching_object: 1.5165
     Episode_Reward/lifting_object: 165.3427
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 2.15s
                      Time elapsed: 00:49:53
                               ETA: 00:32:24

################################################################################
                     [1m Learning iteration 1213/2000 [0m                     

                       Computation: 41226 steps/s (collection: 2.245s, learning 0.139s)
             Mean action noise std: 2.78
          Mean value_function loss: 260.7335
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 62.7461
                       Mean reward: 823.09
               Mean episode length: 222.30
    Episode_Reward/reaching_object: 1.4991
     Episode_Reward/lifting_object: 162.7639
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 2.38s
                      Time elapsed: 00:49:56
                               ETA: 00:32:22

################################################################################
                     [1m Learning iteration 1214/2000 [0m                     

                       Computation: 42023 steps/s (collection: 2.241s, learning 0.098s)
             Mean action noise std: 2.78
          Mean value_function loss: 251.6594
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 62.7657
                       Mean reward: 779.92
               Mean episode length: 215.96
    Episode_Reward/reaching_object: 1.4696
     Episode_Reward/lifting_object: 158.8871
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 2.34s
                      Time elapsed: 00:49:58
                               ETA: 00:32:19

################################################################################
                     [1m Learning iteration 1215/2000 [0m                     

                       Computation: 42477 steps/s (collection: 2.157s, learning 0.158s)
             Mean action noise std: 2.78
          Mean value_function loss: 215.5325
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 62.7756
                       Mean reward: 800.62
               Mean episode length: 216.75
    Episode_Reward/reaching_object: 1.5101
     Episode_Reward/lifting_object: 164.7393
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 2.31s
                      Time elapsed: 00:50:00
                               ETA: 00:32:17

################################################################################
                     [1m Learning iteration 1216/2000 [0m                     

                       Computation: 42201 steps/s (collection: 2.203s, learning 0.127s)
             Mean action noise std: 2.78
          Mean value_function loss: 188.6686
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 62.7890
                       Mean reward: 863.47
               Mean episode length: 232.49
    Episode_Reward/reaching_object: 1.5343
     Episode_Reward/lifting_object: 166.6520
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0705
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 2.33s
                      Time elapsed: 00:50:03
                               ETA: 00:32:14

################################################################################
                     [1m Learning iteration 1217/2000 [0m                     

                       Computation: 44208 steps/s (collection: 2.093s, learning 0.131s)
             Mean action noise std: 2.78
          Mean value_function loss: 230.4599
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 62.8036
                       Mean reward: 828.10
               Mean episode length: 225.05
    Episode_Reward/reaching_object: 1.5069
     Episode_Reward/lifting_object: 163.6286
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0708
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 2.22s
                      Time elapsed: 00:50:05
                               ETA: 00:32:11

################################################################################
                     [1m Learning iteration 1218/2000 [0m                     

                       Computation: 43493 steps/s (collection: 2.170s, learning 0.090s)
             Mean action noise std: 2.78
          Mean value_function loss: 206.3275
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 62.8182
                       Mean reward: 852.50
               Mean episode length: 228.79
    Episode_Reward/reaching_object: 1.5180
     Episode_Reward/lifting_object: 165.4404
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 2.26s
                      Time elapsed: 00:50:07
                               ETA: 00:32:09

################################################################################
                     [1m Learning iteration 1219/2000 [0m                     

                       Computation: 40813 steps/s (collection: 2.261s, learning 0.148s)
             Mean action noise std: 2.78
          Mean value_function loss: 196.6762
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 62.8254
                       Mean reward: 791.96
               Mean episode length: 217.91
    Episode_Reward/reaching_object: 1.5247
     Episode_Reward/lifting_object: 165.7116
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 2.41s
                      Time elapsed: 00:50:09
                               ETA: 00:32:06

################################################################################
                     [1m Learning iteration 1220/2000 [0m                     

                       Computation: 41348 steps/s (collection: 2.246s, learning 0.132s)
             Mean action noise std: 2.78
          Mean value_function loss: 260.1441
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 62.8267
                       Mean reward: 854.03
               Mean episode length: 229.53
    Episode_Reward/reaching_object: 1.4888
     Episode_Reward/lifting_object: 161.0761
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0686
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 2.38s
                      Time elapsed: 00:50:12
                               ETA: 00:32:04

################################################################################
                     [1m Learning iteration 1221/2000 [0m                     

                       Computation: 44337 steps/s (collection: 2.113s, learning 0.104s)
             Mean action noise std: 2.78
          Mean value_function loss: 214.0200
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 62.8278
                       Mean reward: 811.55
               Mean episode length: 219.72
    Episode_Reward/reaching_object: 1.4958
     Episode_Reward/lifting_object: 163.3863
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 2.22s
                      Time elapsed: 00:50:14
                               ETA: 00:32:01

################################################################################
                     [1m Learning iteration 1222/2000 [0m                     

                       Computation: 45525 steps/s (collection: 2.051s, learning 0.108s)
             Mean action noise std: 2.79
          Mean value_function loss: 231.5916
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 62.8303
                       Mean reward: 808.01
               Mean episode length: 219.53
    Episode_Reward/reaching_object: 1.4747
     Episode_Reward/lifting_object: 160.5208
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0692
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 2.16s
                      Time elapsed: 00:50:16
                               ETA: 00:31:59

################################################################################
                     [1m Learning iteration 1223/2000 [0m                     

                       Computation: 44854 steps/s (collection: 2.094s, learning 0.098s)
             Mean action noise std: 2.79
          Mean value_function loss: 223.8741
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 62.8357
                       Mean reward: 860.07
               Mean episode length: 232.87
    Episode_Reward/reaching_object: 1.5247
     Episode_Reward/lifting_object: 166.5466
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 2.19s
                      Time elapsed: 00:50:18
                               ETA: 00:31:56

################################################################################
                     [1m Learning iteration 1224/2000 [0m                     

                       Computation: 37191 steps/s (collection: 2.443s, learning 0.200s)
             Mean action noise std: 2.79
          Mean value_function loss: 246.8856
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 62.8423
                       Mean reward: 855.12
               Mean episode length: 230.60
    Episode_Reward/reaching_object: 1.5205
     Episode_Reward/lifting_object: 165.1391
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 2.64s
                      Time elapsed: 00:50:21
                               ETA: 00:31:54

################################################################################
                     [1m Learning iteration 1225/2000 [0m                     

                       Computation: 39428 steps/s (collection: 2.305s, learning 0.189s)
             Mean action noise std: 2.79
          Mean value_function loss: 203.2951
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 62.8466
                       Mean reward: 810.80
               Mean episode length: 220.35
    Episode_Reward/reaching_object: 1.4849
     Episode_Reward/lifting_object: 161.3528
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0694
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 2.49s
                      Time elapsed: 00:50:24
                               ETA: 00:31:51

################################################################################
                     [1m Learning iteration 1226/2000 [0m                     

                       Computation: 37986 steps/s (collection: 2.438s, learning 0.150s)
             Mean action noise std: 2.79
          Mean value_function loss: 196.1533
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 62.8518
                       Mean reward: 834.29
               Mean episode length: 224.20
    Episode_Reward/reaching_object: 1.5083
     Episode_Reward/lifting_object: 164.9536
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 2.59s
                      Time elapsed: 00:50:26
                               ETA: 00:31:49

################################################################################
                     [1m Learning iteration 1227/2000 [0m                     

                       Computation: 38689 steps/s (collection: 2.312s, learning 0.229s)
             Mean action noise std: 2.79
          Mean value_function loss: 207.0255
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 62.8571
                       Mean reward: 866.88
               Mean episode length: 233.53
    Episode_Reward/reaching_object: 1.4951
     Episode_Reward/lifting_object: 162.7726
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0701
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 2.54s
                      Time elapsed: 00:50:29
                               ETA: 00:31:46

################################################################################
                     [1m Learning iteration 1228/2000 [0m                     

                       Computation: 38023 steps/s (collection: 2.399s, learning 0.186s)
             Mean action noise std: 2.79
          Mean value_function loss: 241.1847
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 62.8697
                       Mean reward: 798.34
               Mean episode length: 219.07
    Episode_Reward/reaching_object: 1.4613
     Episode_Reward/lifting_object: 159.4308
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 2.59s
                      Time elapsed: 00:50:31
                               ETA: 00:31:44

################################################################################
                     [1m Learning iteration 1229/2000 [0m                     

                       Computation: 38697 steps/s (collection: 2.352s, learning 0.189s)
             Mean action noise std: 2.79
          Mean value_function loss: 204.3570
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 62.8837
                       Mean reward: 862.36
               Mean episode length: 232.43
    Episode_Reward/reaching_object: 1.5158
     Episode_Reward/lifting_object: 165.7573
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0700
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 2.54s
                      Time elapsed: 00:50:34
                               ETA: 00:31:41

################################################################################
                     [1m Learning iteration 1230/2000 [0m                     

                       Computation: 39025 steps/s (collection: 2.336s, learning 0.183s)
             Mean action noise std: 2.79
          Mean value_function loss: 226.7620
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 62.8947
                       Mean reward: 841.36
               Mean episode length: 227.73
    Episode_Reward/reaching_object: 1.5056
     Episode_Reward/lifting_object: 164.8902
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0705
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 2.52s
                      Time elapsed: 00:50:36
                               ETA: 00:31:39

################################################################################
                     [1m Learning iteration 1231/2000 [0m                     

                       Computation: 37281 steps/s (collection: 2.477s, learning 0.160s)
             Mean action noise std: 2.79
          Mean value_function loss: 236.4894
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 62.9023
                       Mean reward: 876.50
               Mean episode length: 235.94
    Episode_Reward/reaching_object: 1.5066
     Episode_Reward/lifting_object: 165.2180
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 2.64s
                      Time elapsed: 00:50:39
                               ETA: 00:31:37

################################################################################
                     [1m Learning iteration 1232/2000 [0m                     

                       Computation: 37743 steps/s (collection: 2.419s, learning 0.185s)
             Mean action noise std: 2.80
          Mean value_function loss: 229.5853
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 62.9130
                       Mean reward: 813.62
               Mean episode length: 219.35
    Episode_Reward/reaching_object: 1.4986
     Episode_Reward/lifting_object: 164.7352
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0695
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 2.60s
                      Time elapsed: 00:50:42
                               ETA: 00:31:34

################################################################################
                     [1m Learning iteration 1233/2000 [0m                     

                       Computation: 39738 steps/s (collection: 2.326s, learning 0.148s)
             Mean action noise std: 2.80
          Mean value_function loss: 212.1304
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 62.9273
                       Mean reward: 844.21
               Mean episode length: 227.94
    Episode_Reward/reaching_object: 1.5245
     Episode_Reward/lifting_object: 167.6844
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0701
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 2.47s
                      Time elapsed: 00:50:44
                               ETA: 00:31:32

################################################################################
                     [1m Learning iteration 1234/2000 [0m                     

                       Computation: 41898 steps/s (collection: 2.224s, learning 0.122s)
             Mean action noise std: 2.80
          Mean value_function loss: 201.9884
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 62.9374
                       Mean reward: 822.08
               Mean episode length: 223.27
    Episode_Reward/reaching_object: 1.5029
     Episode_Reward/lifting_object: 164.7814
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 2.35s
                      Time elapsed: 00:50:46
                               ETA: 00:31:29

################################################################################
                     [1m Learning iteration 1235/2000 [0m                     

                       Computation: 38610 steps/s (collection: 2.402s, learning 0.144s)
             Mean action noise std: 2.80
          Mean value_function loss: 200.4926
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 62.9451
                       Mean reward: 847.45
               Mean episode length: 227.44
    Episode_Reward/reaching_object: 1.4937
     Episode_Reward/lifting_object: 164.1906
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0700
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 2.55s
                      Time elapsed: 00:50:49
                               ETA: 00:31:27

################################################################################
                     [1m Learning iteration 1236/2000 [0m                     

                       Computation: 40440 steps/s (collection: 2.273s, learning 0.158s)
             Mean action noise std: 2.80
          Mean value_function loss: 184.7758
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 62.9548
                       Mean reward: 835.81
               Mean episode length: 224.54
    Episode_Reward/reaching_object: 1.5181
     Episode_Reward/lifting_object: 166.3716
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 2.43s
                      Time elapsed: 00:50:51
                               ETA: 00:31:24

################################################################################
                     [1m Learning iteration 1237/2000 [0m                     

                       Computation: 37315 steps/s (collection: 2.415s, learning 0.220s)
             Mean action noise std: 2.80
          Mean value_function loss: 209.5998
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.9623
                       Mean reward: 834.01
               Mean episode length: 226.08
    Episode_Reward/reaching_object: 1.5289
     Episode_Reward/lifting_object: 167.0846
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0720
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 2.63s
                      Time elapsed: 00:50:54
                               ETA: 00:31:22

################################################################################
                     [1m Learning iteration 1238/2000 [0m                     

                       Computation: 37650 steps/s (collection: 2.412s, learning 0.199s)
             Mean action noise std: 2.80
          Mean value_function loss: 196.9674
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 62.9806
                       Mean reward: 814.55
               Mean episode length: 222.35
    Episode_Reward/reaching_object: 1.4792
     Episode_Reward/lifting_object: 161.6707
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 2.61s
                      Time elapsed: 00:50:57
                               ETA: 00:31:20

################################################################################
                     [1m Learning iteration 1239/2000 [0m                     

                       Computation: 34577 steps/s (collection: 2.737s, learning 0.106s)
             Mean action noise std: 2.81
          Mean value_function loss: 203.4711
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 62.9992
                       Mean reward: 834.52
               Mean episode length: 227.53
    Episode_Reward/reaching_object: 1.5440
     Episode_Reward/lifting_object: 169.5420
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0722
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 2.84s
                      Time elapsed: 00:50:59
                               ETA: 00:31:17

################################################################################
                     [1m Learning iteration 1240/2000 [0m                     

                       Computation: 43460 steps/s (collection: 2.158s, learning 0.104s)
             Mean action noise std: 2.81
          Mean value_function loss: 196.4519
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 63.0150
                       Mean reward: 851.13
               Mean episode length: 229.74
    Episode_Reward/reaching_object: 1.5087
     Episode_Reward/lifting_object: 165.2192
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 2.26s
                      Time elapsed: 00:51:02
                               ETA: 00:31:15

################################################################################
                     [1m Learning iteration 1241/2000 [0m                     

                       Computation: 41343 steps/s (collection: 2.217s, learning 0.161s)
             Mean action noise std: 2.81
          Mean value_function loss: 213.0164
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 63.0272
                       Mean reward: 869.81
               Mean episode length: 233.69
    Episode_Reward/reaching_object: 1.5268
     Episode_Reward/lifting_object: 166.6736
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0714
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 2.38s
                      Time elapsed: 00:51:04
                               ETA: 00:31:12

################################################################################
                     [1m Learning iteration 1242/2000 [0m                     

                       Computation: 41447 steps/s (collection: 2.237s, learning 0.135s)
             Mean action noise std: 2.81
          Mean value_function loss: 248.2770
               Mean surrogate loss: 0.0096
                 Mean entropy loss: 63.0400
                       Mean reward: 852.08
               Mean episode length: 230.80
    Episode_Reward/reaching_object: 1.5180
     Episode_Reward/lifting_object: 165.9330
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0705
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 2.37s
                      Time elapsed: 00:51:06
                               ETA: 00:31:10

################################################################################
                     [1m Learning iteration 1243/2000 [0m                     

                       Computation: 40235 steps/s (collection: 2.282s, learning 0.162s)
             Mean action noise std: 2.81
          Mean value_function loss: 264.5167
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 63.0414
                       Mean reward: 835.92
               Mean episode length: 226.49
    Episode_Reward/reaching_object: 1.4608
     Episode_Reward/lifting_object: 159.5472
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 2.44s
                      Time elapsed: 00:51:09
                               ETA: 00:31:07

################################################################################
                     [1m Learning iteration 1244/2000 [0m                     

                       Computation: 45790 steps/s (collection: 2.049s, learning 0.098s)
             Mean action noise std: 2.81
          Mean value_function loss: 209.2661
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 63.0445
                       Mean reward: 840.42
               Mean episode length: 226.28
    Episode_Reward/reaching_object: 1.4661
     Episode_Reward/lifting_object: 159.7268
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 2.15s
                      Time elapsed: 00:51:11
                               ETA: 00:31:05

################################################################################
                     [1m Learning iteration 1245/2000 [0m                     

                       Computation: 41049 steps/s (collection: 2.238s, learning 0.157s)
             Mean action noise std: 2.81
          Mean value_function loss: 204.3229
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 63.0575
                       Mean reward: 808.43
               Mean episode length: 221.01
    Episode_Reward/reaching_object: 1.5059
     Episode_Reward/lifting_object: 164.8657
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 2.39s
                      Time elapsed: 00:51:13
                               ETA: 00:31:02

################################################################################
                     [1m Learning iteration 1246/2000 [0m                     

                       Computation: 44075 steps/s (collection: 2.124s, learning 0.107s)
             Mean action noise std: 2.82
          Mean value_function loss: 209.1251
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 63.0771
                       Mean reward: 811.53
               Mean episode length: 217.82
    Episode_Reward/reaching_object: 1.4754
     Episode_Reward/lifting_object: 161.7544
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 2.23s
                      Time elapsed: 00:51:16
                               ETA: 00:30:59

################################################################################
                     [1m Learning iteration 1247/2000 [0m                     

                       Computation: 41796 steps/s (collection: 2.172s, learning 0.180s)
             Mean action noise std: 2.82
          Mean value_function loss: 181.5672
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 63.0905
                       Mean reward: 862.99
               Mean episode length: 232.31
    Episode_Reward/reaching_object: 1.5131
     Episode_Reward/lifting_object: 165.7887
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 2.35s
                      Time elapsed: 00:51:18
                               ETA: 00:30:57

################################################################################
                     [1m Learning iteration 1248/2000 [0m                     

                       Computation: 35811 steps/s (collection: 2.529s, learning 0.216s)
             Mean action noise std: 2.82
          Mean value_function loss: 204.4402
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 63.1024
                       Mean reward: 841.68
               Mean episode length: 226.28
    Episode_Reward/reaching_object: 1.5541
     Episode_Reward/lifting_object: 170.8882
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0714
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 2.75s
                      Time elapsed: 00:51:21
                               ETA: 00:30:55

################################################################################
                     [1m Learning iteration 1249/2000 [0m                     

                       Computation: 36600 steps/s (collection: 2.525s, learning 0.161s)
             Mean action noise std: 2.82
          Mean value_function loss: 202.6958
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 63.1171
                       Mean reward: 831.91
               Mean episode length: 227.84
    Episode_Reward/reaching_object: 1.5099
     Episode_Reward/lifting_object: 164.3307
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0725
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 2.69s
                      Time elapsed: 00:51:23
                               ETA: 00:30:52

################################################################################
                     [1m Learning iteration 1250/2000 [0m                     

                       Computation: 36874 steps/s (collection: 2.522s, learning 0.144s)
             Mean action noise std: 2.82
          Mean value_function loss: 176.8066
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 63.1294
                       Mean reward: 855.48
               Mean episode length: 231.14
    Episode_Reward/reaching_object: 1.5085
     Episode_Reward/lifting_object: 165.1911
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0710
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 2.67s
                      Time elapsed: 00:51:26
                               ETA: 00:30:50

################################################################################
                     [1m Learning iteration 1251/2000 [0m                     

                       Computation: 38057 steps/s (collection: 2.479s, learning 0.104s)
             Mean action noise std: 2.83
          Mean value_function loss: 223.9028
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 63.1588
                       Mean reward: 834.84
               Mean episode length: 226.33
    Episode_Reward/reaching_object: 1.5053
     Episode_Reward/lifting_object: 164.5467
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 2.58s
                      Time elapsed: 00:51:29
                               ETA: 00:30:48

################################################################################
                     [1m Learning iteration 1252/2000 [0m                     

                       Computation: 39435 steps/s (collection: 2.340s, learning 0.153s)
             Mean action noise std: 2.83
          Mean value_function loss: 190.5911
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 63.1831
                       Mean reward: 854.21
               Mean episode length: 229.43
    Episode_Reward/reaching_object: 1.5072
     Episode_Reward/lifting_object: 164.3680
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0722
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 2.49s
                      Time elapsed: 00:51:31
                               ETA: 00:30:45

################################################################################
                     [1m Learning iteration 1253/2000 [0m                     

                       Computation: 39938 steps/s (collection: 2.285s, learning 0.177s)
             Mean action noise std: 2.83
          Mean value_function loss: 217.9087
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 63.1991
                       Mean reward: 818.05
               Mean episode length: 221.35
    Episode_Reward/reaching_object: 1.4815
     Episode_Reward/lifting_object: 162.0352
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 2.46s
                      Time elapsed: 00:51:34
                               ETA: 00:30:43

################################################################################
                     [1m Learning iteration 1254/2000 [0m                     

                       Computation: 37963 steps/s (collection: 2.429s, learning 0.160s)
             Mean action noise std: 2.83
          Mean value_function loss: 228.2930
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 63.2196
                       Mean reward: 843.04
               Mean episode length: 227.84
    Episode_Reward/reaching_object: 1.5124
     Episode_Reward/lifting_object: 165.0809
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0718
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 2.59s
                      Time elapsed: 00:51:36
                               ETA: 00:30:40

################################################################################
                     [1m Learning iteration 1255/2000 [0m                     

                       Computation: 39974 steps/s (collection: 2.300s, learning 0.160s)
             Mean action noise std: 2.83
          Mean value_function loss: 275.2175
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 63.2301
                       Mean reward: 816.31
               Mean episode length: 221.11
    Episode_Reward/reaching_object: 1.5091
     Episode_Reward/lifting_object: 165.0565
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 2.46s
                      Time elapsed: 00:51:39
                               ETA: 00:30:38

################################################################################
                     [1m Learning iteration 1256/2000 [0m                     

                       Computation: 41344 steps/s (collection: 2.265s, learning 0.113s)
             Mean action noise std: 2.83
          Mean value_function loss: 247.9259
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 63.2411
                       Mean reward: 767.84
               Mean episode length: 209.00
    Episode_Reward/reaching_object: 1.4437
     Episode_Reward/lifting_object: 157.9494
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 2.38s
                      Time elapsed: 00:51:41
                               ETA: 00:30:35

################################################################################
                     [1m Learning iteration 1257/2000 [0m                     

                       Computation: 39509 steps/s (collection: 2.336s, learning 0.153s)
             Mean action noise std: 2.84
          Mean value_function loss: 244.2024
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 63.2599
                       Mean reward: 792.60
               Mean episode length: 216.25
    Episode_Reward/reaching_object: 1.5014
     Episode_Reward/lifting_object: 164.7677
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0704
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 2.49s
                      Time elapsed: 00:51:44
                               ETA: 00:30:33

################################################################################
                     [1m Learning iteration 1258/2000 [0m                     

                       Computation: 42016 steps/s (collection: 2.188s, learning 0.152s)
             Mean action noise std: 2.84
          Mean value_function loss: 185.8129
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 63.2757
                       Mean reward: 787.49
               Mean episode length: 215.76
    Episode_Reward/reaching_object: 1.4916
     Episode_Reward/lifting_object: 163.2252
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 2.34s
                      Time elapsed: 00:51:46
                               ETA: 00:30:30

################################################################################
                     [1m Learning iteration 1259/2000 [0m                     

                       Computation: 42738 steps/s (collection: 2.183s, learning 0.117s)
             Mean action noise std: 2.84
          Mean value_function loss: 244.0206
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 63.2932
                       Mean reward: 850.56
               Mean episode length: 227.97
    Episode_Reward/reaching_object: 1.4971
     Episode_Reward/lifting_object: 163.1549
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 2.30s
                      Time elapsed: 00:51:48
                               ETA: 00:30:28

################################################################################
                     [1m Learning iteration 1260/2000 [0m                     

                       Computation: 41010 steps/s (collection: 2.231s, learning 0.166s)
             Mean action noise std: 2.84
          Mean value_function loss: 208.7276
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 63.3048
                       Mean reward: 827.55
               Mean episode length: 223.98
    Episode_Reward/reaching_object: 1.4883
     Episode_Reward/lifting_object: 162.6825
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 2.40s
                      Time elapsed: 00:51:51
                               ETA: 00:30:25

################################################################################
                     [1m Learning iteration 1261/2000 [0m                     

                       Computation: 41004 steps/s (collection: 2.224s, learning 0.174s)
             Mean action noise std: 2.84
          Mean value_function loss: 231.8693
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 63.3093
                       Mean reward: 875.30
               Mean episode length: 234.21
    Episode_Reward/reaching_object: 1.4695
     Episode_Reward/lifting_object: 160.2050
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 2.40s
                      Time elapsed: 00:51:53
                               ETA: 00:30:23

################################################################################
                     [1m Learning iteration 1262/2000 [0m                     

                       Computation: 41405 steps/s (collection: 2.216s, learning 0.158s)
             Mean action noise std: 2.84
          Mean value_function loss: 194.0838
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 63.3158
                       Mean reward: 861.88
               Mean episode length: 230.56
    Episode_Reward/reaching_object: 1.5593
     Episode_Reward/lifting_object: 171.1707
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 2.37s
                      Time elapsed: 00:51:55
                               ETA: 00:30:20

################################################################################
                     [1m Learning iteration 1263/2000 [0m                     

                       Computation: 31458 steps/s (collection: 2.893s, learning 0.232s)
             Mean action noise std: 2.84
          Mean value_function loss: 257.1685
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 63.3321
                       Mean reward: 804.47
               Mean episode length: 216.43
    Episode_Reward/reaching_object: 1.4870
     Episode_Reward/lifting_object: 162.1430
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 3.12s
                      Time elapsed: 00:51:58
                               ETA: 00:30:18

################################################################################
                     [1m Learning iteration 1264/2000 [0m                     

                       Computation: 40588 steps/s (collection: 2.280s, learning 0.142s)
             Mean action noise std: 2.85
          Mean value_function loss: 232.4186
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 63.3579
                       Mean reward: 837.27
               Mean episode length: 224.47
    Episode_Reward/reaching_object: 1.4866
     Episode_Reward/lifting_object: 161.7764
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0725
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 2.42s
                      Time elapsed: 00:52:01
                               ETA: 00:30:16

################################################################################
                     [1m Learning iteration 1265/2000 [0m                     

                       Computation: 42478 steps/s (collection: 2.218s, learning 0.096s)
             Mean action noise std: 2.85
          Mean value_function loss: 280.6715
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 63.3714
                       Mean reward: 827.84
               Mean episode length: 225.78
    Episode_Reward/reaching_object: 1.5039
     Episode_Reward/lifting_object: 163.9760
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0730
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 2.31s
                      Time elapsed: 00:52:03
                               ETA: 00:30:13

################################################################################
                     [1m Learning iteration 1266/2000 [0m                     

                       Computation: 38112 steps/s (collection: 2.401s, learning 0.178s)
             Mean action noise std: 2.85
          Mean value_function loss: 232.3432
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 63.3756
                       Mean reward: 820.21
               Mean episode length: 224.83
    Episode_Reward/reaching_object: 1.4853
     Episode_Reward/lifting_object: 162.5727
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0725
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 2.58s
                      Time elapsed: 00:52:06
                               ETA: 00:30:11

################################################################################
                     [1m Learning iteration 1267/2000 [0m                     

                       Computation: 38223 steps/s (collection: 2.422s, learning 0.150s)
             Mean action noise std: 2.85
          Mean value_function loss: 230.7520
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 63.3838
                       Mean reward: 830.47
               Mean episode length: 224.99
    Episode_Reward/reaching_object: 1.4766
     Episode_Reward/lifting_object: 161.5783
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 2.57s
                      Time elapsed: 00:52:08
                               ETA: 00:30:08

################################################################################
                     [1m Learning iteration 1268/2000 [0m                     

                       Computation: 36432 steps/s (collection: 2.566s, learning 0.133s)
             Mean action noise std: 2.85
          Mean value_function loss: 274.9087
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 63.3925
                       Mean reward: 832.62
               Mean episode length: 226.25
    Episode_Reward/reaching_object: 1.4790
     Episode_Reward/lifting_object: 161.4690
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0726
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 2.70s
                      Time elapsed: 00:52:11
                               ETA: 00:30:06

################################################################################
                     [1m Learning iteration 1269/2000 [0m                     

                       Computation: 36491 steps/s (collection: 2.527s, learning 0.167s)
             Mean action noise std: 2.85
          Mean value_function loss: 295.2189
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 63.4055
                       Mean reward: 764.43
               Mean episode length: 208.74
    Episode_Reward/reaching_object: 1.4437
     Episode_Reward/lifting_object: 157.0055
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0702
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 2.69s
                      Time elapsed: 00:52:14
                               ETA: 00:30:04

################################################################################
                     [1m Learning iteration 1270/2000 [0m                     

                       Computation: 35923 steps/s (collection: 2.574s, learning 0.163s)
             Mean action noise std: 2.85
          Mean value_function loss: 254.8154
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 63.4144
                       Mean reward: 840.80
               Mean episode length: 226.40
    Episode_Reward/reaching_object: 1.5025
     Episode_Reward/lifting_object: 164.5464
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0723
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 2.74s
                      Time elapsed: 00:52:16
                               ETA: 00:30:01

################################################################################
                     [1m Learning iteration 1271/2000 [0m                     

                       Computation: 36304 steps/s (collection: 2.511s, learning 0.197s)
             Mean action noise std: 2.85
          Mean value_function loss: 280.5920
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 63.4245
                       Mean reward: 796.19
               Mean episode length: 215.18
    Episode_Reward/reaching_object: 1.4426
     Episode_Reward/lifting_object: 157.9424
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0714
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 2.71s
                      Time elapsed: 00:52:19
                               ETA: 00:29:59

################################################################################
                     [1m Learning iteration 1272/2000 [0m                     

                       Computation: 36016 steps/s (collection: 2.541s, learning 0.188s)
             Mean action noise std: 2.86
          Mean value_function loss: 259.6991
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 63.4282
                       Mean reward: 774.89
               Mean episode length: 208.64
    Episode_Reward/reaching_object: 1.4516
     Episode_Reward/lifting_object: 158.8181
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 2.73s
                      Time elapsed: 00:52:22
                               ETA: 00:29:57

################################################################################
                     [1m Learning iteration 1273/2000 [0m                     

                       Computation: 34501 steps/s (collection: 2.624s, learning 0.226s)
             Mean action noise std: 2.86
          Mean value_function loss: 286.1609
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 63.4394
                       Mean reward: 797.01
               Mean episode length: 216.53
    Episode_Reward/reaching_object: 1.4290
     Episode_Reward/lifting_object: 156.4675
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0704
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 2.85s
                      Time elapsed: 00:52:25
                               ETA: 00:29:54

################################################################################
                     [1m Learning iteration 1274/2000 [0m                     

                       Computation: 34080 steps/s (collection: 2.673s, learning 0.211s)
             Mean action noise std: 2.86
          Mean value_function loss: 224.1870
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 63.4481
                       Mean reward: 814.01
               Mean episode length: 221.68
    Episode_Reward/reaching_object: 1.4661
     Episode_Reward/lifting_object: 160.7950
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 2.88s
                      Time elapsed: 00:52:28
                               ETA: 00:29:52

################################################################################
                     [1m Learning iteration 1275/2000 [0m                     

                       Computation: 39982 steps/s (collection: 2.325s, learning 0.134s)
             Mean action noise std: 2.86
          Mean value_function loss: 227.8845
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 63.4557
                       Mean reward: 867.50
               Mean episode length: 231.10
    Episode_Reward/reaching_object: 1.5183
     Episode_Reward/lifting_object: 167.4403
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0730
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 2.46s
                      Time elapsed: 00:52:30
                               ETA: 00:29:50

################################################################################
                     [1m Learning iteration 1276/2000 [0m                     

                       Computation: 37323 steps/s (collection: 2.492s, learning 0.142s)
             Mean action noise std: 2.86
          Mean value_function loss: 232.9967
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 63.4654
                       Mean reward: 791.23
               Mean episode length: 214.77
    Episode_Reward/reaching_object: 1.4706
     Episode_Reward/lifting_object: 162.1806
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 2.63s
                      Time elapsed: 00:52:33
                               ETA: 00:29:47

################################################################################
                     [1m Learning iteration 1277/2000 [0m                     

                       Computation: 35460 steps/s (collection: 2.627s, learning 0.145s)
             Mean action noise std: 2.86
          Mean value_function loss: 215.8782
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 63.4776
                       Mean reward: 824.23
               Mean episode length: 221.27
    Episode_Reward/reaching_object: 1.4533
     Episode_Reward/lifting_object: 159.7870
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 2.77s
                      Time elapsed: 00:52:36
                               ETA: 00:29:45

################################################################################
                     [1m Learning iteration 1278/2000 [0m                     

                       Computation: 38252 steps/s (collection: 2.402s, learning 0.168s)
             Mean action noise std: 2.86
          Mean value_function loss: 208.1874
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 63.4841
                       Mean reward: 825.51
               Mean episode length: 223.64
    Episode_Reward/reaching_object: 1.5004
     Episode_Reward/lifting_object: 165.3415
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0737
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 2.57s
                      Time elapsed: 00:52:38
                               ETA: 00:29:43

################################################################################
                     [1m Learning iteration 1279/2000 [0m                     

                       Computation: 37378 steps/s (collection: 2.393s, learning 0.237s)
             Mean action noise std: 2.86
          Mean value_function loss: 221.3606
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 63.4949
                       Mean reward: 818.28
               Mean episode length: 219.95
    Episode_Reward/reaching_object: 1.4689
     Episode_Reward/lifting_object: 162.2926
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 2.63s
                      Time elapsed: 00:52:41
                               ETA: 00:29:40

################################################################################
                     [1m Learning iteration 1280/2000 [0m                     

                       Computation: 37500 steps/s (collection: 2.431s, learning 0.190s)
             Mean action noise std: 2.86
          Mean value_function loss: 228.8212
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 63.5056
                       Mean reward: 859.42
               Mean episode length: 229.69
    Episode_Reward/reaching_object: 1.4876
     Episode_Reward/lifting_object: 163.5162
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 2.62s
                      Time elapsed: 00:52:43
                               ETA: 00:29:38

################################################################################
                     [1m Learning iteration 1281/2000 [0m                     

                       Computation: 36107 steps/s (collection: 2.526s, learning 0.196s)
             Mean action noise std: 2.87
          Mean value_function loss: 205.4716
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 63.5221
                       Mean reward: 835.75
               Mean episode length: 226.23
    Episode_Reward/reaching_object: 1.4984
     Episode_Reward/lifting_object: 164.9988
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0739
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 2.72s
                      Time elapsed: 00:52:46
                               ETA: 00:29:35

################################################################################
                     [1m Learning iteration 1282/2000 [0m                     

                       Computation: 39182 steps/s (collection: 2.380s, learning 0.129s)
             Mean action noise std: 2.87
          Mean value_function loss: 228.9310
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 63.5352
                       Mean reward: 789.89
               Mean episode length: 213.43
    Episode_Reward/reaching_object: 1.4733
     Episode_Reward/lifting_object: 162.0128
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 2.51s
                      Time elapsed: 00:52:49
                               ETA: 00:29:33

################################################################################
                     [1m Learning iteration 1283/2000 [0m                     

                       Computation: 37140 steps/s (collection: 2.445s, learning 0.202s)
             Mean action noise std: 2.87
          Mean value_function loss: 220.4711
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 63.5423
                       Mean reward: 833.74
               Mean episode length: 227.35
    Episode_Reward/reaching_object: 1.5123
     Episode_Reward/lifting_object: 165.9225
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0734
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 2.65s
                      Time elapsed: 00:52:51
                               ETA: 00:29:31

################################################################################
                     [1m Learning iteration 1284/2000 [0m                     

                       Computation: 38542 steps/s (collection: 2.413s, learning 0.137s)
             Mean action noise std: 2.87
          Mean value_function loss: 198.1100
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 63.5496
                       Mean reward: 784.45
               Mean episode length: 213.29
    Episode_Reward/reaching_object: 1.5077
     Episode_Reward/lifting_object: 166.5158
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0731
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 2.55s
                      Time elapsed: 00:52:54
                               ETA: 00:29:28

################################################################################
                     [1m Learning iteration 1285/2000 [0m                     

                       Computation: 39028 steps/s (collection: 2.354s, learning 0.165s)
             Mean action noise std: 2.87
          Mean value_function loss: 188.4307
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 63.5567
                       Mean reward: 803.07
               Mean episode length: 218.01
    Episode_Reward/reaching_object: 1.4937
     Episode_Reward/lifting_object: 164.1342
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0741
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 2.52s
                      Time elapsed: 00:52:56
                               ETA: 00:29:26

################################################################################
                     [1m Learning iteration 1286/2000 [0m                     

                       Computation: 37166 steps/s (collection: 2.433s, learning 0.212s)
             Mean action noise std: 2.87
          Mean value_function loss: 237.2356
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 63.5675
                       Mean reward: 838.81
               Mean episode length: 226.06
    Episode_Reward/reaching_object: 1.4592
     Episode_Reward/lifting_object: 160.7523
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0729
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 2.64s
                      Time elapsed: 00:52:59
                               ETA: 00:29:23

################################################################################
                     [1m Learning iteration 1287/2000 [0m                     

                       Computation: 38896 steps/s (collection: 2.354s, learning 0.173s)
             Mean action noise std: 2.87
          Mean value_function loss: 225.1164
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 63.5771
                       Mean reward: 817.59
               Mean episode length: 220.60
    Episode_Reward/reaching_object: 1.5033
     Episode_Reward/lifting_object: 166.8374
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 2.53s
                      Time elapsed: 00:53:01
                               ETA: 00:29:21

################################################################################
                     [1m Learning iteration 1288/2000 [0m                     

                       Computation: 36216 steps/s (collection: 2.496s, learning 0.219s)
             Mean action noise std: 2.87
          Mean value_function loss: 223.6324
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 63.5874
                       Mean reward: 808.14
               Mean episode length: 217.92
    Episode_Reward/reaching_object: 1.4827
     Episode_Reward/lifting_object: 163.8981
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 2.71s
                      Time elapsed: 00:53:04
                               ETA: 00:29:19

################################################################################
                     [1m Learning iteration 1289/2000 [0m                     

                       Computation: 36471 steps/s (collection: 2.469s, learning 0.227s)
             Mean action noise std: 2.88
          Mean value_function loss: 239.5228
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 63.5982
                       Mean reward: 825.23
               Mean episode length: 222.88
    Episode_Reward/reaching_object: 1.4742
     Episode_Reward/lifting_object: 162.0402
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0734
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 2.70s
                      Time elapsed: 00:53:07
                               ETA: 00:29:16

################################################################################
                     [1m Learning iteration 1290/2000 [0m                     

                       Computation: 38872 steps/s (collection: 2.344s, learning 0.185s)
             Mean action noise std: 2.88
          Mean value_function loss: 238.6154
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 63.6165
                       Mean reward: 859.68
               Mean episode length: 232.23
    Episode_Reward/reaching_object: 1.4817
     Episode_Reward/lifting_object: 163.5687
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0740
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 2.53s
                      Time elapsed: 00:53:09
                               ETA: 00:29:14

################################################################################
                     [1m Learning iteration 1291/2000 [0m                     

                       Computation: 38303 steps/s (collection: 2.360s, learning 0.207s)
             Mean action noise std: 2.88
          Mean value_function loss: 241.1834
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 63.6327
                       Mean reward: 854.94
               Mean episode length: 228.99
    Episode_Reward/reaching_object: 1.4814
     Episode_Reward/lifting_object: 164.1614
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0743
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 2.57s
                      Time elapsed: 00:53:12
                               ETA: 00:29:11

################################################################################
                     [1m Learning iteration 1292/2000 [0m                     

                       Computation: 38355 steps/s (collection: 2.386s, learning 0.177s)
             Mean action noise std: 2.88
          Mean value_function loss: 264.5218
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 63.6389
                       Mean reward: 818.55
               Mean episode length: 220.52
    Episode_Reward/reaching_object: 1.4670
     Episode_Reward/lifting_object: 162.5802
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 2.56s
                      Time elapsed: 00:53:15
                               ETA: 00:29:09

################################################################################
                     [1m Learning iteration 1293/2000 [0m                     

                       Computation: 40202 steps/s (collection: 2.259s, learning 0.187s)
             Mean action noise std: 2.88
          Mean value_function loss: 238.0937
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 63.6448
                       Mean reward: 862.28
               Mean episode length: 233.03
    Episode_Reward/reaching_object: 1.4760
     Episode_Reward/lifting_object: 162.8523
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 2.45s
                      Time elapsed: 00:53:17
                               ETA: 00:29:07

################################################################################
                     [1m Learning iteration 1294/2000 [0m                     

                       Computation: 39391 steps/s (collection: 2.371s, learning 0.125s)
             Mean action noise std: 2.88
          Mean value_function loss: 237.7581
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 63.6515
                       Mean reward: 824.89
               Mean episode length: 223.40
    Episode_Reward/reaching_object: 1.4459
     Episode_Reward/lifting_object: 159.8044
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 2.50s
                      Time elapsed: 00:53:19
                               ETA: 00:29:04

################################################################################
                     [1m Learning iteration 1295/2000 [0m                     

                       Computation: 40836 steps/s (collection: 2.241s, learning 0.167s)
             Mean action noise std: 2.88
          Mean value_function loss: 202.8909
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 63.6624
                       Mean reward: 824.52
               Mean episode length: 224.11
    Episode_Reward/reaching_object: 1.4781
     Episode_Reward/lifting_object: 163.9393
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 2.41s
                      Time elapsed: 00:53:22
                               ETA: 00:29:02

################################################################################
                     [1m Learning iteration 1296/2000 [0m                     

                       Computation: 38657 steps/s (collection: 2.328s, learning 0.215s)
             Mean action noise std: 2.88
          Mean value_function loss: 251.1257
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 63.6750
                       Mean reward: 835.36
               Mean episode length: 225.28
    Episode_Reward/reaching_object: 1.4909
     Episode_Reward/lifting_object: 164.7674
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 2.54s
                      Time elapsed: 00:53:24
                               ETA: 00:28:59

################################################################################
                     [1m Learning iteration 1297/2000 [0m                     

                       Computation: 36424 steps/s (collection: 2.518s, learning 0.181s)
             Mean action noise std: 2.89
          Mean value_function loss: 245.9786
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 63.6855
                       Mean reward: 819.31
               Mean episode length: 222.79
    Episode_Reward/reaching_object: 1.4597
     Episode_Reward/lifting_object: 161.7284
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0729
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 2.70s
                      Time elapsed: 00:53:27
                               ETA: 00:28:57

################################################################################
                     [1m Learning iteration 1298/2000 [0m                     

                       Computation: 39368 steps/s (collection: 2.347s, learning 0.151s)
             Mean action noise std: 2.89
          Mean value_function loss: 239.8504
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 63.7010
                       Mean reward: 848.00
               Mean episode length: 226.85
    Episode_Reward/reaching_object: 1.4783
     Episode_Reward/lifting_object: 164.5836
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0726
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 2.50s
                      Time elapsed: 00:53:30
                               ETA: 00:28:54

################################################################################
                     [1m Learning iteration 1299/2000 [0m                     

                       Computation: 37305 steps/s (collection: 2.446s, learning 0.189s)
             Mean action noise std: 2.89
          Mean value_function loss: 210.5549
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 63.7156
                       Mean reward: 827.80
               Mean episode length: 224.37
    Episode_Reward/reaching_object: 1.4339
     Episode_Reward/lifting_object: 158.0714
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 2.64s
                      Time elapsed: 00:53:32
                               ETA: 00:28:52

################################################################################
                     [1m Learning iteration 1300/2000 [0m                     

                       Computation: 36022 steps/s (collection: 2.568s, learning 0.161s)
             Mean action noise std: 2.89
          Mean value_function loss: 239.8574
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 63.7315
                       Mean reward: 813.15
               Mean episode length: 220.59
    Episode_Reward/reaching_object: 1.4636
     Episode_Reward/lifting_object: 162.5980
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 2.73s
                      Time elapsed: 00:53:35
                               ETA: 00:28:50

################################################################################
                     [1m Learning iteration 1301/2000 [0m                     

                       Computation: 39598 steps/s (collection: 2.363s, learning 0.119s)
             Mean action noise std: 2.89
          Mean value_function loss: 237.4387
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 63.7473
                       Mean reward: 841.55
               Mean episode length: 225.53
    Episode_Reward/reaching_object: 1.4947
     Episode_Reward/lifting_object: 166.0048
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0740
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 2.48s
                      Time elapsed: 00:53:37
                               ETA: 00:28:47

################################################################################
                     [1m Learning iteration 1302/2000 [0m                     

                       Computation: 39009 steps/s (collection: 2.323s, learning 0.197s)
             Mean action noise std: 2.89
          Mean value_function loss: 244.5691
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 63.7676
                       Mean reward: 818.25
               Mean episode length: 224.08
    Episode_Reward/reaching_object: 1.4278
     Episode_Reward/lifting_object: 158.1153
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 2.52s
                      Time elapsed: 00:53:40
                               ETA: 00:28:45

################################################################################
                     [1m Learning iteration 1303/2000 [0m                     

                       Computation: 40529 steps/s (collection: 2.290s, learning 0.136s)
             Mean action noise std: 2.90
          Mean value_function loss: 190.3595
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 63.7734
                       Mean reward: 876.12
               Mean episode length: 234.06
    Episode_Reward/reaching_object: 1.4938
     Episode_Reward/lifting_object: 165.8611
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0723
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 2.43s
                      Time elapsed: 00:53:42
                               ETA: 00:28:42

################################################################################
                     [1m Learning iteration 1304/2000 [0m                     

                       Computation: 42033 steps/s (collection: 2.231s, learning 0.108s)
             Mean action noise std: 2.90
          Mean value_function loss: 211.8496
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 63.7756
                       Mean reward: 844.30
               Mean episode length: 227.16
    Episode_Reward/reaching_object: 1.4932
     Episode_Reward/lifting_object: 166.3242
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0742
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 2.34s
                      Time elapsed: 00:53:45
                               ETA: 00:28:40

################################################################################
                     [1m Learning iteration 1305/2000 [0m                     

                       Computation: 41318 steps/s (collection: 2.267s, learning 0.113s)
             Mean action noise std: 2.90
          Mean value_function loss: 294.0567
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 63.7796
                       Mean reward: 769.07
               Mean episode length: 210.94
    Episode_Reward/reaching_object: 1.4313
     Episode_Reward/lifting_object: 158.5743
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0721
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 2.38s
                      Time elapsed: 00:53:47
                               ETA: 00:28:37

################################################################################
                     [1m Learning iteration 1306/2000 [0m                     

                       Computation: 40486 steps/s (collection: 2.267s, learning 0.161s)
             Mean action noise std: 2.90
          Mean value_function loss: 200.7009
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 63.7911
                       Mean reward: 870.45
               Mean episode length: 231.63
    Episode_Reward/reaching_object: 1.4681
     Episode_Reward/lifting_object: 162.8167
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0729
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 2.43s
                      Time elapsed: 00:53:50
                               ETA: 00:28:35

################################################################################
                     [1m Learning iteration 1307/2000 [0m                     

                       Computation: 38758 steps/s (collection: 2.408s, learning 0.129s)
             Mean action noise std: 2.90
          Mean value_function loss: 249.8274
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 63.8034
                       Mean reward: 813.38
               Mean episode length: 219.30
    Episode_Reward/reaching_object: 1.4639
     Episode_Reward/lifting_object: 161.9020
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 2.54s
                      Time elapsed: 00:53:52
                               ETA: 00:28:32

################################################################################
                     [1m Learning iteration 1308/2000 [0m                     

                       Computation: 39220 steps/s (collection: 2.308s, learning 0.199s)
             Mean action noise std: 2.90
          Mean value_function loss: 262.7129
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 63.8107
                       Mean reward: 810.29
               Mean episode length: 219.80
    Episode_Reward/reaching_object: 1.4590
     Episode_Reward/lifting_object: 161.0097
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0730
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 2.51s
                      Time elapsed: 00:53:55
                               ETA: 00:28:30

################################################################################
                     [1m Learning iteration 1309/2000 [0m                     

                       Computation: 36976 steps/s (collection: 2.489s, learning 0.169s)
             Mean action noise std: 2.90
          Mean value_function loss: 226.8882
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 63.8235
                       Mean reward: 864.86
               Mean episode length: 232.76
    Episode_Reward/reaching_object: 1.4732
     Episode_Reward/lifting_object: 163.6675
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0725
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 2.66s
                      Time elapsed: 00:53:57
                               ETA: 00:28:27

################################################################################
                     [1m Learning iteration 1310/2000 [0m                     

                       Computation: 39945 steps/s (collection: 2.293s, learning 0.168s)
             Mean action noise std: 2.90
          Mean value_function loss: 225.5217
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 63.8393
                       Mean reward: 865.68
               Mean episode length: 233.57
    Episode_Reward/reaching_object: 1.4536
     Episode_Reward/lifting_object: 160.9866
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 2.46s
                      Time elapsed: 00:54:00
                               ETA: 00:28:25

################################################################################
                     [1m Learning iteration 1311/2000 [0m                     

                       Computation: 35322 steps/s (collection: 2.608s, learning 0.176s)
             Mean action noise std: 2.91
          Mean value_function loss: 194.0631
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 63.8613
                       Mean reward: 824.28
               Mean episode length: 221.93
    Episode_Reward/reaching_object: 1.4916
     Episode_Reward/lifting_object: 165.8661
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 2.78s
                      Time elapsed: 00:54:03
                               ETA: 00:28:23

################################################################################
                     [1m Learning iteration 1312/2000 [0m                     

                       Computation: 42110 steps/s (collection: 2.217s, learning 0.118s)
             Mean action noise std: 2.91
          Mean value_function loss: 206.4776
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 63.8809
                       Mean reward: 781.59
               Mean episode length: 214.27
    Episode_Reward/reaching_object: 1.4844
     Episode_Reward/lifting_object: 164.3873
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0745
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 2.33s
                      Time elapsed: 00:54:05
                               ETA: 00:28:20

################################################################################
                     [1m Learning iteration 1313/2000 [0m                     

                       Computation: 37514 steps/s (collection: 2.474s, learning 0.147s)
             Mean action noise std: 2.91
          Mean value_function loss: 212.6587
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 63.8962
                       Mean reward: 795.56
               Mean episode length: 214.92
    Episode_Reward/reaching_object: 1.4577
     Episode_Reward/lifting_object: 161.5001
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0720
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 2.62s
                      Time elapsed: 00:54:07
                               ETA: 00:28:18

################################################################################
                     [1m Learning iteration 1314/2000 [0m                     

                       Computation: 40308 steps/s (collection: 2.287s, learning 0.152s)
             Mean action noise std: 2.91
          Mean value_function loss: 243.1329
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 63.9080
                       Mean reward: 828.34
               Mean episode length: 223.63
    Episode_Reward/reaching_object: 1.4353
     Episode_Reward/lifting_object: 159.2334
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0718
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 2.44s
                      Time elapsed: 00:54:10
                               ETA: 00:28:15

################################################################################
                     [1m Learning iteration 1315/2000 [0m                     

                       Computation: 37872 steps/s (collection: 2.440s, learning 0.156s)
             Mean action noise std: 2.91
          Mean value_function loss: 215.9739
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 63.9098
                       Mean reward: 826.87
               Mean episode length: 222.26
    Episode_Reward/reaching_object: 1.4913
     Episode_Reward/lifting_object: 165.3031
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0739
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 2.60s
                      Time elapsed: 00:54:12
                               ETA: 00:28:13

################################################################################
                     [1m Learning iteration 1316/2000 [0m                     

                       Computation: 39878 steps/s (collection: 2.280s, learning 0.185s)
             Mean action noise std: 2.91
          Mean value_function loss: 248.8346
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 63.9151
                       Mean reward: 754.99
               Mean episode length: 207.50
    Episode_Reward/reaching_object: 1.4642
     Episode_Reward/lifting_object: 162.7174
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 2.47s
                      Time elapsed: 00:54:15
                               ETA: 00:28:10

################################################################################
                     [1m Learning iteration 1317/2000 [0m                     

                       Computation: 39874 steps/s (collection: 2.310s, learning 0.155s)
             Mean action noise std: 2.91
          Mean value_function loss: 200.7453
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 63.9280
                       Mean reward: 864.87
               Mean episode length: 231.74
    Episode_Reward/reaching_object: 1.5067
     Episode_Reward/lifting_object: 167.7439
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 2.47s
                      Time elapsed: 00:54:17
                               ETA: 00:28:08

################################################################################
                     [1m Learning iteration 1318/2000 [0m                     

                       Computation: 34335 steps/s (collection: 2.669s, learning 0.194s)
             Mean action noise std: 2.92
          Mean value_function loss: 208.0293
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 63.9503
                       Mean reward: 859.84
               Mean episode length: 231.03
    Episode_Reward/reaching_object: 1.5241
     Episode_Reward/lifting_object: 169.6845
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 2.86s
                      Time elapsed: 00:54:20
                               ETA: 00:28:06

################################################################################
                     [1m Learning iteration 1319/2000 [0m                     

                       Computation: 32681 steps/s (collection: 2.811s, learning 0.197s)
             Mean action noise std: 2.92
          Mean value_function loss: 247.3105
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 63.9750
                       Mean reward: 777.46
               Mean episode length: 214.06
    Episode_Reward/reaching_object: 1.4536
     Episode_Reward/lifting_object: 160.7697
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 3.01s
                      Time elapsed: 00:54:23
                               ETA: 00:28:03

################################################################################
                     [1m Learning iteration 1320/2000 [0m                     

                       Computation: 36277 steps/s (collection: 2.476s, learning 0.234s)
             Mean action noise std: 2.92
          Mean value_function loss: 222.8014
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 63.9942
                       Mean reward: 821.32
               Mean episode length: 224.32
    Episode_Reward/reaching_object: 1.4755
     Episode_Reward/lifting_object: 163.5202
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0747
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 2.71s
                      Time elapsed: 00:54:26
                               ETA: 00:28:01

################################################################################
                     [1m Learning iteration 1321/2000 [0m                     

                       Computation: 35102 steps/s (collection: 2.648s, learning 0.152s)
             Mean action noise std: 2.92
          Mean value_function loss: 243.2771
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 64.0082
                       Mean reward: 846.90
               Mean episode length: 228.95
    Episode_Reward/reaching_object: 1.4725
     Episode_Reward/lifting_object: 163.4357
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 2.80s
                      Time elapsed: 00:54:29
                               ETA: 00:27:59

################################################################################
                     [1m Learning iteration 1322/2000 [0m                     

                       Computation: 39667 steps/s (collection: 2.237s, learning 0.242s)
             Mean action noise std: 2.93
          Mean value_function loss: 203.7220
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 64.0225
                       Mean reward: 862.89
               Mean episode length: 231.78
    Episode_Reward/reaching_object: 1.4740
     Episode_Reward/lifting_object: 163.8942
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0731
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 2.48s
                      Time elapsed: 00:54:31
                               ETA: 00:27:56

################################################################################
                     [1m Learning iteration 1323/2000 [0m                     

                       Computation: 38173 steps/s (collection: 2.466s, learning 0.110s)
             Mean action noise std: 2.93
          Mean value_function loss: 221.8219
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 64.0417
                       Mean reward: 835.71
               Mean episode length: 227.86
    Episode_Reward/reaching_object: 1.4699
     Episode_Reward/lifting_object: 163.0671
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0737
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 2.58s
                      Time elapsed: 00:54:34
                               ETA: 00:27:54

################################################################################
                     [1m Learning iteration 1324/2000 [0m                     

                       Computation: 39493 steps/s (collection: 2.345s, learning 0.145s)
             Mean action noise std: 2.93
          Mean value_function loss: 230.3046
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 64.0578
                       Mean reward: 798.41
               Mean episode length: 217.13
    Episode_Reward/reaching_object: 1.4603
     Episode_Reward/lifting_object: 162.7068
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 2.49s
                      Time elapsed: 00:54:36
                               ETA: 00:27:51

################################################################################
                     [1m Learning iteration 1325/2000 [0m                     

                       Computation: 34091 steps/s (collection: 2.655s, learning 0.228s)
             Mean action noise std: 2.93
          Mean value_function loss: 210.4734
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 64.0678
                       Mean reward: 851.70
               Mean episode length: 230.72
    Episode_Reward/reaching_object: 1.4927
     Episode_Reward/lifting_object: 165.9393
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0743
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 2.88s
                      Time elapsed: 00:54:39
                               ETA: 00:27:49

################################################################################
                     [1m Learning iteration 1326/2000 [0m                     

                       Computation: 33043 steps/s (collection: 2.616s, learning 0.359s)
             Mean action noise std: 2.93
          Mean value_function loss: 240.2801
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 64.0790
                       Mean reward: 800.10
               Mean episode length: 218.93
    Episode_Reward/reaching_object: 1.4757
     Episode_Reward/lifting_object: 164.1102
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0752
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 2.98s
                      Time elapsed: 00:54:42
                               ETA: 00:27:47

################################################################################
                     [1m Learning iteration 1327/2000 [0m                     

                       Computation: 34790 steps/s (collection: 2.620s, learning 0.205s)
             Mean action noise std: 2.93
          Mean value_function loss: 237.2784
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 64.0921
                       Mean reward: 796.96
               Mean episode length: 218.05
    Episode_Reward/reaching_object: 1.4778
     Episode_Reward/lifting_object: 164.7711
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0749
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 2.83s
                      Time elapsed: 00:54:45
                               ETA: 00:27:45

################################################################################
                     [1m Learning iteration 1328/2000 [0m                     

                       Computation: 36467 steps/s (collection: 2.556s, learning 0.140s)
             Mean action noise std: 2.93
          Mean value_function loss: 193.7093
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 64.0985
                       Mean reward: 780.62
               Mean episode length: 214.08
    Episode_Reward/reaching_object: 1.4696
     Episode_Reward/lifting_object: 163.7248
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0750
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 2.70s
                      Time elapsed: 00:54:48
                               ETA: 00:27:42

################################################################################
                     [1m Learning iteration 1329/2000 [0m                     

                       Computation: 37259 steps/s (collection: 2.413s, learning 0.226s)
             Mean action noise std: 2.94
          Mean value_function loss: 228.9295
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.1135
                       Mean reward: 786.54
               Mean episode length: 214.18
    Episode_Reward/reaching_object: 1.4607
     Episode_Reward/lifting_object: 162.8883
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0722
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 2.64s
                      Time elapsed: 00:54:50
                               ETA: 00:27:40

################################################################################
                     [1m Learning iteration 1330/2000 [0m                     

                       Computation: 36211 steps/s (collection: 2.565s, learning 0.150s)
             Mean action noise std: 2.94
          Mean value_function loss: 236.4991
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 64.1368
                       Mean reward: 853.95
               Mean episode length: 229.56
    Episode_Reward/reaching_object: 1.4708
     Episode_Reward/lifting_object: 164.5146
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0732
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 2.71s
                      Time elapsed: 00:54:53
                               ETA: 00:27:37

################################################################################
                     [1m Learning iteration 1331/2000 [0m                     

                       Computation: 33898 steps/s (collection: 2.638s, learning 0.262s)
             Mean action noise std: 2.94
          Mean value_function loss: 221.8658
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 64.1523
                       Mean reward: 877.59
               Mean episode length: 235.22
    Episode_Reward/reaching_object: 1.4733
     Episode_Reward/lifting_object: 164.2046
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0730
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 2.90s
                      Time elapsed: 00:54:56
                               ETA: 00:27:35

################################################################################
                     [1m Learning iteration 1332/2000 [0m                     

                       Computation: 35798 steps/s (collection: 2.530s, learning 0.216s)
             Mean action noise std: 2.94
          Mean value_function loss: 229.8723
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 64.1598
                       Mean reward: 804.51
               Mean episode length: 220.02
    Episode_Reward/reaching_object: 1.4490
     Episode_Reward/lifting_object: 160.9395
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 2.75s
                      Time elapsed: 00:54:59
                               ETA: 00:27:33

################################################################################
                     [1m Learning iteration 1333/2000 [0m                     

                       Computation: 16902 steps/s (collection: 5.696s, learning 0.120s)
             Mean action noise std: 2.94
          Mean value_function loss: 211.3875
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 64.1694
                       Mean reward: 839.94
               Mean episode length: 226.10
    Episode_Reward/reaching_object: 1.4497
     Episode_Reward/lifting_object: 161.7877
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0743
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 5.82s
                      Time elapsed: 00:55:05
                               ETA: 00:27:32

################################################################################
                     [1m Learning iteration 1334/2000 [0m                     

                       Computation: 13624 steps/s (collection: 7.054s, learning 0.162s)
             Mean action noise std: 2.94
          Mean value_function loss: 215.2221
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 64.1800
                       Mean reward: 834.76
               Mean episode length: 225.12
    Episode_Reward/reaching_object: 1.4687
     Episode_Reward/lifting_object: 163.8498
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0730
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 7.22s
                      Time elapsed: 00:55:12
                               ETA: 00:27:32

################################################################################
                     [1m Learning iteration 1335/2000 [0m                     

                       Computation: 13575 steps/s (collection: 7.120s, learning 0.122s)
             Mean action noise std: 2.95
          Mean value_function loss: 214.3016
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 64.1928
                       Mean reward: 837.35
               Mean episode length: 225.01
    Episode_Reward/reaching_object: 1.4800
     Episode_Reward/lifting_object: 164.7473
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0740
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 7.24s
                      Time elapsed: 00:55:19
                               ETA: 00:27:32

################################################################################
                     [1m Learning iteration 1336/2000 [0m                     

                       Computation: 14472 steps/s (collection: 6.661s, learning 0.132s)
             Mean action noise std: 2.95
          Mean value_function loss: 225.3755
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 64.2085
                       Mean reward: 814.76
               Mean episode length: 220.92
    Episode_Reward/reaching_object: 1.4506
     Episode_Reward/lifting_object: 161.8714
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 6.79s
                      Time elapsed: 00:55:26
                               ETA: 00:27:31

################################################################################
                     [1m Learning iteration 1337/2000 [0m                     

                       Computation: 13737 steps/s (collection: 7.030s, learning 0.126s)
             Mean action noise std: 2.95
          Mean value_function loss: 235.7598
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 64.2211
                       Mean reward: 786.56
               Mean episode length: 213.54
    Episode_Reward/reaching_object: 1.4433
     Episode_Reward/lifting_object: 160.6086
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0726
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 7.16s
                      Time elapsed: 00:55:33
                               ETA: 00:27:31

################################################################################
                     [1m Learning iteration 1338/2000 [0m                     

                       Computation: 12692 steps/s (collection: 7.558s, learning 0.187s)
             Mean action noise std: 2.95
          Mean value_function loss: 244.2089
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 64.2304
                       Mean reward: 835.41
               Mean episode length: 225.23
    Episode_Reward/reaching_object: 1.4642
     Episode_Reward/lifting_object: 163.2496
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0732
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 7.75s
                      Time elapsed: 00:55:41
                               ETA: 00:27:31

################################################################################
                     [1m Learning iteration 1339/2000 [0m                     

                       Computation: 13554 steps/s (collection: 7.123s, learning 0.129s)
             Mean action noise std: 2.95
          Mean value_function loss: 270.4700
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 64.2403
                       Mean reward: 809.07
               Mean episode length: 219.73
    Episode_Reward/reaching_object: 1.4353
     Episode_Reward/lifting_object: 159.9306
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0731
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 7.25s
                      Time elapsed: 00:55:48
                               ETA: 00:27:31

################################################################################
                     [1m Learning iteration 1340/2000 [0m                     

                       Computation: 14205 steps/s (collection: 6.798s, learning 0.122s)
             Mean action noise std: 2.95
          Mean value_function loss: 249.1822
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 64.2617
                       Mean reward: 810.22
               Mean episode length: 220.24
    Episode_Reward/reaching_object: 1.4453
     Episode_Reward/lifting_object: 161.3649
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 6.92s
                      Time elapsed: 00:55:55
                               ETA: 00:27:31

################################################################################
                     [1m Learning iteration 1341/2000 [0m                     

                       Computation: 12282 steps/s (collection: 7.858s, learning 0.145s)
             Mean action noise std: 2.96
          Mean value_function loss: 228.8517
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 64.2854
                       Mean reward: 793.23
               Mean episode length: 217.18
    Episode_Reward/reaching_object: 1.4294
     Episode_Reward/lifting_object: 159.6764
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 8.00s
                      Time elapsed: 00:56:03
                               ETA: 00:27:31

################################################################################
                     [1m Learning iteration 1342/2000 [0m                     

                       Computation: 40883 steps/s (collection: 2.283s, learning 0.121s)
             Mean action noise std: 2.96
          Mean value_function loss: 215.0504
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 64.3012
                       Mean reward: 785.63
               Mean episode length: 214.92
    Episode_Reward/reaching_object: 1.4359
     Episode_Reward/lifting_object: 159.6808
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0731
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 2.40s
                      Time elapsed: 00:56:05
                               ETA: 00:27:29

################################################################################
                     [1m Learning iteration 1343/2000 [0m                     

                       Computation: 43538 steps/s (collection: 2.097s, learning 0.161s)
             Mean action noise std: 2.96
          Mean value_function loss: 221.3153
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 64.3118
                       Mean reward: 810.33
               Mean episode length: 220.30
    Episode_Reward/reaching_object: 1.4417
     Episode_Reward/lifting_object: 161.1103
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0739
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 2.26s
                      Time elapsed: 00:56:08
                               ETA: 00:27:26

################################################################################
                     [1m Learning iteration 1344/2000 [0m                     

                       Computation: 43273 steps/s (collection: 2.125s, learning 0.147s)
             Mean action noise std: 2.96
          Mean value_function loss: 216.4008
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 64.3241
                       Mean reward: 835.42
               Mean episode length: 226.03
    Episode_Reward/reaching_object: 1.4648
     Episode_Reward/lifting_object: 164.6718
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0750
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 2.27s
                      Time elapsed: 00:56:10
                               ETA: 00:27:23

################################################################################
                     [1m Learning iteration 1345/2000 [0m                     

                       Computation: 44571 steps/s (collection: 2.098s, learning 0.107s)
             Mean action noise std: 2.96
          Mean value_function loss: 219.2569
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 64.3362
                       Mean reward: 811.84
               Mean episode length: 219.42
    Episode_Reward/reaching_object: 1.4435
     Episode_Reward/lifting_object: 161.8812
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0729
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 2.21s
                      Time elapsed: 00:56:12
                               ETA: 00:27:21

################################################################################
                     [1m Learning iteration 1346/2000 [0m                     

                       Computation: 45023 steps/s (collection: 2.072s, learning 0.112s)
             Mean action noise std: 2.96
          Mean value_function loss: 305.4769
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 64.3463
                       Mean reward: 813.82
               Mean episode length: 218.97
    Episode_Reward/reaching_object: 1.3867
     Episode_Reward/lifting_object: 155.0500
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0730
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 2.18s
                      Time elapsed: 00:56:14
                               ETA: 00:27:18

################################################################################
                     [1m Learning iteration 1347/2000 [0m                     

                       Computation: 45241 steps/s (collection: 2.085s, learning 0.088s)
             Mean action noise std: 2.97
          Mean value_function loss: 240.8494
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 64.3588
                       Mean reward: 823.08
               Mean episode length: 222.72
    Episode_Reward/reaching_object: 1.4383
     Episode_Reward/lifting_object: 160.9299
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0743
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 2.17s
                      Time elapsed: 00:56:16
                               ETA: 00:27:15

################################################################################
                     [1m Learning iteration 1348/2000 [0m                     

                       Computation: 44417 steps/s (collection: 2.048s, learning 0.165s)
             Mean action noise std: 2.97
          Mean value_function loss: 260.6358
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 64.3790
                       Mean reward: 771.70
               Mean episode length: 211.68
    Episode_Reward/reaching_object: 1.4374
     Episode_Reward/lifting_object: 160.5887
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 2.21s
                      Time elapsed: 00:56:19
                               ETA: 00:27:13

################################################################################
                     [1m Learning iteration 1349/2000 [0m                     

                       Computation: 38468 steps/s (collection: 2.352s, learning 0.203s)
             Mean action noise std: 2.97
          Mean value_function loss: 252.6395
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 64.4014
                       Mean reward: 835.03
               Mean episode length: 226.41
    Episode_Reward/reaching_object: 1.4319
     Episode_Reward/lifting_object: 161.2668
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 2.56s
                      Time elapsed: 00:56:21
                               ETA: 00:27:10

################################################################################
                     [1m Learning iteration 1350/2000 [0m                     

                       Computation: 36304 steps/s (collection: 2.537s, learning 0.171s)
             Mean action noise std: 2.97
          Mean value_function loss: 208.5706
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 64.4178
                       Mean reward: 838.81
               Mean episode length: 223.81
    Episode_Reward/reaching_object: 1.4129
     Episode_Reward/lifting_object: 159.7046
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0725
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 2.71s
                      Time elapsed: 00:56:24
                               ETA: 00:27:08

################################################################################
                     [1m Learning iteration 1351/2000 [0m                     

                       Computation: 38684 steps/s (collection: 2.400s, learning 0.142s)
             Mean action noise std: 2.97
          Mean value_function loss: 254.7790
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 64.4333
                       Mean reward: 748.15
               Mean episode length: 205.50
    Episode_Reward/reaching_object: 1.3937
     Episode_Reward/lifting_object: 156.3158
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 2.54s
                      Time elapsed: 00:56:26
                               ETA: 00:27:05

################################################################################
                     [1m Learning iteration 1352/2000 [0m                     

                       Computation: 41032 steps/s (collection: 2.273s, learning 0.123s)
             Mean action noise std: 2.97
          Mean value_function loss: 271.2617
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 64.4373
                       Mean reward: 784.66
               Mean episode length: 212.73
    Episode_Reward/reaching_object: 1.4153
     Episode_Reward/lifting_object: 158.8728
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0723
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 2.40s
                      Time elapsed: 00:56:29
                               ETA: 00:27:03

################################################################################
                     [1m Learning iteration 1353/2000 [0m                     

                       Computation: 43119 steps/s (collection: 2.171s, learning 0.109s)
             Mean action noise std: 2.98
          Mean value_function loss: 237.3567
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.4449
                       Mean reward: 866.16
               Mean episode length: 231.86
    Episode_Reward/reaching_object: 1.4651
     Episode_Reward/lifting_object: 165.2079
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0749
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 2.28s
                      Time elapsed: 00:56:31
                               ETA: 00:27:00

################################################################################
                     [1m Learning iteration 1354/2000 [0m                     

                       Computation: 40979 steps/s (collection: 2.270s, learning 0.129s)
             Mean action noise std: 2.98
          Mean value_function loss: 232.8365
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 64.4569
                       Mean reward: 843.59
               Mean episode length: 226.74
    Episode_Reward/reaching_object: 1.4557
     Episode_Reward/lifting_object: 164.4187
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0762
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 2.40s
                      Time elapsed: 00:56:33
                               ETA: 00:26:58

################################################################################
                     [1m Learning iteration 1355/2000 [0m                     

                       Computation: 42087 steps/s (collection: 2.220s, learning 0.116s)
             Mean action noise std: 2.98
          Mean value_function loss: 256.3996
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 64.4686
                       Mean reward: 826.94
               Mean episode length: 223.55
    Episode_Reward/reaching_object: 1.4442
     Episode_Reward/lifting_object: 162.1817
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0741
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 2.34s
                      Time elapsed: 00:56:36
                               ETA: 00:26:55

################################################################################
                     [1m Learning iteration 1356/2000 [0m                     

                       Computation: 43777 steps/s (collection: 2.141s, learning 0.104s)
             Mean action noise std: 2.98
          Mean value_function loss: 241.4715
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 64.4751
                       Mean reward: 829.60
               Mean episode length: 223.86
    Episode_Reward/reaching_object: 1.4172
     Episode_Reward/lifting_object: 159.3815
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 2.25s
                      Time elapsed: 00:56:38
                               ETA: 00:26:52

################################################################################
                     [1m Learning iteration 1357/2000 [0m                     

                       Computation: 41570 steps/s (collection: 2.199s, learning 0.166s)
             Mean action noise std: 2.98
          Mean value_function loss: 235.0278
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 64.4887
                       Mean reward: 794.28
               Mean episode length: 216.49
    Episode_Reward/reaching_object: 1.4046
     Episode_Reward/lifting_object: 157.5858
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0743
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 2.36s
                      Time elapsed: 00:56:40
                               ETA: 00:26:50

################################################################################
                     [1m Learning iteration 1358/2000 [0m                     

                       Computation: 41295 steps/s (collection: 2.250s, learning 0.130s)
             Mean action noise std: 2.98
          Mean value_function loss: 243.7699
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 64.5015
                       Mean reward: 838.90
               Mean episode length: 225.93
    Episode_Reward/reaching_object: 1.4452
     Episode_Reward/lifting_object: 163.0324
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0749
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 2.38s
                      Time elapsed: 00:56:43
                               ETA: 00:26:47

################################################################################
                     [1m Learning iteration 1359/2000 [0m                     

                       Computation: 40275 steps/s (collection: 2.319s, learning 0.122s)
             Mean action noise std: 2.98
          Mean value_function loss: 334.5637
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 64.5194
                       Mean reward: 772.86
               Mean episode length: 211.47
    Episode_Reward/reaching_object: 1.4259
     Episode_Reward/lifting_object: 160.2967
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 2.44s
                      Time elapsed: 00:56:45
                               ETA: 00:26:45

################################################################################
                     [1m Learning iteration 1360/2000 [0m                     

                       Computation: 38840 steps/s (collection: 2.372s, learning 0.159s)
             Mean action noise std: 2.99
          Mean value_function loss: 250.9829
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 64.5289
                       Mean reward: 755.26
               Mean episode length: 205.21
    Episode_Reward/reaching_object: 1.4207
     Episode_Reward/lifting_object: 159.9168
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0742
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 2.53s
                      Time elapsed: 00:56:48
                               ETA: 00:26:42

################################################################################
                     [1m Learning iteration 1361/2000 [0m                     

                       Computation: 40582 steps/s (collection: 2.263s, learning 0.159s)
             Mean action noise std: 2.99
          Mean value_function loss: 240.0041
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 64.5378
                       Mean reward: 807.97
               Mean episode length: 217.92
    Episode_Reward/reaching_object: 1.4158
     Episode_Reward/lifting_object: 159.5508
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0734
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 2.42s
                      Time elapsed: 00:56:50
                               ETA: 00:26:40

################################################################################
                     [1m Learning iteration 1362/2000 [0m                     

                       Computation: 39340 steps/s (collection: 2.312s, learning 0.187s)
             Mean action noise std: 2.99
          Mean value_function loss: 243.8717
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 64.5476
                       Mean reward: 821.62
               Mean episode length: 220.42
    Episode_Reward/reaching_object: 1.4468
     Episode_Reward/lifting_object: 162.6431
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0751
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 2.50s
                      Time elapsed: 00:56:53
                               ETA: 00:26:37

################################################################################
                     [1m Learning iteration 1363/2000 [0m                     

                       Computation: 42987 steps/s (collection: 2.169s, learning 0.118s)
             Mean action noise std: 2.99
          Mean value_function loss: 207.2814
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 64.5630
                       Mean reward: 838.36
               Mean episode length: 227.22
    Episode_Reward/reaching_object: 1.4852
     Episode_Reward/lifting_object: 166.9873
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0750
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 2.29s
                      Time elapsed: 00:56:55
                               ETA: 00:26:35

################################################################################
                     [1m Learning iteration 1364/2000 [0m                     

                       Computation: 40782 steps/s (collection: 2.302s, learning 0.109s)
             Mean action noise std: 2.99
          Mean value_function loss: 235.7110
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 64.5729
                       Mean reward: 873.98
               Mean episode length: 233.68
    Episode_Reward/reaching_object: 1.4559
     Episode_Reward/lifting_object: 163.3075
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0749
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 2.41s
                      Time elapsed: 00:56:57
                               ETA: 00:26:32

################################################################################
                     [1m Learning iteration 1365/2000 [0m                     

                       Computation: 43131 steps/s (collection: 2.148s, learning 0.131s)
             Mean action noise std: 2.99
          Mean value_function loss: 219.6321
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 64.5834
                       Mean reward: 834.86
               Mean episode length: 225.99
    Episode_Reward/reaching_object: 1.4864
     Episode_Reward/lifting_object: 166.2418
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0764
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 2.28s
                      Time elapsed: 00:57:00
                               ETA: 00:26:29

################################################################################
                     [1m Learning iteration 1366/2000 [0m                     

                       Computation: 41807 steps/s (collection: 2.177s, learning 0.174s)
             Mean action noise std: 2.99
          Mean value_function loss: 226.4215
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 64.5925
                       Mean reward: 780.55
               Mean episode length: 213.57
    Episode_Reward/reaching_object: 1.4338
     Episode_Reward/lifting_object: 160.2960
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0731
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 2.35s
                      Time elapsed: 00:57:02
                               ETA: 00:26:27

################################################################################
                     [1m Learning iteration 1367/2000 [0m                     

                       Computation: 39573 steps/s (collection: 2.326s, learning 0.158s)
             Mean action noise std: 3.00
          Mean value_function loss: 284.8389
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 64.6088
                       Mean reward: 828.93
               Mean episode length: 224.31
    Episode_Reward/reaching_object: 1.4560
     Episode_Reward/lifting_object: 162.8577
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 2.48s
                      Time elapsed: 00:57:04
                               ETA: 00:26:24

################################################################################
                     [1m Learning iteration 1368/2000 [0m                     

                       Computation: 42931 steps/s (collection: 2.151s, learning 0.139s)
             Mean action noise std: 3.00
          Mean value_function loss: 233.1103
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 64.6247
                       Mean reward: 834.75
               Mean episode length: 227.55
    Episode_Reward/reaching_object: 1.4449
     Episode_Reward/lifting_object: 160.5961
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 2.29s
                      Time elapsed: 00:57:07
                               ETA: 00:26:22

################################################################################
                     [1m Learning iteration 1369/2000 [0m                     

                       Computation: 45344 steps/s (collection: 2.057s, learning 0.111s)
             Mean action noise std: 3.00
          Mean value_function loss: 189.0156
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 64.6332
                       Mean reward: 857.09
               Mean episode length: 229.66
    Episode_Reward/reaching_object: 1.4728
     Episode_Reward/lifting_object: 164.9954
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 2.17s
                      Time elapsed: 00:57:09
                               ETA: 00:26:19

################################################################################
                     [1m Learning iteration 1370/2000 [0m                     

                       Computation: 45188 steps/s (collection: 2.039s, learning 0.137s)
             Mean action noise std: 3.00
          Mean value_function loss: 206.5956
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 64.6436
                       Mean reward: 862.87
               Mean episode length: 231.90
    Episode_Reward/reaching_object: 1.4932
     Episode_Reward/lifting_object: 167.2987
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0757
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 2.18s
                      Time elapsed: 00:57:11
                               ETA: 00:26:16

################################################################################
                     [1m Learning iteration 1371/2000 [0m                     

                       Computation: 42656 steps/s (collection: 2.178s, learning 0.126s)
             Mean action noise std: 3.00
          Mean value_function loss: 250.9722
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 64.6608
                       Mean reward: 783.33
               Mean episode length: 215.56
    Episode_Reward/reaching_object: 1.4665
     Episode_Reward/lifting_object: 162.9719
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0756
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 2.30s
                      Time elapsed: 00:57:13
                               ETA: 00:26:14

################################################################################
                     [1m Learning iteration 1372/2000 [0m                     

                       Computation: 44956 steps/s (collection: 2.087s, learning 0.100s)
             Mean action noise std: 3.00
          Mean value_function loss: 215.4776
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 64.6705
                       Mean reward: 855.87
               Mean episode length: 231.33
    Episode_Reward/reaching_object: 1.4737
     Episode_Reward/lifting_object: 164.5306
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 2.19s
                      Time elapsed: 00:57:16
                               ETA: 00:26:11

################################################################################
                     [1m Learning iteration 1373/2000 [0m                     

                       Computation: 42540 steps/s (collection: 2.190s, learning 0.121s)
             Mean action noise std: 3.00
          Mean value_function loss: 211.7986
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 64.6782
                       Mean reward: 832.63
               Mean episode length: 224.71
    Episode_Reward/reaching_object: 1.4992
     Episode_Reward/lifting_object: 167.5938
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0753
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 2.31s
                      Time elapsed: 00:57:18
                               ETA: 00:26:09

################################################################################
                     [1m Learning iteration 1374/2000 [0m                     

                       Computation: 38276 steps/s (collection: 2.431s, learning 0.137s)
             Mean action noise std: 3.00
          Mean value_function loss: 200.5414
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 64.6831
                       Mean reward: 848.90
               Mean episode length: 229.91
    Episode_Reward/reaching_object: 1.4849
     Episode_Reward/lifting_object: 166.3391
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 2.57s
                      Time elapsed: 00:57:20
                               ETA: 00:26:06

################################################################################
                     [1m Learning iteration 1375/2000 [0m                     

                       Computation: 37932 steps/s (collection: 2.416s, learning 0.176s)
             Mean action noise std: 3.01
          Mean value_function loss: 259.1769
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 64.6920
                       Mean reward: 841.98
               Mean episode length: 225.97
    Episode_Reward/reaching_object: 1.4622
     Episode_Reward/lifting_object: 162.8563
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0740
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 2.59s
                      Time elapsed: 00:57:23
                               ETA: 00:26:04

################################################################################
                     [1m Learning iteration 1376/2000 [0m                     

                       Computation: 38035 steps/s (collection: 2.444s, learning 0.141s)
             Mean action noise std: 3.01
          Mean value_function loss: 213.3512
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 64.7004
                       Mean reward: 833.43
               Mean episode length: 226.12
    Episode_Reward/reaching_object: 1.4839
     Episode_Reward/lifting_object: 165.7487
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0739
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 2.58s
                      Time elapsed: 00:57:26
                               ETA: 00:26:01

################################################################################
                     [1m Learning iteration 1377/2000 [0m                     

                       Computation: 41231 steps/s (collection: 2.268s, learning 0.117s)
             Mean action noise std: 3.01
          Mean value_function loss: 229.0804
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 64.7083
                       Mean reward: 842.52
               Mean episode length: 228.46
    Episode_Reward/reaching_object: 1.4724
     Episode_Reward/lifting_object: 164.5045
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 2.38s
                      Time elapsed: 00:57:28
                               ETA: 00:25:59

################################################################################
                     [1m Learning iteration 1378/2000 [0m                     

                       Computation: 42008 steps/s (collection: 2.187s, learning 0.154s)
             Mean action noise std: 3.01
          Mean value_function loss: 199.5314
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 64.7217
                       Mean reward: 870.30
               Mean episode length: 232.72
    Episode_Reward/reaching_object: 1.4802
     Episode_Reward/lifting_object: 166.0833
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 2.34s
                      Time elapsed: 00:57:30
                               ETA: 00:25:56

################################################################################
                     [1m Learning iteration 1379/2000 [0m                     

                       Computation: 41069 steps/s (collection: 2.242s, learning 0.152s)
             Mean action noise std: 3.01
          Mean value_function loss: 217.9138
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 64.7314
                       Mean reward: 762.02
               Mean episode length: 213.18
    Episode_Reward/reaching_object: 1.4695
     Episode_Reward/lifting_object: 163.9134
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0758
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 2.39s
                      Time elapsed: 00:57:33
                               ETA: 00:25:53

################################################################################
                     [1m Learning iteration 1380/2000 [0m                     

                       Computation: 42036 steps/s (collection: 2.228s, learning 0.111s)
             Mean action noise std: 3.01
          Mean value_function loss: 195.6842
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 64.7362
                       Mean reward: 849.12
               Mean episode length: 230.23
    Episode_Reward/reaching_object: 1.4746
     Episode_Reward/lifting_object: 164.8528
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0741
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 2.34s
                      Time elapsed: 00:57:35
                               ETA: 00:25:51

################################################################################
                     [1m Learning iteration 1381/2000 [0m                     

                       Computation: 43223 steps/s (collection: 2.114s, learning 0.160s)
             Mean action noise std: 3.01
          Mean value_function loss: 255.1135
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 64.7425
                       Mean reward: 796.29
               Mean episode length: 215.79
    Episode_Reward/reaching_object: 1.4436
     Episode_Reward/lifting_object: 161.9929
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0742
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 2.27s
                      Time elapsed: 00:57:37
                               ETA: 00:25:48

################################################################################
                     [1m Learning iteration 1382/2000 [0m                     

                       Computation: 41136 steps/s (collection: 2.204s, learning 0.186s)
             Mean action noise std: 3.01
          Mean value_function loss: 222.5008
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 64.7513
                       Mean reward: 829.16
               Mean episode length: 223.99
    Episode_Reward/reaching_object: 1.4628
     Episode_Reward/lifting_object: 163.9380
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0729
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 2.39s
                      Time elapsed: 00:57:40
                               ETA: 00:25:46

################################################################################
                     [1m Learning iteration 1383/2000 [0m                     

                       Computation: 41829 steps/s (collection: 2.202s, learning 0.148s)
             Mean action noise std: 3.01
          Mean value_function loss: 280.0643
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 64.7567
                       Mean reward: 859.88
               Mean episode length: 230.27
    Episode_Reward/reaching_object: 1.4579
     Episode_Reward/lifting_object: 163.7307
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0723
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 2.35s
                      Time elapsed: 00:57:42
                               ETA: 00:25:43

################################################################################
                     [1m Learning iteration 1384/2000 [0m                     

                       Computation: 44265 steps/s (collection: 2.114s, learning 0.107s)
             Mean action noise std: 3.01
          Mean value_function loss: 195.4498
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 64.7602
                       Mean reward: 827.63
               Mean episode length: 223.84
    Episode_Reward/reaching_object: 1.4584
     Episode_Reward/lifting_object: 163.4522
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 2.22s
                      Time elapsed: 00:57:44
                               ETA: 00:25:41

################################################################################
                     [1m Learning iteration 1385/2000 [0m                     

                       Computation: 41198 steps/s (collection: 2.206s, learning 0.180s)
             Mean action noise std: 3.02
          Mean value_function loss: 228.6969
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 64.7638
                       Mean reward: 871.26
               Mean episode length: 232.37
    Episode_Reward/reaching_object: 1.4920
     Episode_Reward/lifting_object: 167.7715
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0746
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 2.39s
                      Time elapsed: 00:57:47
                               ETA: 00:25:38

################################################################################
                     [1m Learning iteration 1386/2000 [0m                     

                       Computation: 40073 steps/s (collection: 2.291s, learning 0.163s)
             Mean action noise std: 3.02
          Mean value_function loss: 224.4266
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 64.7654
                       Mean reward: 810.20
               Mean episode length: 220.09
    Episode_Reward/reaching_object: 1.4662
     Episode_Reward/lifting_object: 164.2101
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0743
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 2.45s
                      Time elapsed: 00:57:49
                               ETA: 00:25:35

################################################################################
                     [1m Learning iteration 1387/2000 [0m                     

                       Computation: 39057 steps/s (collection: 2.343s, learning 0.174s)
             Mean action noise std: 3.02
          Mean value_function loss: 265.1243
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 64.7691
                       Mean reward: 784.79
               Mean episode length: 213.55
    Episode_Reward/reaching_object: 1.4147
     Episode_Reward/lifting_object: 157.9361
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 2.52s
                      Time elapsed: 00:57:52
                               ETA: 00:25:33

################################################################################
                     [1m Learning iteration 1388/2000 [0m                     

                       Computation: 41811 steps/s (collection: 2.245s, learning 0.107s)
             Mean action noise std: 3.02
          Mean value_function loss: 249.1278
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 64.7753
                       Mean reward: 783.66
               Mean episode length: 212.21
    Episode_Reward/reaching_object: 1.4492
     Episode_Reward/lifting_object: 161.9497
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 2.35s
                      Time elapsed: 00:57:54
                               ETA: 00:25:30

################################################################################
                     [1m Learning iteration 1389/2000 [0m                     

                       Computation: 40845 steps/s (collection: 2.259s, learning 0.148s)
             Mean action noise std: 3.02
          Mean value_function loss: 227.3704
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 64.7881
                       Mean reward: 832.53
               Mean episode length: 223.68
    Episode_Reward/reaching_object: 1.4267
     Episode_Reward/lifting_object: 159.7487
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0721
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 2.41s
                      Time elapsed: 00:57:56
                               ETA: 00:25:28

################################################################################
                     [1m Learning iteration 1390/2000 [0m                     

                       Computation: 41630 steps/s (collection: 2.227s, learning 0.135s)
             Mean action noise std: 3.02
          Mean value_function loss: 255.1814
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 64.8076
                       Mean reward: 814.71
               Mean episode length: 220.13
    Episode_Reward/reaching_object: 1.4742
     Episode_Reward/lifting_object: 164.8192
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0734
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 2.36s
                      Time elapsed: 00:57:59
                               ETA: 00:25:25

################################################################################
                     [1m Learning iteration 1391/2000 [0m                     

                       Computation: 41716 steps/s (collection: 2.235s, learning 0.121s)
             Mean action noise std: 3.02
          Mean value_function loss: 263.4262
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 64.8231
                       Mean reward: 797.53
               Mean episode length: 215.76
    Episode_Reward/reaching_object: 1.4464
     Episode_Reward/lifting_object: 161.1689
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0732
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 2.36s
                      Time elapsed: 00:58:01
                               ETA: 00:25:23

################################################################################
                     [1m Learning iteration 1392/2000 [0m                     

                       Computation: 43461 steps/s (collection: 2.146s, learning 0.116s)
             Mean action noise std: 3.02
          Mean value_function loss: 329.5383
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 64.8321
                       Mean reward: 781.00
               Mean episode length: 212.68
    Episode_Reward/reaching_object: 1.4076
     Episode_Reward/lifting_object: 156.5769
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 2.26s
                      Time elapsed: 00:58:03
                               ETA: 00:25:20

################################################################################
                     [1m Learning iteration 1393/2000 [0m                     

                       Computation: 40687 steps/s (collection: 2.247s, learning 0.170s)
             Mean action noise std: 3.03
          Mean value_function loss: 260.9832
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 64.8416
                       Mean reward: 810.29
               Mean episode length: 218.99
    Episode_Reward/reaching_object: 1.4523
     Episode_Reward/lifting_object: 162.5038
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0718
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 2.42s
                      Time elapsed: 00:58:06
                               ETA: 00:25:18

################################################################################
                     [1m Learning iteration 1394/2000 [0m                     

                       Computation: 41333 steps/s (collection: 2.284s, learning 0.095s)
             Mean action noise std: 3.03
          Mean value_function loss: 215.6083
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 64.8506
                       Mean reward: 813.74
               Mean episode length: 218.68
    Episode_Reward/reaching_object: 1.4578
     Episode_Reward/lifting_object: 162.7914
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 2.38s
                      Time elapsed: 00:58:08
                               ETA: 00:25:15

################################################################################
                     [1m Learning iteration 1395/2000 [0m                     

                       Computation: 41652 steps/s (collection: 2.251s, learning 0.110s)
             Mean action noise std: 3.03
          Mean value_function loss: 228.5827
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 64.8573
                       Mean reward: 841.12
               Mean episode length: 226.02
    Episode_Reward/reaching_object: 1.4876
     Episode_Reward/lifting_object: 166.6426
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0741
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 2.36s
                      Time elapsed: 00:58:11
                               ETA: 00:25:12

################################################################################
                     [1m Learning iteration 1396/2000 [0m                     

                       Computation: 38874 steps/s (collection: 2.386s, learning 0.143s)
             Mean action noise std: 3.03
          Mean value_function loss: 205.1349
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 64.8648
                       Mean reward: 843.54
               Mean episode length: 228.75
    Episode_Reward/reaching_object: 1.4533
     Episode_Reward/lifting_object: 160.8218
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0734
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 2.53s
                      Time elapsed: 00:58:13
                               ETA: 00:25:10

################################################################################
                     [1m Learning iteration 1397/2000 [0m                     

                       Computation: 41781 steps/s (collection: 2.245s, learning 0.108s)
             Mean action noise std: 3.03
          Mean value_function loss: 219.2383
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 64.8727
                       Mean reward: 798.64
               Mean episode length: 217.48
    Episode_Reward/reaching_object: 1.4523
     Episode_Reward/lifting_object: 161.4004
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0729
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 2.35s
                      Time elapsed: 00:58:15
                               ETA: 00:25:07

################################################################################
                     [1m Learning iteration 1398/2000 [0m                     

                       Computation: 40490 steps/s (collection: 2.259s, learning 0.169s)
             Mean action noise std: 3.03
          Mean value_function loss: 258.4766
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 64.8805
                       Mean reward: 825.56
               Mean episode length: 223.51
    Episode_Reward/reaching_object: 1.4726
     Episode_Reward/lifting_object: 163.4340
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0749
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 2.43s
                      Time elapsed: 00:58:18
                               ETA: 00:25:05

################################################################################
                     [1m Learning iteration 1399/2000 [0m                     

                       Computation: 39620 steps/s (collection: 2.318s, learning 0.163s)
             Mean action noise std: 3.03
          Mean value_function loss: 203.5829
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 64.8907
                       Mean reward: 840.94
               Mean episode length: 227.40
    Episode_Reward/reaching_object: 1.4602
     Episode_Reward/lifting_object: 161.1651
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0739
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 2.48s
                      Time elapsed: 00:58:20
                               ETA: 00:25:02

################################################################################
                     [1m Learning iteration 1400/2000 [0m                     

                       Computation: 40347 steps/s (collection: 2.201s, learning 0.236s)
             Mean action noise std: 3.03
          Mean value_function loss: 186.6230
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 64.8979
                       Mean reward: 837.51
               Mean episode length: 224.23
    Episode_Reward/reaching_object: 1.5177
     Episode_Reward/lifting_object: 169.5281
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0750
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 2.44s
                      Time elapsed: 00:58:23
                               ETA: 00:25:00

################################################################################
                     [1m Learning iteration 1401/2000 [0m                     

                       Computation: 34393 steps/s (collection: 2.675s, learning 0.183s)
             Mean action noise std: 3.03
          Mean value_function loss: 222.1173
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 64.9041
                       Mean reward: 834.16
               Mean episode length: 224.51
    Episode_Reward/reaching_object: 1.4779
     Episode_Reward/lifting_object: 164.0451
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 2.86s
                      Time elapsed: 00:58:26
                               ETA: 00:24:58

################################################################################
                     [1m Learning iteration 1402/2000 [0m                     

                       Computation: 37398 steps/s (collection: 2.497s, learning 0.131s)
             Mean action noise std: 3.04
          Mean value_function loss: 215.9911
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 64.9161
                       Mean reward: 849.72
               Mean episode length: 230.06
    Episode_Reward/reaching_object: 1.4995
     Episode_Reward/lifting_object: 166.3253
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.0756
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 2.63s
                      Time elapsed: 00:58:28
                               ETA: 00:24:55

################################################################################
                     [1m Learning iteration 1403/2000 [0m                     

                       Computation: 40849 steps/s (collection: 2.262s, learning 0.144s)
             Mean action noise std: 3.04
          Mean value_function loss: 267.1925
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 64.9310
                       Mean reward: 852.01
               Mean episode length: 228.12
    Episode_Reward/reaching_object: 1.4891
     Episode_Reward/lifting_object: 165.8707
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0749
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 2.41s
                      Time elapsed: 00:58:31
                               ETA: 00:24:53

################################################################################
                     [1m Learning iteration 1404/2000 [0m                     

                       Computation: 39870 steps/s (collection: 2.321s, learning 0.144s)
             Mean action noise std: 3.04
          Mean value_function loss: 249.3763
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 64.9356
                       Mean reward: 824.07
               Mean episode length: 223.11
    Episode_Reward/reaching_object: 1.4620
     Episode_Reward/lifting_object: 160.9255
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0737
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 2.47s
                      Time elapsed: 00:58:33
                               ETA: 00:24:50

################################################################################
                     [1m Learning iteration 1405/2000 [0m                     

                       Computation: 41772 steps/s (collection: 2.220s, learning 0.133s)
             Mean action noise std: 3.04
          Mean value_function loss: 200.4048
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 64.9412
                       Mean reward: 807.62
               Mean episode length: 218.12
    Episode_Reward/reaching_object: 1.4578
     Episode_Reward/lifting_object: 162.2066
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0725
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 2.35s
                      Time elapsed: 00:58:36
                               ETA: 00:24:47

################################################################################
                     [1m Learning iteration 1406/2000 [0m                     

                       Computation: 40170 steps/s (collection: 2.325s, learning 0.122s)
             Mean action noise std: 3.04
          Mean value_function loss: 266.4692
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 64.9594
                       Mean reward: 819.52
               Mean episode length: 220.03
    Episode_Reward/reaching_object: 1.4535
     Episode_Reward/lifting_object: 161.7400
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0731
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 2.45s
                      Time elapsed: 00:58:38
                               ETA: 00:24:45

################################################################################
                     [1m Learning iteration 1407/2000 [0m                     

                       Computation: 41397 steps/s (collection: 2.232s, learning 0.143s)
             Mean action noise std: 3.04
          Mean value_function loss: 203.8531
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 64.9771
                       Mean reward: 849.27
               Mean episode length: 228.14
    Episode_Reward/reaching_object: 1.4857
     Episode_Reward/lifting_object: 165.5988
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 2.37s
                      Time elapsed: 00:58:40
                               ETA: 00:24:42

################################################################################
                     [1m Learning iteration 1408/2000 [0m                     

                       Computation: 43882 steps/s (collection: 2.135s, learning 0.105s)
             Mean action noise std: 3.04
          Mean value_function loss: 189.7456
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 64.9849
                       Mean reward: 788.87
               Mean episode length: 215.43
    Episode_Reward/reaching_object: 1.4749
     Episode_Reward/lifting_object: 163.6499
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 2.24s
                      Time elapsed: 00:58:43
                               ETA: 00:24:40

################################################################################
                     [1m Learning iteration 1409/2000 [0m                     

                       Computation: 40613 steps/s (collection: 2.234s, learning 0.186s)
             Mean action noise std: 3.05
          Mean value_function loss: 196.7479
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 64.9945
                       Mean reward: 784.04
               Mean episode length: 215.33
    Episode_Reward/reaching_object: 1.4638
     Episode_Reward/lifting_object: 162.7497
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 2.42s
                      Time elapsed: 00:58:45
                               ETA: 00:24:37

################################################################################
                     [1m Learning iteration 1410/2000 [0m                     

                       Computation: 40555 steps/s (collection: 2.286s, learning 0.138s)
             Mean action noise std: 3.05
          Mean value_function loss: 218.6491
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 65.0095
                       Mean reward: 856.86
               Mean episode length: 229.65
    Episode_Reward/reaching_object: 1.4873
     Episode_Reward/lifting_object: 165.6716
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 2.42s
                      Time elapsed: 00:58:47
                               ETA: 00:24:35

################################################################################
                     [1m Learning iteration 1411/2000 [0m                     

                       Computation: 42846 steps/s (collection: 2.150s, learning 0.145s)
             Mean action noise std: 3.05
          Mean value_function loss: 179.4214
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 65.0226
                       Mean reward: 809.36
               Mean episode length: 221.43
    Episode_Reward/reaching_object: 1.4908
     Episode_Reward/lifting_object: 165.9908
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.0726
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 2.29s
                      Time elapsed: 00:58:50
                               ETA: 00:24:32

################################################################################
                     [1m Learning iteration 1412/2000 [0m                     

                       Computation: 41852 steps/s (collection: 2.204s, learning 0.145s)
             Mean action noise std: 3.05
          Mean value_function loss: 187.0067
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 65.0264
                       Mean reward: 814.91
               Mean episode length: 220.75
    Episode_Reward/reaching_object: 1.4680
     Episode_Reward/lifting_object: 163.5459
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0731
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 2.35s
                      Time elapsed: 00:58:52
                               ETA: 00:24:30

################################################################################
                     [1m Learning iteration 1413/2000 [0m                     

                       Computation: 42329 steps/s (collection: 2.182s, learning 0.140s)
             Mean action noise std: 3.05
          Mean value_function loss: 254.0916
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 65.0301
                       Mean reward: 853.51
               Mean episode length: 228.51
    Episode_Reward/reaching_object: 1.4753
     Episode_Reward/lifting_object: 163.5040
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0737
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 2.32s
                      Time elapsed: 00:58:54
                               ETA: 00:24:27

################################################################################
                     [1m Learning iteration 1414/2000 [0m                     

                       Computation: 41633 steps/s (collection: 2.238s, learning 0.124s)
             Mean action noise std: 3.05
          Mean value_function loss: 191.9177
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 65.0357
                       Mean reward: 821.56
               Mean episode length: 221.01
    Episode_Reward/reaching_object: 1.5040
     Episode_Reward/lifting_object: 167.1759
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 2.36s
                      Time elapsed: 00:58:57
                               ETA: 00:24:24

################################################################################
                     [1m Learning iteration 1415/2000 [0m                     

                       Computation: 39126 steps/s (collection: 2.344s, learning 0.169s)
             Mean action noise std: 3.05
          Mean value_function loss: 235.6864
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 65.0437
                       Mean reward: 845.85
               Mean episode length: 228.48
    Episode_Reward/reaching_object: 1.4716
     Episode_Reward/lifting_object: 162.7852
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0737
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 2.51s
                      Time elapsed: 00:58:59
                               ETA: 00:24:22

################################################################################
                     [1m Learning iteration 1416/2000 [0m                     

                       Computation: 43468 steps/s (collection: 2.164s, learning 0.098s)
             Mean action noise std: 3.05
          Mean value_function loss: 197.4438
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 65.0570
                       Mean reward: 839.24
               Mean episode length: 228.88
    Episode_Reward/reaching_object: 1.4652
     Episode_Reward/lifting_object: 161.8288
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0734
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 2.26s
                      Time elapsed: 00:59:02
                               ETA: 00:24:19

################################################################################
                     [1m Learning iteration 1417/2000 [0m                     

                       Computation: 43093 steps/s (collection: 2.128s, learning 0.153s)
             Mean action noise std: 3.06
          Mean value_function loss: 209.0384
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 65.0739
                       Mean reward: 806.51
               Mean episode length: 218.57
    Episode_Reward/reaching_object: 1.4843
     Episode_Reward/lifting_object: 164.5034
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0739
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 2.28s
                      Time elapsed: 00:59:04
                               ETA: 00:24:17

################################################################################
                     [1m Learning iteration 1418/2000 [0m                     

                       Computation: 42014 steps/s (collection: 2.146s, learning 0.194s)
             Mean action noise std: 3.06
          Mean value_function loss: 226.9395
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 65.0880
                       Mean reward: 834.74
               Mean episode length: 224.08
    Episode_Reward/reaching_object: 1.4539
     Episode_Reward/lifting_object: 161.5210
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0720
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 2.34s
                      Time elapsed: 00:59:06
                               ETA: 00:24:14

################################################################################
                     [1m Learning iteration 1419/2000 [0m                     

                       Computation: 42349 steps/s (collection: 2.222s, learning 0.099s)
             Mean action noise std: 3.06
          Mean value_function loss: 171.6161
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 65.0984
                       Mean reward: 813.55
               Mean episode length: 220.89
    Episode_Reward/reaching_object: 1.4937
     Episode_Reward/lifting_object: 165.7331
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 2.32s
                      Time elapsed: 00:59:08
                               ETA: 00:24:12

################################################################################
                     [1m Learning iteration 1420/2000 [0m                     

                       Computation: 43147 steps/s (collection: 2.173s, learning 0.105s)
             Mean action noise std: 3.06
          Mean value_function loss: 165.8602
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 65.1037
                       Mean reward: 853.47
               Mean episode length: 229.15
    Episode_Reward/reaching_object: 1.5136
     Episode_Reward/lifting_object: 168.8193
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0753
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 2.28s
                      Time elapsed: 00:59:11
                               ETA: 00:24:09

################################################################################
                     [1m Learning iteration 1421/2000 [0m                     

                       Computation: 41592 steps/s (collection: 2.252s, learning 0.112s)
             Mean action noise std: 3.06
          Mean value_function loss: 211.7602
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 65.1132
                       Mean reward: 808.10
               Mean episode length: 219.37
    Episode_Reward/reaching_object: 1.4814
     Episode_Reward/lifting_object: 163.7713
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0750
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 2.36s
                      Time elapsed: 00:59:13
                               ETA: 00:24:06

################################################################################
                     [1m Learning iteration 1422/2000 [0m                     

                       Computation: 41583 steps/s (collection: 2.172s, learning 0.192s)
             Mean action noise std: 3.06
          Mean value_function loss: 238.2545
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 65.1312
                       Mean reward: 843.74
               Mean episode length: 225.67
    Episode_Reward/reaching_object: 1.4861
     Episode_Reward/lifting_object: 165.4376
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0741
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 2.36s
                      Time elapsed: 00:59:16
                               ETA: 00:24:04

################################################################################
                     [1m Learning iteration 1423/2000 [0m                     

                       Computation: 39891 steps/s (collection: 2.343s, learning 0.121s)
             Mean action noise std: 3.07
          Mean value_function loss: 197.6208
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 65.1527
                       Mean reward: 837.43
               Mean episode length: 226.93
    Episode_Reward/reaching_object: 1.5354
     Episode_Reward/lifting_object: 171.2120
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0767
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 2.46s
                      Time elapsed: 00:59:18
                               ETA: 00:24:01

################################################################################
                     [1m Learning iteration 1424/2000 [0m                     

                       Computation: 42053 steps/s (collection: 2.182s, learning 0.156s)
             Mean action noise std: 3.07
          Mean value_function loss: 211.9445
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 65.1667
                       Mean reward: 830.57
               Mean episode length: 222.58
    Episode_Reward/reaching_object: 1.4892
     Episode_Reward/lifting_object: 166.2344
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0749
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 2.34s
                      Time elapsed: 00:59:20
                               ETA: 00:23:59

################################################################################
                     [1m Learning iteration 1425/2000 [0m                     

                       Computation: 41150 steps/s (collection: 2.251s, learning 0.138s)
             Mean action noise std: 3.07
          Mean value_function loss: 224.8696
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 65.1802
                       Mean reward: 837.21
               Mean episode length: 225.26
    Episode_Reward/reaching_object: 1.5056
     Episode_Reward/lifting_object: 167.5070
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0864
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 2.39s
                      Time elapsed: 00:59:23
                               ETA: 00:23:56

################################################################################
                     [1m Learning iteration 1426/2000 [0m                     

                       Computation: 34933 steps/s (collection: 2.669s, learning 0.145s)
             Mean action noise std: 3.07
          Mean value_function loss: 184.1270
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 65.1924
                       Mean reward: 809.48
               Mean episode length: 219.83
    Episode_Reward/reaching_object: 1.5087
     Episode_Reward/lifting_object: 167.7535
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.0767
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 2.81s
                      Time elapsed: 00:59:26
                               ETA: 00:23:54

################################################################################
                     [1m Learning iteration 1427/2000 [0m                     

                       Computation: 40603 steps/s (collection: 2.285s, learning 0.136s)
             Mean action noise std: 3.07
          Mean value_function loss: 179.9344
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 65.2031
                       Mean reward: 845.77
               Mean episode length: 227.15
    Episode_Reward/reaching_object: 1.5115
     Episode_Reward/lifting_object: 169.1849
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0533
          Episode_Reward/joint_vel: -0.0741
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 2.42s
                      Time elapsed: 00:59:28
                               ETA: 00:23:51

################################################################################
                     [1m Learning iteration 1428/2000 [0m                     

                       Computation: 42326 steps/s (collection: 2.197s, learning 0.126s)
             Mean action noise std: 3.07
          Mean value_function loss: 215.9487
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 65.2091
                       Mean reward: 817.02
               Mean episode length: 221.78
    Episode_Reward/reaching_object: 1.5059
     Episode_Reward/lifting_object: 167.5927
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0752
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 2.32s
                      Time elapsed: 00:59:30
                               ETA: 00:23:49

################################################################################
                     [1m Learning iteration 1429/2000 [0m                     

                       Computation: 38394 steps/s (collection: 2.367s, learning 0.194s)
             Mean action noise std: 3.07
          Mean value_function loss: 189.4725
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 65.2149
                       Mean reward: 868.37
               Mean episode length: 233.16
    Episode_Reward/reaching_object: 1.4819
     Episode_Reward/lifting_object: 165.1381
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0760
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 2.56s
                      Time elapsed: 00:59:33
                               ETA: 00:23:46

################################################################################
                     [1m Learning iteration 1430/2000 [0m                     

                       Computation: 39925 steps/s (collection: 2.333s, learning 0.129s)
             Mean action noise std: 3.07
          Mean value_function loss: 220.1270
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 65.2230
                       Mean reward: 795.24
               Mean episode length: 216.26
    Episode_Reward/reaching_object: 1.4613
     Episode_Reward/lifting_object: 162.4977
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 2.46s
                      Time elapsed: 00:59:35
                               ETA: 00:23:44

################################################################################
                     [1m Learning iteration 1431/2000 [0m                     

                       Computation: 40733 steps/s (collection: 2.294s, learning 0.120s)
             Mean action noise std: 3.08
          Mean value_function loss: 223.7725
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 65.2347
                       Mean reward: 807.69
               Mean episode length: 219.02
    Episode_Reward/reaching_object: 1.4897
     Episode_Reward/lifting_object: 165.5512
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 2.41s
                      Time elapsed: 00:59:38
                               ETA: 00:23:41

################################################################################
                     [1m Learning iteration 1432/2000 [0m                     

                       Computation: 41090 steps/s (collection: 2.262s, learning 0.131s)
             Mean action noise std: 3.08
          Mean value_function loss: 217.1466
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 65.2386
                       Mean reward: 859.96
               Mean episode length: 231.03
    Episode_Reward/reaching_object: 1.4880
     Episode_Reward/lifting_object: 165.5694
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0747
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 2.39s
                      Time elapsed: 00:59:40
                               ETA: 00:23:39

################################################################################
                     [1m Learning iteration 1433/2000 [0m                     

                       Computation: 39805 steps/s (collection: 2.336s, learning 0.134s)
             Mean action noise std: 3.08
          Mean value_function loss: 218.4776
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 65.2401
                       Mean reward: 865.72
               Mean episode length: 234.16
    Episode_Reward/reaching_object: 1.4751
     Episode_Reward/lifting_object: 164.4214
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0743
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 2.47s
                      Time elapsed: 00:59:43
                               ETA: 00:23:36

################################################################################
                     [1m Learning iteration 1434/2000 [0m                     

                       Computation: 38022 steps/s (collection: 2.341s, learning 0.245s)
             Mean action noise std: 3.08
          Mean value_function loss: 182.1772
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 65.2419
                       Mean reward: 878.51
               Mean episode length: 234.49
    Episode_Reward/reaching_object: 1.4865
     Episode_Reward/lifting_object: 165.3754
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0747
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 2.59s
                      Time elapsed: 00:59:45
                               ETA: 00:23:34

################################################################################
                     [1m Learning iteration 1435/2000 [0m                     

                       Computation: 40067 steps/s (collection: 2.340s, learning 0.113s)
             Mean action noise std: 3.08
          Mean value_function loss: 204.2301
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 65.2451
                       Mean reward: 884.32
               Mean episode length: 235.78
    Episode_Reward/reaching_object: 1.5059
     Episode_Reward/lifting_object: 168.6050
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0760
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 2.45s
                      Time elapsed: 00:59:48
                               ETA: 00:23:31

################################################################################
                     [1m Learning iteration 1436/2000 [0m                     

                       Computation: 42517 steps/s (collection: 2.192s, learning 0.120s)
             Mean action noise std: 3.08
          Mean value_function loss: 215.4834
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 65.2559
                       Mean reward: 821.28
               Mean episode length: 220.57
    Episode_Reward/reaching_object: 1.4917
     Episode_Reward/lifting_object: 166.8352
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0533
          Episode_Reward/joint_vel: -0.0762
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 2.31s
                      Time elapsed: 00:59:50
                               ETA: 00:23:29

################################################################################
                     [1m Learning iteration 1437/2000 [0m                     

                       Computation: 41657 steps/s (collection: 2.188s, learning 0.172s)
             Mean action noise std: 3.08
          Mean value_function loss: 230.2618
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 65.2716
                       Mean reward: 836.86
               Mean episode length: 225.95
    Episode_Reward/reaching_object: 1.4706
     Episode_Reward/lifting_object: 163.1675
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0761
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 2.36s
                      Time elapsed: 00:59:52
                               ETA: 00:23:26

################################################################################
                     [1m Learning iteration 1438/2000 [0m                     

                       Computation: 40909 steps/s (collection: 2.264s, learning 0.139s)
             Mean action noise std: 3.08
          Mean value_function loss: 237.8021
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 65.2780
                       Mean reward: 881.01
               Mean episode length: 236.82
    Episode_Reward/reaching_object: 1.4730
     Episode_Reward/lifting_object: 163.9851
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0750
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 2.40s
                      Time elapsed: 00:59:55
                               ETA: 00:23:24

################################################################################
                     [1m Learning iteration 1439/2000 [0m                     

                       Computation: 40276 steps/s (collection: 2.287s, learning 0.154s)
             Mean action noise std: 3.08
          Mean value_function loss: 246.8781
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 65.2852
                       Mean reward: 820.80
               Mean episode length: 221.67
    Episode_Reward/reaching_object: 1.4446
     Episode_Reward/lifting_object: 161.1982
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0746
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 2.44s
                      Time elapsed: 00:59:57
                               ETA: 00:23:21

################################################################################
                     [1m Learning iteration 1440/2000 [0m                     

                       Computation: 39128 steps/s (collection: 2.409s, learning 0.103s)
             Mean action noise std: 3.09
          Mean value_function loss: 237.5932
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 65.2998
                       Mean reward: 831.05
               Mean episode length: 223.35
    Episode_Reward/reaching_object: 1.4406
     Episode_Reward/lifting_object: 160.1194
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.0746
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 2.51s
                      Time elapsed: 01:00:00
                               ETA: 00:23:19

################################################################################
                     [1m Learning iteration 1441/2000 [0m                     

                       Computation: 42826 steps/s (collection: 2.181s, learning 0.115s)
             Mean action noise std: 3.09
          Mean value_function loss: 220.3363
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 65.3123
                       Mean reward: 838.24
               Mean episode length: 225.99
    Episode_Reward/reaching_object: 1.4552
     Episode_Reward/lifting_object: 162.0091
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0763
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 2.30s
                      Time elapsed: 01:00:02
                               ETA: 00:23:16

################################################################################
                     [1m Learning iteration 1442/2000 [0m                     

                       Computation: 42762 steps/s (collection: 2.180s, learning 0.119s)
             Mean action noise std: 3.09
          Mean value_function loss: 259.7392
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 65.3162
                       Mean reward: 824.23
               Mean episode length: 221.84
    Episode_Reward/reaching_object: 1.4734
     Episode_Reward/lifting_object: 164.6196
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0759
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 2.30s
                      Time elapsed: 01:00:04
                               ETA: 00:23:13

################################################################################
                     [1m Learning iteration 1443/2000 [0m                     

                       Computation: 38547 steps/s (collection: 2.387s, learning 0.164s)
             Mean action noise std: 3.09
          Mean value_function loss: 219.9551
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 65.3242
                       Mean reward: 794.34
               Mean episode length: 214.24
    Episode_Reward/reaching_object: 1.4725
     Episode_Reward/lifting_object: 164.1330
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0768
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 2.55s
                      Time elapsed: 01:00:07
                               ETA: 00:23:11

################################################################################
                     [1m Learning iteration 1444/2000 [0m                     

                       Computation: 42775 steps/s (collection: 2.181s, learning 0.118s)
             Mean action noise std: 3.09
          Mean value_function loss: 221.8494
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 65.3318
                       Mean reward: 859.57
               Mean episode length: 230.87
    Episode_Reward/reaching_object: 1.4904
     Episode_Reward/lifting_object: 166.4127
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0778
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 2.30s
                      Time elapsed: 01:00:09
                               ETA: 00:23:08

################################################################################
                     [1m Learning iteration 1445/2000 [0m                     

                       Computation: 41678 steps/s (collection: 2.185s, learning 0.174s)
             Mean action noise std: 3.09
          Mean value_function loss: 208.1421
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 65.3406
                       Mean reward: 878.44
               Mean episode length: 237.19
    Episode_Reward/reaching_object: 1.4968
     Episode_Reward/lifting_object: 166.4569
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0775
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 2.36s
                      Time elapsed: 01:00:11
                               ETA: 00:23:06

################################################################################
                     [1m Learning iteration 1446/2000 [0m                     

                       Computation: 43396 steps/s (collection: 2.142s, learning 0.123s)
             Mean action noise std: 3.09
          Mean value_function loss: 214.1134
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 65.3449
                       Mean reward: 812.42
               Mean episode length: 220.09
    Episode_Reward/reaching_object: 1.4632
     Episode_Reward/lifting_object: 162.4995
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.0769
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 2.27s
                      Time elapsed: 01:00:14
                               ETA: 00:23:03

################################################################################
                     [1m Learning iteration 1447/2000 [0m                     

                       Computation: 42331 steps/s (collection: 2.189s, learning 0.133s)
             Mean action noise std: 3.09
          Mean value_function loss: 174.1119
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 65.3524
                       Mean reward: 837.34
               Mean episode length: 225.51
    Episode_Reward/reaching_object: 1.5281
     Episode_Reward/lifting_object: 170.7034
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0795
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 2.32s
                      Time elapsed: 01:00:16
                               ETA: 00:23:01

################################################################################
                     [1m Learning iteration 1448/2000 [0m                     

                       Computation: 39511 steps/s (collection: 2.309s, learning 0.179s)
             Mean action noise std: 3.09
          Mean value_function loss: 215.8656
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 65.3641
                       Mean reward: 859.12
               Mean episode length: 229.95
    Episode_Reward/reaching_object: 1.5046
     Episode_Reward/lifting_object: 167.1944
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0786
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 2.49s
                      Time elapsed: 01:00:18
                               ETA: 00:22:58

################################################################################
                     [1m Learning iteration 1449/2000 [0m                     

                       Computation: 42922 steps/s (collection: 2.167s, learning 0.124s)
             Mean action noise std: 3.09
          Mean value_function loss: 221.1015
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 65.3723
                       Mean reward: 803.81
               Mean episode length: 216.86
    Episode_Reward/reaching_object: 1.4832
     Episode_Reward/lifting_object: 164.3779
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0533
          Episode_Reward/joint_vel: -0.0764
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 2.29s
                      Time elapsed: 01:00:21
                               ETA: 00:22:56

################################################################################
                     [1m Learning iteration 1450/2000 [0m                     

                       Computation: 41352 steps/s (collection: 2.253s, learning 0.125s)
             Mean action noise std: 3.10
          Mean value_function loss: 190.0050
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 65.3770
                       Mean reward: 844.63
               Mean episode length: 229.93
    Episode_Reward/reaching_object: 1.5026
     Episode_Reward/lifting_object: 166.2945
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0788
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 2.38s
                      Time elapsed: 01:00:23
                               ETA: 00:22:53

################################################################################
                     [1m Learning iteration 1451/2000 [0m                     

                       Computation: 39632 steps/s (collection: 2.328s, learning 0.153s)
             Mean action noise std: 3.10
          Mean value_function loss: 213.2638
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 65.3841
                       Mean reward: 828.63
               Mean episode length: 223.60
    Episode_Reward/reaching_object: 1.4961
     Episode_Reward/lifting_object: 165.8033
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0772
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 2.48s
                      Time elapsed: 01:00:26
                               ETA: 00:22:51

################################################################################
                     [1m Learning iteration 1452/2000 [0m                     

                       Computation: 41326 steps/s (collection: 2.258s, learning 0.120s)
             Mean action noise std: 3.10
          Mean value_function loss: 187.8234
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 65.3908
                       Mean reward: 861.29
               Mean episode length: 231.37
    Episode_Reward/reaching_object: 1.5127
     Episode_Reward/lifting_object: 167.1131
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0778
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 2.38s
                      Time elapsed: 01:00:28
                               ETA: 00:22:48

################################################################################
                     [1m Learning iteration 1453/2000 [0m                     

                       Computation: 41878 steps/s (collection: 2.199s, learning 0.149s)
             Mean action noise std: 3.10
          Mean value_function loss: 190.3699
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 65.4003
                       Mean reward: 840.41
               Mean episode length: 224.93
    Episode_Reward/reaching_object: 1.5331
     Episode_Reward/lifting_object: 170.3345
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0786
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 2.35s
                      Time elapsed: 01:00:30
                               ETA: 00:22:45

################################################################################
                     [1m Learning iteration 1454/2000 [0m                     

                       Computation: 42081 steps/s (collection: 2.189s, learning 0.147s)
             Mean action noise std: 3.10
          Mean value_function loss: 202.3236
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 65.4106
                       Mean reward: 854.33
               Mean episode length: 229.28
    Episode_Reward/reaching_object: 1.5371
     Episode_Reward/lifting_object: 170.7604
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0785
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 2.34s
                      Time elapsed: 01:00:33
                               ETA: 00:22:43

################################################################################
                     [1m Learning iteration 1455/2000 [0m                     

                       Computation: 42338 steps/s (collection: 2.205s, learning 0.117s)
             Mean action noise std: 3.10
          Mean value_function loss: 231.1001
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 65.4131
                       Mean reward: 836.16
               Mean episode length: 224.52
    Episode_Reward/reaching_object: 1.4829
     Episode_Reward/lifting_object: 164.3462
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0764
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 2.32s
                      Time elapsed: 01:00:35
                               ETA: 00:22:40

################################################################################
                     [1m Learning iteration 1456/2000 [0m                     

                       Computation: 38921 steps/s (collection: 2.347s, learning 0.179s)
             Mean action noise std: 3.10
          Mean value_function loss: 228.8891
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 65.4148
                       Mean reward: 868.19
               Mean episode length: 232.13
    Episode_Reward/reaching_object: 1.5201
     Episode_Reward/lifting_object: 168.6056
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0790
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 2.53s
                      Time elapsed: 01:00:38
                               ETA: 00:22:38

################################################################################
                     [1m Learning iteration 1457/2000 [0m                     

                       Computation: 40939 steps/s (collection: 2.263s, learning 0.139s)
             Mean action noise std: 3.10
          Mean value_function loss: 215.6936
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 65.4180
                       Mean reward: 827.18
               Mean episode length: 223.22
    Episode_Reward/reaching_object: 1.4886
     Episode_Reward/lifting_object: 165.6678
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0775
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 2.40s
                      Time elapsed: 01:00:40
                               ETA: 00:22:35

################################################################################
                     [1m Learning iteration 1458/2000 [0m                     

                       Computation: 39044 steps/s (collection: 2.345s, learning 0.173s)
             Mean action noise std: 3.10
          Mean value_function loss: 228.5586
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 65.4251
                       Mean reward: 831.26
               Mean episode length: 224.66
    Episode_Reward/reaching_object: 1.5103
     Episode_Reward/lifting_object: 167.9055
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0780
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 2.52s
                      Time elapsed: 01:00:42
                               ETA: 00:22:33

################################################################################
                     [1m Learning iteration 1459/2000 [0m                     

                       Computation: 41487 steps/s (collection: 2.223s, learning 0.146s)
             Mean action noise std: 3.10
          Mean value_function loss: 204.7031
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 65.4338
                       Mean reward: 838.13
               Mean episode length: 226.27
    Episode_Reward/reaching_object: 1.5041
     Episode_Reward/lifting_object: 166.8430
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0772
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 2.37s
                      Time elapsed: 01:00:45
                               ETA: 00:22:30

################################################################################
                     [1m Learning iteration 1460/2000 [0m                     

                       Computation: 41702 steps/s (collection: 2.250s, learning 0.108s)
             Mean action noise std: 3.11
          Mean value_function loss: 176.6065
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 65.4474
                       Mean reward: 857.69
               Mean episode length: 229.66
    Episode_Reward/reaching_object: 1.5267
     Episode_Reward/lifting_object: 169.9743
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0780
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 2.36s
                      Time elapsed: 01:00:47
                               ETA: 00:22:28

################################################################################
                     [1m Learning iteration 1461/2000 [0m                     

                       Computation: 42918 steps/s (collection: 2.181s, learning 0.109s)
             Mean action noise std: 3.11
          Mean value_function loss: 213.5209
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 65.4552
                       Mean reward: 874.82
               Mean episode length: 233.07
    Episode_Reward/reaching_object: 1.5366
     Episode_Reward/lifting_object: 170.3553
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0794
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 2.29s
                      Time elapsed: 01:00:49
                               ETA: 00:22:25

################################################################################
                     [1m Learning iteration 1462/2000 [0m                     

                       Computation: 42664 steps/s (collection: 2.177s, learning 0.127s)
             Mean action noise std: 3.11
          Mean value_function loss: 272.2634
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 65.4575
                       Mean reward: 837.05
               Mean episode length: 227.22
    Episode_Reward/reaching_object: 1.4793
     Episode_Reward/lifting_object: 163.7570
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0773
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 2.30s
                      Time elapsed: 01:00:52
                               ETA: 00:22:23

################################################################################
                     [1m Learning iteration 1463/2000 [0m                     

                       Computation: 42650 steps/s (collection: 2.206s, learning 0.099s)
             Mean action noise std: 3.11
          Mean value_function loss: 179.8297
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 65.4683
                       Mean reward: 850.63
               Mean episode length: 231.93
    Episode_Reward/reaching_object: 1.5176
     Episode_Reward/lifting_object: 168.3667
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0778
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 2.30s
                      Time elapsed: 01:00:54
                               ETA: 00:22:20

################################################################################
                     [1m Learning iteration 1464/2000 [0m                     

                       Computation: 41617 steps/s (collection: 2.192s, learning 0.171s)
             Mean action noise std: 3.11
          Mean value_function loss: 151.1429
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 65.4835
                       Mean reward: 909.68
               Mean episode length: 241.72
    Episode_Reward/reaching_object: 1.5288
     Episode_Reward/lifting_object: 170.5670
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0781
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 2.36s
                      Time elapsed: 01:00:56
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 1465/2000 [0m                     

                       Computation: 42339 steps/s (collection: 2.173s, learning 0.149s)
             Mean action noise std: 3.11
          Mean value_function loss: 223.6256
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 65.4913
                       Mean reward: 834.63
               Mean episode length: 224.49
    Episode_Reward/reaching_object: 1.4653
     Episode_Reward/lifting_object: 162.5571
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0760
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 2.32s
                      Time elapsed: 01:00:59
                               ETA: 00:22:15

################################################################################
                     [1m Learning iteration 1466/2000 [0m                     

                       Computation: 42703 steps/s (collection: 2.178s, learning 0.124s)
             Mean action noise std: 3.11
          Mean value_function loss: 209.4026
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 65.4993
                       Mean reward: 830.05
               Mean episode length: 224.38
    Episode_Reward/reaching_object: 1.5251
     Episode_Reward/lifting_object: 170.0196
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0783
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 2.30s
                      Time elapsed: 01:01:01
                               ETA: 00:22:12

################################################################################
                     [1m Learning iteration 1467/2000 [0m                     

                       Computation: 43787 steps/s (collection: 2.128s, learning 0.117s)
             Mean action noise std: 3.12
          Mean value_function loss: 197.4624
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 65.5198
                       Mean reward: 820.87
               Mean episode length: 220.45
    Episode_Reward/reaching_object: 1.4911
     Episode_Reward/lifting_object: 166.6985
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0781
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 2.25s
                      Time elapsed: 01:01:03
                               ETA: 00:22:10

################################################################################
                     [1m Learning iteration 1468/2000 [0m                     

                       Computation: 41581 steps/s (collection: 2.193s, learning 0.171s)
             Mean action noise std: 3.12
          Mean value_function loss: 258.9082
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 65.5375
                       Mean reward: 799.27
               Mean episode length: 217.76
    Episode_Reward/reaching_object: 1.4673
     Episode_Reward/lifting_object: 162.9747
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0770
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 2.36s
                      Time elapsed: 01:01:06
                               ETA: 00:22:07

################################################################################
                     [1m Learning iteration 1469/2000 [0m                     

                       Computation: 40851 steps/s (collection: 2.297s, learning 0.109s)
             Mean action noise std: 3.12
          Mean value_function loss: 218.6433
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 65.5412
                       Mean reward: 853.92
               Mean episode length: 229.88
    Episode_Reward/reaching_object: 1.4965
     Episode_Reward/lifting_object: 166.9680
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0773
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 2.41s
                      Time elapsed: 01:01:08
                               ETA: 00:22:05

################################################################################
                     [1m Learning iteration 1470/2000 [0m                     

                       Computation: 43499 steps/s (collection: 2.112s, learning 0.148s)
             Mean action noise std: 3.12
          Mean value_function loss: 200.9683
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 65.5505
                       Mean reward: 857.35
               Mean episode length: 229.61
    Episode_Reward/reaching_object: 1.4924
     Episode_Reward/lifting_object: 165.9308
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0771
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 2.26s
                      Time elapsed: 01:01:10
                               ETA: 00:22:02

################################################################################
                     [1m Learning iteration 1471/2000 [0m                     

                       Computation: 42184 steps/s (collection: 2.187s, learning 0.143s)
             Mean action noise std: 3.12
          Mean value_function loss: 206.2155
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 65.5628
                       Mean reward: 859.05
               Mean episode length: 229.51
    Episode_Reward/reaching_object: 1.5077
     Episode_Reward/lifting_object: 168.2318
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0774
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 2.33s
                      Time elapsed: 01:01:13
                               ETA: 00:22:00

################################################################################
                     [1m Learning iteration 1472/2000 [0m                     

                       Computation: 39629 steps/s (collection: 2.349s, learning 0.132s)
             Mean action noise std: 3.12
          Mean value_function loss: 203.0219
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 65.5722
                       Mean reward: 822.92
               Mean episode length: 222.97
    Episode_Reward/reaching_object: 1.5034
     Episode_Reward/lifting_object: 167.6123
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0788
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 2.48s
                      Time elapsed: 01:01:15
                               ETA: 00:21:57

################################################################################
                     [1m Learning iteration 1473/2000 [0m                     

                       Computation: 41010 steps/s (collection: 2.244s, learning 0.153s)
             Mean action noise std: 3.12
          Mean value_function loss: 180.8223
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 65.5816
                       Mean reward: 891.72
               Mean episode length: 238.42
    Episode_Reward/reaching_object: 1.5082
     Episode_Reward/lifting_object: 168.1511
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0773
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 2.40s
                      Time elapsed: 01:01:18
                               ETA: 00:21:55

################################################################################
                     [1m Learning iteration 1474/2000 [0m                     

                       Computation: 41564 steps/s (collection: 2.240s, learning 0.126s)
             Mean action noise std: 3.12
          Mean value_function loss: 232.0727
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 65.5946
                       Mean reward: 818.38
               Mean episode length: 220.61
    Episode_Reward/reaching_object: 1.4767
     Episode_Reward/lifting_object: 164.3800
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0773
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 2.37s
                      Time elapsed: 01:01:20
                               ETA: 00:21:52

################################################################################
                     [1m Learning iteration 1475/2000 [0m                     

                       Computation: 41756 steps/s (collection: 2.214s, learning 0.141s)
             Mean action noise std: 3.13
          Mean value_function loss: 203.7696
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 65.6064
                       Mean reward: 825.05
               Mean episode length: 223.46
    Episode_Reward/reaching_object: 1.5022
     Episode_Reward/lifting_object: 167.6064
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0803
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 2.35s
                      Time elapsed: 01:01:22
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 1476/2000 [0m                     

                       Computation: 38238 steps/s (collection: 2.415s, learning 0.156s)
             Mean action noise std: 3.13
          Mean value_function loss: 192.5715
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 65.6159
                       Mean reward: 842.19
               Mean episode length: 226.19
    Episode_Reward/reaching_object: 1.4878
     Episode_Reward/lifting_object: 166.4188
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0791
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 2.57s
                      Time elapsed: 01:01:25
                               ETA: 00:21:47

################################################################################
                     [1m Learning iteration 1477/2000 [0m                     

                       Computation: 42024 steps/s (collection: 2.227s, learning 0.113s)
             Mean action noise std: 3.13
          Mean value_function loss: 233.8277
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 65.6211
                       Mean reward: 845.87
               Mean episode length: 227.70
    Episode_Reward/reaching_object: 1.4861
     Episode_Reward/lifting_object: 165.7241
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0797
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 2.34s
                      Time elapsed: 01:01:27
                               ETA: 00:21:44

################################################################################
                     [1m Learning iteration 1478/2000 [0m                     

                       Computation: 42524 steps/s (collection: 2.192s, learning 0.120s)
             Mean action noise std: 3.13
          Mean value_function loss: 233.5232
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 65.6277
                       Mean reward: 789.26
               Mean episode length: 211.58
    Episode_Reward/reaching_object: 1.4595
     Episode_Reward/lifting_object: 163.2154
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0761
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 2.31s
                      Time elapsed: 01:01:30
                               ETA: 00:21:42

################################################################################
                     [1m Learning iteration 1479/2000 [0m                     

                       Computation: 41574 steps/s (collection: 2.224s, learning 0.141s)
             Mean action noise std: 3.13
          Mean value_function loss: 208.2863
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 65.6422
                       Mean reward: 853.94
               Mean episode length: 230.21
    Episode_Reward/reaching_object: 1.4753
     Episode_Reward/lifting_object: 164.8345
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0778
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 2.36s
                      Time elapsed: 01:01:32
                               ETA: 00:21:39

################################################################################
                     [1m Learning iteration 1480/2000 [0m                     

                       Computation: 42287 steps/s (collection: 2.222s, learning 0.103s)
             Mean action noise std: 3.13
          Mean value_function loss: 308.5072
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 65.6492
                       Mean reward: 783.49
               Mean episode length: 211.23
    Episode_Reward/reaching_object: 1.4529
     Episode_Reward/lifting_object: 162.0527
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0771
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 2.32s
                      Time elapsed: 01:01:34
                               ETA: 00:21:37

################################################################################
                     [1m Learning iteration 1481/2000 [0m                     

                       Computation: 41110 steps/s (collection: 2.210s, learning 0.182s)
             Mean action noise std: 3.13
          Mean value_function loss: 230.9054
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 65.6520
                       Mean reward: 853.75
               Mean episode length: 228.90
    Episode_Reward/reaching_object: 1.4804
     Episode_Reward/lifting_object: 165.2873
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0775
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 2.39s
                      Time elapsed: 01:01:37
                               ETA: 00:21:34

################################################################################
                     [1m Learning iteration 1482/2000 [0m                     

                       Computation: 43189 steps/s (collection: 2.149s, learning 0.128s)
             Mean action noise std: 3.13
          Mean value_function loss: 215.6634
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 65.6586
                       Mean reward: 855.49
               Mean episode length: 229.29
    Episode_Reward/reaching_object: 1.4783
     Episode_Reward/lifting_object: 165.0608
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0799
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 2.28s
                      Time elapsed: 01:01:39
                               ETA: 00:21:32

################################################################################
                     [1m Learning iteration 1483/2000 [0m                     

                       Computation: 42178 steps/s (collection: 2.219s, learning 0.112s)
             Mean action noise std: 3.13
          Mean value_function loss: 191.5915
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 65.6678
                       Mean reward: 791.24
               Mean episode length: 214.17
    Episode_Reward/reaching_object: 1.4979
     Episode_Reward/lifting_object: 167.5550
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0788
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 2.33s
                      Time elapsed: 01:01:41
                               ETA: 00:21:29

################################################################################
                     [1m Learning iteration 1484/2000 [0m                     

                       Computation: 39445 steps/s (collection: 2.330s, learning 0.163s)
             Mean action noise std: 3.14
          Mean value_function loss: 211.1338
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 65.6802
                       Mean reward: 830.87
               Mean episode length: 225.69
    Episode_Reward/reaching_object: 1.5153
     Episode_Reward/lifting_object: 168.7476
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0794
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 2.49s
                      Time elapsed: 01:01:44
                               ETA: 00:21:27

################################################################################
                     [1m Learning iteration 1485/2000 [0m                     

                       Computation: 43112 steps/s (collection: 2.145s, learning 0.136s)
             Mean action noise std: 3.14
          Mean value_function loss: 216.0720
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 65.6905
                       Mean reward: 868.31
               Mean episode length: 231.49
    Episode_Reward/reaching_object: 1.4964
     Episode_Reward/lifting_object: 166.9816
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0790
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 2.28s
                      Time elapsed: 01:01:46
                               ETA: 00:21:24

################################################################################
                     [1m Learning iteration 1486/2000 [0m                     

                       Computation: 41301 steps/s (collection: 2.228s, learning 0.152s)
             Mean action noise std: 3.14
          Mean value_function loss: 229.5432
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 65.6939
                       Mean reward: 835.71
               Mean episode length: 222.72
    Episode_Reward/reaching_object: 1.4830
     Episode_Reward/lifting_object: 165.4707
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0788
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 2.38s
                      Time elapsed: 01:01:48
                               ETA: 00:21:22

################################################################################
                     [1m Learning iteration 1487/2000 [0m                     

                       Computation: 40002 steps/s (collection: 2.310s, learning 0.147s)
             Mean action noise std: 3.14
          Mean value_function loss: 235.3198
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 65.7055
                       Mean reward: 807.27
               Mean episode length: 218.43
    Episode_Reward/reaching_object: 1.4681
     Episode_Reward/lifting_object: 161.2791
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0781
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 2.46s
                      Time elapsed: 01:01:51
                               ETA: 00:21:19

################################################################################
                     [1m Learning iteration 1488/2000 [0m                     

                       Computation: 40969 steps/s (collection: 2.274s, learning 0.125s)
             Mean action noise std: 3.14
          Mean value_function loss: 267.1875
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 65.7165
                       Mean reward: 801.02
               Mean episode length: 216.77
    Episode_Reward/reaching_object: 1.4704
     Episode_Reward/lifting_object: 163.2054
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0783
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 2.40s
                      Time elapsed: 01:01:53
                               ETA: 00:21:16

################################################################################
                     [1m Learning iteration 1489/2000 [0m                     

                       Computation: 40970 steps/s (collection: 2.268s, learning 0.131s)
             Mean action noise std: 3.14
          Mean value_function loss: 259.6145
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 65.7329
                       Mean reward: 827.51
               Mean episode length: 222.36
    Episode_Reward/reaching_object: 1.4398
     Episode_Reward/lifting_object: 159.5932
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0763
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 2.40s
                      Time elapsed: 01:01:56
                               ETA: 00:21:14

################################################################################
                     [1m Learning iteration 1490/2000 [0m                     

                       Computation: 41407 steps/s (collection: 2.242s, learning 0.133s)
             Mean action noise std: 3.14
          Mean value_function loss: 248.0031
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 65.7398
                       Mean reward: 855.99
               Mean episode length: 229.73
    Episode_Reward/reaching_object: 1.4920
     Episode_Reward/lifting_object: 165.5250
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0784
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 2.37s
                      Time elapsed: 01:01:58
                               ETA: 00:21:11

################################################################################
                     [1m Learning iteration 1491/2000 [0m                     

                       Computation: 42223 steps/s (collection: 2.154s, learning 0.175s)
             Mean action noise std: 3.14
          Mean value_function loss: 244.2357
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 65.7434
                       Mean reward: 786.35
               Mean episode length: 214.37
    Episode_Reward/reaching_object: 1.4323
     Episode_Reward/lifting_object: 158.9188
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0533
          Episode_Reward/joint_vel: -0.0764
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 2.33s
                      Time elapsed: 01:02:00
                               ETA: 00:21:09

################################################################################
                     [1m Learning iteration 1492/2000 [0m                     

                       Computation: 35458 steps/s (collection: 2.633s, learning 0.139s)
             Mean action noise std: 3.15
          Mean value_function loss: 263.2963
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 65.7515
                       Mean reward: 808.51
               Mean episode length: 217.48
    Episode_Reward/reaching_object: 1.4516
     Episode_Reward/lifting_object: 160.2424
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0778
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 2.77s
                      Time elapsed: 01:02:03
                               ETA: 00:21:06

################################################################################
                     [1m Learning iteration 1493/2000 [0m                     

                       Computation: 42207 steps/s (collection: 2.216s, learning 0.113s)
             Mean action noise std: 3.15
          Mean value_function loss: 254.5797
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 65.7627
                       Mean reward: 819.71
               Mean episode length: 223.03
    Episode_Reward/reaching_object: 1.4662
     Episode_Reward/lifting_object: 161.6904
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0779
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 2.33s
                      Time elapsed: 01:02:05
                               ETA: 00:21:04

################################################################################
                     [1m Learning iteration 1494/2000 [0m                     

                       Computation: 37112 steps/s (collection: 2.436s, learning 0.213s)
             Mean action noise std: 3.15
          Mean value_function loss: 226.9181
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 65.7720
                       Mean reward: 864.84
               Mean episode length: 231.48
    Episode_Reward/reaching_object: 1.4795
     Episode_Reward/lifting_object: 163.4943
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0784
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 2.65s
                      Time elapsed: 01:02:08
                               ETA: 00:21:01

################################################################################
                     [1m Learning iteration 1495/2000 [0m                     

                       Computation: 35474 steps/s (collection: 2.514s, learning 0.257s)
             Mean action noise std: 3.15
          Mean value_function loss: 211.4239
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 65.7831
                       Mean reward: 849.00
               Mean episode length: 228.94
    Episode_Reward/reaching_object: 1.4744
     Episode_Reward/lifting_object: 163.0994
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0774
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 2.77s
                      Time elapsed: 01:02:11
                               ETA: 00:20:59

################################################################################
                     [1m Learning iteration 1496/2000 [0m                     

                       Computation: 38856 steps/s (collection: 2.409s, learning 0.120s)
             Mean action noise std: 3.15
          Mean value_function loss: 281.5593
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 65.7930
                       Mean reward: 766.74
               Mean episode length: 209.94
    Episode_Reward/reaching_object: 1.4604
     Episode_Reward/lifting_object: 161.6236
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0768
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 2.53s
                      Time elapsed: 01:02:13
                               ETA: 00:20:57

################################################################################
                     [1m Learning iteration 1497/2000 [0m                     

                       Computation: 43616 steps/s (collection: 2.118s, learning 0.136s)
             Mean action noise std: 3.15
          Mean value_function loss: 236.3236
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 65.8005
                       Mean reward: 832.78
               Mean episode length: 223.38
    Episode_Reward/reaching_object: 1.4397
     Episode_Reward/lifting_object: 158.4274
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0759
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 2.25s
                      Time elapsed: 01:02:16
                               ETA: 00:20:54

################################################################################
                     [1m Learning iteration 1498/2000 [0m                     

                       Computation: 38880 steps/s (collection: 2.323s, learning 0.205s)
             Mean action noise std: 3.15
          Mean value_function loss: 221.7900
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 65.8083
                       Mean reward: 828.40
               Mean episode length: 222.95
    Episode_Reward/reaching_object: 1.5019
     Episode_Reward/lifting_object: 166.3242
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0787
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 2.53s
                      Time elapsed: 01:02:18
                               ETA: 00:20:52

################################################################################
                     [1m Learning iteration 1499/2000 [0m                     

                       Computation: 37409 steps/s (collection: 2.446s, learning 0.182s)
             Mean action noise std: 3.15
          Mean value_function loss: 256.4930
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 65.8168
                       Mean reward: 781.74
               Mean episode length: 212.17
    Episode_Reward/reaching_object: 1.4256
     Episode_Reward/lifting_object: 157.5943
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 2.63s
                      Time elapsed: 01:02:21
                               ETA: 00:20:49

################################################################################
                     [1m Learning iteration 1500/2000 [0m                     

                       Computation: 37641 steps/s (collection: 2.434s, learning 0.177s)
             Mean action noise std: 3.16
          Mean value_function loss: 258.9469
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 65.8249
                       Mean reward: 816.13
               Mean episode length: 219.89
    Episode_Reward/reaching_object: 1.4861
     Episode_Reward/lifting_object: 164.7899
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0778
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 2.61s
                      Time elapsed: 01:02:23
                               ETA: 00:20:47

################################################################################
                     [1m Learning iteration 1501/2000 [0m                     

                       Computation: 38034 steps/s (collection: 2.447s, learning 0.138s)
             Mean action noise std: 3.16
          Mean value_function loss: 219.7123
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 65.8377
                       Mean reward: 830.88
               Mean episode length: 223.10
    Episode_Reward/reaching_object: 1.5015
     Episode_Reward/lifting_object: 167.7783
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0790
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 2.58s
                      Time elapsed: 01:02:26
                               ETA: 00:20:44

################################################################################
                     [1m Learning iteration 1502/2000 [0m                     

                       Computation: 36464 steps/s (collection: 2.487s, learning 0.209s)
             Mean action noise std: 3.16
          Mean value_function loss: 212.9759
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 65.8430
                       Mean reward: 852.47
               Mean episode length: 228.70
    Episode_Reward/reaching_object: 1.4871
     Episode_Reward/lifting_object: 165.1124
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0776
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 2.70s
                      Time elapsed: 01:02:29
                               ETA: 00:20:42

################################################################################
                     [1m Learning iteration 1503/2000 [0m                     

                       Computation: 39533 steps/s (collection: 2.363s, learning 0.124s)
             Mean action noise std: 3.16
          Mean value_function loss: 227.8629
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 65.8478
                       Mean reward: 807.81
               Mean episode length: 219.91
    Episode_Reward/reaching_object: 1.4799
     Episode_Reward/lifting_object: 164.5378
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0781
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 2.49s
                      Time elapsed: 01:02:31
                               ETA: 00:20:39

################################################################################
                     [1m Learning iteration 1504/2000 [0m                     

                       Computation: 42782 steps/s (collection: 2.168s, learning 0.130s)
             Mean action noise std: 3.16
          Mean value_function loss: 237.9208
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 65.8536
                       Mean reward: 798.23
               Mean episode length: 215.45
    Episode_Reward/reaching_object: 1.4639
     Episode_Reward/lifting_object: 162.8396
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0549
          Episode_Reward/joint_vel: -0.0762
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 2.30s
                      Time elapsed: 01:02:33
                               ETA: 00:20:37

################################################################################
                     [1m Learning iteration 1505/2000 [0m                     

                       Computation: 39136 steps/s (collection: 2.410s, learning 0.102s)
             Mean action noise std: 3.16
          Mean value_function loss: 221.3489
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 65.8596
                       Mean reward: 807.80
               Mean episode length: 218.19
    Episode_Reward/reaching_object: 1.4503
     Episode_Reward/lifting_object: 160.4836
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0763
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 2.51s
                      Time elapsed: 01:02:36
                               ETA: 00:20:34

################################################################################
                     [1m Learning iteration 1506/2000 [0m                     

                       Computation: 43741 steps/s (collection: 2.145s, learning 0.103s)
             Mean action noise std: 3.16
          Mean value_function loss: 239.4425
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 65.8649
                       Mean reward: 771.35
               Mean episode length: 212.56
    Episode_Reward/reaching_object: 1.4447
     Episode_Reward/lifting_object: 159.7118
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0758
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 2.25s
                      Time elapsed: 01:02:38
                               ETA: 00:20:32

################################################################################
                     [1m Learning iteration 1507/2000 [0m                     

                       Computation: 43822 steps/s (collection: 2.140s, learning 0.103s)
             Mean action noise std: 3.16
          Mean value_function loss: 197.7941
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 65.8683
                       Mean reward: 849.32
               Mean episode length: 229.78
    Episode_Reward/reaching_object: 1.4420
     Episode_Reward/lifting_object: 160.0418
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0763
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 2.24s
                      Time elapsed: 01:02:40
                               ETA: 00:20:29

################################################################################
                     [1m Learning iteration 1508/2000 [0m                     

                       Computation: 42959 steps/s (collection: 2.112s, learning 0.177s)
             Mean action noise std: 3.16
          Mean value_function loss: 213.5847
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 65.8702
                       Mean reward: 835.04
               Mean episode length: 225.44
    Episode_Reward/reaching_object: 1.4946
     Episode_Reward/lifting_object: 166.2925
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0771
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 2.29s
                      Time elapsed: 01:02:43
                               ETA: 00:20:26

################################################################################
                     [1m Learning iteration 1509/2000 [0m                     

                       Computation: 41701 steps/s (collection: 2.248s, learning 0.109s)
             Mean action noise std: 3.16
          Mean value_function loss: 234.9165
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 65.8717
                       Mean reward: 820.58
               Mean episode length: 220.92
    Episode_Reward/reaching_object: 1.4482
     Episode_Reward/lifting_object: 160.8423
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0761
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 2.36s
                      Time elapsed: 01:02:45
                               ETA: 00:20:24

################################################################################
                     [1m Learning iteration 1510/2000 [0m                     

                       Computation: 42196 steps/s (collection: 2.200s, learning 0.130s)
             Mean action noise std: 3.16
          Mean value_function loss: 221.6326
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 65.8759
                       Mean reward: 820.15
               Mean episode length: 222.68
    Episode_Reward/reaching_object: 1.4754
     Episode_Reward/lifting_object: 164.1847
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0769
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 2.33s
                      Time elapsed: 01:02:47
                               ETA: 00:20:21

################################################################################
                     [1m Learning iteration 1511/2000 [0m                     

                       Computation: 40308 steps/s (collection: 2.269s, learning 0.170s)
             Mean action noise std: 3.16
          Mean value_function loss: 204.0413
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 65.8906
                       Mean reward: 826.26
               Mean episode length: 223.85
    Episode_Reward/reaching_object: 1.4871
     Episode_Reward/lifting_object: 165.1089
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0564
          Episode_Reward/joint_vel: -0.0777
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 2.44s
                      Time elapsed: 01:02:50
                               ETA: 00:20:19

################################################################################
                     [1m Learning iteration 1512/2000 [0m                     

                       Computation: 40153 steps/s (collection: 2.320s, learning 0.129s)
             Mean action noise std: 3.17
          Mean value_function loss: 192.0040
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 65.9094
                       Mean reward: 849.40
               Mean episode length: 228.91
    Episode_Reward/reaching_object: 1.4870
     Episode_Reward/lifting_object: 165.1423
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0779
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 2.45s
                      Time elapsed: 01:02:52
                               ETA: 00:20:16

################################################################################
                     [1m Learning iteration 1513/2000 [0m                     

                       Computation: 42232 steps/s (collection: 2.197s, learning 0.131s)
             Mean action noise std: 3.17
          Mean value_function loss: 188.6965
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 65.9189
                       Mean reward: 823.92
               Mean episode length: 220.87
    Episode_Reward/reaching_object: 1.4701
     Episode_Reward/lifting_object: 163.7244
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0767
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 2.33s
                      Time elapsed: 01:02:55
                               ETA: 00:20:14

################################################################################
                     [1m Learning iteration 1514/2000 [0m                     

                       Computation: 40968 steps/s (collection: 2.254s, learning 0.145s)
             Mean action noise std: 3.17
          Mean value_function loss: 202.9880
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 65.9284
                       Mean reward: 840.59
               Mean episode length: 227.84
    Episode_Reward/reaching_object: 1.4614
     Episode_Reward/lifting_object: 161.7081
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0770
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 2.40s
                      Time elapsed: 01:02:57
                               ETA: 00:20:11

################################################################################
                     [1m Learning iteration 1515/2000 [0m                     

                       Computation: 42297 steps/s (collection: 2.177s, learning 0.147s)
             Mean action noise std: 3.17
          Mean value_function loss: 182.4415
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 65.9340
                       Mean reward: 802.84
               Mean episode length: 217.33
    Episode_Reward/reaching_object: 1.5125
     Episode_Reward/lifting_object: 168.0135
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0780
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 2.32s
                      Time elapsed: 01:02:59
                               ETA: 00:20:09

################################################################################
                     [1m Learning iteration 1516/2000 [0m                     

                       Computation: 39819 steps/s (collection: 2.256s, learning 0.213s)
             Mean action noise std: 3.17
          Mean value_function loss: 211.9209
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 65.9393
                       Mean reward: 811.00
               Mean episode length: 218.58
    Episode_Reward/reaching_object: 1.4810
     Episode_Reward/lifting_object: 163.7142
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0769
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 2.47s
                      Time elapsed: 01:03:02
                               ETA: 00:20:06

################################################################################
                     [1m Learning iteration 1517/2000 [0m                     

                       Computation: 42290 steps/s (collection: 2.159s, learning 0.165s)
             Mean action noise std: 3.17
          Mean value_function loss: 199.6159
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 65.9466
                       Mean reward: 872.06
               Mean episode length: 235.04
    Episode_Reward/reaching_object: 1.4794
     Episode_Reward/lifting_object: 163.8148
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0783
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 2.32s
                      Time elapsed: 01:03:04
                               ETA: 00:20:04

################################################################################
                     [1m Learning iteration 1518/2000 [0m                     

                       Computation: 42408 steps/s (collection: 2.180s, learning 0.138s)
             Mean action noise std: 3.17
          Mean value_function loss: 188.7363
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 65.9542
                       Mean reward: 827.30
               Mean episode length: 222.45
    Episode_Reward/reaching_object: 1.4767
     Episode_Reward/lifting_object: 163.2962
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0775
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 2.32s
                      Time elapsed: 01:03:06
                               ETA: 00:20:01

################################################################################
                     [1m Learning iteration 1519/2000 [0m                     

                       Computation: 40228 steps/s (collection: 2.253s, learning 0.191s)
             Mean action noise std: 3.17
          Mean value_function loss: 150.5297
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 65.9610
                       Mean reward: 845.40
               Mean episode length: 226.46
    Episode_Reward/reaching_object: 1.5178
     Episode_Reward/lifting_object: 169.0245
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0795
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 2.44s
                      Time elapsed: 01:03:09
                               ETA: 00:19:59

################################################################################
                     [1m Learning iteration 1520/2000 [0m                     

                       Computation: 42834 steps/s (collection: 2.164s, learning 0.131s)
             Mean action noise std: 3.17
          Mean value_function loss: 191.8160
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 65.9693
                       Mean reward: 821.65
               Mean episode length: 222.09
    Episode_Reward/reaching_object: 1.5025
     Episode_Reward/lifting_object: 166.7229
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0795
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 2.29s
                      Time elapsed: 01:03:11
                               ETA: 00:19:56

################################################################################
                     [1m Learning iteration 1521/2000 [0m                     

                       Computation: 41328 steps/s (collection: 2.216s, learning 0.163s)
             Mean action noise std: 3.17
          Mean value_function loss: 253.3373
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 65.9761
                       Mean reward: 830.20
               Mean episode length: 223.75
    Episode_Reward/reaching_object: 1.5081
     Episode_Reward/lifting_object: 167.0243
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0791
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 2.38s
                      Time elapsed: 01:03:14
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 1522/2000 [0m                     

                       Computation: 42902 steps/s (collection: 2.160s, learning 0.132s)
             Mean action noise std: 3.18
          Mean value_function loss: 238.6974
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 65.9861
                       Mean reward: 784.56
               Mean episode length: 212.18
    Episode_Reward/reaching_object: 1.4582
     Episode_Reward/lifting_object: 161.9223
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0766
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 2.29s
                      Time elapsed: 01:03:16
                               ETA: 00:19:51

################################################################################
                     [1m Learning iteration 1523/2000 [0m                     

                       Computation: 40956 steps/s (collection: 2.296s, learning 0.104s)
             Mean action noise std: 3.18
          Mean value_function loss: 236.0580
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 65.9963
                       Mean reward: 867.40
               Mean episode length: 231.41
    Episode_Reward/reaching_object: 1.4737
     Episode_Reward/lifting_object: 162.9592
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0782
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 2.40s
                      Time elapsed: 01:03:18
                               ETA: 00:19:48

################################################################################
                     [1m Learning iteration 1524/2000 [0m                     

                       Computation: 40878 steps/s (collection: 2.249s, learning 0.156s)
             Mean action noise std: 3.18
          Mean value_function loss: 220.2080
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 65.9997
                       Mean reward: 805.03
               Mean episode length: 217.82
    Episode_Reward/reaching_object: 1.4844
     Episode_Reward/lifting_object: 163.1699
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0789
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 2.40s
                      Time elapsed: 01:03:21
                               ETA: 00:19:46

################################################################################
                     [1m Learning iteration 1525/2000 [0m                     

                       Computation: 40623 steps/s (collection: 2.278s, learning 0.142s)
             Mean action noise std: 3.18
          Mean value_function loss: 259.1383
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 66.0022
                       Mean reward: 784.29
               Mean episode length: 213.40
    Episode_Reward/reaching_object: 1.4645
     Episode_Reward/lifting_object: 161.7029
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0780
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 2.42s
                      Time elapsed: 01:03:23
                               ETA: 00:19:43

################################################################################
                     [1m Learning iteration 1526/2000 [0m                     

                       Computation: 42298 steps/s (collection: 2.181s, learning 0.143s)
             Mean action noise std: 3.18
          Mean value_function loss: 242.7815
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 66.0041
                       Mean reward: 831.68
               Mean episode length: 222.78
    Episode_Reward/reaching_object: 1.4832
     Episode_Reward/lifting_object: 163.7101
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0784
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 2.32s
                      Time elapsed: 01:03:25
                               ETA: 00:19:41

################################################################################
                     [1m Learning iteration 1527/2000 [0m                     

                       Computation: 39389 steps/s (collection: 2.387s, learning 0.109s)
             Mean action noise std: 3.18
          Mean value_function loss: 233.5322
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 66.0093
                       Mean reward: 821.16
               Mean episode length: 222.84
    Episode_Reward/reaching_object: 1.4869
     Episode_Reward/lifting_object: 163.9654
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0564
          Episode_Reward/joint_vel: -0.0787
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 2.50s
                      Time elapsed: 01:03:28
                               ETA: 00:19:38

################################################################################
                     [1m Learning iteration 1528/2000 [0m                     

                       Computation: 38832 steps/s (collection: 2.417s, learning 0.114s)
             Mean action noise std: 3.18
          Mean value_function loss: 258.6835
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 66.0133
                       Mean reward: 835.77
               Mean episode length: 223.83
    Episode_Reward/reaching_object: 1.4660
     Episode_Reward/lifting_object: 162.4038
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0777
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 2.53s
                      Time elapsed: 01:03:30
                               ETA: 00:19:36

################################################################################
                     [1m Learning iteration 1529/2000 [0m                     

                       Computation: 41060 steps/s (collection: 2.236s, learning 0.158s)
             Mean action noise std: 3.18
          Mean value_function loss: 213.6550
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 66.0195
                       Mean reward: 853.93
               Mean episode length: 229.07
    Episode_Reward/reaching_object: 1.5074
     Episode_Reward/lifting_object: 166.9942
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0797
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 2.39s
                      Time elapsed: 01:03:33
                               ETA: 00:19:33

################################################################################
                     [1m Learning iteration 1530/2000 [0m                     

                       Computation: 41987 steps/s (collection: 2.219s, learning 0.122s)
             Mean action noise std: 3.18
          Mean value_function loss: 217.6437
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 66.0356
                       Mean reward: 814.53
               Mean episode length: 220.05
    Episode_Reward/reaching_object: 1.4756
     Episode_Reward/lifting_object: 163.3288
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0786
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 2.34s
                      Time elapsed: 01:03:35
                               ETA: 00:19:31

################################################################################
                     [1m Learning iteration 1531/2000 [0m                     

                       Computation: 40330 steps/s (collection: 2.311s, learning 0.126s)
             Mean action noise std: 3.19
          Mean value_function loss: 198.8539
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 66.0556
                       Mean reward: 878.46
               Mean episode length: 235.79
    Episode_Reward/reaching_object: 1.4944
     Episode_Reward/lifting_object: 165.1215
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0799
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 2.44s
                      Time elapsed: 01:03:38
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 1532/2000 [0m                     

                       Computation: 40091 steps/s (collection: 2.258s, learning 0.194s)
             Mean action noise std: 3.19
          Mean value_function loss: 217.2710
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 66.0656
                       Mean reward: 861.14
               Mean episode length: 230.85
    Episode_Reward/reaching_object: 1.4987
     Episode_Reward/lifting_object: 165.8594
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0799
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 2.45s
                      Time elapsed: 01:03:40
                               ETA: 00:19:26

################################################################################
                     [1m Learning iteration 1533/2000 [0m                     

                       Computation: 39235 steps/s (collection: 2.348s, learning 0.157s)
             Mean action noise std: 3.19
          Mean value_function loss: 242.7104
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 66.0746
                       Mean reward: 823.67
               Mean episode length: 222.05
    Episode_Reward/reaching_object: 1.5010
     Episode_Reward/lifting_object: 165.3420
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0572
          Episode_Reward/joint_vel: -0.0804
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 2.51s
                      Time elapsed: 01:03:43
                               ETA: 00:19:23

################################################################################
                     [1m Learning iteration 1534/2000 [0m                     

                       Computation: 39594 steps/s (collection: 2.301s, learning 0.182s)
             Mean action noise std: 3.19
          Mean value_function loss: 224.3130
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 66.0845
                       Mean reward: 832.83
               Mean episode length: 223.50
    Episode_Reward/reaching_object: 1.4850
     Episode_Reward/lifting_object: 164.4263
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0795
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 2.48s
                      Time elapsed: 01:03:45
                               ETA: 00:19:21

################################################################################
                     [1m Learning iteration 1535/2000 [0m                     

                       Computation: 40401 steps/s (collection: 2.302s, learning 0.131s)
             Mean action noise std: 3.19
          Mean value_function loss: 199.4274
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 66.0900
                       Mean reward: 884.12
               Mean episode length: 237.19
    Episode_Reward/reaching_object: 1.4972
     Episode_Reward/lifting_object: 165.8758
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0794
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 2.43s
                      Time elapsed: 01:03:48
                               ETA: 00:19:18

################################################################################
                     [1m Learning iteration 1536/2000 [0m                     

                       Computation: 41093 steps/s (collection: 2.229s, learning 0.163s)
             Mean action noise std: 3.19
          Mean value_function loss: 201.2880
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 66.1010
                       Mean reward: 828.59
               Mean episode length: 222.65
    Episode_Reward/reaching_object: 1.4765
     Episode_Reward/lifting_object: 164.2674
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0564
          Episode_Reward/joint_vel: -0.0784
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 2.39s
                      Time elapsed: 01:03:50
                               ETA: 00:19:16

################################################################################
                     [1m Learning iteration 1537/2000 [0m                     

                       Computation: 40566 steps/s (collection: 2.260s, learning 0.163s)
             Mean action noise std: 3.19
          Mean value_function loss: 219.4845
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 66.1117
                       Mean reward: 819.64
               Mean episode length: 221.54
    Episode_Reward/reaching_object: 1.4737
     Episode_Reward/lifting_object: 163.0566
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0564
          Episode_Reward/joint_vel: -0.0793
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 2.42s
                      Time elapsed: 01:03:52
                               ETA: 00:19:13

################################################################################
                     [1m Learning iteration 1538/2000 [0m                     

                       Computation: 42074 steps/s (collection: 2.216s, learning 0.121s)
             Mean action noise std: 3.19
          Mean value_function loss: 205.7417
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 66.1226
                       Mean reward: 827.39
               Mean episode length: 223.52
    Episode_Reward/reaching_object: 1.4668
     Episode_Reward/lifting_object: 162.4818
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0565
          Episode_Reward/joint_vel: -0.0798
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 2.34s
                      Time elapsed: 01:03:55
                               ETA: 00:19:11

################################################################################
                     [1m Learning iteration 1539/2000 [0m                     

                       Computation: 42061 steps/s (collection: 2.196s, learning 0.142s)
             Mean action noise std: 3.19
          Mean value_function loss: 181.0769
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 66.1308
                       Mean reward: 836.48
               Mean episode length: 225.94
    Episode_Reward/reaching_object: 1.4673
     Episode_Reward/lifting_object: 161.6660
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0800
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 2.34s
                      Time elapsed: 01:03:57
                               ETA: 00:19:08

################################################################################
                     [1m Learning iteration 1540/2000 [0m                     

                       Computation: 41158 steps/s (collection: 2.251s, learning 0.137s)
             Mean action noise std: 3.20
          Mean value_function loss: 185.7243
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 66.1420
                       Mean reward: 835.42
               Mean episode length: 227.59
    Episode_Reward/reaching_object: 1.4920
     Episode_Reward/lifting_object: 165.3349
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 2.39s
                      Time elapsed: 01:03:59
                               ETA: 00:19:06

################################################################################
                     [1m Learning iteration 1541/2000 [0m                     

                       Computation: 42649 steps/s (collection: 2.174s, learning 0.131s)
             Mean action noise std: 3.20
          Mean value_function loss: 158.0673
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 66.1567
                       Mean reward: 843.65
               Mean episode length: 227.14
    Episode_Reward/reaching_object: 1.5202
     Episode_Reward/lifting_object: 169.8840
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0584
          Episode_Reward/joint_vel: -0.0818
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 2.30s
                      Time elapsed: 01:04:02
                               ETA: 00:19:03

################################################################################
                     [1m Learning iteration 1542/2000 [0m                     

                       Computation: 40344 steps/s (collection: 2.270s, learning 0.167s)
             Mean action noise std: 3.20
          Mean value_function loss: 215.2792
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 66.1739
                       Mean reward: 801.88
               Mean episode length: 215.13
    Episode_Reward/reaching_object: 1.4921
     Episode_Reward/lifting_object: 166.7751
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0572
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 2.44s
                      Time elapsed: 01:04:04
                               ETA: 00:19:01

################################################################################
                     [1m Learning iteration 1543/2000 [0m                     

                       Computation: 40947 steps/s (collection: 2.280s, learning 0.121s)
             Mean action noise std: 3.20
          Mean value_function loss: 188.3277
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 66.1813
                       Mean reward: 855.89
               Mean episode length: 229.86
    Episode_Reward/reaching_object: 1.4797
     Episode_Reward/lifting_object: 164.8500
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0799
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 2.40s
                      Time elapsed: 01:04:07
                               ETA: 00:18:58

################################################################################
                     [1m Learning iteration 1544/2000 [0m                     

                       Computation: 41347 steps/s (collection: 2.212s, learning 0.166s)
             Mean action noise std: 3.20
          Mean value_function loss: 167.9158
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 66.1853
                       Mean reward: 845.89
               Mean episode length: 227.87
    Episode_Reward/reaching_object: 1.5200
     Episode_Reward/lifting_object: 169.2213
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0825
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 2.38s
                      Time elapsed: 01:04:09
                               ETA: 00:18:56

################################################################################
                     [1m Learning iteration 1545/2000 [0m                     

                       Computation: 38904 steps/s (collection: 2.426s, learning 0.101s)
             Mean action noise std: 3.20
          Mean value_function loss: 205.7357
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 66.1947
                       Mean reward: 820.68
               Mean episode length: 221.83
    Episode_Reward/reaching_object: 1.5077
     Episode_Reward/lifting_object: 168.0157
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0830
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 2.53s
                      Time elapsed: 01:04:11
                               ETA: 00:18:53

################################################################################
                     [1m Learning iteration 1546/2000 [0m                     

                       Computation: 37254 steps/s (collection: 2.511s, learning 0.128s)
             Mean action noise std: 3.21
          Mean value_function loss: 223.3234
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 66.2133
                       Mean reward: 828.72
               Mean episode length: 222.97
    Episode_Reward/reaching_object: 1.4936
     Episode_Reward/lifting_object: 166.6625
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0813
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 2.64s
                      Time elapsed: 01:04:14
                               ETA: 00:18:51

################################################################################
                     [1m Learning iteration 1547/2000 [0m                     

                       Computation: 40103 steps/s (collection: 2.300s, learning 0.152s)
             Mean action noise std: 3.21
          Mean value_function loss: 227.1459
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 66.2239
                       Mean reward: 817.18
               Mean episode length: 220.53
    Episode_Reward/reaching_object: 1.4532
     Episode_Reward/lifting_object: 161.8127
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0797
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 2.45s
                      Time elapsed: 01:04:17
                               ETA: 00:18:48

################################################################################
                     [1m Learning iteration 1548/2000 [0m                     

                       Computation: 36815 steps/s (collection: 2.475s, learning 0.196s)
             Mean action noise std: 3.21
          Mean value_function loss: 267.1803
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 66.2282
                       Mean reward: 808.41
               Mean episode length: 219.31
    Episode_Reward/reaching_object: 1.4569
     Episode_Reward/lifting_object: 161.4921
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0801
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 2.67s
                      Time elapsed: 01:04:19
                               ETA: 00:18:46

################################################################################
                     [1m Learning iteration 1549/2000 [0m                     

                       Computation: 40064 steps/s (collection: 2.296s, learning 0.158s)
             Mean action noise std: 3.21
          Mean value_function loss: 190.8419
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 66.2405
                       Mean reward: 850.15
               Mean episode length: 231.20
    Episode_Reward/reaching_object: 1.5061
     Episode_Reward/lifting_object: 166.8373
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0828
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 2.45s
                      Time elapsed: 01:04:22
                               ETA: 00:18:43

################################################################################
                     [1m Learning iteration 1550/2000 [0m                     

                       Computation: 37250 steps/s (collection: 2.416s, learning 0.223s)
             Mean action noise std: 3.21
          Mean value_function loss: 218.8926
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 66.2575
                       Mean reward: 825.76
               Mean episode length: 221.91
    Episode_Reward/reaching_object: 1.4693
     Episode_Reward/lifting_object: 163.9657
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0806
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 2.64s
                      Time elapsed: 01:04:24
                               ETA: 00:18:41

################################################################################
                     [1m Learning iteration 1551/2000 [0m                     

                       Computation: 31984 steps/s (collection: 2.814s, learning 0.259s)
             Mean action noise std: 3.21
          Mean value_function loss: 182.0594
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 66.2676
                       Mean reward: 822.11
               Mean episode length: 221.25
    Episode_Reward/reaching_object: 1.4526
     Episode_Reward/lifting_object: 161.4146
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0800
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 3.07s
                      Time elapsed: 01:04:27
                               ETA: 00:18:38

################################################################################
                     [1m Learning iteration 1552/2000 [0m                     

                       Computation: 31643 steps/s (collection: 2.947s, learning 0.159s)
             Mean action noise std: 3.21
          Mean value_function loss: 175.0753
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 66.2764
                       Mean reward: 829.80
               Mean episode length: 222.88
    Episode_Reward/reaching_object: 1.4794
     Episode_Reward/lifting_object: 164.7340
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0810
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 3.11s
                      Time elapsed: 01:04:30
                               ETA: 00:18:36

################################################################################
                     [1m Learning iteration 1553/2000 [0m                     

                       Computation: 37043 steps/s (collection: 2.533s, learning 0.121s)
             Mean action noise std: 3.21
          Mean value_function loss: 218.2958
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 66.2810
                       Mean reward: 843.88
               Mean episode length: 226.27
    Episode_Reward/reaching_object: 1.4865
     Episode_Reward/lifting_object: 166.1344
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0818
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 2.65s
                      Time elapsed: 01:04:33
                               ETA: 00:18:34

################################################################################
                     [1m Learning iteration 1554/2000 [0m                     

                       Computation: 37391 steps/s (collection: 2.442s, learning 0.187s)
             Mean action noise std: 3.22
          Mean value_function loss: 187.4108
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 66.2905
                       Mean reward: 866.31
               Mean episode length: 231.58
    Episode_Reward/reaching_object: 1.4964
     Episode_Reward/lifting_object: 166.3442
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 2.63s
                      Time elapsed: 01:04:36
                               ETA: 00:18:31

################################################################################
                     [1m Learning iteration 1555/2000 [0m                     

                       Computation: 38617 steps/s (collection: 2.372s, learning 0.174s)
             Mean action noise std: 3.22
          Mean value_function loss: 176.0443
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 66.3031
                       Mean reward: 870.37
               Mean episode length: 234.46
    Episode_Reward/reaching_object: 1.4964
     Episode_Reward/lifting_object: 166.8776
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0816
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 2.55s
                      Time elapsed: 01:04:38
                               ETA: 00:18:29

################################################################################
                     [1m Learning iteration 1556/2000 [0m                     

                       Computation: 37703 steps/s (collection: 2.464s, learning 0.144s)
             Mean action noise std: 3.22
          Mean value_function loss: 156.9869
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 66.3113
                       Mean reward: 854.84
               Mean episode length: 228.27
    Episode_Reward/reaching_object: 1.5155
     Episode_Reward/lifting_object: 168.5822
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0825
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 2.61s
                      Time elapsed: 01:04:41
                               ETA: 00:18:26

################################################################################
                     [1m Learning iteration 1557/2000 [0m                     

                       Computation: 35663 steps/s (collection: 2.602s, learning 0.154s)
             Mean action noise std: 3.22
          Mean value_function loss: 212.8497
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 66.3145
                       Mean reward: 820.12
               Mean episode length: 220.58
    Episode_Reward/reaching_object: 1.4454
     Episode_Reward/lifting_object: 160.6264
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0793
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 2.76s
                      Time elapsed: 01:04:44
                               ETA: 00:18:24

################################################################################
                     [1m Learning iteration 1558/2000 [0m                     

                       Computation: 38707 steps/s (collection: 2.352s, learning 0.188s)
             Mean action noise std: 3.22
          Mean value_function loss: 170.3615
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 66.3190
                       Mean reward: 876.62
               Mean episode length: 234.66
    Episode_Reward/reaching_object: 1.5318
     Episode_Reward/lifting_object: 170.5709
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0596
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 2.54s
                      Time elapsed: 01:04:46
                               ETA: 00:18:21

################################################################################
                     [1m Learning iteration 1559/2000 [0m                     

                       Computation: 39154 steps/s (collection: 2.369s, learning 0.142s)
             Mean action noise std: 3.22
          Mean value_function loss: 169.9883
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 66.3226
                       Mean reward: 843.44
               Mean episode length: 226.77
    Episode_Reward/reaching_object: 1.5475
     Episode_Reward/lifting_object: 172.5938
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 2.51s
                      Time elapsed: 01:04:49
                               ETA: 00:18:19

################################################################################
                     [1m Learning iteration 1560/2000 [0m                     

                       Computation: 38716 steps/s (collection: 2.405s, learning 0.134s)
             Mean action noise std: 3.22
          Mean value_function loss: 186.3672
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 66.3282
                       Mean reward: 865.90
               Mean episode length: 232.44
    Episode_Reward/reaching_object: 1.4851
     Episode_Reward/lifting_object: 165.5255
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0810
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 2.54s
                      Time elapsed: 01:04:51
                               ETA: 00:18:16

################################################################################
                     [1m Learning iteration 1561/2000 [0m                     

                       Computation: 35987 steps/s (collection: 2.495s, learning 0.237s)
             Mean action noise std: 3.22
          Mean value_function loss: 212.4842
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 66.3313
                       Mean reward: 870.79
               Mean episode length: 231.75
    Episode_Reward/reaching_object: 1.5056
     Episode_Reward/lifting_object: 167.2301
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0812
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 2.73s
                      Time elapsed: 01:04:54
                               ETA: 00:18:14

################################################################################
                     [1m Learning iteration 1562/2000 [0m                     

                       Computation: 37130 steps/s (collection: 2.462s, learning 0.186s)
             Mean action noise std: 3.22
          Mean value_function loss: 237.0268
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 66.3359
                       Mean reward: 813.34
               Mean episode length: 218.38
    Episode_Reward/reaching_object: 1.4940
     Episode_Reward/lifting_object: 165.9088
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0811
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 2.65s
                      Time elapsed: 01:04:57
                               ETA: 00:18:12

################################################################################
                     [1m Learning iteration 1563/2000 [0m                     

                       Computation: 39893 steps/s (collection: 2.310s, learning 0.154s)
             Mean action noise std: 3.22
          Mean value_function loss: 206.9351
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 66.3456
                       Mean reward: 859.13
               Mean episode length: 232.48
    Episode_Reward/reaching_object: 1.5169
     Episode_Reward/lifting_object: 167.7118
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0830
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 2.46s
                      Time elapsed: 01:04:59
                               ETA: 00:18:09

################################################################################
                     [1m Learning iteration 1564/2000 [0m                     

                       Computation: 38120 steps/s (collection: 2.429s, learning 0.150s)
             Mean action noise std: 3.22
          Mean value_function loss: 220.4968
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 66.3564
                       Mean reward: 839.59
               Mean episode length: 225.02
    Episode_Reward/reaching_object: 1.4864
     Episode_Reward/lifting_object: 164.3720
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0576
          Episode_Reward/joint_vel: -0.0808
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 2.58s
                      Time elapsed: 01:05:02
                               ETA: 00:18:07

################################################################################
                     [1m Learning iteration 1565/2000 [0m                     

                       Computation: 39282 steps/s (collection: 2.331s, learning 0.171s)
             Mean action noise std: 3.23
          Mean value_function loss: 242.9388
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 66.3638
                       Mean reward: 828.35
               Mean episode length: 223.74
    Episode_Reward/reaching_object: 1.4992
     Episode_Reward/lifting_object: 166.1954
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0821
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 2.50s
                      Time elapsed: 01:05:04
                               ETA: 00:18:04

################################################################################
                     [1m Learning iteration 1566/2000 [0m                     

                       Computation: 40538 steps/s (collection: 2.260s, learning 0.165s)
             Mean action noise std: 3.23
          Mean value_function loss: 249.3014
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 66.3742
                       Mean reward: 792.07
               Mean episode length: 215.60
    Episode_Reward/reaching_object: 1.4539
     Episode_Reward/lifting_object: 159.8727
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 2.42s
                      Time elapsed: 01:05:07
                               ETA: 00:18:02

################################################################################
                     [1m Learning iteration 1567/2000 [0m                     

                       Computation: 38713 steps/s (collection: 2.412s, learning 0.127s)
             Mean action noise std: 3.23
          Mean value_function loss: 178.3828
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 66.3840
                       Mean reward: 833.50
               Mean episode length: 223.63
    Episode_Reward/reaching_object: 1.5079
     Episode_Reward/lifting_object: 166.7330
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0584
          Episode_Reward/joint_vel: -0.0819
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 2.54s
                      Time elapsed: 01:05:09
                               ETA: 00:17:59

################################################################################
                     [1m Learning iteration 1568/2000 [0m                     

                       Computation: 39683 steps/s (collection: 2.344s, learning 0.134s)
             Mean action noise std: 3.23
          Mean value_function loss: 248.1089
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 66.3899
                       Mean reward: 813.67
               Mean episode length: 218.40
    Episode_Reward/reaching_object: 1.4776
     Episode_Reward/lifting_object: 163.5521
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0805
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 2.48s
                      Time elapsed: 01:05:12
                               ETA: 00:17:57

################################################################################
                     [1m Learning iteration 1569/2000 [0m                     

                       Computation: 38470 steps/s (collection: 2.387s, learning 0.168s)
             Mean action noise std: 3.23
          Mean value_function loss: 228.8270
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 66.3949
                       Mean reward: 858.55
               Mean episode length: 231.25
    Episode_Reward/reaching_object: 1.4809
     Episode_Reward/lifting_object: 162.8580
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0576
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 2.56s
                      Time elapsed: 01:05:14
                               ETA: 00:17:54

################################################################################
                     [1m Learning iteration 1570/2000 [0m                     

                       Computation: 39819 steps/s (collection: 2.357s, learning 0.112s)
             Mean action noise std: 3.23
          Mean value_function loss: 211.1425
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 66.4010
                       Mean reward: 836.12
               Mean episode length: 224.82
    Episode_Reward/reaching_object: 1.4620
     Episode_Reward/lifting_object: 161.3115
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0798
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 2.47s
                      Time elapsed: 01:05:17
                               ETA: 00:17:52

################################################################################
                     [1m Learning iteration 1571/2000 [0m                     

                       Computation: 37839 steps/s (collection: 2.424s, learning 0.174s)
             Mean action noise std: 3.23
          Mean value_function loss: 206.1898
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 66.4062
                       Mean reward: 835.78
               Mean episode length: 225.50
    Episode_Reward/reaching_object: 1.5012
     Episode_Reward/lifting_object: 166.1068
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0584
          Episode_Reward/joint_vel: -0.0816
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 2.60s
                      Time elapsed: 01:05:19
                               ETA: 00:17:49

################################################################################
                     [1m Learning iteration 1572/2000 [0m                     

                       Computation: 38336 steps/s (collection: 2.413s, learning 0.151s)
             Mean action noise std: 3.23
          Mean value_function loss: 221.4612
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 66.4128
                       Mean reward: 856.41
               Mean episode length: 229.73
    Episode_Reward/reaching_object: 1.4886
     Episode_Reward/lifting_object: 163.6611
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0815
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 2.56s
                      Time elapsed: 01:05:22
                               ETA: 00:17:47

################################################################################
                     [1m Learning iteration 1573/2000 [0m                     

                       Computation: 41681 steps/s (collection: 2.204s, learning 0.154s)
             Mean action noise std: 3.23
          Mean value_function loss: 206.4120
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 66.4238
                       Mean reward: 845.47
               Mean episode length: 226.23
    Episode_Reward/reaching_object: 1.4802
     Episode_Reward/lifting_object: 163.4437
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0576
          Episode_Reward/joint_vel: -0.0809
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 2.36s
                      Time elapsed: 01:05:24
                               ETA: 00:17:44

################################################################################
                     [1m Learning iteration 1574/2000 [0m                     

                       Computation: 40007 steps/s (collection: 2.321s, learning 0.137s)
             Mean action noise std: 3.24
          Mean value_function loss: 254.7052
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 66.4326
                       Mean reward: 761.53
               Mean episode length: 207.09
    Episode_Reward/reaching_object: 1.4804
     Episode_Reward/lifting_object: 162.7528
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0810
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 2.46s
                      Time elapsed: 01:05:27
                               ETA: 00:17:42

################################################################################
                     [1m Learning iteration 1575/2000 [0m                     

                       Computation: 39046 steps/s (collection: 2.308s, learning 0.210s)
             Mean action noise std: 3.24
          Mean value_function loss: 185.3798
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 66.4373
                       Mean reward: 846.21
               Mean episode length: 226.95
    Episode_Reward/reaching_object: 1.5239
     Episode_Reward/lifting_object: 168.6531
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 2.52s
                      Time elapsed: 01:05:29
                               ETA: 00:17:39

################################################################################
                     [1m Learning iteration 1576/2000 [0m                     

                       Computation: 39709 steps/s (collection: 2.307s, learning 0.169s)
             Mean action noise std: 3.24
          Mean value_function loss: 195.0571
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 66.4448
                       Mean reward: 825.06
               Mean episode length: 224.39
    Episode_Reward/reaching_object: 1.5238
     Episode_Reward/lifting_object: 167.5468
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0836
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 2.48s
                      Time elapsed: 01:05:32
                               ETA: 00:17:37

################################################################################
                     [1m Learning iteration 1577/2000 [0m                     

                       Computation: 39445 steps/s (collection: 2.308s, learning 0.184s)
             Mean action noise std: 3.24
          Mean value_function loss: 194.5875
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 66.4586
                       Mean reward: 829.52
               Mean episode length: 224.44
    Episode_Reward/reaching_object: 1.5128
     Episode_Reward/lifting_object: 167.3986
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 2.49s
                      Time elapsed: 01:05:34
                               ETA: 00:17:34

################################################################################
                     [1m Learning iteration 1578/2000 [0m                     

                       Computation: 40160 steps/s (collection: 2.331s, learning 0.117s)
             Mean action noise std: 3.24
          Mean value_function loss: 188.7991
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 66.4821
                       Mean reward: 805.39
               Mean episode length: 216.86
    Episode_Reward/reaching_object: 1.4945
     Episode_Reward/lifting_object: 165.0847
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 2.45s
                      Time elapsed: 01:05:37
                               ETA: 00:17:32

################################################################################
                     [1m Learning iteration 1579/2000 [0m                     

                       Computation: 37586 steps/s (collection: 2.450s, learning 0.165s)
             Mean action noise std: 3.24
          Mean value_function loss: 185.6830
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 66.4926
                       Mean reward: 861.84
               Mean episode length: 231.86
    Episode_Reward/reaching_object: 1.5384
     Episode_Reward/lifting_object: 170.3643
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0844
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 2.62s
                      Time elapsed: 01:05:39
                               ETA: 00:17:29

################################################################################
                     [1m Learning iteration 1580/2000 [0m                     

                       Computation: 37177 steps/s (collection: 2.515s, learning 0.130s)
             Mean action noise std: 3.25
          Mean value_function loss: 214.2567
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 66.5069
                       Mean reward: 841.07
               Mean episode length: 227.38
    Episode_Reward/reaching_object: 1.5026
     Episode_Reward/lifting_object: 166.0501
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0831
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 2.64s
                      Time elapsed: 01:05:42
                               ETA: 00:17:27

################################################################################
                     [1m Learning iteration 1581/2000 [0m                     

                       Computation: 36095 steps/s (collection: 2.543s, learning 0.181s)
             Mean action noise std: 3.25
          Mean value_function loss: 197.3979
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 66.5178
                       Mean reward: 795.97
               Mean episode length: 215.49
    Episode_Reward/reaching_object: 1.4778
     Episode_Reward/lifting_object: 163.1323
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0811
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 2.72s
                      Time elapsed: 01:05:45
                               ETA: 00:17:24

################################################################################
                     [1m Learning iteration 1582/2000 [0m                     

                       Computation: 39899 steps/s (collection: 2.303s, learning 0.161s)
             Mean action noise std: 3.25
          Mean value_function loss: 217.0167
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 66.5278
                       Mean reward: 822.97
               Mean episode length: 220.99
    Episode_Reward/reaching_object: 1.4875
     Episode_Reward/lifting_object: 164.0000
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0819
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 2.46s
                      Time elapsed: 01:05:47
                               ETA: 00:17:22

################################################################################
                     [1m Learning iteration 1583/2000 [0m                     

                       Computation: 39925 steps/s (collection: 2.287s, learning 0.176s)
             Mean action noise std: 3.25
          Mean value_function loss: 178.8473
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 66.5369
                       Mean reward: 841.30
               Mean episode length: 225.17
    Episode_Reward/reaching_object: 1.5164
     Episode_Reward/lifting_object: 168.4417
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 2.46s
                      Time elapsed: 01:05:49
                               ETA: 00:17:19

################################################################################
                     [1m Learning iteration 1584/2000 [0m                     

                       Computation: 38844 steps/s (collection: 2.396s, learning 0.135s)
             Mean action noise std: 3.25
          Mean value_function loss: 216.0523
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 66.5450
                       Mean reward: 864.19
               Mean episode length: 230.74
    Episode_Reward/reaching_object: 1.5144
     Episode_Reward/lifting_object: 167.0573
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 2.53s
                      Time elapsed: 01:05:52
                               ETA: 00:17:17

################################################################################
                     [1m Learning iteration 1585/2000 [0m                     

                       Computation: 38765 steps/s (collection: 2.375s, learning 0.161s)
             Mean action noise std: 3.25
          Mean value_function loss: 197.7049
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 66.5564
                       Mean reward: 825.33
               Mean episode length: 222.49
    Episode_Reward/reaching_object: 1.4874
     Episode_Reward/lifting_object: 164.2430
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0828
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 2.54s
                      Time elapsed: 01:05:55
                               ETA: 00:17:14

################################################################################
                     [1m Learning iteration 1586/2000 [0m                     

                       Computation: 38689 steps/s (collection: 2.352s, learning 0.189s)
             Mean action noise std: 3.25
          Mean value_function loss: 187.4685
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 66.5666
                       Mean reward: 830.74
               Mean episode length: 222.89
    Episode_Reward/reaching_object: 1.5184
     Episode_Reward/lifting_object: 168.7228
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0828
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 2.54s
                      Time elapsed: 01:05:57
                               ETA: 00:17:12

################################################################################
                     [1m Learning iteration 1587/2000 [0m                     

                       Computation: 37072 steps/s (collection: 2.537s, learning 0.115s)
             Mean action noise std: 3.25
          Mean value_function loss: 227.2546
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 66.5751
                       Mean reward: 876.41
               Mean episode length: 234.23
    Episode_Reward/reaching_object: 1.5116
     Episode_Reward/lifting_object: 167.8990
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0832
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 2.65s
                      Time elapsed: 01:06:00
                               ETA: 00:17:09

################################################################################
                     [1m Learning iteration 1588/2000 [0m                     

                       Computation: 39169 steps/s (collection: 2.335s, learning 0.175s)
             Mean action noise std: 3.25
          Mean value_function loss: 240.6769
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 66.5785
                       Mean reward: 864.74
               Mean episode length: 231.39
    Episode_Reward/reaching_object: 1.4851
     Episode_Reward/lifting_object: 164.7716
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 2.51s
                      Time elapsed: 01:06:02
                               ETA: 00:17:07

################################################################################
                     [1m Learning iteration 1589/2000 [0m                     

                       Computation: 38160 steps/s (collection: 2.385s, learning 0.192s)
             Mean action noise std: 3.25
          Mean value_function loss: 267.0667
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 66.5795
                       Mean reward: 780.23
               Mean episode length: 211.23
    Episode_Reward/reaching_object: 1.4585
     Episode_Reward/lifting_object: 161.2323
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0802
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 2.58s
                      Time elapsed: 01:06:05
                               ETA: 00:17:04

################################################################################
                     [1m Learning iteration 1590/2000 [0m                     

                       Computation: 40155 steps/s (collection: 2.293s, learning 0.155s)
             Mean action noise std: 3.26
          Mean value_function loss: 237.3722
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 66.5816
                       Mean reward: 819.48
               Mean episode length: 221.36
    Episode_Reward/reaching_object: 1.4604
     Episode_Reward/lifting_object: 161.9167
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 2.45s
                      Time elapsed: 01:06:07
                               ETA: 00:17:02

################################################################################
                     [1m Learning iteration 1591/2000 [0m                     

                       Computation: 39632 steps/s (collection: 2.318s, learning 0.163s)
             Mean action noise std: 3.26
          Mean value_function loss: 160.0084
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 66.5898
                       Mean reward: 882.94
               Mean episode length: 235.57
    Episode_Reward/reaching_object: 1.5309
     Episode_Reward/lifting_object: 170.4553
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0605
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 2.48s
                      Time elapsed: 01:06:10
                               ETA: 00:16:59

################################################################################
                     [1m Learning iteration 1592/2000 [0m                     

                       Computation: 40003 steps/s (collection: 2.276s, learning 0.181s)
             Mean action noise std: 3.26
          Mean value_function loss: 184.1119
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 66.6058
                       Mean reward: 805.61
               Mean episode length: 217.78
    Episode_Reward/reaching_object: 1.4645
     Episode_Reward/lifting_object: 162.9785
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0811
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 2.46s
                      Time elapsed: 01:06:12
                               ETA: 00:16:57

################################################################################
                     [1m Learning iteration 1593/2000 [0m                     

                       Computation: 40177 steps/s (collection: 2.307s, learning 0.140s)
             Mean action noise std: 3.26
          Mean value_function loss: 185.9631
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 66.6204
                       Mean reward: 871.52
               Mean episode length: 232.08
    Episode_Reward/reaching_object: 1.5117
     Episode_Reward/lifting_object: 167.9786
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 2.45s
                      Time elapsed: 01:06:15
                               ETA: 00:16:54

################################################################################
                     [1m Learning iteration 1594/2000 [0m                     

                       Computation: 38527 steps/s (collection: 2.379s, learning 0.173s)
             Mean action noise std: 3.26
          Mean value_function loss: 210.7030
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 66.6284
                       Mean reward: 793.95
               Mean episode length: 215.34
    Episode_Reward/reaching_object: 1.5116
     Episode_Reward/lifting_object: 167.2113
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0831
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 2.55s
                      Time elapsed: 01:06:17
                               ETA: 00:16:52

################################################################################
                     [1m Learning iteration 1595/2000 [0m                     

                       Computation: 39584 steps/s (collection: 2.347s, learning 0.136s)
             Mean action noise std: 3.26
          Mean value_function loss: 168.4359
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 66.6378
                       Mean reward: 861.11
               Mean episode length: 230.01
    Episode_Reward/reaching_object: 1.5237
     Episode_Reward/lifting_object: 169.3499
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0605
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 2.48s
                      Time elapsed: 01:06:20
                               ETA: 00:16:50

################################################################################
                     [1m Learning iteration 1596/2000 [0m                     

                       Computation: 39667 steps/s (collection: 2.278s, learning 0.201s)
             Mean action noise std: 3.27
          Mean value_function loss: 178.8506
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 66.6491
                       Mean reward: 854.64
               Mean episode length: 228.83
    Episode_Reward/reaching_object: 1.4825
     Episode_Reward/lifting_object: 164.2728
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0819
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 2.48s
                      Time elapsed: 01:06:22
                               ETA: 00:16:47

################################################################################
                     [1m Learning iteration 1597/2000 [0m                     

                       Computation: 41127 steps/s (collection: 2.265s, learning 0.125s)
             Mean action noise std: 3.27
          Mean value_function loss: 181.2533
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 66.6568
                       Mean reward: 842.00
               Mean episode length: 227.28
    Episode_Reward/reaching_object: 1.4937
     Episode_Reward/lifting_object: 165.5791
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 2.39s
                      Time elapsed: 01:06:25
                               ETA: 00:16:44

################################################################################
                     [1m Learning iteration 1598/2000 [0m                     

                       Computation: 39488 steps/s (collection: 2.354s, learning 0.135s)
             Mean action noise std: 3.27
          Mean value_function loss: 225.0644
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 66.6596
                       Mean reward: 836.73
               Mean episode length: 226.36
    Episode_Reward/reaching_object: 1.5043
     Episode_Reward/lifting_object: 166.6032
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 2.49s
                      Time elapsed: 01:06:27
                               ETA: 00:16:42

################################################################################
                     [1m Learning iteration 1599/2000 [0m                     

                       Computation: 39481 steps/s (collection: 2.314s, learning 0.176s)
             Mean action noise std: 3.27
          Mean value_function loss: 251.8132
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 66.6652
                       Mean reward: 809.89
               Mean episode length: 218.98
    Episode_Reward/reaching_object: 1.4734
     Episode_Reward/lifting_object: 163.0261
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0811
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 2.49s
                      Time elapsed: 01:06:30
                               ETA: 00:16:39

################################################################################
                     [1m Learning iteration 1600/2000 [0m                     

                       Computation: 39085 steps/s (collection: 2.380s, learning 0.135s)
             Mean action noise std: 3.27
          Mean value_function loss: 217.9093
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 66.6723
                       Mean reward: 861.17
               Mean episode length: 231.13
    Episode_Reward/reaching_object: 1.5153
     Episode_Reward/lifting_object: 167.9350
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 2.52s
                      Time elapsed: 01:06:32
                               ETA: 00:16:37

################################################################################
                     [1m Learning iteration 1601/2000 [0m                     

                       Computation: 38320 steps/s (collection: 2.380s, learning 0.185s)
             Mean action noise std: 3.27
          Mean value_function loss: 200.1024
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 66.6816
                       Mean reward: 829.55
               Mean episode length: 227.34
    Episode_Reward/reaching_object: 1.4581
     Episode_Reward/lifting_object: 160.0189
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0810
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 2.57s
                      Time elapsed: 01:06:35
                               ETA: 00:16:35

################################################################################
                     [1m Learning iteration 1602/2000 [0m                     

                       Computation: 39619 steps/s (collection: 2.302s, learning 0.180s)
             Mean action noise std: 3.27
          Mean value_function loss: 253.3412
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 66.6911
                       Mean reward: 802.92
               Mean episode length: 217.24
    Episode_Reward/reaching_object: 1.4430
     Episode_Reward/lifting_object: 159.0894
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0795
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 2.48s
                      Time elapsed: 01:06:37
                               ETA: 00:16:32

################################################################################
                     [1m Learning iteration 1603/2000 [0m                     

                       Computation: 36878 steps/s (collection: 2.450s, learning 0.215s)
             Mean action noise std: 3.27
          Mean value_function loss: 198.3689
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 66.7015
                       Mean reward: 820.52
               Mean episode length: 221.25
    Episode_Reward/reaching_object: 1.4851
     Episode_Reward/lifting_object: 164.4233
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0596
          Episode_Reward/joint_vel: -0.0812
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 2.67s
                      Time elapsed: 01:06:40
                               ETA: 00:16:30

################################################################################
                     [1m Learning iteration 1604/2000 [0m                     

                       Computation: 36772 steps/s (collection: 2.569s, learning 0.105s)
             Mean action noise std: 3.27
          Mean value_function loss: 244.8650
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 66.7085
                       Mean reward: 793.56
               Mean episode length: 216.00
    Episode_Reward/reaching_object: 1.4677
     Episode_Reward/lifting_object: 162.0924
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0809
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 2.67s
                      Time elapsed: 01:06:42
                               ETA: 00:16:27

################################################################################
                     [1m Learning iteration 1605/2000 [0m                     

                       Computation: 38527 steps/s (collection: 2.387s, learning 0.165s)
             Mean action noise std: 3.27
          Mean value_function loss: 247.0086
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 66.7183
                       Mean reward: 825.36
               Mean episode length: 221.99
    Episode_Reward/reaching_object: 1.4587
     Episode_Reward/lifting_object: 161.6276
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 2.55s
                      Time elapsed: 01:06:45
                               ETA: 00:16:25

################################################################################
                     [1m Learning iteration 1606/2000 [0m                     

                       Computation: 36479 steps/s (collection: 2.532s, learning 0.163s)
             Mean action noise std: 3.28
          Mean value_function loss: 213.8576
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 66.7284
                       Mean reward: 785.99
               Mean episode length: 213.11
    Episode_Reward/reaching_object: 1.4474
     Episode_Reward/lifting_object: 159.9124
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0794
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 2.69s
                      Time elapsed: 01:06:48
                               ETA: 00:16:22

################################################################################
                     [1m Learning iteration 1607/2000 [0m                     

                       Computation: 38205 steps/s (collection: 2.425s, learning 0.149s)
             Mean action noise std: 3.28
          Mean value_function loss: 184.0221
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 66.7310
                       Mean reward: 831.06
               Mean episode length: 223.65
    Episode_Reward/reaching_object: 1.4931
     Episode_Reward/lifting_object: 165.8398
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 2.57s
                      Time elapsed: 01:06:50
                               ETA: 00:16:20

################################################################################
                     [1m Learning iteration 1608/2000 [0m                     

                       Computation: 39366 steps/s (collection: 2.364s, learning 0.134s)
             Mean action noise std: 3.28
          Mean value_function loss: 227.6638
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 66.7366
                       Mean reward: 842.35
               Mean episode length: 226.56
    Episode_Reward/reaching_object: 1.4955
     Episode_Reward/lifting_object: 165.2029
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0819
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 2.50s
                      Time elapsed: 01:06:53
                               ETA: 00:16:17

################################################################################
                     [1m Learning iteration 1609/2000 [0m                     

                       Computation: 39869 steps/s (collection: 2.296s, learning 0.170s)
             Mean action noise std: 3.28
          Mean value_function loss: 209.7220
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 66.7500
                       Mean reward: 816.27
               Mean episode length: 222.77
    Episode_Reward/reaching_object: 1.4659
     Episode_Reward/lifting_object: 161.4832
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0813
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 2.47s
                      Time elapsed: 01:06:55
                               ETA: 00:16:15

################################################################################
                     [1m Learning iteration 1610/2000 [0m                     

                       Computation: 38638 steps/s (collection: 2.425s, learning 0.120s)
             Mean action noise std: 3.28
          Mean value_function loss: 229.6230
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 66.7716
                       Mean reward: 872.20
               Mean episode length: 234.50
    Episode_Reward/reaching_object: 1.5090
     Episode_Reward/lifting_object: 167.0023
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0823
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 2.54s
                      Time elapsed: 01:06:58
                               ETA: 00:16:12

################################################################################
                     [1m Learning iteration 1611/2000 [0m                     

                       Computation: 39546 steps/s (collection: 2.304s, learning 0.182s)
             Mean action noise std: 3.28
          Mean value_function loss: 153.3093
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 66.7895
                       Mean reward: 839.85
               Mean episode length: 224.52
    Episode_Reward/reaching_object: 1.4960
     Episode_Reward/lifting_object: 166.2793
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 2.49s
                      Time elapsed: 01:07:00
                               ETA: 00:16:10

################################################################################
                     [1m Learning iteration 1612/2000 [0m                     

                       Computation: 33894 steps/s (collection: 2.656s, learning 0.245s)
             Mean action noise std: 3.28
          Mean value_function loss: 189.0932
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 66.7959
                       Mean reward: 836.78
               Mean episode length: 226.08
    Episode_Reward/reaching_object: 1.4947
     Episode_Reward/lifting_object: 166.0686
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0822
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 2.90s
                      Time elapsed: 01:07:03
                               ETA: 00:16:07

################################################################################
                     [1m Learning iteration 1613/2000 [0m                     

                       Computation: 32711 steps/s (collection: 2.780s, learning 0.225s)
             Mean action noise std: 3.28
          Mean value_function loss: 193.3759
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 66.7981
                       Mean reward: 829.67
               Mean episode length: 226.91
    Episode_Reward/reaching_object: 1.4658
     Episode_Reward/lifting_object: 162.3716
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0812
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 3.01s
                      Time elapsed: 01:07:06
                               ETA: 00:16:05

################################################################################
                     [1m Learning iteration 1614/2000 [0m                     

                       Computation: 28585 steps/s (collection: 3.176s, learning 0.263s)
             Mean action noise std: 3.28
          Mean value_function loss: 184.3874
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 66.8001
                       Mean reward: 846.80
               Mean episode length: 227.49
    Episode_Reward/reaching_object: 1.4740
     Episode_Reward/lifting_object: 163.9834
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0811
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 3.44s
                      Time elapsed: 01:07:10
                               ETA: 00:16:03

################################################################################
                     [1m Learning iteration 1615/2000 [0m                     

                       Computation: 27585 steps/s (collection: 3.396s, learning 0.168s)
             Mean action noise std: 3.28
          Mean value_function loss: 194.3393
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 66.8020
                       Mean reward: 831.04
               Mean episode length: 225.42
    Episode_Reward/reaching_object: 1.5178
     Episode_Reward/lifting_object: 168.2876
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0616
          Episode_Reward/joint_vel: -0.0831
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 3.56s
                      Time elapsed: 01:07:13
                               ETA: 00:16:00

################################################################################
                     [1m Learning iteration 1616/2000 [0m                     

                       Computation: 32556 steps/s (collection: 2.821s, learning 0.198s)
             Mean action noise std: 3.29
          Mean value_function loss: 197.5729
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 66.8033
                       Mean reward: 811.37
               Mean episode length: 220.52
    Episode_Reward/reaching_object: 1.5030
     Episode_Reward/lifting_object: 167.2992
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0615
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 3.02s
                      Time elapsed: 01:07:16
                               ETA: 00:15:58

################################################################################
                     [1m Learning iteration 1617/2000 [0m                     

                       Computation: 30938 steps/s (collection: 2.963s, learning 0.215s)
             Mean action noise std: 3.29
          Mean value_function loss: 222.3174
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 66.8045
                       Mean reward: 837.72
               Mean episode length: 224.65
    Episode_Reward/reaching_object: 1.5022
     Episode_Reward/lifting_object: 167.0711
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0613
          Episode_Reward/joint_vel: -0.0818
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 3.18s
                      Time elapsed: 01:07:19
                               ETA: 00:15:56

################################################################################
                     [1m Learning iteration 1618/2000 [0m                     

                       Computation: 32784 steps/s (collection: 2.768s, learning 0.230s)
             Mean action noise std: 3.29
          Mean value_function loss: 237.7123
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 66.8058
                       Mean reward: 819.83
               Mean episode length: 221.85
    Episode_Reward/reaching_object: 1.4863
     Episode_Reward/lifting_object: 165.0301
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 3.00s
                      Time elapsed: 01:07:22
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 1619/2000 [0m                     

                       Computation: 31474 steps/s (collection: 2.764s, learning 0.360s)
             Mean action noise std: 3.29
          Mean value_function loss: 201.5048
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 66.8070
                       Mean reward: 842.00
               Mean episode length: 227.04
    Episode_Reward/reaching_object: 1.5036
     Episode_Reward/lifting_object: 166.6129
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0617
          Episode_Reward/joint_vel: -0.0825
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 3.12s
                      Time elapsed: 01:07:25
                               ETA: 00:15:51

################################################################################
                     [1m Learning iteration 1620/2000 [0m                     

                       Computation: 32607 steps/s (collection: 2.707s, learning 0.308s)
             Mean action noise std: 3.29
          Mean value_function loss: 199.8227
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 66.8088
                       Mean reward: 883.38
               Mean episode length: 236.31
    Episode_Reward/reaching_object: 1.5170
     Episode_Reward/lifting_object: 168.7373
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0831
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 3.01s
                      Time elapsed: 01:07:28
                               ETA: 00:15:49

################################################################################
                     [1m Learning iteration 1621/2000 [0m                     

                       Computation: 29370 steps/s (collection: 3.162s, learning 0.185s)
             Mean action noise std: 3.29
          Mean value_function loss: 281.4816
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 66.8157
                       Mean reward: 787.87
               Mean episode length: 215.72
    Episode_Reward/reaching_object: 1.4720
     Episode_Reward/lifting_object: 163.1028
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0811
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 3.35s
                      Time elapsed: 01:07:32
                               ETA: 00:15:46

################################################################################
                     [1m Learning iteration 1622/2000 [0m                     

                       Computation: 30765 steps/s (collection: 3.006s, learning 0.189s)
             Mean action noise std: 3.29
          Mean value_function loss: 209.3118
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 66.8248
                       Mean reward: 835.85
               Mean episode length: 225.00
    Episode_Reward/reaching_object: 1.4571
     Episode_Reward/lifting_object: 161.4775
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 3.20s
                      Time elapsed: 01:07:35
                               ETA: 00:15:44

################################################################################
                     [1m Learning iteration 1623/2000 [0m                     

                       Computation: 36625 steps/s (collection: 2.551s, learning 0.133s)
             Mean action noise std: 3.29
          Mean value_function loss: 210.0292
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 66.8288
                       Mean reward: 817.51
               Mean episode length: 220.89
    Episode_Reward/reaching_object: 1.4811
     Episode_Reward/lifting_object: 163.9630
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0605
          Episode_Reward/joint_vel: -0.0814
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 2.68s
                      Time elapsed: 01:07:38
                               ETA: 00:15:42

################################################################################
                     [1m Learning iteration 1624/2000 [0m                     

                       Computation: 37539 steps/s (collection: 2.480s, learning 0.139s)
             Mean action noise std: 3.29
          Mean value_function loss: 194.0689
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 66.8360
                       Mean reward: 816.10
               Mean episode length: 221.10
    Episode_Reward/reaching_object: 1.4681
     Episode_Reward/lifting_object: 162.9997
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0798
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 2.62s
                      Time elapsed: 01:07:40
                               ETA: 00:15:39

################################################################################
                     [1m Learning iteration 1625/2000 [0m                     

                       Computation: 37104 steps/s (collection: 2.495s, learning 0.155s)
             Mean action noise std: 3.29
          Mean value_function loss: 240.4128
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 66.8522
                       Mean reward: 870.03
               Mean episode length: 232.35
    Episode_Reward/reaching_object: 1.4901
     Episode_Reward/lifting_object: 165.5808
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0812
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 2.65s
                      Time elapsed: 01:07:43
                               ETA: 00:15:37

################################################################################
                     [1m Learning iteration 1626/2000 [0m                     

                       Computation: 40326 steps/s (collection: 2.269s, learning 0.169s)
             Mean action noise std: 3.30
          Mean value_function loss: 259.3248
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 66.8726
                       Mean reward: 841.37
               Mean episode length: 226.93
    Episode_Reward/reaching_object: 1.4616
     Episode_Reward/lifting_object: 161.6945
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0802
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 2.44s
                      Time elapsed: 01:07:45
                               ETA: 00:15:34

################################################################################
                     [1m Learning iteration 1627/2000 [0m                     

                       Computation: 39568 steps/s (collection: 2.329s, learning 0.155s)
             Mean action noise std: 3.30
          Mean value_function loss: 279.7873
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 66.8917
                       Mean reward: 852.53
               Mean episode length: 228.89
    Episode_Reward/reaching_object: 1.4573
     Episode_Reward/lifting_object: 161.1496
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0798
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 2.48s
                      Time elapsed: 01:07:48
                               ETA: 00:15:32

################################################################################
                     [1m Learning iteration 1628/2000 [0m                     

                       Computation: 40059 steps/s (collection: 2.308s, learning 0.146s)
             Mean action noise std: 3.30
          Mean value_function loss: 218.7480
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 66.9031
                       Mean reward: 847.70
               Mean episode length: 226.77
    Episode_Reward/reaching_object: 1.4634
     Episode_Reward/lifting_object: 161.6670
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0804
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 2.45s
                      Time elapsed: 01:07:50
                               ETA: 00:15:29

################################################################################
                     [1m Learning iteration 1629/2000 [0m                     

                       Computation: 38902 steps/s (collection: 2.403s, learning 0.124s)
             Mean action noise std: 3.30
          Mean value_function loss: 244.4530
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 66.9168
                       Mean reward: 806.62
               Mean episode length: 218.78
    Episode_Reward/reaching_object: 1.4876
     Episode_Reward/lifting_object: 165.0191
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0812
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 2.53s
                      Time elapsed: 01:07:53
                               ETA: 00:15:27

################################################################################
                     [1m Learning iteration 1630/2000 [0m                     

                       Computation: 39114 steps/s (collection: 2.353s, learning 0.161s)
             Mean action noise std: 3.30
          Mean value_function loss: 264.1963
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 66.9228
                       Mean reward: 822.56
               Mean episode length: 221.59
    Episode_Reward/reaching_object: 1.4832
     Episode_Reward/lifting_object: 163.8435
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0608
          Episode_Reward/joint_vel: -0.0812
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 2.51s
                      Time elapsed: 01:07:55
                               ETA: 00:15:24

################################################################################
                     [1m Learning iteration 1631/2000 [0m                     

                       Computation: 39456 steps/s (collection: 2.352s, learning 0.140s)
             Mean action noise std: 3.30
          Mean value_function loss: 237.5645
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 66.9320
                       Mean reward: 853.64
               Mean episode length: 228.52
    Episode_Reward/reaching_object: 1.4726
     Episode_Reward/lifting_object: 162.9891
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0804
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 2.49s
                      Time elapsed: 01:07:58
                               ETA: 00:15:22

################################################################################
                     [1m Learning iteration 1632/2000 [0m                     

                       Computation: 41492 steps/s (collection: 2.257s, learning 0.113s)
             Mean action noise std: 3.30
          Mean value_function loss: 186.1786
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 66.9350
                       Mean reward: 883.55
               Mean episode length: 235.91
    Episode_Reward/reaching_object: 1.5021
     Episode_Reward/lifting_object: 166.2497
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 2.37s
                      Time elapsed: 01:08:00
                               ETA: 00:15:19

################################################################################
                     [1m Learning iteration 1633/2000 [0m                     

                       Computation: 38879 steps/s (collection: 2.351s, learning 0.177s)
             Mean action noise std: 3.30
          Mean value_function loss: 193.7325
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 66.9419
                       Mean reward: 818.37
               Mean episode length: 222.52
    Episode_Reward/reaching_object: 1.4916
     Episode_Reward/lifting_object: 165.6257
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 2.53s
                      Time elapsed: 01:08:03
                               ETA: 00:15:17

################################################################################
                     [1m Learning iteration 1634/2000 [0m                     

                       Computation: 39328 steps/s (collection: 2.375s, learning 0.124s)
             Mean action noise std: 3.30
          Mean value_function loss: 202.1080
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 66.9493
                       Mean reward: 828.34
               Mean episode length: 222.37
    Episode_Reward/reaching_object: 1.5261
     Episode_Reward/lifting_object: 169.9144
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 2.50s
                      Time elapsed: 01:08:05
                               ETA: 00:15:14

################################################################################
                     [1m Learning iteration 1635/2000 [0m                     

                       Computation: 38907 steps/s (collection: 2.368s, learning 0.158s)
             Mean action noise std: 3.31
          Mean value_function loss: 194.1511
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 66.9597
                       Mean reward: 866.62
               Mean episode length: 232.53
    Episode_Reward/reaching_object: 1.5040
     Episode_Reward/lifting_object: 166.8895
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0821
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 2.53s
                      Time elapsed: 01:08:08
                               ETA: 00:15:12

################################################################################
                     [1m Learning iteration 1636/2000 [0m                     

                       Computation: 38493 steps/s (collection: 2.397s, learning 0.157s)
             Mean action noise std: 3.31
          Mean value_function loss: 200.5285
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 66.9724
                       Mean reward: 849.98
               Mean episode length: 226.92
    Episode_Reward/reaching_object: 1.5096
     Episode_Reward/lifting_object: 168.8316
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0815
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 2.55s
                      Time elapsed: 01:08:10
                               ETA: 00:15:09

################################################################################
                     [1m Learning iteration 1637/2000 [0m                     

                       Computation: 32766 steps/s (collection: 2.840s, learning 0.160s)
             Mean action noise std: 3.31
          Mean value_function loss: 206.6766
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 66.9751
                       Mean reward: 787.82
               Mean episode length: 211.45
    Episode_Reward/reaching_object: 1.4889
     Episode_Reward/lifting_object: 166.4431
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0821
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 3.00s
                      Time elapsed: 01:08:13
                               ETA: 00:15:07

################################################################################
                     [1m Learning iteration 1638/2000 [0m                     

                       Computation: 37243 steps/s (collection: 2.547s, learning 0.093s)
             Mean action noise std: 3.31
          Mean value_function loss: 205.6070
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 66.9762
                       Mean reward: 786.44
               Mean episode length: 212.94
    Episode_Reward/reaching_object: 1.4684
     Episode_Reward/lifting_object: 163.8438
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0813
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 2.64s
                      Time elapsed: 01:08:16
                               ETA: 00:15:04

################################################################################
                     [1m Learning iteration 1639/2000 [0m                     

                       Computation: 44043 steps/s (collection: 2.129s, learning 0.103s)
             Mean action noise std: 3.31
          Mean value_function loss: 218.2742
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 66.9773
                       Mean reward: 856.29
               Mean episode length: 229.51
    Episode_Reward/reaching_object: 1.4627
     Episode_Reward/lifting_object: 163.1037
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0806
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 2.23s
                      Time elapsed: 01:08:18
                               ETA: 00:15:02

################################################################################
                     [1m Learning iteration 1640/2000 [0m                     

                       Computation: 42144 steps/s (collection: 2.199s, learning 0.134s)
             Mean action noise std: 3.31
          Mean value_function loss: 208.5655
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 66.9782
                       Mean reward: 805.08
               Mean episode length: 217.64
    Episode_Reward/reaching_object: 1.4488
     Episode_Reward/lifting_object: 161.1914
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.0800
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 2.33s
                      Time elapsed: 01:08:21
                               ETA: 00:14:59

################################################################################
                     [1m Learning iteration 1641/2000 [0m                     

                       Computation: 43583 steps/s (collection: 2.150s, learning 0.105s)
             Mean action noise std: 3.31
          Mean value_function loss: 221.8584
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 66.9791
                       Mean reward: 848.60
               Mean episode length: 226.63
    Episode_Reward/reaching_object: 1.5269
     Episode_Reward/lifting_object: 171.6847
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0634
          Episode_Reward/joint_vel: -0.0821
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 2.26s
                      Time elapsed: 01:08:23
                               ETA: 00:14:57

################################################################################
                     [1m Learning iteration 1642/2000 [0m                     

                       Computation: 44151 steps/s (collection: 2.130s, learning 0.097s)
             Mean action noise std: 3.31
          Mean value_function loss: 192.5235
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 66.9805
                       Mean reward: 864.64
               Mean episode length: 231.40
    Episode_Reward/reaching_object: 1.4856
     Episode_Reward/lifting_object: 165.7563
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0813
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 2.23s
                      Time elapsed: 01:08:25
                               ETA: 00:14:54

################################################################################
                     [1m Learning iteration 1643/2000 [0m                     

                       Computation: 44810 steps/s (collection: 2.098s, learning 0.096s)
             Mean action noise std: 3.31
          Mean value_function loss: 204.1104
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 66.9860
                       Mean reward: 848.28
               Mean episode length: 228.64
    Episode_Reward/reaching_object: 1.4872
     Episode_Reward/lifting_object: 165.4730
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0818
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 2.19s
                      Time elapsed: 01:08:27
                               ETA: 00:14:52

################################################################################
                     [1m Learning iteration 1644/2000 [0m                     

                       Computation: 44109 steps/s (collection: 2.119s, learning 0.110s)
             Mean action noise std: 3.31
          Mean value_function loss: 188.5786
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.0019
                       Mean reward: 865.46
               Mean episode length: 231.87
    Episode_Reward/reaching_object: 1.4803
     Episode_Reward/lifting_object: 164.7066
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 2.23s
                      Time elapsed: 01:08:29
                               ETA: 00:14:49

################################################################################
                     [1m Learning iteration 1645/2000 [0m                     

                       Computation: 42842 steps/s (collection: 2.178s, learning 0.117s)
             Mean action noise std: 3.31
          Mean value_function loss: 169.2873
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 67.0179
                       Mean reward: 815.27
               Mean episode length: 220.29
    Episode_Reward/reaching_object: 1.4960
     Episode_Reward/lifting_object: 166.2186
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 2.29s
                      Time elapsed: 01:08:32
                               ETA: 00:14:46

################################################################################
                     [1m Learning iteration 1646/2000 [0m                     

                       Computation: 40592 steps/s (collection: 2.305s, learning 0.117s)
             Mean action noise std: 3.32
          Mean value_function loss: 210.4337
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 67.0350
                       Mean reward: 816.65
               Mean episode length: 220.51
    Episode_Reward/reaching_object: 1.4836
     Episode_Reward/lifting_object: 165.4104
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 2.42s
                      Time elapsed: 01:08:34
                               ETA: 00:14:44

################################################################################
                     [1m Learning iteration 1647/2000 [0m                     

                       Computation: 41026 steps/s (collection: 2.247s, learning 0.150s)
             Mean action noise std: 3.32
          Mean value_function loss: 168.3908
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 67.0438
                       Mean reward: 845.65
               Mean episode length: 227.55
    Episode_Reward/reaching_object: 1.5321
     Episode_Reward/lifting_object: 170.7548
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0845
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 2.40s
                      Time elapsed: 01:08:37
                               ETA: 00:14:41

################################################################################
                     [1m Learning iteration 1648/2000 [0m                     

                       Computation: 42863 steps/s (collection: 2.176s, learning 0.117s)
             Mean action noise std: 3.32
          Mean value_function loss: 213.1854
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 67.0551
                       Mean reward: 870.20
               Mean episode length: 234.93
    Episode_Reward/reaching_object: 1.4908
     Episode_Reward/lifting_object: 166.1797
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 2.29s
                      Time elapsed: 01:08:39
                               ETA: 00:14:39

################################################################################
                     [1m Learning iteration 1649/2000 [0m                     

                       Computation: 42725 steps/s (collection: 2.197s, learning 0.104s)
             Mean action noise std: 3.32
          Mean value_function loss: 197.4608
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 67.0681
                       Mean reward: 858.03
               Mean episode length: 231.13
    Episode_Reward/reaching_object: 1.4985
     Episode_Reward/lifting_object: 167.2610
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 2.30s
                      Time elapsed: 01:08:41
                               ETA: 00:14:36

################################################################################
                     [1m Learning iteration 1650/2000 [0m                     

                       Computation: 42765 steps/s (collection: 2.200s, learning 0.099s)
             Mean action noise std: 3.32
          Mean value_function loss: 186.4773
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 67.0734
                       Mean reward: 855.97
               Mean episode length: 230.11
    Episode_Reward/reaching_object: 1.4775
     Episode_Reward/lifting_object: 164.2631
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0831
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 2.30s
                      Time elapsed: 01:08:43
                               ETA: 00:14:34

################################################################################
                     [1m Learning iteration 1651/2000 [0m                     

                       Computation: 43581 steps/s (collection: 2.157s, learning 0.099s)
             Mean action noise std: 3.32
          Mean value_function loss: 188.8570
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 67.0748
                       Mean reward: 856.16
               Mean episode length: 230.77
    Episode_Reward/reaching_object: 1.5110
     Episode_Reward/lifting_object: 168.7757
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 2.26s
                      Time elapsed: 01:08:46
                               ETA: 00:14:31

################################################################################
                     [1m Learning iteration 1652/2000 [0m                     

                       Computation: 43755 steps/s (collection: 2.150s, learning 0.097s)
             Mean action noise std: 3.32
          Mean value_function loss: 197.4245
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 67.0774
                       Mean reward: 798.96
               Mean episode length: 217.65
    Episode_Reward/reaching_object: 1.4719
     Episode_Reward/lifting_object: 163.5776
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0842
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 2.25s
                      Time elapsed: 01:08:48
                               ETA: 00:14:29

################################################################################
                     [1m Learning iteration 1653/2000 [0m                     

                       Computation: 43608 steps/s (collection: 2.145s, learning 0.109s)
             Mean action noise std: 3.32
          Mean value_function loss: 161.8618
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 67.0825
                       Mean reward: 856.71
               Mean episode length: 229.71
    Episode_Reward/reaching_object: 1.5071
     Episode_Reward/lifting_object: 167.9731
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 2.25s
                      Time elapsed: 01:08:50
                               ETA: 00:14:26

################################################################################
                     [1m Learning iteration 1654/2000 [0m                     

                       Computation: 43325 steps/s (collection: 2.160s, learning 0.109s)
             Mean action noise std: 3.32
          Mean value_function loss: 162.1580
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.0954
                       Mean reward: 823.97
               Mean episode length: 222.14
    Episode_Reward/reaching_object: 1.5045
     Episode_Reward/lifting_object: 168.0903
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0634
          Episode_Reward/joint_vel: -0.0854
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 2.27s
                      Time elapsed: 01:08:52
                               ETA: 00:14:24

################################################################################
                     [1m Learning iteration 1655/2000 [0m                     

                       Computation: 43954 steps/s (collection: 2.141s, learning 0.095s)
             Mean action noise std: 3.32
          Mean value_function loss: 199.0304
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 67.1046
                       Mean reward: 852.90
               Mean episode length: 230.69
    Episode_Reward/reaching_object: 1.5146
     Episode_Reward/lifting_object: 168.6100
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0637
          Episode_Reward/joint_vel: -0.0859
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 2.24s
                      Time elapsed: 01:08:55
                               ETA: 00:14:21

################################################################################
                     [1m Learning iteration 1656/2000 [0m                     

                       Computation: 42406 steps/s (collection: 2.219s, learning 0.100s)
             Mean action noise std: 3.33
          Mean value_function loss: 166.0220
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.1123
                       Mean reward: 871.87
               Mean episode length: 233.25
    Episode_Reward/reaching_object: 1.5298
     Episode_Reward/lifting_object: 170.4439
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0858
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 2.32s
                      Time elapsed: 01:08:57
                               ETA: 00:14:18

################################################################################
                     [1m Learning iteration 1657/2000 [0m                     

                       Computation: 42696 steps/s (collection: 2.199s, learning 0.103s)
             Mean action noise std: 3.33
          Mean value_function loss: 161.0752
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 67.1278
                       Mean reward: 824.11
               Mean episode length: 222.53
    Episode_Reward/reaching_object: 1.4794
     Episode_Reward/lifting_object: 164.4486
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 2.30s
                      Time elapsed: 01:08:59
                               ETA: 00:14:16

################################################################################
                     [1m Learning iteration 1658/2000 [0m                     

                       Computation: 35845 steps/s (collection: 2.582s, learning 0.160s)
             Mean action noise std: 3.33
          Mean value_function loss: 177.7260
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.1461
                       Mean reward: 843.57
               Mean episode length: 227.14
    Episode_Reward/reaching_object: 1.4947
     Episode_Reward/lifting_object: 166.0831
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 2.74s
                      Time elapsed: 01:09:02
                               ETA: 00:14:13

################################################################################
                     [1m Learning iteration 1659/2000 [0m                     

                       Computation: 35466 steps/s (collection: 2.603s, learning 0.169s)
             Mean action noise std: 3.33
          Mean value_function loss: 177.4488
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 67.1562
                       Mean reward: 823.51
               Mean episode length: 221.21
    Episode_Reward/reaching_object: 1.4689
     Episode_Reward/lifting_object: 163.4629
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 2.77s
                      Time elapsed: 01:09:05
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 1660/2000 [0m                     

                       Computation: 36831 steps/s (collection: 2.507s, learning 0.162s)
             Mean action noise std: 3.33
          Mean value_function loss: 222.7556
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 67.1680
                       Mean reward: 821.09
               Mean episode length: 221.94
    Episode_Reward/reaching_object: 1.5241
     Episode_Reward/lifting_object: 170.0520
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0637
          Episode_Reward/joint_vel: -0.0843
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 2.67s
                      Time elapsed: 01:09:08
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 1661/2000 [0m                     

                       Computation: 41165 steps/s (collection: 2.266s, learning 0.122s)
             Mean action noise std: 3.33
          Mean value_function loss: 166.8981
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 67.1730
                       Mean reward: 899.69
               Mean episode length: 238.69
    Episode_Reward/reaching_object: 1.5168
     Episode_Reward/lifting_object: 169.4247
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0857
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 2.39s
                      Time elapsed: 01:09:10
                               ETA: 00:14:06

################################################################################
                     [1m Learning iteration 1662/2000 [0m                     

                       Computation: 39261 steps/s (collection: 2.355s, learning 0.149s)
             Mean action noise std: 3.33
          Mean value_function loss: 222.2015
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 67.1761
                       Mean reward: 855.22
               Mean episode length: 228.78
    Episode_Reward/reaching_object: 1.5196
     Episode_Reward/lifting_object: 169.7138
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0853
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 2.50s
                      Time elapsed: 01:09:12
                               ETA: 00:14:04

################################################################################
                     [1m Learning iteration 1663/2000 [0m                     

                       Computation: 40703 steps/s (collection: 2.222s, learning 0.193s)
             Mean action noise std: 3.34
          Mean value_function loss: 221.6081
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 67.1840
                       Mean reward: 794.51
               Mean episode length: 218.90
    Episode_Reward/reaching_object: 1.4863
     Episode_Reward/lifting_object: 164.8687
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 2.42s
                      Time elapsed: 01:09:15
                               ETA: 00:14:01

################################################################################
                     [1m Learning iteration 1664/2000 [0m                     

                       Computation: 37353 steps/s (collection: 2.464s, learning 0.168s)
             Mean action noise std: 3.34
          Mean value_function loss: 203.8834
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 67.1896
                       Mean reward: 804.71
               Mean episode length: 218.88
    Episode_Reward/reaching_object: 1.4571
     Episode_Reward/lifting_object: 161.8163
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 2.63s
                      Time elapsed: 01:09:17
                               ETA: 00:13:59

################################################################################
                     [1m Learning iteration 1665/2000 [0m                     

                       Computation: 37360 steps/s (collection: 2.512s, learning 0.120s)
             Mean action noise std: 3.34
          Mean value_function loss: 187.2205
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 67.2006
                       Mean reward: 862.84
               Mean episode length: 232.28
    Episode_Reward/reaching_object: 1.4842
     Episode_Reward/lifting_object: 165.0149
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 2.63s
                      Time elapsed: 01:09:20
                               ETA: 00:13:56

################################################################################
                     [1m Learning iteration 1666/2000 [0m                     

                       Computation: 23745 steps/s (collection: 4.007s, learning 0.133s)
             Mean action noise std: 3.34
          Mean value_function loss: 208.1717
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 67.2054
                       Mean reward: 799.52
               Mean episode length: 216.52
    Episode_Reward/reaching_object: 1.4772
     Episode_Reward/lifting_object: 164.7573
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0829
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 4.14s
                      Time elapsed: 01:09:24
                               ETA: 00:13:54

################################################################################
                     [1m Learning iteration 1667/2000 [0m                     

                       Computation: 13458 steps/s (collection: 7.175s, learning 0.129s)
             Mean action noise std: 3.34
          Mean value_function loss: 168.4059
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 67.2119
                       Mean reward: 834.02
               Mean episode length: 224.34
    Episode_Reward/reaching_object: 1.4980
     Episode_Reward/lifting_object: 166.7721
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 7.30s
                      Time elapsed: 01:09:32
                               ETA: 00:13:52

################################################################################
                     [1m Learning iteration 1668/2000 [0m                     

                       Computation: 13677 steps/s (collection: 7.062s, learning 0.125s)
             Mean action noise std: 3.34
          Mean value_function loss: 169.9485
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 67.2243
                       Mean reward: 825.11
               Mean episode length: 222.45
    Episode_Reward/reaching_object: 1.5185
     Episode_Reward/lifting_object: 169.9634
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0637
          Episode_Reward/joint_vel: -0.0842
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 7.19s
                      Time elapsed: 01:09:39
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 1669/2000 [0m                     

                       Computation: 13403 steps/s (collection: 7.135s, learning 0.199s)
             Mean action noise std: 3.34
          Mean value_function loss: 223.1571
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 67.2291
                       Mean reward: 825.36
               Mean episode length: 224.48
    Episode_Reward/reaching_object: 1.4793
     Episode_Reward/lifting_object: 164.9301
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 7.33s
                      Time elapsed: 01:09:46
                               ETA: 00:13:49

################################################################################
                     [1m Learning iteration 1670/2000 [0m                     

                       Computation: 13563 steps/s (collection: 7.106s, learning 0.142s)
             Mean action noise std: 3.34
          Mean value_function loss: 192.2984
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 67.2347
                       Mean reward: 828.46
               Mean episode length: 222.84
    Episode_Reward/reaching_object: 1.4817
     Episode_Reward/lifting_object: 164.7814
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0850
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 7.25s
                      Time elapsed: 01:09:53
                               ETA: 00:13:48

################################################################################
                     [1m Learning iteration 1671/2000 [0m                     

                       Computation: 13478 steps/s (collection: 7.135s, learning 0.158s)
             Mean action noise std: 3.34
          Mean value_function loss: 200.2842
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 67.2432
                       Mean reward: 868.96
               Mean episode length: 232.42
    Episode_Reward/reaching_object: 1.5158
     Episode_Reward/lifting_object: 168.9369
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0863
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 7.29s
                      Time elapsed: 01:10:01
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 1672/2000 [0m                     

                       Computation: 13747 steps/s (collection: 7.033s, learning 0.118s)
             Mean action noise std: 3.35
          Mean value_function loss: 201.6269
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 67.2577
                       Mean reward: 827.75
               Mean episode length: 225.48
    Episode_Reward/reaching_object: 1.4879
     Episode_Reward/lifting_object: 165.5535
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 7.15s
                      Time elapsed: 01:10:08
                               ETA: 00:13:45

################################################################################
                     [1m Learning iteration 1673/2000 [0m                     

                       Computation: 13446 steps/s (collection: 7.173s, learning 0.138s)
             Mean action noise std: 3.35
          Mean value_function loss: 205.9328
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.2674
                       Mean reward: 849.98
               Mean episode length: 228.15
    Episode_Reward/reaching_object: 1.4706
     Episode_Reward/lifting_object: 163.1310
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 7.31s
                      Time elapsed: 01:10:15
                               ETA: 00:13:43

################################################################################
                     [1m Learning iteration 1674/2000 [0m                     

                       Computation: 13931 steps/s (collection: 6.908s, learning 0.148s)
             Mean action noise std: 3.35
          Mean value_function loss: 209.1769
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 67.2779
                       Mean reward: 811.86
               Mean episode length: 218.38
    Episode_Reward/reaching_object: 1.4918
     Episode_Reward/lifting_object: 166.5655
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0848
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 7.06s
                      Time elapsed: 01:10:22
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 1675/2000 [0m                     

                       Computation: 17286 steps/s (collection: 5.600s, learning 0.087s)
             Mean action noise std: 3.35
          Mean value_function loss: 183.9237
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 67.2862
                       Mean reward: 898.76
               Mean episode length: 239.97
    Episode_Reward/reaching_object: 1.5074
     Episode_Reward/lifting_object: 168.4872
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0637
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 5.69s
                      Time elapsed: 01:10:28
                               ETA: 00:13:39

################################################################################
                     [1m Learning iteration 1676/2000 [0m                     

                       Computation: 44742 steps/s (collection: 2.089s, learning 0.109s)
             Mean action noise std: 3.35
          Mean value_function loss: 206.8288
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 67.2980
                       Mean reward: 814.81
               Mean episode length: 220.22
    Episode_Reward/reaching_object: 1.4813
     Episode_Reward/lifting_object: 165.2005
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 2.20s
                      Time elapsed: 01:10:30
                               ETA: 00:13:37

################################################################################
                     [1m Learning iteration 1677/2000 [0m                     

                       Computation: 44808 steps/s (collection: 2.078s, learning 0.116s)
             Mean action noise std: 3.35
          Mean value_function loss: 206.3638
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 67.3130
                       Mean reward: 854.37
               Mean episode length: 228.45
    Episode_Reward/reaching_object: 1.4908
     Episode_Reward/lifting_object: 166.2931
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 2.19s
                      Time elapsed: 01:10:32
                               ETA: 00:13:34

################################################################################
                     [1m Learning iteration 1678/2000 [0m                     

                       Computation: 40790 steps/s (collection: 2.302s, learning 0.108s)
             Mean action noise std: 3.35
          Mean value_function loss: 194.8059
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 67.3163
                       Mean reward: 788.80
               Mean episode length: 213.91
    Episode_Reward/reaching_object: 1.4660
     Episode_Reward/lifting_object: 163.8151
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 2.41s
                      Time elapsed: 01:10:35
                               ETA: 00:13:32

################################################################################
                     [1m Learning iteration 1679/2000 [0m                     

                       Computation: 42927 steps/s (collection: 2.166s, learning 0.124s)
             Mean action noise std: 3.36
          Mean value_function loss: 230.8712
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 67.3241
                       Mean reward: 806.74
               Mean episode length: 217.92
    Episode_Reward/reaching_object: 1.4713
     Episode_Reward/lifting_object: 165.0374
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0845
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 2.29s
                      Time elapsed: 01:10:37
                               ETA: 00:13:29

################################################################################
                     [1m Learning iteration 1680/2000 [0m                     

                       Computation: 39056 steps/s (collection: 2.313s, learning 0.204s)
             Mean action noise std: 3.36
          Mean value_function loss: 219.1402
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 67.3359
                       Mean reward: 790.48
               Mean episode length: 216.49
    Episode_Reward/reaching_object: 1.4482
     Episode_Reward/lifting_object: 161.9871
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 2.52s
                      Time elapsed: 01:10:39
                               ETA: 00:13:27

################################################################################
                     [1m Learning iteration 1681/2000 [0m                     

                       Computation: 41296 steps/s (collection: 2.204s, learning 0.176s)
             Mean action noise std: 3.36
          Mean value_function loss: 213.5902
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 67.3598
                       Mean reward: 851.53
               Mean episode length: 228.73
    Episode_Reward/reaching_object: 1.4683
     Episode_Reward/lifting_object: 164.1758
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0832
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 2.38s
                      Time elapsed: 01:10:42
                               ETA: 00:13:24

################################################################################
                     [1m Learning iteration 1682/2000 [0m                     

                       Computation: 39734 steps/s (collection: 2.264s, learning 0.210s)
             Mean action noise std: 3.36
          Mean value_function loss: 209.7387
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 67.3814
                       Mean reward: 884.60
               Mean episode length: 235.67
    Episode_Reward/reaching_object: 1.4987
     Episode_Reward/lifting_object: 168.3969
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0856
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 2.47s
                      Time elapsed: 01:10:44
                               ETA: 00:13:22

################################################################################
                     [1m Learning iteration 1683/2000 [0m                     

                       Computation: 41171 steps/s (collection: 2.219s, learning 0.169s)
             Mean action noise std: 3.36
          Mean value_function loss: 185.9303
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 67.3868
                       Mean reward: 887.64
               Mean episode length: 237.82
    Episode_Reward/reaching_object: 1.5188
     Episode_Reward/lifting_object: 170.5466
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0650
          Episode_Reward/joint_vel: -0.0872
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 2.39s
                      Time elapsed: 01:10:47
                               ETA: 00:13:19

################################################################################
                     [1m Learning iteration 1684/2000 [0m                     

                       Computation: 37447 steps/s (collection: 2.412s, learning 0.213s)
             Mean action noise std: 3.36
          Mean value_function loss: 213.0792
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 67.3957
                       Mean reward: 816.69
               Mean episode length: 220.68
    Episode_Reward/reaching_object: 1.4577
     Episode_Reward/lifting_object: 163.4952
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0843
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 2.63s
                      Time elapsed: 01:10:49
                               ETA: 00:13:16

################################################################################
                     [1m Learning iteration 1685/2000 [0m                     

                       Computation: 33876 steps/s (collection: 2.681s, learning 0.221s)
             Mean action noise std: 3.37
          Mean value_function loss: 172.1433
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 67.4053
                       Mean reward: 850.83
               Mean episode length: 228.73
    Episode_Reward/reaching_object: 1.5070
     Episode_Reward/lifting_object: 169.0246
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0862
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 2.90s
                      Time elapsed: 01:10:52
                               ETA: 00:13:14

################################################################################
                     [1m Learning iteration 1686/2000 [0m                     

                       Computation: 32236 steps/s (collection: 2.792s, learning 0.257s)
             Mean action noise std: 3.37
          Mean value_function loss: 194.6710
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 67.4081
                       Mean reward: 816.59
               Mean episode length: 222.32
    Episode_Reward/reaching_object: 1.4883
     Episode_Reward/lifting_object: 166.4739
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0852
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 3.05s
                      Time elapsed: 01:10:55
                               ETA: 00:13:12

################################################################################
                     [1m Learning iteration 1687/2000 [0m                     

                       Computation: 32458 steps/s (collection: 2.793s, learning 0.236s)
             Mean action noise std: 3.37
          Mean value_function loss: 252.2405
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 67.4133
                       Mean reward: 767.99
               Mean episode length: 210.87
    Episode_Reward/reaching_object: 1.4553
     Episode_Reward/lifting_object: 163.2251
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0831
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 3.03s
                      Time elapsed: 01:10:58
                               ETA: 00:13:09

################################################################################
                     [1m Learning iteration 1688/2000 [0m                     

                       Computation: 23653 steps/s (collection: 3.836s, learning 0.320s)
             Mean action noise std: 3.37
          Mean value_function loss: 222.8450
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 67.4250
                       Mean reward: 808.46
               Mean episode length: 218.06
    Episode_Reward/reaching_object: 1.4631
     Episode_Reward/lifting_object: 163.8056
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 4.16s
                      Time elapsed: 01:11:02
                               ETA: 00:13:07

################################################################################
                     [1m Learning iteration 1689/2000 [0m                     

                       Computation: 29542 steps/s (collection: 3.152s, learning 0.176s)
             Mean action noise std: 3.37
          Mean value_function loss: 215.6645
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 67.4340
                       Mean reward: 869.86
               Mean episode length: 232.09
    Episode_Reward/reaching_object: 1.4929
     Episode_Reward/lifting_object: 167.2391
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 3.33s
                      Time elapsed: 01:11:06
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 1690/2000 [0m                     

                       Computation: 29238 steps/s (collection: 3.034s, learning 0.329s)
             Mean action noise std: 3.37
          Mean value_function loss: 183.5649
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 67.4387
                       Mean reward: 826.81
               Mean episode length: 224.74
    Episode_Reward/reaching_object: 1.4843
     Episode_Reward/lifting_object: 165.8184
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 3.36s
                      Time elapsed: 01:11:09
                               ETA: 00:13:02

################################################################################
                     [1m Learning iteration 1691/2000 [0m                     

                       Computation: 28958 steps/s (collection: 3.144s, learning 0.251s)
             Mean action noise std: 3.37
          Mean value_function loss: 233.2091
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 67.4446
                       Mean reward: 855.79
               Mean episode length: 229.79
    Episode_Reward/reaching_object: 1.4804
     Episode_Reward/lifting_object: 166.2559
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0634
          Episode_Reward/joint_vel: -0.0829
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 3.39s
                      Time elapsed: 01:11:12
                               ETA: 00:13:00

################################################################################
                     [1m Learning iteration 1692/2000 [0m                     

                       Computation: 26535 steps/s (collection: 3.398s, learning 0.307s)
             Mean action noise std: 3.37
          Mean value_function loss: 171.4994
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 67.4510
                       Mean reward: 918.26
               Mean episode length: 242.59
    Episode_Reward/reaching_object: 1.5146
     Episode_Reward/lifting_object: 171.0583
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0649
          Episode_Reward/joint_vel: -0.0844
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 3.70s
                      Time elapsed: 01:11:16
                               ETA: 00:12:58

################################################################################
                     [1m Learning iteration 1693/2000 [0m                     

                       Computation: 26621 steps/s (collection: 3.416s, learning 0.276s)
             Mean action noise std: 3.37
          Mean value_function loss: 193.2348
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 67.4586
                       Mean reward: 823.17
               Mean episode length: 221.64
    Episode_Reward/reaching_object: 1.4713
     Episode_Reward/lifting_object: 165.0450
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0634
          Episode_Reward/joint_vel: -0.0830
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 3.69s
                      Time elapsed: 01:11:20
                               ETA: 00:12:55

################################################################################
                     [1m Learning iteration 1694/2000 [0m                     

                       Computation: 30476 steps/s (collection: 3.108s, learning 0.118s)
             Mean action noise std: 3.37
          Mean value_function loss: 229.2765
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 67.4628
                       Mean reward: 801.09
               Mean episode length: 216.40
    Episode_Reward/reaching_object: 1.4659
     Episode_Reward/lifting_object: 164.2693
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.0828
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 3.23s
                      Time elapsed: 01:11:23
                               ETA: 00:12:53

################################################################################
                     [1m Learning iteration 1695/2000 [0m                     

                       Computation: 32632 steps/s (collection: 2.868s, learning 0.145s)
             Mean action noise std: 3.37
          Mean value_function loss: 203.1679
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 67.4645
                       Mean reward: 837.40
               Mean episode length: 225.90
    Episode_Reward/reaching_object: 1.4806
     Episode_Reward/lifting_object: 166.0347
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0637
          Episode_Reward/joint_vel: -0.0835
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 3.01s
                      Time elapsed: 01:11:26
                               ETA: 00:12:50

################################################################################
                     [1m Learning iteration 1696/2000 [0m                     

                       Computation: 31095 steps/s (collection: 2.872s, learning 0.290s)
             Mean action noise std: 3.37
          Mean value_function loss: 194.9597
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 67.4656
                       Mean reward: 837.21
               Mean episode length: 224.15
    Episode_Reward/reaching_object: 1.4679
     Episode_Reward/lifting_object: 164.5955
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.0828
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 3.16s
                      Time elapsed: 01:11:29
                               ETA: 00:12:48

################################################################################
                     [1m Learning iteration 1697/2000 [0m                     

                       Computation: 33157 steps/s (collection: 2.860s, learning 0.105s)
             Mean action noise std: 3.37
          Mean value_function loss: 217.3650
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 67.4669
                       Mean reward: 816.14
               Mean episode length: 220.33
    Episode_Reward/reaching_object: 1.4875
     Episode_Reward/lifting_object: 167.7472
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0642
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 2.96s
                      Time elapsed: 01:11:32
                               ETA: 00:12:46

################################################################################
                     [1m Learning iteration 1698/2000 [0m                     

                       Computation: 44719 steps/s (collection: 2.088s, learning 0.110s)
             Mean action noise std: 3.37
          Mean value_function loss: 247.4698
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 67.4680
                       Mean reward: 809.69
               Mean episode length: 218.19
    Episode_Reward/reaching_object: 1.4766
     Episode_Reward/lifting_object: 166.3965
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0843
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 2.20s
                      Time elapsed: 01:11:34
                               ETA: 00:12:43

################################################################################
                     [1m Learning iteration 1699/2000 [0m                     

                       Computation: 44085 steps/s (collection: 2.117s, learning 0.113s)
             Mean action noise std: 3.37
          Mean value_function loss: 248.2586
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 67.4701
                       Mean reward: 787.73
               Mean episode length: 215.34
    Episode_Reward/reaching_object: 1.3952
     Episode_Reward/lifting_object: 155.9976
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0803
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 2.23s
                      Time elapsed: 01:11:37
                               ETA: 00:12:40

################################################################################
                     [1m Learning iteration 1700/2000 [0m                     

                       Computation: 43776 steps/s (collection: 2.141s, learning 0.105s)
             Mean action noise std: 3.38
          Mean value_function loss: 243.3920
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 67.4715
                       Mean reward: 763.90
               Mean episode length: 206.92
    Episode_Reward/reaching_object: 1.4230
     Episode_Reward/lifting_object: 160.4309
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0616
          Episode_Reward/joint_vel: -0.0795
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 2.25s
                      Time elapsed: 01:11:39
                               ETA: 00:12:38

################################################################################
                     [1m Learning iteration 1701/2000 [0m                     

                       Computation: 43852 steps/s (collection: 2.141s, learning 0.101s)
             Mean action noise std: 3.38
          Mean value_function loss: 217.3030
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 67.4728
                       Mean reward: 814.25
               Mean episode length: 220.76
    Episode_Reward/reaching_object: 1.4319
     Episode_Reward/lifting_object: 161.0469
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0809
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 2.24s
                      Time elapsed: 01:11:41
                               ETA: 00:12:35

################################################################################
                     [1m Learning iteration 1702/2000 [0m                     

                       Computation: 44547 steps/s (collection: 2.106s, learning 0.101s)
             Mean action noise std: 3.38
          Mean value_function loss: 233.5581
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 67.4744
                       Mean reward: 830.35
               Mean episode length: 222.72
    Episode_Reward/reaching_object: 1.4640
     Episode_Reward/lifting_object: 164.9370
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 2.21s
                      Time elapsed: 01:11:43
                               ETA: 00:12:33

################################################################################
                     [1m Learning iteration 1703/2000 [0m                     

                       Computation: 44295 steps/s (collection: 2.109s, learning 0.111s)
             Mean action noise std: 3.38
          Mean value_function loss: 212.8770
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 67.4760
                       Mean reward: 851.17
               Mean episode length: 229.30
    Episode_Reward/reaching_object: 1.4743
     Episode_Reward/lifting_object: 166.4605
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 2.22s
                      Time elapsed: 01:11:46
                               ETA: 00:12:30

################################################################################
                     [1m Learning iteration 1704/2000 [0m                     

                       Computation: 43424 steps/s (collection: 2.148s, learning 0.116s)
             Mean action noise std: 3.38
          Mean value_function loss: 195.4808
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 67.4771
                       Mean reward: 855.14
               Mean episode length: 229.09
    Episode_Reward/reaching_object: 1.4748
     Episode_Reward/lifting_object: 166.7810
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0830
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 2.26s
                      Time elapsed: 01:11:48
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 1705/2000 [0m                     

                       Computation: 44214 steps/s (collection: 2.119s, learning 0.105s)
             Mean action noise std: 3.38
          Mean value_function loss: 186.1576
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 67.4781
                       Mean reward: 860.51
               Mean episode length: 230.44
    Episode_Reward/reaching_object: 1.4752
     Episode_Reward/lifting_object: 166.1997
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0832
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 2.22s
                      Time elapsed: 01:11:50
                               ETA: 00:12:25

################################################################################
                     [1m Learning iteration 1706/2000 [0m                     

                       Computation: 44186 steps/s (collection: 2.106s, learning 0.119s)
             Mean action noise std: 3.38
          Mean value_function loss: 236.8335
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 67.4793
                       Mean reward: 845.10
               Mean episode length: 228.22
    Episode_Reward/reaching_object: 1.4800
     Episode_Reward/lifting_object: 166.6425
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0821
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 2.22s
                      Time elapsed: 01:11:52
                               ETA: 00:12:22

################################################################################
                     [1m Learning iteration 1707/2000 [0m                     

                       Computation: 44698 steps/s (collection: 2.084s, learning 0.115s)
             Mean action noise std: 3.38
          Mean value_function loss: 254.7355
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 67.4802
                       Mean reward: 786.99
               Mean episode length: 213.08
    Episode_Reward/reaching_object: 1.4318
     Episode_Reward/lifting_object: 161.0180
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0814
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 2.20s
                      Time elapsed: 01:11:55
                               ETA: 00:12:20

################################################################################
                     [1m Learning iteration 1708/2000 [0m                     

                       Computation: 44817 steps/s (collection: 2.089s, learning 0.105s)
             Mean action noise std: 3.38
          Mean value_function loss: 254.0548
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 67.4825
                       Mean reward: 829.84
               Mean episode length: 223.99
    Episode_Reward/reaching_object: 1.4531
     Episode_Reward/lifting_object: 164.1240
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 2.19s
                      Time elapsed: 01:11:57
                               ETA: 00:12:17

################################################################################
                     [1m Learning iteration 1709/2000 [0m                     

                       Computation: 43322 steps/s (collection: 2.174s, learning 0.095s)
             Mean action noise std: 3.38
          Mean value_function loss: 271.9266
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 67.4879
                       Mean reward: 830.57
               Mean episode length: 224.18
    Episode_Reward/reaching_object: 1.4240
     Episode_Reward/lifting_object: 160.1617
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0622
          Episode_Reward/joint_vel: -0.0813
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 2.27s
                      Time elapsed: 01:11:59
                               ETA: 00:12:15

################################################################################
                     [1m Learning iteration 1710/2000 [0m                     

                       Computation: 43968 steps/s (collection: 2.124s, learning 0.112s)
             Mean action noise std: 3.38
          Mean value_function loss: 321.6396
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 67.4945
                       Mean reward: 789.71
               Mean episode length: 213.86
    Episode_Reward/reaching_object: 1.3919
     Episode_Reward/lifting_object: 156.5339
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 2.24s
                      Time elapsed: 01:12:01
                               ETA: 00:12:12

################################################################################
                     [1m Learning iteration 1711/2000 [0m                     

                       Computation: 43849 steps/s (collection: 2.118s, learning 0.124s)
             Mean action noise std: 3.38
          Mean value_function loss: 322.0948
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.5001
                       Mean reward: 811.76
               Mean episode length: 219.41
    Episode_Reward/reaching_object: 1.4053
     Episode_Reward/lifting_object: 158.4286
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0616
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 2.24s
                      Time elapsed: 01:12:03
                               ETA: 00:12:09

################################################################################
                     [1m Learning iteration 1712/2000 [0m                     

                       Computation: 44678 steps/s (collection: 2.099s, learning 0.101s)
             Mean action noise std: 3.38
          Mean value_function loss: 262.8096
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 67.5103
                       Mean reward: 815.19
               Mean episode length: 221.21
    Episode_Reward/reaching_object: 1.3884
     Episode_Reward/lifting_object: 156.9729
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0795
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 2.20s
                      Time elapsed: 01:12:06
                               ETA: 00:12:07

################################################################################
                     [1m Learning iteration 1713/2000 [0m                     

                       Computation: 44737 steps/s (collection: 2.104s, learning 0.094s)
             Mean action noise std: 3.38
          Mean value_function loss: 263.0333
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 67.5170
                       Mean reward: 839.78
               Mean episode length: 225.54
    Episode_Reward/reaching_object: 1.4318
     Episode_Reward/lifting_object: 162.3008
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0808
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 2.20s
                      Time elapsed: 01:12:08
                               ETA: 00:12:04

################################################################################
                     [1m Learning iteration 1714/2000 [0m                     

                       Computation: 43693 steps/s (collection: 2.146s, learning 0.104s)
             Mean action noise std: 3.38
          Mean value_function loss: 263.3841
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 67.5218
                       Mean reward: 830.54
               Mean episode length: 222.97
    Episode_Reward/reaching_object: 1.4047
     Episode_Reward/lifting_object: 158.8954
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0809
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 2.25s
                      Time elapsed: 01:12:10
                               ETA: 00:12:02

################################################################################
                     [1m Learning iteration 1715/2000 [0m                     

                       Computation: 43553 steps/s (collection: 2.155s, learning 0.102s)
             Mean action noise std: 3.39
          Mean value_function loss: 266.5344
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 67.5307
                       Mean reward: 812.38
               Mean episode length: 219.74
    Episode_Reward/reaching_object: 1.3850
     Episode_Reward/lifting_object: 157.5567
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0789
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 2.26s
                      Time elapsed: 01:12:12
                               ETA: 00:11:59

################################################################################
                     [1m Learning iteration 1716/2000 [0m                     

                       Computation: 43884 steps/s (collection: 2.127s, learning 0.114s)
             Mean action noise std: 3.39
          Mean value_function loss: 267.6378
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 67.5493
                       Mean reward: 822.36
               Mean episode length: 222.18
    Episode_Reward/reaching_object: 1.4268
     Episode_Reward/lifting_object: 162.3241
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 2.24s
                      Time elapsed: 01:12:15
                               ETA: 00:11:57

################################################################################
                     [1m Learning iteration 1717/2000 [0m                     

                       Computation: 43614 steps/s (collection: 2.154s, learning 0.100s)
             Mean action noise std: 3.39
          Mean value_function loss: 258.2341
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 67.5569
                       Mean reward: 810.98
               Mean episode length: 219.11
    Episode_Reward/reaching_object: 1.4179
     Episode_Reward/lifting_object: 160.8510
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0622
          Episode_Reward/joint_vel: -0.0812
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 2.25s
                      Time elapsed: 01:12:17
                               ETA: 00:11:54

################################################################################
                     [1m Learning iteration 1718/2000 [0m                     

                       Computation: 44937 steps/s (collection: 2.091s, learning 0.097s)
             Mean action noise std: 3.39
          Mean value_function loss: 213.6395
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.5647
                       Mean reward: 852.97
               Mean episode length: 229.08
    Episode_Reward/reaching_object: 1.4513
     Episode_Reward/lifting_object: 164.2931
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0812
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 2.19s
                      Time elapsed: 01:12:19
                               ETA: 00:11:51

################################################################################
                     [1m Learning iteration 1719/2000 [0m                     

                       Computation: 44670 steps/s (collection: 2.109s, learning 0.092s)
             Mean action noise std: 3.39
          Mean value_function loss: 251.0062
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 67.5791
                       Mean reward: 869.99
               Mean episode length: 232.28
    Episode_Reward/reaching_object: 1.4353
     Episode_Reward/lifting_object: 163.0541
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0802
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 2.20s
                      Time elapsed: 01:12:21
                               ETA: 00:11:49

################################################################################
                     [1m Learning iteration 1720/2000 [0m                     

                       Computation: 44786 steps/s (collection: 2.101s, learning 0.094s)
             Mean action noise std: 3.39
          Mean value_function loss: 252.2297
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 67.5896
                       Mean reward: 838.09
               Mean episode length: 225.87
    Episode_Reward/reaching_object: 1.3904
     Episode_Reward/lifting_object: 157.4681
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0615
          Episode_Reward/joint_vel: -0.0794
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 2.19s
                      Time elapsed: 01:12:23
                               ETA: 00:11:46

################################################################################
                     [1m Learning iteration 1721/2000 [0m                     

                       Computation: 44559 steps/s (collection: 2.098s, learning 0.108s)
             Mean action noise std: 3.40
          Mean value_function loss: 281.7015
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 67.6044
                       Mean reward: 778.32
               Mean episode length: 211.73
    Episode_Reward/reaching_object: 1.4017
     Episode_Reward/lifting_object: 159.1682
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0793
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 2.21s
                      Time elapsed: 01:12:26
                               ETA: 00:11:44

################################################################################
                     [1m Learning iteration 1722/2000 [0m                     

                       Computation: 43769 steps/s (collection: 2.132s, learning 0.114s)
             Mean action noise std: 3.40
          Mean value_function loss: 261.6782
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 67.6179
                       Mean reward: 813.41
               Mean episode length: 220.20
    Episode_Reward/reaching_object: 1.4227
     Episode_Reward/lifting_object: 162.2164
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0793
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 2.25s
                      Time elapsed: 01:12:28
                               ETA: 00:11:41

################################################################################
                     [1m Learning iteration 1723/2000 [0m                     

                       Computation: 43385 steps/s (collection: 2.145s, learning 0.121s)
             Mean action noise std: 3.40
          Mean value_function loss: 242.7852
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 67.6240
                       Mean reward: 800.84
               Mean episode length: 216.76
    Episode_Reward/reaching_object: 1.3801
     Episode_Reward/lifting_object: 156.4998
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0779
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 2.27s
                      Time elapsed: 01:12:30
                               ETA: 00:11:39

################################################################################
                     [1m Learning iteration 1724/2000 [0m                     

                       Computation: 44273 steps/s (collection: 2.109s, learning 0.112s)
             Mean action noise std: 3.40
          Mean value_function loss: 216.7033
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 67.6292
                       Mean reward: 832.16
               Mean episode length: 223.49
    Episode_Reward/reaching_object: 1.4413
     Episode_Reward/lifting_object: 164.1381
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0797
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 2.22s
                      Time elapsed: 01:12:32
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 1725/2000 [0m                     

                       Computation: 43532 steps/s (collection: 2.147s, learning 0.111s)
             Mean action noise std: 3.40
          Mean value_function loss: 235.9188
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.6362
                       Mean reward: 829.24
               Mean episode length: 221.80
    Episode_Reward/reaching_object: 1.4051
     Episode_Reward/lifting_object: 160.3545
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0781
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 2.26s
                      Time elapsed: 01:12:35
                               ETA: 00:11:33

################################################################################
                     [1m Learning iteration 1726/2000 [0m                     

                       Computation: 43054 steps/s (collection: 2.153s, learning 0.131s)
             Mean action noise std: 3.40
          Mean value_function loss: 205.7364
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 67.6429
                       Mean reward: 840.93
               Mean episode length: 226.14
    Episode_Reward/reaching_object: 1.4298
     Episode_Reward/lifting_object: 162.6259
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0803
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 2.28s
                      Time elapsed: 01:12:37
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 1727/2000 [0m                     

                       Computation: 42761 steps/s (collection: 2.183s, learning 0.116s)
             Mean action noise std: 3.40
          Mean value_function loss: 220.1653
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 67.6467
                       Mean reward: 845.29
               Mean episode length: 225.37
    Episode_Reward/reaching_object: 1.4438
     Episode_Reward/lifting_object: 165.1251
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 2.30s
                      Time elapsed: 01:12:39
                               ETA: 00:11:28

################################################################################
                     [1m Learning iteration 1728/2000 [0m                     

                       Computation: 43502 steps/s (collection: 2.140s, learning 0.120s)
             Mean action noise std: 3.40
          Mean value_function loss: 201.6540
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 67.6496
                       Mean reward: 849.06
               Mean episode length: 230.37
    Episode_Reward/reaching_object: 1.4641
     Episode_Reward/lifting_object: 166.8607
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0816
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 2.26s
                      Time elapsed: 01:12:41
                               ETA: 00:11:26

################################################################################
                     [1m Learning iteration 1729/2000 [0m                     

                       Computation: 42572 steps/s (collection: 2.183s, learning 0.127s)
             Mean action noise std: 3.40
          Mean value_function loss: 191.5348
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 67.6537
                       Mean reward: 806.25
               Mean episode length: 216.71
    Episode_Reward/reaching_object: 1.4595
     Episode_Reward/lifting_object: 166.6472
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 2.31s
                      Time elapsed: 01:12:44
                               ETA: 00:11:23

################################################################################
                     [1m Learning iteration 1730/2000 [0m                     

                       Computation: 44026 steps/s (collection: 2.128s, learning 0.104s)
             Mean action noise std: 3.40
          Mean value_function loss: 257.3223
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 67.6578
                       Mean reward: 839.08
               Mean episode length: 226.62
    Episode_Reward/reaching_object: 1.4178
     Episode_Reward/lifting_object: 161.2216
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0797
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 2.23s
                      Time elapsed: 01:12:46
                               ETA: 00:11:21

################################################################################
                     [1m Learning iteration 1731/2000 [0m                     

                       Computation: 44876 steps/s (collection: 2.079s, learning 0.112s)
             Mean action noise std: 3.40
          Mean value_function loss: 238.8939
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 67.6665
                       Mean reward: 827.81
               Mean episode length: 223.91
    Episode_Reward/reaching_object: 1.4396
     Episode_Reward/lifting_object: 163.5634
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0810
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 2.19s
                      Time elapsed: 01:12:48
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 1732/2000 [0m                     

                       Computation: 43308 steps/s (collection: 2.175s, learning 0.095s)
             Mean action noise std: 3.41
          Mean value_function loss: 200.4021
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 67.6738
                       Mean reward: 786.71
               Mean episode length: 213.33
    Episode_Reward/reaching_object: 1.4639
     Episode_Reward/lifting_object: 166.4741
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 2.27s
                      Time elapsed: 01:12:50
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 1733/2000 [0m                     

                       Computation: 43307 steps/s (collection: 2.145s, learning 0.125s)
             Mean action noise std: 3.41
          Mean value_function loss: 228.0422
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 67.6768
                       Mean reward: 801.33
               Mean episode length: 217.32
    Episode_Reward/reaching_object: 1.4070
     Episode_Reward/lifting_object: 159.9378
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0801
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 2.27s
                      Time elapsed: 01:12:53
                               ETA: 00:11:13

################################################################################
                     [1m Learning iteration 1734/2000 [0m                     

                       Computation: 43856 steps/s (collection: 2.134s, learning 0.107s)
             Mean action noise std: 3.41
          Mean value_function loss: 255.6853
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 67.6851
                       Mean reward: 768.84
               Mean episode length: 207.59
    Episode_Reward/reaching_object: 1.4033
     Episode_Reward/lifting_object: 158.9135
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0796
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 2.24s
                      Time elapsed: 01:12:55
                               ETA: 00:11:10

################################################################################
                     [1m Learning iteration 1735/2000 [0m                     

                       Computation: 43615 steps/s (collection: 2.130s, learning 0.124s)
             Mean action noise std: 3.41
          Mean value_function loss: 278.3316
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 67.7023
                       Mean reward: 812.42
               Mean episode length: 219.43
    Episode_Reward/reaching_object: 1.4298
     Episode_Reward/lifting_object: 161.7368
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.0813
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 2.25s
                      Time elapsed: 01:12:57
                               ETA: 00:11:08

################################################################################
                     [1m Learning iteration 1736/2000 [0m                     

                       Computation: 43334 steps/s (collection: 2.154s, learning 0.114s)
             Mean action noise std: 3.41
          Mean value_function loss: 296.2410
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.7243
                       Mean reward: 816.20
               Mean episode length: 220.43
    Episode_Reward/reaching_object: 1.3910
     Episode_Reward/lifting_object: 157.6411
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0804
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 2.27s
                      Time elapsed: 01:13:00
                               ETA: 00:11:05

################################################################################
                     [1m Learning iteration 1737/2000 [0m                     

                       Computation: 42880 steps/s (collection: 2.150s, learning 0.143s)
             Mean action noise std: 3.42
          Mean value_function loss: 252.6239
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 67.7414
                       Mean reward: 816.01
               Mean episode length: 219.23
    Episode_Reward/reaching_object: 1.4029
     Episode_Reward/lifting_object: 158.9083
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0808
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 2.29s
                      Time elapsed: 01:13:02
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 1738/2000 [0m                     

                       Computation: 43702 steps/s (collection: 2.133s, learning 0.116s)
             Mean action noise std: 3.42
          Mean value_function loss: 214.9684
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 67.7526
                       Mean reward: 844.21
               Mean episode length: 226.43
    Episode_Reward/reaching_object: 1.4574
     Episode_Reward/lifting_object: 164.7432
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0645
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 2.25s
                      Time elapsed: 01:13:04
                               ETA: 00:11:00

################################################################################
                     [1m Learning iteration 1739/2000 [0m                     

                       Computation: 43734 steps/s (collection: 2.139s, learning 0.109s)
             Mean action noise std: 3.42
          Mean value_function loss: 293.8696
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 67.7679
                       Mean reward: 790.49
               Mean episode length: 212.62
    Episode_Reward/reaching_object: 1.3842
     Episode_Reward/lifting_object: 156.0382
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0613
          Episode_Reward/joint_vel: -0.0805
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 2.25s
                      Time elapsed: 01:13:06
                               ETA: 00:10:58

################################################################################
                     [1m Learning iteration 1740/2000 [0m                     

                       Computation: 44392 steps/s (collection: 2.112s, learning 0.103s)
             Mean action noise std: 3.42
          Mean value_function loss: 257.6119
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 67.7717
                       Mean reward: 806.51
               Mean episode length: 216.53
    Episode_Reward/reaching_object: 1.4546
     Episode_Reward/lifting_object: 164.8113
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0831
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 2.21s
                      Time elapsed: 01:13:09
                               ETA: 00:10:55

################################################################################
                     [1m Learning iteration 1741/2000 [0m                     

                       Computation: 43816 steps/s (collection: 2.129s, learning 0.115s)
             Mean action noise std: 3.42
          Mean value_function loss: 260.7249
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 67.7814
                       Mean reward: 766.92
               Mean episode length: 208.99
    Episode_Reward/reaching_object: 1.3983
     Episode_Reward/lifting_object: 157.2271
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0809
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 2.24s
                      Time elapsed: 01:13:11
                               ETA: 00:10:52

################################################################################
                     [1m Learning iteration 1742/2000 [0m                     

                       Computation: 44053 steps/s (collection: 2.129s, learning 0.103s)
             Mean action noise std: 3.42
          Mean value_function loss: 238.6127
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 67.8036
                       Mean reward: 866.54
               Mean episode length: 232.45
    Episode_Reward/reaching_object: 1.4706
     Episode_Reward/lifting_object: 166.3570
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0649
          Episode_Reward/joint_vel: -0.0829
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 2.23s
                      Time elapsed: 01:13:13
                               ETA: 00:10:50

################################################################################
                     [1m Learning iteration 1743/2000 [0m                     

                       Computation: 43552 steps/s (collection: 2.139s, learning 0.118s)
             Mean action noise std: 3.43
          Mean value_function loss: 213.7106
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 67.8204
                       Mean reward: 804.48
               Mean episode length: 218.27
    Episode_Reward/reaching_object: 1.4259
     Episode_Reward/lifting_object: 160.3248
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.0812
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 2.26s
                      Time elapsed: 01:13:15
                               ETA: 00:10:47

################################################################################
                     [1m Learning iteration 1744/2000 [0m                     

                       Computation: 43731 steps/s (collection: 2.112s, learning 0.136s)
             Mean action noise std: 3.43
          Mean value_function loss: 231.9074
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 67.8275
                       Mean reward: 803.70
               Mean episode length: 215.61
    Episode_Reward/reaching_object: 1.4869
     Episode_Reward/lifting_object: 167.8189
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0654
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 2.25s
                      Time elapsed: 01:13:17
                               ETA: 00:10:45

################################################################################
                     [1m Learning iteration 1745/2000 [0m                     

                       Computation: 43607 steps/s (collection: 2.142s, learning 0.113s)
             Mean action noise std: 3.43
          Mean value_function loss: 244.4286
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 67.8389
                       Mean reward: 801.31
               Mean episode length: 217.28
    Episode_Reward/reaching_object: 1.4519
     Episode_Reward/lifting_object: 163.2066
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0819
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 2.25s
                      Time elapsed: 01:13:20
                               ETA: 00:10:42

################################################################################
                     [1m Learning iteration 1746/2000 [0m                     

                       Computation: 43775 steps/s (collection: 2.140s, learning 0.106s)
             Mean action noise std: 3.43
          Mean value_function loss: 219.6257
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 67.8456
                       Mean reward: 818.98
               Mean episode length: 221.83
    Episode_Reward/reaching_object: 1.4902
     Episode_Reward/lifting_object: 167.0570
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0832
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 2.25s
                      Time elapsed: 01:13:22
                               ETA: 00:10:40

################################################################################
                     [1m Learning iteration 1747/2000 [0m                     

                       Computation: 44248 steps/s (collection: 2.112s, learning 0.110s)
             Mean action noise std: 3.43
          Mean value_function loss: 211.2849
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 67.8507
                       Mean reward: 841.91
               Mean episode length: 227.67
    Episode_Reward/reaching_object: 1.4670
     Episode_Reward/lifting_object: 164.7646
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0651
          Episode_Reward/joint_vel: -0.0830
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 2.22s
                      Time elapsed: 01:13:24
                               ETA: 00:10:37

################################################################################
                     [1m Learning iteration 1748/2000 [0m                     

                       Computation: 42738 steps/s (collection: 2.167s, learning 0.133s)
             Mean action noise std: 3.43
          Mean value_function loss: 269.3901
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 67.8570
                       Mean reward: 804.86
               Mean episode length: 217.04
    Episode_Reward/reaching_object: 1.4401
     Episode_Reward/lifting_object: 161.5875
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.0832
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 2.30s
                      Time elapsed: 01:13:27
                               ETA: 00:10:34

################################################################################
                     [1m Learning iteration 1749/2000 [0m                     

                       Computation: 43631 steps/s (collection: 2.140s, learning 0.114s)
             Mean action noise std: 3.43
          Mean value_function loss: 230.4108
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 67.8612
                       Mean reward: 824.17
               Mean episode length: 222.27
    Episode_Reward/reaching_object: 1.4515
     Episode_Reward/lifting_object: 163.2923
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0649
          Episode_Reward/joint_vel: -0.0843
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 2.25s
                      Time elapsed: 01:13:29
                               ETA: 00:10:32

################################################################################
                     [1m Learning iteration 1750/2000 [0m                     

                       Computation: 43297 steps/s (collection: 2.155s, learning 0.116s)
             Mean action noise std: 3.43
          Mean value_function loss: 269.4813
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 67.8676
                       Mean reward: 821.33
               Mean episode length: 222.05
    Episode_Reward/reaching_object: 1.4366
     Episode_Reward/lifting_object: 161.5069
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 2.27s
                      Time elapsed: 01:13:31
                               ETA: 00:10:29

################################################################################
                     [1m Learning iteration 1751/2000 [0m                     

                       Computation: 43771 steps/s (collection: 2.140s, learning 0.106s)
             Mean action noise std: 3.43
          Mean value_function loss: 222.2978
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 67.8743
                       Mean reward: 880.29
               Mean episode length: 235.78
    Episode_Reward/reaching_object: 1.4455
     Episode_Reward/lifting_object: 162.3615
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 2.25s
                      Time elapsed: 01:13:33
                               ETA: 00:10:27

################################################################################
                     [1m Learning iteration 1752/2000 [0m                     

                       Computation: 43713 steps/s (collection: 2.140s, learning 0.109s)
             Mean action noise std: 3.44
          Mean value_function loss: 229.4012
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 67.8862
                       Mean reward: 808.23
               Mean episode length: 220.41
    Episode_Reward/reaching_object: 1.4221
     Episode_Reward/lifting_object: 159.9154
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.0835
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 2.25s
                      Time elapsed: 01:13:36
                               ETA: 00:10:24

################################################################################
                     [1m Learning iteration 1753/2000 [0m                     

                       Computation: 42682 steps/s (collection: 2.179s, learning 0.125s)
             Mean action noise std: 3.44
          Mean value_function loss: 257.1450
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 67.8996
                       Mean reward: 822.55
               Mean episode length: 222.00
    Episode_Reward/reaching_object: 1.4015
     Episode_Reward/lifting_object: 156.6741
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0821
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 2.30s
                      Time elapsed: 01:13:38
                               ETA: 00:10:22

################################################################################
                     [1m Learning iteration 1754/2000 [0m                     

                       Computation: 42147 steps/s (collection: 2.204s, learning 0.128s)
             Mean action noise std: 3.44
          Mean value_function loss: 215.0610
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 67.9098
                       Mean reward: 820.93
               Mean episode length: 221.17
    Episode_Reward/reaching_object: 1.4656
     Episode_Reward/lifting_object: 165.2032
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0657
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 2.33s
                      Time elapsed: 01:13:40
                               ETA: 00:10:19

################################################################################
                     [1m Learning iteration 1755/2000 [0m                     

                       Computation: 42560 steps/s (collection: 2.192s, learning 0.118s)
             Mean action noise std: 3.44
          Mean value_function loss: 224.6890
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 67.9171
                       Mean reward: 807.78
               Mean episode length: 218.61
    Episode_Reward/reaching_object: 1.4574
     Episode_Reward/lifting_object: 164.5835
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.0845
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 2.31s
                      Time elapsed: 01:13:42
                               ETA: 00:10:17

################################################################################
                     [1m Learning iteration 1756/2000 [0m                     

                       Computation: 42238 steps/s (collection: 2.216s, learning 0.112s)
             Mean action noise std: 3.44
          Mean value_function loss: 306.5022
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 67.9221
                       Mean reward: 793.93
               Mean episode length: 217.17
    Episode_Reward/reaching_object: 1.4143
     Episode_Reward/lifting_object: 159.0967
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0637
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 2.33s
                      Time elapsed: 01:13:45
                               ETA: 00:10:14

################################################################################
                     [1m Learning iteration 1757/2000 [0m                     

                       Computation: 43356 steps/s (collection: 2.139s, learning 0.128s)
             Mean action noise std: 3.44
          Mean value_function loss: 232.8708
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 67.9245
                       Mean reward: 829.90
               Mean episode length: 222.63
    Episode_Reward/reaching_object: 1.4406
     Episode_Reward/lifting_object: 161.6517
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 2.27s
                      Time elapsed: 01:13:47
                               ETA: 00:10:12

################################################################################
                     [1m Learning iteration 1758/2000 [0m                     

                       Computation: 43751 steps/s (collection: 2.132s, learning 0.115s)
             Mean action noise std: 3.44
          Mean value_function loss: 215.6832
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 67.9258
                       Mean reward: 809.63
               Mean episode length: 219.28
    Episode_Reward/reaching_object: 1.4533
     Episode_Reward/lifting_object: 163.9301
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 2.25s
                      Time elapsed: 01:13:49
                               ETA: 00:10:09

################################################################################
                     [1m Learning iteration 1759/2000 [0m                     

                       Computation: 43183 steps/s (collection: 2.150s, learning 0.126s)
             Mean action noise std: 3.44
          Mean value_function loss: 274.5221
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 67.9272
                       Mean reward: 807.12
               Mean episode length: 218.46
    Episode_Reward/reaching_object: 1.4187
     Episode_Reward/lifting_object: 160.1025
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0642
          Episode_Reward/joint_vel: -0.0823
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 2.28s
                      Time elapsed: 01:13:52
                               ETA: 00:10:06

################################################################################
                     [1m Learning iteration 1760/2000 [0m                     

                       Computation: 43632 steps/s (collection: 2.137s, learning 0.116s)
             Mean action noise std: 3.44
          Mean value_function loss: 226.3400
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 67.9287
                       Mean reward: 788.36
               Mean episode length: 215.13
    Episode_Reward/reaching_object: 1.4504
     Episode_Reward/lifting_object: 162.7848
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 2.25s
                      Time elapsed: 01:13:54
                               ETA: 00:10:04

################################################################################
                     [1m Learning iteration 1761/2000 [0m                     

                       Computation: 44344 steps/s (collection: 2.113s, learning 0.104s)
             Mean action noise std: 3.44
          Mean value_function loss: 260.9637
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 67.9315
                       Mean reward: 842.20
               Mean episode length: 225.98
    Episode_Reward/reaching_object: 1.3981
     Episode_Reward/lifting_object: 157.4601
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0634
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 2.22s
                      Time elapsed: 01:13:56
                               ETA: 00:10:01

################################################################################
                     [1m Learning iteration 1762/2000 [0m                     

                       Computation: 44310 steps/s (collection: 2.119s, learning 0.100s)
             Mean action noise std: 3.44
          Mean value_function loss: 259.3827
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 67.9368
                       Mean reward: 816.62
               Mean episode length: 219.93
    Episode_Reward/reaching_object: 1.4396
     Episode_Reward/lifting_object: 162.8911
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0651
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 2.22s
                      Time elapsed: 01:13:58
                               ETA: 00:09:59

################################################################################
                     [1m Learning iteration 1763/2000 [0m                     

                       Computation: 44262 steps/s (collection: 2.121s, learning 0.100s)
             Mean action noise std: 3.44
          Mean value_function loss: 273.0542
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 67.9437
                       Mean reward: 823.63
               Mean episode length: 221.17
    Episode_Reward/reaching_object: 1.4161
     Episode_Reward/lifting_object: 160.1107
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 2.22s
                      Time elapsed: 01:14:01
                               ETA: 00:09:56

################################################################################
                     [1m Learning iteration 1764/2000 [0m                     

                       Computation: 43664 steps/s (collection: 2.127s, learning 0.124s)
             Mean action noise std: 3.45
          Mean value_function loss: 235.7324
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 67.9515
                       Mean reward: 769.01
               Mean episode length: 210.99
    Episode_Reward/reaching_object: 1.4209
     Episode_Reward/lifting_object: 159.8157
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0645
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 2.25s
                      Time elapsed: 01:14:03
                               ETA: 00:09:54

################################################################################
                     [1m Learning iteration 1765/2000 [0m                     

                       Computation: 43117 steps/s (collection: 2.158s, learning 0.122s)
             Mean action noise std: 3.45
          Mean value_function loss: 237.7165
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 67.9680
                       Mean reward: 841.94
               Mean episode length: 226.50
    Episode_Reward/reaching_object: 1.4624
     Episode_Reward/lifting_object: 164.7475
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0835
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 2.28s
                      Time elapsed: 01:14:05
                               ETA: 00:09:51

################################################################################
                     [1m Learning iteration 1766/2000 [0m                     

                       Computation: 43497 steps/s (collection: 2.143s, learning 0.117s)
             Mean action noise std: 3.45
          Mean value_function loss: 220.2105
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 67.9854
                       Mean reward: 857.58
               Mean episode length: 229.22
    Episode_Reward/reaching_object: 1.4874
     Episode_Reward/lifting_object: 167.3163
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0667
          Episode_Reward/joint_vel: -0.0850
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 2.26s
                      Time elapsed: 01:14:07
                               ETA: 00:09:49

################################################################################
                     [1m Learning iteration 1767/2000 [0m                     

                       Computation: 43828 steps/s (collection: 2.134s, learning 0.109s)
             Mean action noise std: 3.45
          Mean value_function loss: 215.4686
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 67.9962
                       Mean reward: 807.32
               Mean episode length: 218.67
    Episode_Reward/reaching_object: 1.4658
     Episode_Reward/lifting_object: 164.7548
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 2.24s
                      Time elapsed: 01:14:10
                               ETA: 00:09:46

################################################################################
                     [1m Learning iteration 1768/2000 [0m                     

                       Computation: 43588 steps/s (collection: 2.132s, learning 0.124s)
             Mean action noise std: 3.45
          Mean value_function loss: 215.4843
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 68.0045
                       Mean reward: 823.20
               Mean episode length: 221.76
    Episode_Reward/reaching_object: 1.4579
     Episode_Reward/lifting_object: 162.6150
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0850
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 2.26s
                      Time elapsed: 01:14:12
                               ETA: 00:09:43

################################################################################
                     [1m Learning iteration 1769/2000 [0m                     

                       Computation: 43712 steps/s (collection: 2.120s, learning 0.129s)
             Mean action noise std: 3.45
          Mean value_function loss: 256.2290
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 68.0165
                       Mean reward: 784.49
               Mean episode length: 212.61
    Episode_Reward/reaching_object: 1.4461
     Episode_Reward/lifting_object: 161.4057
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0654
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 2.25s
                      Time elapsed: 01:14:14
                               ETA: 00:09:41

################################################################################
                     [1m Learning iteration 1770/2000 [0m                     

                       Computation: 42201 steps/s (collection: 2.198s, learning 0.132s)
             Mean action noise std: 3.46
          Mean value_function loss: 222.8626
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 68.0234
                       Mean reward: 790.33
               Mean episode length: 215.44
    Episode_Reward/reaching_object: 1.4696
     Episode_Reward/lifting_object: 163.9659
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0662
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 2.33s
                      Time elapsed: 01:14:16
                               ETA: 00:09:38

################################################################################
                     [1m Learning iteration 1771/2000 [0m                     

                       Computation: 42595 steps/s (collection: 2.199s, learning 0.109s)
             Mean action noise std: 3.46
          Mean value_function loss: 294.6693
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 68.0322
                       Mean reward: 769.30
               Mean episode length: 210.75
    Episode_Reward/reaching_object: 1.4366
     Episode_Reward/lifting_object: 159.6727
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0650
          Episode_Reward/joint_vel: -0.0836
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 2.31s
                      Time elapsed: 01:14:19
                               ETA: 00:09:36

################################################################################
                     [1m Learning iteration 1772/2000 [0m                     

                       Computation: 42579 steps/s (collection: 2.181s, learning 0.128s)
             Mean action noise std: 3.46
          Mean value_function loss: 259.0686
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 68.0428
                       Mean reward: 867.28
               Mean episode length: 232.09
    Episode_Reward/reaching_object: 1.4633
     Episode_Reward/lifting_object: 163.7997
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0657
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 2.31s
                      Time elapsed: 01:14:21
                               ETA: 00:09:33

################################################################################
                     [1m Learning iteration 1773/2000 [0m                     

                       Computation: 42622 steps/s (collection: 2.177s, learning 0.129s)
             Mean action noise std: 3.46
          Mean value_function loss: 251.0494
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 68.0489
                       Mean reward: 833.71
               Mean episode length: 224.62
    Episode_Reward/reaching_object: 1.4913
     Episode_Reward/lifting_object: 166.9603
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0670
          Episode_Reward/joint_vel: -0.0859
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 2.31s
                      Time elapsed: 01:14:23
                               ETA: 00:09:31

################################################################################
                     [1m Learning iteration 1774/2000 [0m                     

                       Computation: 43401 steps/s (collection: 2.166s, learning 0.099s)
             Mean action noise std: 3.46
          Mean value_function loss: 261.8574
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 68.0575
                       Mean reward: 797.26
               Mean episode length: 213.88
    Episode_Reward/reaching_object: 1.4195
     Episode_Reward/lifting_object: 158.2870
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 2.26s
                      Time elapsed: 01:14:26
                               ETA: 00:09:28

################################################################################
                     [1m Learning iteration 1775/2000 [0m                     

                       Computation: 40449 steps/s (collection: 2.334s, learning 0.097s)
             Mean action noise std: 3.46
          Mean value_function loss: 287.4193
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 68.0708
                       Mean reward: 826.06
               Mean episode length: 222.62
    Episode_Reward/reaching_object: 1.4308
     Episode_Reward/lifting_object: 159.2954
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0645
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 2.43s
                      Time elapsed: 01:14:28
                               ETA: 00:09:26

################################################################################
                     [1m Learning iteration 1776/2000 [0m                     

                       Computation: 42235 steps/s (collection: 2.223s, learning 0.104s)
             Mean action noise std: 3.46
          Mean value_function loss: 257.1707
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 68.0780
                       Mean reward: 842.56
               Mean episode length: 224.85
    Episode_Reward/reaching_object: 1.4388
     Episode_Reward/lifting_object: 160.7999
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0819
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 2.33s
                      Time elapsed: 01:14:30
                               ETA: 00:09:23

################################################################################
                     [1m Learning iteration 1777/2000 [0m                     

                       Computation: 44298 steps/s (collection: 2.118s, learning 0.102s)
             Mean action noise std: 3.47
          Mean value_function loss: 229.4255
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 68.0861
                       Mean reward: 850.12
               Mean episode length: 229.30
    Episode_Reward/reaching_object: 1.4562
     Episode_Reward/lifting_object: 162.9984
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 2.22s
                      Time elapsed: 01:14:33
                               ETA: 00:09:21

################################################################################
                     [1m Learning iteration 1778/2000 [0m                     

                       Computation: 46564 steps/s (collection: 2.018s, learning 0.094s)
             Mean action noise std: 3.47
          Mean value_function loss: 205.2425
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 68.0945
                       Mean reward: 783.39
               Mean episode length: 212.95
    Episode_Reward/reaching_object: 1.4676
     Episode_Reward/lifting_object: 164.1130
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0661
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 2.11s
                      Time elapsed: 01:14:35
                               ETA: 00:09:18

################################################################################
                     [1m Learning iteration 1779/2000 [0m                     

                       Computation: 45633 steps/s (collection: 2.059s, learning 0.096s)
             Mean action noise std: 3.47
          Mean value_function loss: 200.6766
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 68.0989
                       Mean reward: 824.83
               Mean episode length: 223.07
    Episode_Reward/reaching_object: 1.4854
     Episode_Reward/lifting_object: 166.1478
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0859
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 2.15s
                      Time elapsed: 01:14:37
                               ETA: 00:09:15

################################################################################
                     [1m Learning iteration 1780/2000 [0m                     

                       Computation: 44672 steps/s (collection: 2.085s, learning 0.115s)
             Mean action noise std: 3.47
          Mean value_function loss: 196.1627
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 68.1058
                       Mean reward: 908.99
               Mean episode length: 242.94
    Episode_Reward/reaching_object: 1.4901
     Episode_Reward/lifting_object: 166.8062
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0673
          Episode_Reward/joint_vel: -0.0861
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 2.20s
                      Time elapsed: 01:14:39
                               ETA: 00:09:13

################################################################################
                     [1m Learning iteration 1781/2000 [0m                     

                       Computation: 44121 steps/s (collection: 2.111s, learning 0.117s)
             Mean action noise std: 3.47
          Mean value_function loss: 218.2370
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 68.1146
                       Mean reward: 858.39
               Mean episode length: 229.01
    Episode_Reward/reaching_object: 1.4611
     Episode_Reward/lifting_object: 163.8784
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0661
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 2.23s
                      Time elapsed: 01:14:41
                               ETA: 00:09:10

################################################################################
                     [1m Learning iteration 1782/2000 [0m                     

                       Computation: 46137 steps/s (collection: 2.025s, learning 0.106s)
             Mean action noise std: 3.47
          Mean value_function loss: 177.3907
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 68.1281
                       Mean reward: 830.41
               Mean episode length: 224.46
    Episode_Reward/reaching_object: 1.4972
     Episode_Reward/lifting_object: 168.2882
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0853
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 2.13s
                      Time elapsed: 01:14:43
                               ETA: 00:09:08

################################################################################
                     [1m Learning iteration 1783/2000 [0m                     

                       Computation: 46664 steps/s (collection: 2.018s, learning 0.089s)
             Mean action noise std: 3.47
          Mean value_function loss: 157.6073
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 68.1389
                       Mean reward: 846.02
               Mean episode length: 228.19
    Episode_Reward/reaching_object: 1.4896
     Episode_Reward/lifting_object: 167.4966
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0853
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 2.11s
                      Time elapsed: 01:14:45
                               ETA: 00:09:05

################################################################################
                     [1m Learning iteration 1784/2000 [0m                     

                       Computation: 46130 steps/s (collection: 2.039s, learning 0.092s)
             Mean action noise std: 3.48
          Mean value_function loss: 190.1341
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 68.1502
                       Mean reward: 822.70
               Mean episode length: 222.28
    Episode_Reward/reaching_object: 1.5036
     Episode_Reward/lifting_object: 168.5670
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0681
          Episode_Reward/joint_vel: -0.0859
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 2.13s
                      Time elapsed: 01:14:48
                               ETA: 00:09:03

################################################################################
                     [1m Learning iteration 1785/2000 [0m                     

                       Computation: 45660 steps/s (collection: 2.058s, learning 0.095s)
             Mean action noise std: 3.48
          Mean value_function loss: 168.9093
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 68.1556
                       Mean reward: 868.57
               Mean episode length: 232.17
    Episode_Reward/reaching_object: 1.5113
     Episode_Reward/lifting_object: 169.7941
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0685
          Episode_Reward/joint_vel: -0.0867
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 2.15s
                      Time elapsed: 01:14:50
                               ETA: 00:09:00

################################################################################
                     [1m Learning iteration 1786/2000 [0m                     

                       Computation: 44908 steps/s (collection: 2.091s, learning 0.098s)
             Mean action noise std: 3.48
          Mean value_function loss: 186.8111
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 68.1575
                       Mean reward: 822.16
               Mean episode length: 221.63
    Episode_Reward/reaching_object: 1.4814
     Episode_Reward/lifting_object: 166.7223
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0861
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 2.19s
                      Time elapsed: 01:14:52
                               ETA: 00:08:57

################################################################################
                     [1m Learning iteration 1787/2000 [0m                     

                       Computation: 43250 steps/s (collection: 2.159s, learning 0.114s)
             Mean action noise std: 3.48
          Mean value_function loss: 219.9758
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 68.1591
                       Mean reward: 853.88
               Mean episode length: 228.27
    Episode_Reward/reaching_object: 1.4931
     Episode_Reward/lifting_object: 168.1408
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0680
          Episode_Reward/joint_vel: -0.0854
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 2.27s
                      Time elapsed: 01:14:54
                               ETA: 00:08:55

################################################################################
                     [1m Learning iteration 1788/2000 [0m                     

                       Computation: 45294 steps/s (collection: 2.064s, learning 0.107s)
             Mean action noise std: 3.48
          Mean value_function loss: 172.4848
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 68.1606
                       Mean reward: 862.51
               Mean episode length: 230.08
    Episode_Reward/reaching_object: 1.4828
     Episode_Reward/lifting_object: 166.5456
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0851
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 2.17s
                      Time elapsed: 01:14:56
                               ETA: 00:08:52

################################################################################
                     [1m Learning iteration 1789/2000 [0m                     

                       Computation: 46089 steps/s (collection: 2.039s, learning 0.094s)
             Mean action noise std: 3.48
          Mean value_function loss: 215.8468
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 68.1617
                       Mean reward: 829.47
               Mean episode length: 224.92
    Episode_Reward/reaching_object: 1.3911
     Episode_Reward/lifting_object: 156.0000
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0825
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 2.13s
                      Time elapsed: 01:14:59
                               ETA: 00:08:50

################################################################################
                     [1m Learning iteration 1790/2000 [0m                     

                       Computation: 45462 steps/s (collection: 2.068s, learning 0.094s)
             Mean action noise std: 3.48
          Mean value_function loss: 198.9208
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 68.1628
                       Mean reward: 825.30
               Mean episode length: 223.60
    Episode_Reward/reaching_object: 1.4875
     Episode_Reward/lifting_object: 168.0854
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0867
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 2.16s
                      Time elapsed: 01:15:01
                               ETA: 00:08:47

################################################################################
                     [1m Learning iteration 1791/2000 [0m                     

                       Computation: 44723 steps/s (collection: 2.107s, learning 0.091s)
             Mean action noise std: 3.48
          Mean value_function loss: 233.0541
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 68.1644
                       Mean reward: 828.36
               Mean episode length: 222.53
    Episode_Reward/reaching_object: 1.4817
     Episode_Reward/lifting_object: 166.6712
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0860
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 2.20s
                      Time elapsed: 01:15:03
                               ETA: 00:08:45

################################################################################
                     [1m Learning iteration 1792/2000 [0m                     

                       Computation: 45972 steps/s (collection: 2.041s, learning 0.098s)
             Mean action noise std: 3.48
          Mean value_function loss: 210.7314
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 68.1659
                       Mean reward: 825.66
               Mean episode length: 222.39
    Episode_Reward/reaching_object: 1.4459
     Episode_Reward/lifting_object: 163.7471
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0665
          Episode_Reward/joint_vel: -0.0830
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 2.14s
                      Time elapsed: 01:15:05
                               ETA: 00:08:42

################################################################################
                     [1m Learning iteration 1793/2000 [0m                     

                       Computation: 45571 steps/s (collection: 2.065s, learning 0.092s)
             Mean action noise std: 3.48
          Mean value_function loss: 175.1540
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 68.1677
                       Mean reward: 865.01
               Mean episode length: 231.94
    Episode_Reward/reaching_object: 1.4815
     Episode_Reward/lifting_object: 167.9238
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0860
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 2.16s
                      Time elapsed: 01:15:07
                               ETA: 00:08:40

################################################################################
                     [1m Learning iteration 1794/2000 [0m                     

                       Computation: 45474 steps/s (collection: 2.044s, learning 0.118s)
             Mean action noise std: 3.48
          Mean value_function loss: 211.3582
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 68.1714
                       Mean reward: 829.49
               Mean episode length: 223.44
    Episode_Reward/reaching_object: 1.4750
     Episode_Reward/lifting_object: 167.4401
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0679
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 2.16s
                      Time elapsed: 01:15:09
                               ETA: 00:08:37

################################################################################
                     [1m Learning iteration 1795/2000 [0m                     

                       Computation: 46064 steps/s (collection: 2.026s, learning 0.109s)
             Mean action noise std: 3.48
          Mean value_function loss: 256.0351
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 68.1771
                       Mean reward: 779.03
               Mean episode length: 209.92
    Episode_Reward/reaching_object: 1.4014
     Episode_Reward/lifting_object: 158.0483
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0814
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 2.13s
                      Time elapsed: 01:15:11
                               ETA: 00:08:35

################################################################################
                     [1m Learning iteration 1796/2000 [0m                     

                       Computation: 45426 steps/s (collection: 2.057s, learning 0.107s)
             Mean action noise std: 3.48
          Mean value_function loss: 251.0716
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 68.1852
                       Mean reward: 840.59
               Mean episode length: 224.90
    Episode_Reward/reaching_object: 1.4603
     Episode_Reward/lifting_object: 165.0085
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0672
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 2.16s
                      Time elapsed: 01:15:14
                               ETA: 00:08:32

################################################################################
                     [1m Learning iteration 1797/2000 [0m                     

                       Computation: 44262 steps/s (collection: 2.115s, learning 0.106s)
             Mean action noise std: 3.48
          Mean value_function loss: 261.4164
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 68.1959
                       Mean reward: 780.49
               Mean episode length: 211.68
    Episode_Reward/reaching_object: 1.4075
     Episode_Reward/lifting_object: 158.4881
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0654
          Episode_Reward/joint_vel: -0.0815
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 2.22s
                      Time elapsed: 01:15:16
                               ETA: 00:08:29

################################################################################
                     [1m Learning iteration 1798/2000 [0m                     

                       Computation: 43933 steps/s (collection: 2.143s, learning 0.095s)
             Mean action noise std: 3.48
          Mean value_function loss: 220.0188
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 68.2013
                       Mean reward: 819.26
               Mean episode length: 220.47
    Episode_Reward/reaching_object: 1.4457
     Episode_Reward/lifting_object: 164.1186
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0670
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 2.24s
                      Time elapsed: 01:15:18
                               ETA: 00:08:27

################################################################################
                     [1m Learning iteration 1799/2000 [0m                     

                       Computation: 44795 steps/s (collection: 2.097s, learning 0.098s)
             Mean action noise std: 3.49
          Mean value_function loss: 258.8727
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 68.2109
                       Mean reward: 811.10
               Mean episode length: 218.34
    Episode_Reward/reaching_object: 1.3733
     Episode_Reward/lifting_object: 155.4105
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0803
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 2.19s
                      Time elapsed: 01:15:20
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1800/2000 [0m                     

                       Computation: 45380 steps/s (collection: 2.061s, learning 0.106s)
             Mean action noise std: 3.49
          Mean value_function loss: 272.4806
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 68.2280
                       Mean reward: 782.15
               Mean episode length: 212.15
    Episode_Reward/reaching_object: 1.4176
     Episode_Reward/lifting_object: 160.6893
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0660
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 2.17s
                      Time elapsed: 01:15:22
                               ETA: 00:08:22

################################################################################
                     [1m Learning iteration 1801/2000 [0m                     

                       Computation: 44188 steps/s (collection: 2.090s, learning 0.134s)
             Mean action noise std: 3.49
          Mean value_function loss: 263.1746
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 68.2471
                       Mean reward: 817.57
               Mean episode length: 220.51
    Episode_Reward/reaching_object: 1.4450
     Episode_Reward/lifting_object: 164.3646
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0672
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 2.22s
                      Time elapsed: 01:15:25
                               ETA: 00:08:19

################################################################################
                     [1m Learning iteration 1802/2000 [0m                     

                       Computation: 41865 steps/s (collection: 2.239s, learning 0.109s)
             Mean action noise std: 3.49
          Mean value_function loss: 244.2352
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 68.2644
                       Mean reward: 817.44
               Mean episode length: 221.23
    Episode_Reward/reaching_object: 1.4192
     Episode_Reward/lifting_object: 161.3037
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0659
          Episode_Reward/joint_vel: -0.0822
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 2.35s
                      Time elapsed: 01:15:27
                               ETA: 00:08:17

################################################################################
                     [1m Learning iteration 1803/2000 [0m                     

                       Computation: 44033 steps/s (collection: 2.139s, learning 0.093s)
             Mean action noise std: 3.49
          Mean value_function loss: 256.4891
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 68.2755
                       Mean reward: 776.76
               Mean episode length: 212.01
    Episode_Reward/reaching_object: 1.4208
     Episode_Reward/lifting_object: 161.1633
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0663
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 2.23s
                      Time elapsed: 01:15:29
                               ETA: 00:08:14

################################################################################
                     [1m Learning iteration 1804/2000 [0m                     

                       Computation: 44917 steps/s (collection: 2.086s, learning 0.102s)
             Mean action noise std: 3.50
          Mean value_function loss: 273.7002
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 68.2791
                       Mean reward: 827.39
               Mean episode length: 222.78
    Episode_Reward/reaching_object: 1.4061
     Episode_Reward/lifting_object: 159.2384
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0814
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 2.19s
                      Time elapsed: 01:15:31
                               ETA: 00:08:12

################################################################################
                     [1m Learning iteration 1805/2000 [0m                     

                       Computation: 46055 steps/s (collection: 2.029s, learning 0.106s)
             Mean action noise std: 3.50
          Mean value_function loss: 248.2868
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 68.2900
                       Mean reward: 802.25
               Mean episode length: 217.67
    Episode_Reward/reaching_object: 1.4321
     Episode_Reward/lifting_object: 162.3114
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0814
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 2.13s
                      Time elapsed: 01:15:34
                               ETA: 00:08:09

################################################################################
                     [1m Learning iteration 1806/2000 [0m                     

                       Computation: 43900 steps/s (collection: 2.130s, learning 0.110s)
             Mean action noise std: 3.50
          Mean value_function loss: 236.0322
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 68.2989
                       Mean reward: 833.79
               Mean episode length: 224.08
    Episode_Reward/reaching_object: 1.4422
     Episode_Reward/lifting_object: 163.5844
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0672
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 2.24s
                      Time elapsed: 01:15:36
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 1807/2000 [0m                     

                       Computation: 43720 steps/s (collection: 2.143s, learning 0.106s)
             Mean action noise std: 3.50
          Mean value_function loss: 274.1848
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 68.3035
                       Mean reward: 832.13
               Mean episode length: 224.74
    Episode_Reward/reaching_object: 1.4610
     Episode_Reward/lifting_object: 165.9689
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0679
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 2.25s
                      Time elapsed: 01:15:38
                               ETA: 00:08:04

################################################################################
                     [1m Learning iteration 1808/2000 [0m                     

                       Computation: 45645 steps/s (collection: 2.063s, learning 0.091s)
             Mean action noise std: 3.50
          Mean value_function loss: 201.7255
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 68.3110
                       Mean reward: 839.56
               Mean episode length: 225.18
    Episode_Reward/reaching_object: 1.4461
     Episode_Reward/lifting_object: 163.7581
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0674
          Episode_Reward/joint_vel: -0.0822
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 2.15s
                      Time elapsed: 01:15:40
                               ETA: 00:08:01

################################################################################
                     [1m Learning iteration 1809/2000 [0m                     

                       Computation: 44832 steps/s (collection: 2.074s, learning 0.119s)
             Mean action noise std: 3.50
          Mean value_function loss: 199.0577
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 68.3159
                       Mean reward: 830.71
               Mean episode length: 223.56
    Episode_Reward/reaching_object: 1.4828
     Episode_Reward/lifting_object: 168.5037
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0686
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 2.19s
                      Time elapsed: 01:15:42
                               ETA: 00:07:59

################################################################################
                     [1m Learning iteration 1810/2000 [0m                     

                       Computation: 42813 steps/s (collection: 2.164s, learning 0.133s)
             Mean action noise std: 3.50
          Mean value_function loss: 201.5021
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 68.3216
                       Mean reward: 791.33
               Mean episode length: 214.72
    Episode_Reward/reaching_object: 1.4716
     Episode_Reward/lifting_object: 166.2017
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0848
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 2.30s
                      Time elapsed: 01:15:45
                               ETA: 00:07:56

################################################################################
                     [1m Learning iteration 1811/2000 [0m                     

                       Computation: 43059 steps/s (collection: 2.193s, learning 0.090s)
             Mean action noise std: 3.50
          Mean value_function loss: 204.7654
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 68.3303
                       Mean reward: 832.43
               Mean episode length: 224.07
    Episode_Reward/reaching_object: 1.4529
     Episode_Reward/lifting_object: 163.4789
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0674
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 2.28s
                      Time elapsed: 01:15:47
                               ETA: 00:07:54

################################################################################
                     [1m Learning iteration 1812/2000 [0m                     

                       Computation: 43830 steps/s (collection: 2.151s, learning 0.092s)
             Mean action noise std: 3.51
          Mean value_function loss: 192.8381
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 68.3416
                       Mean reward: 833.15
               Mean episode length: 224.09
    Episode_Reward/reaching_object: 1.4609
     Episode_Reward/lifting_object: 164.5611
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0675
          Episode_Reward/joint_vel: -0.0828
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 2.24s
                      Time elapsed: 01:15:49
                               ETA: 00:07:51

################################################################################
                     [1m Learning iteration 1813/2000 [0m                     

                       Computation: 43993 steps/s (collection: 2.134s, learning 0.100s)
             Mean action noise std: 3.51
          Mean value_function loss: 190.3914
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 68.3495
                       Mean reward: 825.66
               Mean episode length: 222.54
    Episode_Reward/reaching_object: 1.4847
     Episode_Reward/lifting_object: 167.1394
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0687
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 2.23s
                      Time elapsed: 01:15:51
                               ETA: 00:07:49

################################################################################
                     [1m Learning iteration 1814/2000 [0m                     

                       Computation: 45805 steps/s (collection: 2.040s, learning 0.107s)
             Mean action noise std: 3.51
          Mean value_function loss: 185.6172
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 68.3569
                       Mean reward: 843.83
               Mean episode length: 226.16
    Episode_Reward/reaching_object: 1.4531
     Episode_Reward/lifting_object: 163.2526
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0670
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 2.15s
                      Time elapsed: 01:15:54
                               ETA: 00:07:46

################################################################################
                     [1m Learning iteration 1815/2000 [0m                     

                       Computation: 45528 steps/s (collection: 2.068s, learning 0.092s)
             Mean action noise std: 3.51
          Mean value_function loss: 141.7180
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 68.3683
                       Mean reward: 862.51
               Mean episode length: 231.99
    Episode_Reward/reaching_object: 1.5223
     Episode_Reward/lifting_object: 171.1552
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0704
          Episode_Reward/joint_vel: -0.0875
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 2.16s
                      Time elapsed: 01:15:56
                               ETA: 00:07:44

################################################################################
                     [1m Learning iteration 1816/2000 [0m                     

                       Computation: 45987 steps/s (collection: 2.043s, learning 0.095s)
             Mean action noise std: 3.51
          Mean value_function loss: 163.7674
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 68.3717
                       Mean reward: 845.05
               Mean episode length: 227.29
    Episode_Reward/reaching_object: 1.5187
     Episode_Reward/lifting_object: 170.5030
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0701
          Episode_Reward/joint_vel: -0.0867
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 2.14s
                      Time elapsed: 01:15:58
                               ETA: 00:07:41

################################################################################
                     [1m Learning iteration 1817/2000 [0m                     

                       Computation: 45731 steps/s (collection: 2.064s, learning 0.086s)
             Mean action noise std: 3.51
          Mean value_function loss: 212.3734
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 68.3776
                       Mean reward: 835.71
               Mean episode length: 225.71
    Episode_Reward/reaching_object: 1.4763
     Episode_Reward/lifting_object: 165.5632
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0685
          Episode_Reward/joint_vel: -0.0859
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 2.15s
                      Time elapsed: 01:16:00
                               ETA: 00:07:39

################################################################################
                     [1m Learning iteration 1818/2000 [0m                     

                       Computation: 45432 steps/s (collection: 2.075s, learning 0.089s)
             Mean action noise std: 3.51
          Mean value_function loss: 204.1331
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 68.3902
                       Mean reward: 812.09
               Mean episode length: 220.00
    Episode_Reward/reaching_object: 1.4542
     Episode_Reward/lifting_object: 162.6155
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 2.16s
                      Time elapsed: 01:16:02
                               ETA: 00:07:36

################################################################################
                     [1m Learning iteration 1819/2000 [0m                     

                       Computation: 45248 steps/s (collection: 2.072s, learning 0.100s)
             Mean action noise std: 3.51
          Mean value_function loss: 185.3590
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 68.4018
                       Mean reward: 821.72
               Mean episode length: 222.45
    Episode_Reward/reaching_object: 1.4737
     Episode_Reward/lifting_object: 165.4550
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0685
          Episode_Reward/joint_vel: -0.0869
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 2.17s
                      Time elapsed: 01:16:04
                               ETA: 00:07:33

################################################################################
                     [1m Learning iteration 1820/2000 [0m                     

                       Computation: 44812 steps/s (collection: 2.100s, learning 0.094s)
             Mean action noise std: 3.52
          Mean value_function loss: 181.6541
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 68.4129
                       Mean reward: 834.22
               Mean episode length: 225.67
    Episode_Reward/reaching_object: 1.4584
     Episode_Reward/lifting_object: 163.3185
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0676
          Episode_Reward/joint_vel: -0.0843
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 2.19s
                      Time elapsed: 01:16:07
                               ETA: 00:07:31

################################################################################
                     [1m Learning iteration 1821/2000 [0m                     

                       Computation: 45914 steps/s (collection: 2.045s, learning 0.096s)
             Mean action noise std: 3.52
          Mean value_function loss: 191.5666
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 68.4166
                       Mean reward: 869.87
               Mean episode length: 232.94
    Episode_Reward/reaching_object: 1.4932
     Episode_Reward/lifting_object: 167.6256
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0690
          Episode_Reward/joint_vel: -0.0857
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 2.14s
                      Time elapsed: 01:16:09
                               ETA: 00:07:28

################################################################################
                     [1m Learning iteration 1822/2000 [0m                     

                       Computation: 45824 steps/s (collection: 2.048s, learning 0.098s)
             Mean action noise std: 3.52
          Mean value_function loss: 205.9667
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 68.4231
                       Mean reward: 816.02
               Mean episode length: 219.45
    Episode_Reward/reaching_object: 1.4655
     Episode_Reward/lifting_object: 164.0771
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 2.15s
                      Time elapsed: 01:16:11
                               ETA: 00:07:26

################################################################################
                     [1m Learning iteration 1823/2000 [0m                     

                       Computation: 45148 steps/s (collection: 2.072s, learning 0.105s)
             Mean action noise std: 3.52
          Mean value_function loss: 243.5751
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 68.4275
                       Mean reward: 818.39
               Mean episode length: 220.43
    Episode_Reward/reaching_object: 1.4578
     Episode_Reward/lifting_object: 163.2783
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0675
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 2.18s
                      Time elapsed: 01:16:13
                               ETA: 00:07:23

################################################################################
                     [1m Learning iteration 1824/2000 [0m                     

                       Computation: 45024 steps/s (collection: 2.075s, learning 0.109s)
             Mean action noise std: 3.52
          Mean value_function loss: 216.4275
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 68.4329
                       Mean reward: 809.20
               Mean episode length: 219.37
    Episode_Reward/reaching_object: 1.4685
     Episode_Reward/lifting_object: 164.4058
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0681
          Episode_Reward/joint_vel: -0.0844
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 2.18s
                      Time elapsed: 01:16:15
                               ETA: 00:07:21

################################################################################
                     [1m Learning iteration 1825/2000 [0m                     

                       Computation: 44675 steps/s (collection: 2.099s, learning 0.101s)
             Mean action noise std: 3.52
          Mean value_function loss: 167.5366
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 68.4380
                       Mean reward: 853.40
               Mean episode length: 229.46
    Episode_Reward/reaching_object: 1.4970
     Episode_Reward/lifting_object: 167.7685
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0848
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 2.20s
                      Time elapsed: 01:16:17
                               ETA: 00:07:18

################################################################################
                     [1m Learning iteration 1826/2000 [0m                     

                       Computation: 44784 steps/s (collection: 2.091s, learning 0.104s)
             Mean action noise std: 3.52
          Mean value_function loss: 201.8359
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 68.4459
                       Mean reward: 857.45
               Mean episode length: 229.91
    Episode_Reward/reaching_object: 1.5229
     Episode_Reward/lifting_object: 170.2528
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0704
          Episode_Reward/joint_vel: -0.0867
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 2.20s
                      Time elapsed: 01:16:20
                               ETA: 00:07:16

################################################################################
                     [1m Learning iteration 1827/2000 [0m                     

                       Computation: 45267 steps/s (collection: 2.078s, learning 0.094s)
             Mean action noise std: 3.52
          Mean value_function loss: 175.9418
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 68.4508
                       Mean reward: 863.81
               Mean episode length: 232.02
    Episode_Reward/reaching_object: 1.4812
     Episode_Reward/lifting_object: 165.5317
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0683
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 2.17s
                      Time elapsed: 01:16:22
                               ETA: 00:07:13

################################################################################
                     [1m Learning iteration 1828/2000 [0m                     

                       Computation: 45444 steps/s (collection: 2.070s, learning 0.094s)
             Mean action noise std: 3.52
          Mean value_function loss: 193.6055
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 68.4578
                       Mean reward: 831.02
               Mean episode length: 224.62
    Episode_Reward/reaching_object: 1.4844
     Episode_Reward/lifting_object: 166.1649
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0685
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 2.16s
                      Time elapsed: 01:16:24
                               ETA: 00:07:11

################################################################################
                     [1m Learning iteration 1829/2000 [0m                     

                       Computation: 43835 steps/s (collection: 2.142s, learning 0.101s)
             Mean action noise std: 3.53
          Mean value_function loss: 187.7136
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 68.4733
                       Mean reward: 813.78
               Mean episode length: 220.89
    Episode_Reward/reaching_object: 1.4612
     Episode_Reward/lifting_object: 162.5369
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0845
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 2.24s
                      Time elapsed: 01:16:26
                               ETA: 00:07:08

################################################################################
                     [1m Learning iteration 1830/2000 [0m                     

                       Computation: 44711 steps/s (collection: 2.104s, learning 0.095s)
             Mean action noise std: 3.53
          Mean value_function loss: 178.9659
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 68.4936
                       Mean reward: 859.65
               Mean episode length: 229.21
    Episode_Reward/reaching_object: 1.4742
     Episode_Reward/lifting_object: 163.3886
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0680
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 2.20s
                      Time elapsed: 01:16:28
                               ETA: 00:07:06

################################################################################
                     [1m Learning iteration 1831/2000 [0m                     

                       Computation: 45870 steps/s (collection: 2.042s, learning 0.102s)
             Mean action noise std: 3.53
          Mean value_function loss: 187.1349
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 68.5156
                       Mean reward: 849.43
               Mean episode length: 228.81
    Episode_Reward/reaching_object: 1.5141
     Episode_Reward/lifting_object: 169.0966
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0698
          Episode_Reward/joint_vel: -0.0864
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 2.14s
                      Time elapsed: 01:16:31
                               ETA: 00:07:03

################################################################################
                     [1m Learning iteration 1832/2000 [0m                     

                       Computation: 45751 steps/s (collection: 2.044s, learning 0.105s)
             Mean action noise std: 3.53
          Mean value_function loss: 190.7672
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 68.5305
                       Mean reward: 826.17
               Mean episode length: 221.62
    Episode_Reward/reaching_object: 1.5001
     Episode_Reward/lifting_object: 166.7282
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0689
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 2.15s
                      Time elapsed: 01:16:33
                               ETA: 00:07:00

################################################################################
                     [1m Learning iteration 1833/2000 [0m                     

                       Computation: 45434 steps/s (collection: 2.058s, learning 0.106s)
             Mean action noise std: 3.53
          Mean value_function loss: 208.2201
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 68.5352
                       Mean reward: 816.22
               Mean episode length: 220.95
    Episode_Reward/reaching_object: 1.5158
     Episode_Reward/lifting_object: 167.9031
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0697
          Episode_Reward/joint_vel: -0.0857
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 2.16s
                      Time elapsed: 01:16:35
                               ETA: 00:06:58

################################################################################
                     [1m Learning iteration 1834/2000 [0m                     

                       Computation: 45703 steps/s (collection: 2.049s, learning 0.102s)
             Mean action noise std: 3.53
          Mean value_function loss: 210.9242
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 68.5437
                       Mean reward: 820.97
               Mean episode length: 221.23
    Episode_Reward/reaching_object: 1.4786
     Episode_Reward/lifting_object: 163.5750
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0681
          Episode_Reward/joint_vel: -0.0843
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 2.15s
                      Time elapsed: 01:16:37
                               ETA: 00:06:55

################################################################################
                     [1m Learning iteration 1835/2000 [0m                     

                       Computation: 45766 steps/s (collection: 2.049s, learning 0.099s)
             Mean action noise std: 3.54
          Mean value_function loss: 198.9953
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 68.5548
                       Mean reward: 827.93
               Mean episode length: 222.67
    Episode_Reward/reaching_object: 1.4914
     Episode_Reward/lifting_object: 165.1208
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0843
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 2.15s
                      Time elapsed: 01:16:39
                               ETA: 00:06:53

################################################################################
                     [1m Learning iteration 1836/2000 [0m                     

                       Computation: 45379 steps/s (collection: 2.058s, learning 0.108s)
             Mean action noise std: 3.54
          Mean value_function loss: 169.3007
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 68.5678
                       Mean reward: 879.42
               Mean episode length: 236.73
    Episode_Reward/reaching_object: 1.5144
     Episode_Reward/lifting_object: 167.0613
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.0869
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 2.17s
                      Time elapsed: 01:16:41
                               ETA: 00:06:50

################################################################################
                     [1m Learning iteration 1837/2000 [0m                     

                       Computation: 43524 steps/s (collection: 2.148s, learning 0.111s)
             Mean action noise std: 3.54
          Mean value_function loss: 229.1200
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 68.5839
                       Mean reward: 807.01
               Mean episode length: 217.22
    Episode_Reward/reaching_object: 1.4758
     Episode_Reward/lifting_object: 162.6309
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0679
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 2.26s
                      Time elapsed: 01:16:44
                               ETA: 00:06:48

################################################################################
                     [1m Learning iteration 1838/2000 [0m                     

                       Computation: 44458 steps/s (collection: 2.098s, learning 0.113s)
             Mean action noise std: 3.54
          Mean value_function loss: 181.8134
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 68.5976
                       Mean reward: 850.74
               Mean episode length: 228.20
    Episode_Reward/reaching_object: 1.5143
     Episode_Reward/lifting_object: 167.6510
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0697
          Episode_Reward/joint_vel: -0.0862
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 2.21s
                      Time elapsed: 01:16:46
                               ETA: 00:06:45

################################################################################
                     [1m Learning iteration 1839/2000 [0m                     

                       Computation: 44958 steps/s (collection: 2.092s, learning 0.095s)
             Mean action noise std: 3.54
          Mean value_function loss: 212.2143
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 68.6119
                       Mean reward: 844.07
               Mean episode length: 227.19
    Episode_Reward/reaching_object: 1.5003
     Episode_Reward/lifting_object: 165.9619
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0688
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 2.19s
                      Time elapsed: 01:16:48
                               ETA: 00:06:43

################################################################################
                     [1m Learning iteration 1840/2000 [0m                     

                       Computation: 43345 steps/s (collection: 2.170s, learning 0.098s)
             Mean action noise std: 3.54
          Mean value_function loss: 245.2764
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 68.6197
                       Mean reward: 842.81
               Mean episode length: 226.43
    Episode_Reward/reaching_object: 1.4849
     Episode_Reward/lifting_object: 164.6850
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0685
          Episode_Reward/joint_vel: -0.0861
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 2.27s
                      Time elapsed: 01:16:50
                               ETA: 00:06:40

################################################################################
                     [1m Learning iteration 1841/2000 [0m                     

                       Computation: 43928 steps/s (collection: 2.145s, learning 0.093s)
             Mean action noise std: 3.55
          Mean value_function loss: 218.4113
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 68.6286
                       Mean reward: 776.19
               Mean episode length: 209.94
    Episode_Reward/reaching_object: 1.4757
     Episode_Reward/lifting_object: 163.4858
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0680
          Episode_Reward/joint_vel: -0.0848
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 2.24s
                      Time elapsed: 01:16:52
                               ETA: 00:06:38

################################################################################
                     [1m Learning iteration 1842/2000 [0m                     

                       Computation: 43785 steps/s (collection: 2.147s, learning 0.098s)
             Mean action noise std: 3.55
          Mean value_function loss: 186.8409
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 68.6360
                       Mean reward: 787.86
               Mean episode length: 215.77
    Episode_Reward/reaching_object: 1.4785
     Episode_Reward/lifting_object: 162.8896
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0685
          Episode_Reward/joint_vel: -0.0859
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 2.25s
                      Time elapsed: 01:16:55
                               ETA: 00:06:35

################################################################################
                     [1m Learning iteration 1843/2000 [0m                     

                       Computation: 43436 steps/s (collection: 2.156s, learning 0.108s)
             Mean action noise std: 3.55
          Mean value_function loss: 242.9713
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 68.6398
                       Mean reward: 852.48
               Mean episode length: 228.83
    Episode_Reward/reaching_object: 1.4956
     Episode_Reward/lifting_object: 165.3825
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0690
          Episode_Reward/joint_vel: -0.0855
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 2.26s
                      Time elapsed: 01:16:57
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 1844/2000 [0m                     

                       Computation: 46294 steps/s (collection: 2.024s, learning 0.100s)
             Mean action noise std: 3.55
          Mean value_function loss: 204.5676
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 68.6457
                       Mean reward: 820.34
               Mean episode length: 220.54
    Episode_Reward/reaching_object: 1.4849
     Episode_Reward/lifting_object: 164.6219
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0687
          Episode_Reward/joint_vel: -0.0856
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 2.12s
                      Time elapsed: 01:16:59
                               ETA: 00:06:30

################################################################################
                     [1m Learning iteration 1845/2000 [0m                     

                       Computation: 45254 steps/s (collection: 2.075s, learning 0.098s)
             Mean action noise std: 3.55
          Mean value_function loss: 201.0343
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 68.6496
                       Mean reward: 845.03
               Mean episode length: 228.24
    Episode_Reward/reaching_object: 1.4990
     Episode_Reward/lifting_object: 165.9427
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0693
          Episode_Reward/joint_vel: -0.0868
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 2.17s
                      Time elapsed: 01:17:01
                               ETA: 00:06:28

################################################################################
                     [1m Learning iteration 1846/2000 [0m                     

                       Computation: 46032 steps/s (collection: 2.045s, learning 0.091s)
             Mean action noise std: 3.55
          Mean value_function loss: 208.0553
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 68.6532
                       Mean reward: 868.28
               Mean episode length: 231.49
    Episode_Reward/reaching_object: 1.5339
     Episode_Reward/lifting_object: 170.5393
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0707
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 2.14s
                      Time elapsed: 01:17:03
                               ETA: 00:06:25

################################################################################
                     [1m Learning iteration 1847/2000 [0m                     

                       Computation: 44774 steps/s (collection: 2.092s, learning 0.103s)
             Mean action noise std: 3.55
          Mean value_function loss: 177.9519
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 68.6597
                       Mean reward: 846.41
               Mean episode length: 226.69
    Episode_Reward/reaching_object: 1.4985
     Episode_Reward/lifting_object: 166.5573
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0697
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 2.20s
                      Time elapsed: 01:17:06
                               ETA: 00:06:23

################################################################################
                     [1m Learning iteration 1848/2000 [0m                     

                       Computation: 45048 steps/s (collection: 2.088s, learning 0.095s)
             Mean action noise std: 3.55
          Mean value_function loss: 202.7423
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 68.6716
                       Mean reward: 839.01
               Mean episode length: 225.31
    Episode_Reward/reaching_object: 1.5104
     Episode_Reward/lifting_object: 167.1453
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0702
          Episode_Reward/joint_vel: -0.0873
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 2.18s
                      Time elapsed: 01:17:08
                               ETA: 00:06:20

################################################################################
                     [1m Learning iteration 1849/2000 [0m                     

                       Computation: 45319 steps/s (collection: 2.077s, learning 0.092s)
             Mean action noise std: 3.55
          Mean value_function loss: 181.9021
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 68.6841
                       Mean reward: 862.13
               Mean episode length: 231.66
    Episode_Reward/reaching_object: 1.5276
     Episode_Reward/lifting_object: 169.2083
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0709
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 2.17s
                      Time elapsed: 01:17:10
                               ETA: 00:06:17

################################################################################
                     [1m Learning iteration 1850/2000 [0m                     

                       Computation: 43328 steps/s (collection: 2.175s, learning 0.094s)
             Mean action noise std: 3.56
          Mean value_function loss: 205.3098
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 68.6984
                       Mean reward: 826.86
               Mean episode length: 221.91
    Episode_Reward/reaching_object: 1.4745
     Episode_Reward/lifting_object: 163.9790
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0689
          Episode_Reward/joint_vel: -0.0861
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 2.27s
                      Time elapsed: 01:17:12
                               ETA: 00:06:15

################################################################################
                     [1m Learning iteration 1851/2000 [0m                     

                       Computation: 45366 steps/s (collection: 2.074s, learning 0.093s)
             Mean action noise std: 3.56
          Mean value_function loss: 234.7210
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 68.7182
                       Mean reward: 836.26
               Mean episode length: 225.85
    Episode_Reward/reaching_object: 1.4482
     Episode_Reward/lifting_object: 160.5229
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0861
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 2.17s
                      Time elapsed: 01:17:14
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 1852/2000 [0m                     

                       Computation: 44497 steps/s (collection: 2.115s, learning 0.094s)
             Mean action noise std: 3.56
          Mean value_function loss: 201.1005
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 68.7263
                       Mean reward: 855.67
               Mean episode length: 229.61
    Episode_Reward/reaching_object: 1.5192
     Episode_Reward/lifting_object: 168.5844
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0708
          Episode_Reward/joint_vel: -0.0889
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 2.21s
                      Time elapsed: 01:17:17
                               ETA: 00:06:10

################################################################################
                     [1m Learning iteration 1853/2000 [0m                     

                       Computation: 43372 steps/s (collection: 2.164s, learning 0.102s)
             Mean action noise std: 3.56
          Mean value_function loss: 209.2982
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 68.7299
                       Mean reward: 840.75
               Mean episode length: 226.25
    Episode_Reward/reaching_object: 1.4864
     Episode_Reward/lifting_object: 165.3295
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0692
          Episode_Reward/joint_vel: -0.0875
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 2.27s
                      Time elapsed: 01:17:19
                               ETA: 00:06:07

################################################################################
                     [1m Learning iteration 1854/2000 [0m                     

                       Computation: 43042 steps/s (collection: 2.188s, learning 0.096s)
             Mean action noise std: 3.56
          Mean value_function loss: 197.3501
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 68.7358
                       Mean reward: 812.31
               Mean episode length: 217.90
    Episode_Reward/reaching_object: 1.4784
     Episode_Reward/lifting_object: 164.5517
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0695
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 2.28s
                      Time elapsed: 01:17:21
                               ETA: 00:06:05

################################################################################
                     [1m Learning iteration 1855/2000 [0m                     

                       Computation: 44887 steps/s (collection: 2.074s, learning 0.116s)
             Mean action noise std: 3.56
          Mean value_function loss: 228.3158
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 68.7497
                       Mean reward: 829.94
               Mean episode length: 224.12
    Episode_Reward/reaching_object: 1.4765
     Episode_Reward/lifting_object: 163.3551
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0695
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 2.19s
                      Time elapsed: 01:17:23
                               ETA: 00:06:02

################################################################################
                     [1m Learning iteration 1856/2000 [0m                     

                       Computation: 45134 steps/s (collection: 2.063s, learning 0.115s)
             Mean action noise std: 3.57
          Mean value_function loss: 238.7887
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 68.7638
                       Mean reward: 798.20
               Mean episode length: 216.84
    Episode_Reward/reaching_object: 1.4548
     Episode_Reward/lifting_object: 161.1556
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0687
          Episode_Reward/joint_vel: -0.0868
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 2.18s
                      Time elapsed: 01:17:26
                               ETA: 00:06:00

################################################################################
                     [1m Learning iteration 1857/2000 [0m                     

                       Computation: 44519 steps/s (collection: 2.115s, learning 0.094s)
             Mean action noise std: 3.57
          Mean value_function loss: 219.9142
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 68.7728
                       Mean reward: 784.51
               Mean episode length: 213.79
    Episode_Reward/reaching_object: 1.4538
     Episode_Reward/lifting_object: 160.6472
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0862
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 2.21s
                      Time elapsed: 01:17:28
                               ETA: 00:05:57

################################################################################
                     [1m Learning iteration 1858/2000 [0m                     

                       Computation: 44629 steps/s (collection: 2.089s, learning 0.113s)
             Mean action noise std: 3.57
          Mean value_function loss: 201.0710
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 68.7838
                       Mean reward: 796.08
               Mean episode length: 215.83
    Episode_Reward/reaching_object: 1.4985
     Episode_Reward/lifting_object: 166.1128
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0705
          Episode_Reward/joint_vel: -0.0891
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 2.20s
                      Time elapsed: 01:17:30
                               ETA: 00:05:55

################################################################################
                     [1m Learning iteration 1859/2000 [0m                     

                       Computation: 44810 steps/s (collection: 2.094s, learning 0.100s)
             Mean action noise std: 3.57
          Mean value_function loss: 208.0226
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 68.7927
                       Mean reward: 818.03
               Mean episode length: 222.30
    Episode_Reward/reaching_object: 1.4710
     Episode_Reward/lifting_object: 163.1574
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0877
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 2.19s
                      Time elapsed: 01:17:32
                               ETA: 00:05:52

################################################################################
                     [1m Learning iteration 1860/2000 [0m                     

                       Computation: 45615 steps/s (collection: 2.060s, learning 0.095s)
             Mean action noise std: 3.57
          Mean value_function loss: 189.5749
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 68.8090
                       Mean reward: 833.51
               Mean episode length: 224.33
    Episode_Reward/reaching_object: 1.5074
     Episode_Reward/lifting_object: 166.6920
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0708
          Episode_Reward/joint_vel: -0.0907
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 2.16s
                      Time elapsed: 01:17:34
                               ETA: 00:05:50

################################################################################
                     [1m Learning iteration 1861/2000 [0m                     

                       Computation: 45475 steps/s (collection: 2.070s, learning 0.092s)
             Mean action noise std: 3.57
          Mean value_function loss: 208.8698
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 68.8178
                       Mean reward: 830.67
               Mean episode length: 223.54
    Episode_Reward/reaching_object: 1.4644
     Episode_Reward/lifting_object: 161.6596
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0877
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 2.16s
                      Time elapsed: 01:17:36
                               ETA: 00:05:47

################################################################################
                     [1m Learning iteration 1862/2000 [0m                     

                       Computation: 45226 steps/s (collection: 2.085s, learning 0.088s)
             Mean action noise std: 3.57
          Mean value_function loss: 212.3848
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 68.8238
                       Mean reward: 845.83
               Mean episode length: 227.24
    Episode_Reward/reaching_object: 1.4892
     Episode_Reward/lifting_object: 165.0172
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0701
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 2.17s
                      Time elapsed: 01:17:39
                               ETA: 00:05:45

################################################################################
                     [1m Learning iteration 1863/2000 [0m                     

                       Computation: 45038 steps/s (collection: 2.086s, learning 0.097s)
             Mean action noise std: 3.58
          Mean value_function loss: 167.2194
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 68.8352
                       Mean reward: 876.22
               Mean episode length: 234.28
    Episode_Reward/reaching_object: 1.5043
     Episode_Reward/lifting_object: 167.1254
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0706
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 2.18s
                      Time elapsed: 01:17:41
                               ETA: 00:05:42

################################################################################
                     [1m Learning iteration 1864/2000 [0m                     

                       Computation: 45657 steps/s (collection: 2.059s, learning 0.094s)
             Mean action noise std: 3.58
          Mean value_function loss: 255.5286
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 68.8414
                       Mean reward: 785.40
               Mean episode length: 211.75
    Episode_Reward/reaching_object: 1.4451
     Episode_Reward/lifting_object: 160.0525
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0683
          Episode_Reward/joint_vel: -0.0867
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 2.15s
                      Time elapsed: 01:17:43
                               ETA: 00:05:40

################################################################################
                     [1m Learning iteration 1865/2000 [0m                     

                       Computation: 43607 steps/s (collection: 2.164s, learning 0.091s)
             Mean action noise std: 3.58
          Mean value_function loss: 222.7120
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 68.8461
                       Mean reward: 872.10
               Mean episode length: 232.63
    Episode_Reward/reaching_object: 1.5287
     Episode_Reward/lifting_object: 169.2934
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0717
          Episode_Reward/joint_vel: -0.0911
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 2.25s
                      Time elapsed: 01:17:45
                               ETA: 00:05:37

################################################################################
                     [1m Learning iteration 1866/2000 [0m                     

                       Computation: 44780 steps/s (collection: 2.103s, learning 0.092s)
             Mean action noise std: 3.58
          Mean value_function loss: 195.0457
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 68.8512
                       Mean reward: 828.65
               Mean episode length: 224.95
    Episode_Reward/reaching_object: 1.5339
     Episode_Reward/lifting_object: 170.1946
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0720
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 2.20s
                      Time elapsed: 01:17:47
                               ETA: 00:05:35

################################################################################
                     [1m Learning iteration 1867/2000 [0m                     

                       Computation: 44633 steps/s (collection: 2.103s, learning 0.099s)
             Mean action noise std: 3.58
          Mean value_function loss: 234.8492
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 68.8629
                       Mean reward: 748.99
               Mean episode length: 204.89
    Episode_Reward/reaching_object: 1.4505
     Episode_Reward/lifting_object: 158.8656
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0873
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 2.20s
                      Time elapsed: 01:17:50
                               ETA: 00:05:32

################################################################################
                     [1m Learning iteration 1868/2000 [0m                     

                       Computation: 43085 steps/s (collection: 2.186s, learning 0.096s)
             Mean action noise std: 3.58
          Mean value_function loss: 222.2255
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 68.8797
                       Mean reward: 856.24
               Mean episode length: 229.27
    Episode_Reward/reaching_object: 1.5035
     Episode_Reward/lifting_object: 165.7897
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0709
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 2.28s
                      Time elapsed: 01:17:52
                               ETA: 00:05:29

################################################################################
                     [1m Learning iteration 1869/2000 [0m                     

                       Computation: 43678 steps/s (collection: 2.144s, learning 0.107s)
             Mean action noise std: 3.58
          Mean value_function loss: 215.3190
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 68.8903
                       Mean reward: 832.32
               Mean episode length: 224.71
    Episode_Reward/reaching_object: 1.4846
     Episode_Reward/lifting_object: 163.3348
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0700
          Episode_Reward/joint_vel: -0.0891
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 2.25s
                      Time elapsed: 01:17:54
                               ETA: 00:05:27

################################################################################
                     [1m Learning iteration 1870/2000 [0m                     

                       Computation: 43717 steps/s (collection: 2.144s, learning 0.105s)
             Mean action noise std: 3.59
          Mean value_function loss: 230.2842
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 68.8999
                       Mean reward: 837.89
               Mean episode length: 225.05
    Episode_Reward/reaching_object: 1.5123
     Episode_Reward/lifting_object: 166.2513
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0708
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 2.25s
                      Time elapsed: 01:17:56
                               ETA: 00:05:24

################################################################################
                     [1m Learning iteration 1871/2000 [0m                     

                       Computation: 44027 steps/s (collection: 2.128s, learning 0.105s)
             Mean action noise std: 3.59
          Mean value_function loss: 190.9688
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 68.9130
                       Mean reward: 821.64
               Mean episode length: 221.31
    Episode_Reward/reaching_object: 1.4733
     Episode_Reward/lifting_object: 162.9854
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0698
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 2.23s
                      Time elapsed: 01:17:59
                               ETA: 00:05:22

################################################################################
                     [1m Learning iteration 1872/2000 [0m                     

                       Computation: 42669 steps/s (collection: 2.199s, learning 0.105s)
             Mean action noise std: 3.59
          Mean value_function loss: 227.0120
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 68.9202
                       Mean reward: 823.99
               Mean episode length: 222.80
    Episode_Reward/reaching_object: 1.4827
     Episode_Reward/lifting_object: 162.3034
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0701
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 2.30s
                      Time elapsed: 01:18:01
                               ETA: 00:05:19

################################################################################
                     [1m Learning iteration 1873/2000 [0m                     

                       Computation: 42233 steps/s (collection: 2.230s, learning 0.098s)
             Mean action noise std: 3.59
          Mean value_function loss: 187.5504
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 68.9241
                       Mean reward: 828.96
               Mean episode length: 223.48
    Episode_Reward/reaching_object: 1.5173
     Episode_Reward/lifting_object: 167.0311
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0716
          Episode_Reward/joint_vel: -0.0906
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 2.33s
                      Time elapsed: 01:18:03
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 1874/2000 [0m                     

                       Computation: 44184 steps/s (collection: 2.108s, learning 0.117s)
             Mean action noise std: 3.59
          Mean value_function loss: 225.1489
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 68.9362
                       Mean reward: 817.40
               Mean episode length: 220.70
    Episode_Reward/reaching_object: 1.5036
     Episode_Reward/lifting_object: 165.5771
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0711
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 2.22s
                      Time elapsed: 01:18:05
                               ETA: 00:05:14

################################################################################
                     [1m Learning iteration 1875/2000 [0m                     

                       Computation: 43069 steps/s (collection: 2.170s, learning 0.113s)
             Mean action noise std: 3.59
          Mean value_function loss: 207.4514
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 68.9492
                       Mean reward: 810.32
               Mean episode length: 220.44
    Episode_Reward/reaching_object: 1.5015
     Episode_Reward/lifting_object: 164.7249
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0708
          Episode_Reward/joint_vel: -0.0885
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 2.28s
                      Time elapsed: 01:18:08
                               ETA: 00:05:12

################################################################################
                     [1m Learning iteration 1876/2000 [0m                     

                       Computation: 43961 steps/s (collection: 2.131s, learning 0.105s)
             Mean action noise std: 3.59
          Mean value_function loss: 202.6000
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 68.9556
                       Mean reward: 875.19
               Mean episode length: 234.47
    Episode_Reward/reaching_object: 1.5121
     Episode_Reward/lifting_object: 166.4154
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0714
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 2.24s
                      Time elapsed: 01:18:10
                               ETA: 00:05:09

################################################################################
                     [1m Learning iteration 1877/2000 [0m                     

                       Computation: 41267 steps/s (collection: 2.272s, learning 0.110s)
             Mean action noise std: 3.60
          Mean value_function loss: 212.0968
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 68.9712
                       Mean reward: 857.13
               Mean episode length: 229.82
    Episode_Reward/reaching_object: 1.5230
     Episode_Reward/lifting_object: 167.5271
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0716
          Episode_Reward/joint_vel: -0.0886
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 2.38s
                      Time elapsed: 01:18:12
                               ETA: 00:05:07

################################################################################
                     [1m Learning iteration 1878/2000 [0m                     

                       Computation: 42911 steps/s (collection: 2.168s, learning 0.123s)
             Mean action noise std: 3.60
          Mean value_function loss: 198.2692
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 68.9814
                       Mean reward: 872.23
               Mean episode length: 234.67
    Episode_Reward/reaching_object: 1.5327
     Episode_Reward/lifting_object: 168.1092
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0724
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 2.29s
                      Time elapsed: 01:18:15
                               ETA: 00:05:04

################################################################################
                     [1m Learning iteration 1879/2000 [0m                     

                       Computation: 44780 steps/s (collection: 2.103s, learning 0.093s)
             Mean action noise std: 3.60
          Mean value_function loss: 211.6232
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 68.9880
                       Mean reward: 798.13
               Mean episode length: 218.03
    Episode_Reward/reaching_object: 1.4838
     Episode_Reward/lifting_object: 162.1399
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0705
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 2.20s
                      Time elapsed: 01:18:17
                               ETA: 00:05:02

################################################################################
                     [1m Learning iteration 1880/2000 [0m                     

                       Computation: 42213 steps/s (collection: 2.221s, learning 0.108s)
             Mean action noise std: 3.60
          Mean value_function loss: 201.7932
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 68.9925
                       Mean reward: 846.69
               Mean episode length: 227.48
    Episode_Reward/reaching_object: 1.5194
     Episode_Reward/lifting_object: 166.0701
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0715
          Episode_Reward/joint_vel: -0.0885
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 2.33s
                      Time elapsed: 01:18:19
                               ETA: 00:04:59

################################################################################
                     [1m Learning iteration 1881/2000 [0m                     

                       Computation: 41751 steps/s (collection: 2.248s, learning 0.106s)
             Mean action noise std: 3.60
          Mean value_function loss: 168.6164
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 68.9943
                       Mean reward: 853.03
               Mean episode length: 228.19
    Episode_Reward/reaching_object: 1.5407
     Episode_Reward/lifting_object: 168.6015
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0725
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 2.35s
                      Time elapsed: 01:18:22
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 1882/2000 [0m                     

                       Computation: 42987 steps/s (collection: 2.179s, learning 0.108s)
             Mean action noise std: 3.60
          Mean value_function loss: 206.9673
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 68.9994
                       Mean reward: 816.71
               Mean episode length: 219.73
    Episode_Reward/reaching_object: 1.5000
     Episode_Reward/lifting_object: 163.7979
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0712
          Episode_Reward/joint_vel: -0.0889
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 2.29s
                      Time elapsed: 01:18:24
                               ETA: 00:04:54

################################################################################
                     [1m Learning iteration 1883/2000 [0m                     

                       Computation: 43195 steps/s (collection: 2.179s, learning 0.097s)
             Mean action noise std: 3.60
          Mean value_function loss: 194.5703
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 69.0145
                       Mean reward: 824.28
               Mean episode length: 222.16
    Episode_Reward/reaching_object: 1.5164
     Episode_Reward/lifting_object: 165.6851
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0718
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 2.28s
                      Time elapsed: 01:18:26
                               ETA: 00:04:52

################################################################################
                     [1m Learning iteration 1884/2000 [0m                     

                       Computation: 42611 steps/s (collection: 2.194s, learning 0.113s)
             Mean action noise std: 3.61
          Mean value_function loss: 199.9678
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 69.0323
                       Mean reward: 829.67
               Mean episode length: 224.05
    Episode_Reward/reaching_object: 1.5053
     Episode_Reward/lifting_object: 164.1371
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0712
          Episode_Reward/joint_vel: -0.0874
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 2.31s
                      Time elapsed: 01:18:28
                               ETA: 00:04:49

################################################################################
                     [1m Learning iteration 1885/2000 [0m                     

                       Computation: 43757 steps/s (collection: 2.151s, learning 0.096s)
             Mean action noise std: 3.61
          Mean value_function loss: 180.3656
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 69.0442
                       Mean reward: 833.02
               Mean episode length: 223.89
    Episode_Reward/reaching_object: 1.5783
     Episode_Reward/lifting_object: 173.1366
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0741
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 2.25s
                      Time elapsed: 01:18:31
                               ETA: 00:04:47

################################################################################
                     [1m Learning iteration 1886/2000 [0m                     

                       Computation: 44416 steps/s (collection: 2.118s, learning 0.096s)
             Mean action noise std: 3.61
          Mean value_function loss: 218.8697
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 69.0543
                       Mean reward: 820.90
               Mean episode length: 221.25
    Episode_Reward/reaching_object: 1.5225
     Episode_Reward/lifting_object: 166.0962
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0717
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 2.21s
                      Time elapsed: 01:18:33
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 1887/2000 [0m                     

                       Computation: 43860 steps/s (collection: 2.131s, learning 0.111s)
             Mean action noise std: 3.61
          Mean value_function loss: 217.0199
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 69.0672
                       Mean reward: 826.26
               Mean episode length: 224.30
    Episode_Reward/reaching_object: 1.5362
     Episode_Reward/lifting_object: 167.9475
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.0905
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 2.24s
                      Time elapsed: 01:18:35
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 1888/2000 [0m                     

                       Computation: 42531 steps/s (collection: 2.185s, learning 0.126s)
             Mean action noise std: 3.61
          Mean value_function loss: 232.9785
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 69.0702
                       Mean reward: 847.03
               Mean episode length: 226.05
    Episode_Reward/reaching_object: 1.5237
     Episode_Reward/lifting_object: 166.6906
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0721
          Episode_Reward/joint_vel: -0.0885
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 2.31s
                      Time elapsed: 01:18:37
                               ETA: 00:04:39

################################################################################
                     [1m Learning iteration 1889/2000 [0m                     

                       Computation: 44221 steps/s (collection: 2.123s, learning 0.100s)
             Mean action noise std: 3.61
          Mean value_function loss: 199.6038
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 69.0720
                       Mean reward: 848.62
               Mean episode length: 226.87
    Episode_Reward/reaching_object: 1.5384
     Episode_Reward/lifting_object: 169.0204
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.0889
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 2.22s
                      Time elapsed: 01:18:40
                               ETA: 00:04:37

################################################################################
                     [1m Learning iteration 1890/2000 [0m                     

                       Computation: 44501 steps/s (collection: 2.110s, learning 0.099s)
             Mean action noise std: 3.61
          Mean value_function loss: 236.5795
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 69.0784
                       Mean reward: 836.53
               Mean episode length: 224.55
    Episode_Reward/reaching_object: 1.4818
     Episode_Reward/lifting_object: 161.5401
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0706
          Episode_Reward/joint_vel: -0.0879
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 2.21s
                      Time elapsed: 01:18:42
                               ETA: 00:04:34

################################################################################
                     [1m Learning iteration 1891/2000 [0m                     

                       Computation: 43660 steps/s (collection: 2.137s, learning 0.114s)
             Mean action noise std: 3.61
          Mean value_function loss: 207.7275
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 69.0899
                       Mean reward: 784.15
               Mean episode length: 212.91
    Episode_Reward/reaching_object: 1.4965
     Episode_Reward/lifting_object: 162.6039
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0707
          Episode_Reward/joint_vel: -0.0871
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 2.25s
                      Time elapsed: 01:18:44
                               ETA: 00:04:32

################################################################################
                     [1m Learning iteration 1892/2000 [0m                     

                       Computation: 42439 steps/s (collection: 2.201s, learning 0.116s)
             Mean action noise std: 3.62
          Mean value_function loss: 206.2331
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 69.1000
                       Mean reward: 893.10
               Mean episode length: 239.05
    Episode_Reward/reaching_object: 1.5288
     Episode_Reward/lifting_object: 166.8374
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0725
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 2.32s
                      Time elapsed: 01:18:46
                               ETA: 00:04:29

################################################################################
                     [1m Learning iteration 1893/2000 [0m                     

                       Computation: 41535 steps/s (collection: 2.269s, learning 0.098s)
             Mean action noise std: 3.62
          Mean value_function loss: 191.3975
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 69.1042
                       Mean reward: 859.04
               Mean episode length: 228.72
    Episode_Reward/reaching_object: 1.5221
     Episode_Reward/lifting_object: 165.7803
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0721
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 2.37s
                      Time elapsed: 01:18:49
                               ETA: 00:04:27

################################################################################
                     [1m Learning iteration 1894/2000 [0m                     

                       Computation: 42790 steps/s (collection: 2.197s, learning 0.101s)
             Mean action noise std: 3.62
          Mean value_function loss: 221.4922
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 69.1143
                       Mean reward: 836.85
               Mean episode length: 225.56
    Episode_Reward/reaching_object: 1.5469
     Episode_Reward/lifting_object: 168.3981
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 2.30s
                      Time elapsed: 01:18:51
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1895/2000 [0m                     

                       Computation: 43899 steps/s (collection: 2.135s, learning 0.104s)
             Mean action noise std: 3.62
          Mean value_function loss: 207.6242
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 69.1294
                       Mean reward: 819.33
               Mean episode length: 221.97
    Episode_Reward/reaching_object: 1.5200
     Episode_Reward/lifting_object: 165.4415
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0719
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 2.24s
                      Time elapsed: 01:18:53
                               ETA: 00:04:22

################################################################################
                     [1m Learning iteration 1896/2000 [0m                     

                       Computation: 43510 steps/s (collection: 2.153s, learning 0.107s)
             Mean action noise std: 3.62
          Mean value_function loss: 191.6265
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 69.1400
                       Mean reward: 863.03
               Mean episode length: 233.67
    Episode_Reward/reaching_object: 1.5435
     Episode_Reward/lifting_object: 167.5452
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0731
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 2.26s
                      Time elapsed: 01:18:56
                               ETA: 00:04:19

################################################################################
                     [1m Learning iteration 1897/2000 [0m                     

                       Computation: 43083 steps/s (collection: 2.177s, learning 0.105s)
             Mean action noise std: 3.62
          Mean value_function loss: 239.2609
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 69.1552
                       Mean reward: 833.67
               Mean episode length: 226.65
    Episode_Reward/reaching_object: 1.4855
     Episode_Reward/lifting_object: 160.4435
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0710
          Episode_Reward/joint_vel: -0.0882
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 2.28s
                      Time elapsed: 01:18:58
                               ETA: 00:04:17

################################################################################
                     [1m Learning iteration 1898/2000 [0m                     

                       Computation: 43533 steps/s (collection: 2.148s, learning 0.110s)
             Mean action noise std: 3.63
          Mean value_function loss: 282.2910
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 69.1730
                       Mean reward: 773.92
               Mean episode length: 211.00
    Episode_Reward/reaching_object: 1.4470
     Episode_Reward/lifting_object: 156.5944
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0851
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 2.26s
                      Time elapsed: 01:19:00
                               ETA: 00:04:14

################################################################################
                     [1m Learning iteration 1899/2000 [0m                     

                       Computation: 43572 steps/s (collection: 2.157s, learning 0.100s)
             Mean action noise std: 3.63
          Mean value_function loss: 236.4764
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 69.1820
                       Mean reward: 775.01
               Mean episode length: 212.06
    Episode_Reward/reaching_object: 1.4791
     Episode_Reward/lifting_object: 160.0352
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0705
          Episode_Reward/joint_vel: -0.0857
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 2.26s
                      Time elapsed: 01:19:02
                               ETA: 00:04:12

################################################################################
                     [1m Learning iteration 1900/2000 [0m                     

                       Computation: 42717 steps/s (collection: 2.210s, learning 0.091s)
             Mean action noise std: 3.63
          Mean value_function loss: 185.6916
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 69.1844
                       Mean reward: 802.66
               Mean episode length: 217.10
    Episode_Reward/reaching_object: 1.5139
     Episode_Reward/lifting_object: 164.7773
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0724
          Episode_Reward/joint_vel: -0.0868
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 2.30s
                      Time elapsed: 01:19:05
                               ETA: 00:04:09

################################################################################
                     [1m Learning iteration 1901/2000 [0m                     

                       Computation: 42923 steps/s (collection: 2.196s, learning 0.094s)
             Mean action noise std: 3.63
          Mean value_function loss: 177.1988
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 69.1855
                       Mean reward: 849.02
               Mean episode length: 227.94
    Episode_Reward/reaching_object: 1.5248
     Episode_Reward/lifting_object: 166.4240
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0726
          Episode_Reward/joint_vel: -0.0871
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 2.29s
                      Time elapsed: 01:19:07
                               ETA: 00:04:07

################################################################################
                     [1m Learning iteration 1902/2000 [0m                     

                       Computation: 41013 steps/s (collection: 2.292s, learning 0.105s)
             Mean action noise std: 3.63
          Mean value_function loss: 181.6038
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 69.1867
                       Mean reward: 811.10
               Mean episode length: 217.70
    Episode_Reward/reaching_object: 1.4952
     Episode_Reward/lifting_object: 162.6171
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0717
          Episode_Reward/joint_vel: -0.0879
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 2.40s
                      Time elapsed: 01:19:09
                               ETA: 00:04:04

################################################################################
                     [1m Learning iteration 1903/2000 [0m                     

                       Computation: 42250 steps/s (collection: 2.229s, learning 0.098s)
             Mean action noise std: 3.63
          Mean value_function loss: 169.7767
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 69.1875
                       Mean reward: 876.68
               Mean episode length: 235.85
    Episode_Reward/reaching_object: 1.5351
     Episode_Reward/lifting_object: 167.6356
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0736
          Episode_Reward/joint_vel: -0.0884
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 2.33s
                      Time elapsed: 01:19:12
                               ETA: 00:04:02

################################################################################
                     [1m Learning iteration 1904/2000 [0m                     

                       Computation: 42625 steps/s (collection: 2.189s, learning 0.118s)
             Mean action noise std: 3.63
          Mean value_function loss: 157.5352
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 69.1885
                       Mean reward: 814.32
               Mean episode length: 220.53
    Episode_Reward/reaching_object: 1.5499
     Episode_Reward/lifting_object: 169.7063
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0743
          Episode_Reward/joint_vel: -0.0889
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 2.31s
                      Time elapsed: 01:19:14
                               ETA: 00:03:59

################################################################################
                     [1m Learning iteration 1905/2000 [0m                     

                       Computation: 42988 steps/s (collection: 2.176s, learning 0.111s)
             Mean action noise std: 3.63
          Mean value_function loss: 185.7391
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 69.1902
                       Mean reward: 853.81
               Mean episode length: 229.14
    Episode_Reward/reaching_object: 1.4930
     Episode_Reward/lifting_object: 163.3552
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0719
          Episode_Reward/joint_vel: -0.0863
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 2.29s
                      Time elapsed: 01:19:16
                               ETA: 00:03:57

################################################################################
                     [1m Learning iteration 1906/2000 [0m                     

                       Computation: 41591 steps/s (collection: 2.241s, learning 0.123s)
             Mean action noise std: 3.63
          Mean value_function loss: 165.8926
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 69.1912
                       Mean reward: 856.31
               Mean episode length: 229.97
    Episode_Reward/reaching_object: 1.5439
     Episode_Reward/lifting_object: 169.0293
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0741
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 2.36s
                      Time elapsed: 01:19:19
                               ETA: 00:03:54

################################################################################
                     [1m Learning iteration 1907/2000 [0m                     

                       Computation: 42389 steps/s (collection: 2.219s, learning 0.100s)
             Mean action noise std: 3.63
          Mean value_function loss: 159.9373
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 69.1940
                       Mean reward: 871.19
               Mean episode length: 233.76
    Episode_Reward/reaching_object: 1.5705
     Episode_Reward/lifting_object: 172.4157
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0751
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 2.32s
                      Time elapsed: 01:19:21
                               ETA: 00:03:52

################################################################################
                     [1m Learning iteration 1908/2000 [0m                     

                       Computation: 42009 steps/s (collection: 2.232s, learning 0.109s)
             Mean action noise std: 3.63
          Mean value_function loss: 211.2009
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 69.2058
                       Mean reward: 848.62
               Mean episode length: 226.93
    Episode_Reward/reaching_object: 1.5158
     Episode_Reward/lifting_object: 165.7661
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0727
          Episode_Reward/joint_vel: -0.0870
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 2.34s
                      Time elapsed: 01:19:23
                               ETA: 00:03:49

################################################################################
                     [1m Learning iteration 1909/2000 [0m                     

                       Computation: 41142 steps/s (collection: 2.258s, learning 0.131s)
             Mean action noise std: 3.63
          Mean value_function loss: 200.7775
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 69.2196
                       Mean reward: 801.35
               Mean episode length: 217.46
    Episode_Reward/reaching_object: 1.5288
     Episode_Reward/lifting_object: 166.9362
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0886
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 2.39s
                      Time elapsed: 01:19:26
                               ETA: 00:03:47

################################################################################
                     [1m Learning iteration 1910/2000 [0m                     

                       Computation: 42873 steps/s (collection: 2.196s, learning 0.097s)
             Mean action noise std: 3.63
          Mean value_function loss: 215.7681
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 69.2335
                       Mean reward: 871.78
               Mean episode length: 232.34
    Episode_Reward/reaching_object: 1.5355
     Episode_Reward/lifting_object: 168.6948
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0891
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 2.29s
                      Time elapsed: 01:19:28
                               ETA: 00:03:44

################################################################################
                     [1m Learning iteration 1911/2000 [0m                     

                       Computation: 42443 steps/s (collection: 2.208s, learning 0.108s)
             Mean action noise std: 3.64
          Mean value_function loss: 197.9448
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 69.2446
                       Mean reward: 847.11
               Mean episode length: 225.56
    Episode_Reward/reaching_object: 1.5486
     Episode_Reward/lifting_object: 170.6638
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0743
          Episode_Reward/joint_vel: -0.0884
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 2.32s
                      Time elapsed: 01:19:30
                               ETA: 00:03:42

################################################################################
                     [1m Learning iteration 1912/2000 [0m                     

                       Computation: 42603 steps/s (collection: 2.205s, learning 0.102s)
             Mean action noise std: 3.64
          Mean value_function loss: 192.8622
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 69.2545
                       Mean reward: 825.64
               Mean episode length: 223.47
    Episode_Reward/reaching_object: 1.4977
     Episode_Reward/lifting_object: 164.1960
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0721
          Episode_Reward/joint_vel: -0.0864
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 2.31s
                      Time elapsed: 01:19:33
                               ETA: 00:03:39

################################################################################
                     [1m Learning iteration 1913/2000 [0m                     

                       Computation: 42885 steps/s (collection: 2.195s, learning 0.098s)
             Mean action noise std: 3.64
          Mean value_function loss: 202.1929
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 69.2572
                       Mean reward: 795.98
               Mean episode length: 215.69
    Episode_Reward/reaching_object: 1.4669
     Episode_Reward/lifting_object: 160.4539
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0710
          Episode_Reward/joint_vel: -0.0859
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 2.29s
                      Time elapsed: 01:19:35
                               ETA: 00:03:37

################################################################################
                     [1m Learning iteration 1914/2000 [0m                     

                       Computation: 43027 steps/s (collection: 2.187s, learning 0.098s)
             Mean action noise std: 3.64
          Mean value_function loss: 171.9373
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 69.2585
                       Mean reward: 870.78
               Mean episode length: 234.41
    Episode_Reward/reaching_object: 1.5723
     Episode_Reward/lifting_object: 172.9672
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0754
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 2.28s
                      Time elapsed: 01:19:37
                               ETA: 00:03:34

################################################################################
                     [1m Learning iteration 1915/2000 [0m                     

                       Computation: 43471 steps/s (collection: 2.160s, learning 0.102s)
             Mean action noise std: 3.64
          Mean value_function loss: 195.5643
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 69.2608
                       Mean reward: 808.92
               Mean episode length: 219.52
    Episode_Reward/reaching_object: 1.5024
     Episode_Reward/lifting_object: 164.6282
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0725
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 2.26s
                      Time elapsed: 01:19:39
                               ETA: 00:03:32

################################################################################
                     [1m Learning iteration 1916/2000 [0m                     

                       Computation: 43793 steps/s (collection: 2.152s, learning 0.093s)
             Mean action noise std: 3.64
          Mean value_function loss: 199.0650
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 69.2639
                       Mean reward: 839.40
               Mean episode length: 225.76
    Episode_Reward/reaching_object: 1.5040
     Episode_Reward/lifting_object: 165.1429
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0725
          Episode_Reward/joint_vel: -0.0877
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 2.24s
                      Time elapsed: 01:19:42
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 1917/2000 [0m                     

                       Computation: 42931 steps/s (collection: 2.178s, learning 0.112s)
             Mean action noise std: 3.64
          Mean value_function loss: 192.7073
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 69.2657
                       Mean reward: 850.44
               Mean episode length: 228.75
    Episode_Reward/reaching_object: 1.5171
     Episode_Reward/lifting_object: 166.7512
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 2.29s
                      Time elapsed: 01:19:44
                               ETA: 00:03:27

################################################################################
                     [1m Learning iteration 1918/2000 [0m                     

                       Computation: 42844 steps/s (collection: 2.171s, learning 0.124s)
             Mean action noise std: 3.64
          Mean value_function loss: 236.4258
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 69.2684
                       Mean reward: 839.67
               Mean episode length: 225.45
    Episode_Reward/reaching_object: 1.5019
     Episode_Reward/lifting_object: 164.7648
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0725
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 2.29s
                      Time elapsed: 01:19:46
                               ETA: 00:03:24

################################################################################
                     [1m Learning iteration 1919/2000 [0m                     

                       Computation: 42101 steps/s (collection: 2.221s, learning 0.114s)
             Mean action noise std: 3.64
          Mean value_function loss: 174.4195
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 69.2787
                       Mean reward: 812.67
               Mean episode length: 221.00
    Episode_Reward/reaching_object: 1.5372
     Episode_Reward/lifting_object: 169.1312
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0743
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 2.33s
                      Time elapsed: 01:19:49
                               ETA: 00:03:22

################################################################################
                     [1m Learning iteration 1920/2000 [0m                     

                       Computation: 43044 steps/s (collection: 2.194s, learning 0.090s)
             Mean action noise std: 3.64
          Mean value_function loss: 175.5950
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 69.2953
                       Mean reward: 809.74
               Mean episode length: 218.81
    Episode_Reward/reaching_object: 1.5418
     Episode_Reward/lifting_object: 169.5687
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0741
          Episode_Reward/joint_vel: -0.0886
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 2.28s
                      Time elapsed: 01:19:51
                               ETA: 00:03:19

################################################################################
                     [1m Learning iteration 1921/2000 [0m                     

                       Computation: 42420 steps/s (collection: 2.211s, learning 0.106s)
             Mean action noise std: 3.65
          Mean value_function loss: 213.7215
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 69.3063
                       Mean reward: 787.12
               Mean episode length: 212.77
    Episode_Reward/reaching_object: 1.5108
     Episode_Reward/lifting_object: 166.2558
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0729
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 2.32s
                      Time elapsed: 01:19:53
                               ETA: 00:03:17

################################################################################
                     [1m Learning iteration 1922/2000 [0m                     

                       Computation: 43329 steps/s (collection: 2.161s, learning 0.108s)
             Mean action noise std: 3.65
          Mean value_function loss: 180.6709
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 69.3134
                       Mean reward: 869.59
               Mean episode length: 233.41
    Episode_Reward/reaching_object: 1.5300
     Episode_Reward/lifting_object: 168.5113
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0739
          Episode_Reward/joint_vel: -0.0888
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 2.27s
                      Time elapsed: 01:19:56
                               ETA: 00:03:14

################################################################################
                     [1m Learning iteration 1923/2000 [0m                     

                       Computation: 42521 steps/s (collection: 2.187s, learning 0.125s)
             Mean action noise std: 3.65
          Mean value_function loss: 152.9290
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 69.3227
                       Mean reward: 861.57
               Mean episode length: 230.12
    Episode_Reward/reaching_object: 1.5482
     Episode_Reward/lifting_object: 171.1024
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0748
          Episode_Reward/joint_vel: -0.0889
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 2.31s
                      Time elapsed: 01:19:58
                               ETA: 00:03:12

################################################################################
                     [1m Learning iteration 1924/2000 [0m                     

                       Computation: 43765 steps/s (collection: 2.145s, learning 0.101s)
             Mean action noise std: 3.65
          Mean value_function loss: 139.0854
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 69.3428
                       Mean reward: 877.22
               Mean episode length: 233.77
    Episode_Reward/reaching_object: 1.5603
     Episode_Reward/lifting_object: 172.4483
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0754
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 2.25s
                      Time elapsed: 01:20:00
                               ETA: 00:03:09

################################################################################
                     [1m Learning iteration 1925/2000 [0m                     

                       Computation: 43930 steps/s (collection: 2.144s, learning 0.094s)
             Mean action noise std: 3.66
          Mean value_function loss: 187.6993
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 69.3650
                       Mean reward: 825.17
               Mean episode length: 222.44
    Episode_Reward/reaching_object: 1.5039
     Episode_Reward/lifting_object: 166.0180
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0873
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 2.24s
                      Time elapsed: 01:20:02
                               ETA: 00:03:07

################################################################################
                     [1m Learning iteration 1926/2000 [0m                     

                       Computation: 43301 steps/s (collection: 2.163s, learning 0.107s)
             Mean action noise std: 3.66
          Mean value_function loss: 161.4804
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 69.3857
                       Mean reward: 862.83
               Mean episode length: 231.18
    Episode_Reward/reaching_object: 1.5215
     Episode_Reward/lifting_object: 168.3260
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0740
          Episode_Reward/joint_vel: -0.0875
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 2.27s
                      Time elapsed: 01:20:05
                               ETA: 00:03:04

################################################################################
                     [1m Learning iteration 1927/2000 [0m                     

                       Computation: 42195 steps/s (collection: 2.211s, learning 0.119s)
             Mean action noise std: 3.66
          Mean value_function loss: 192.7994
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 69.3973
                       Mean reward: 810.97
               Mean episode length: 217.74
    Episode_Reward/reaching_object: 1.5325
     Episode_Reward/lifting_object: 169.9997
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0747
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 2.33s
                      Time elapsed: 01:20:07
                               ETA: 00:03:02

################################################################################
                     [1m Learning iteration 1928/2000 [0m                     

                       Computation: 41491 steps/s (collection: 2.264s, learning 0.105s)
             Mean action noise std: 3.66
          Mean value_function loss: 190.1662
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 69.4124
                       Mean reward: 875.51
               Mean episode length: 233.99
    Episode_Reward/reaching_object: 1.5157
     Episode_Reward/lifting_object: 167.0276
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0746
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 2.37s
                      Time elapsed: 01:20:09
                               ETA: 00:02:59

################################################################################
                     [1m Learning iteration 1929/2000 [0m                     

                       Computation: 42624 steps/s (collection: 2.192s, learning 0.115s)
             Mean action noise std: 3.66
          Mean value_function loss: 203.6963
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 69.4261
                       Mean reward: 815.99
               Mean episode length: 220.38
    Episode_Reward/reaching_object: 1.5138
     Episode_Reward/lifting_object: 167.2301
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0742
          Episode_Reward/joint_vel: -0.0886
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 2.31s
                      Time elapsed: 01:20:12
                               ETA: 00:02:57

################################################################################
                     [1m Learning iteration 1930/2000 [0m                     

                       Computation: 42529 steps/s (collection: 2.198s, learning 0.114s)
             Mean action noise std: 3.67
          Mean value_function loss: 240.9017
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 69.4456
                       Mean reward: 843.66
               Mean episode length: 227.86
    Episode_Reward/reaching_object: 1.5292
     Episode_Reward/lifting_object: 168.8589
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0746
          Episode_Reward/joint_vel: -0.0888
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 2.31s
                      Time elapsed: 01:20:14
                               ETA: 00:02:54

################################################################################
                     [1m Learning iteration 1931/2000 [0m                     

                       Computation: 42425 steps/s (collection: 2.193s, learning 0.125s)
             Mean action noise std: 3.67
          Mean value_function loss: 191.0741
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 69.4557
                       Mean reward: 897.42
               Mean episode length: 238.37
    Episode_Reward/reaching_object: 1.5338
     Episode_Reward/lifting_object: 170.0842
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0750
          Episode_Reward/joint_vel: -0.0885
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 2.32s
                      Time elapsed: 01:20:16
                               ETA: 00:02:52

################################################################################
                     [1m Learning iteration 1932/2000 [0m                     

                       Computation: 41617 steps/s (collection: 2.243s, learning 0.119s)
             Mean action noise std: 3.67
          Mean value_function loss: 189.3774
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 69.4668
                       Mean reward: 827.20
               Mean episode length: 224.22
    Episode_Reward/reaching_object: 1.5088
     Episode_Reward/lifting_object: 167.1671
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0743
          Episode_Reward/joint_vel: -0.0875
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 2.36s
                      Time elapsed: 01:20:19
                               ETA: 00:02:49

################################################################################
                     [1m Learning iteration 1933/2000 [0m                     

                       Computation: 41305 steps/s (collection: 2.275s, learning 0.105s)
             Mean action noise std: 3.67
          Mean value_function loss: 199.8496
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 69.4902
                       Mean reward: 822.28
               Mean episode length: 220.93
    Episode_Reward/reaching_object: 1.4879
     Episode_Reward/lifting_object: 164.2938
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0731
          Episode_Reward/joint_vel: -0.0864
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 2.38s
                      Time elapsed: 01:20:21
                               ETA: 00:02:47

################################################################################
                     [1m Learning iteration 1934/2000 [0m                     

                       Computation: 42615 steps/s (collection: 2.202s, learning 0.105s)
             Mean action noise std: 3.68
          Mean value_function loss: 215.2017
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 69.5207
                       Mean reward: 825.14
               Mean episode length: 221.77
    Episode_Reward/reaching_object: 1.4648
     Episode_Reward/lifting_object: 162.1590
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0722
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 2.31s
                      Time elapsed: 01:20:23
                               ETA: 00:02:44

################################################################################
                     [1m Learning iteration 1935/2000 [0m                     

                       Computation: 42418 steps/s (collection: 2.208s, learning 0.110s)
             Mean action noise std: 3.68
          Mean value_function loss: 201.9962
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 69.5356
                       Mean reward: 894.03
               Mean episode length: 237.85
    Episode_Reward/reaching_object: 1.4964
     Episode_Reward/lifting_object: 165.1821
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0736
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 2.32s
                      Time elapsed: 01:20:26
                               ETA: 00:02:42

################################################################################
                     [1m Learning iteration 1936/2000 [0m                     

                       Computation: 42077 steps/s (collection: 2.222s, learning 0.115s)
             Mean action noise std: 3.68
          Mean value_function loss: 173.8889
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 69.5514
                       Mean reward: 830.97
               Mean episode length: 224.30
    Episode_Reward/reaching_object: 1.4978
     Episode_Reward/lifting_object: 164.8620
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0738
          Episode_Reward/joint_vel: -0.0882
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 2.34s
                      Time elapsed: 01:20:28
                               ETA: 00:02:39

################################################################################
                     [1m Learning iteration 1937/2000 [0m                     

                       Computation: 43205 steps/s (collection: 2.176s, learning 0.099s)
             Mean action noise std: 3.68
          Mean value_function loss: 199.4920
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 69.5728
                       Mean reward: 874.97
               Mean episode length: 233.22
    Episode_Reward/reaching_object: 1.5281
     Episode_Reward/lifting_object: 169.1869
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0753
          Episode_Reward/joint_vel: -0.0884
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 2.28s
                      Time elapsed: 01:20:30
                               ETA: 00:02:37

################################################################################
                     [1m Learning iteration 1938/2000 [0m                     

                       Computation: 42764 steps/s (collection: 2.202s, learning 0.097s)
             Mean action noise std: 3.69
          Mean value_function loss: 185.5463
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 69.5890
                       Mean reward: 874.99
               Mean episode length: 232.79
    Episode_Reward/reaching_object: 1.5109
     Episode_Reward/lifting_object: 167.1043
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0749
          Episode_Reward/joint_vel: -0.0887
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 2.30s
                      Time elapsed: 01:20:32
                               ETA: 00:02:34

################################################################################
                     [1m Learning iteration 1939/2000 [0m                     

                       Computation: 43127 steps/s (collection: 2.174s, learning 0.106s)
             Mean action noise std: 3.69
          Mean value_function loss: 191.0704
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 69.5965
                       Mean reward: 841.86
               Mean episode length: 226.91
    Episode_Reward/reaching_object: 1.5346
     Episode_Reward/lifting_object: 169.7484
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0760
          Episode_Reward/joint_vel: -0.0887
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 2.28s
                      Time elapsed: 01:20:35
                               ETA: 00:02:32

################################################################################
                     [1m Learning iteration 1940/2000 [0m                     

                       Computation: 43209 steps/s (collection: 2.181s, learning 0.095s)
             Mean action noise std: 3.69
          Mean value_function loss: 157.4753
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 69.6069
                       Mean reward: 878.23
               Mean episode length: 233.24
    Episode_Reward/reaching_object: 1.5545
     Episode_Reward/lifting_object: 172.2767
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0768
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 2.28s
                      Time elapsed: 01:20:37
                               ETA: 00:02:29

################################################################################
                     [1m Learning iteration 1941/2000 [0m                     

                       Computation: 43611 steps/s (collection: 2.157s, learning 0.097s)
             Mean action noise std: 3.69
          Mean value_function loss: 193.5334
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 69.6165
                       Mean reward: 832.02
               Mean episode length: 225.28
    Episode_Reward/reaching_object: 1.5085
     Episode_Reward/lifting_object: 166.1035
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0749
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 2.25s
                      Time elapsed: 01:20:39
                               ETA: 00:02:27

################################################################################
                     [1m Learning iteration 1942/2000 [0m                     

                       Computation: 43123 steps/s (collection: 2.183s, learning 0.097s)
             Mean action noise std: 3.69
          Mean value_function loss: 209.7614
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 69.6278
                       Mean reward: 809.88
               Mean episode length: 220.75
    Episode_Reward/reaching_object: 1.5056
     Episode_Reward/lifting_object: 165.2493
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0750
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 2.28s
                      Time elapsed: 01:20:42
                               ETA: 00:02:24

################################################################################
                     [1m Learning iteration 1943/2000 [0m                     

                       Computation: 42934 steps/s (collection: 2.185s, learning 0.105s)
             Mean action noise std: 3.70
          Mean value_function loss: 209.5854
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 69.6453
                       Mean reward: 824.19
               Mean episode length: 220.96
    Episode_Reward/reaching_object: 1.4953
     Episode_Reward/lifting_object: 165.1491
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0742
          Episode_Reward/joint_vel: -0.0885
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 2.29s
                      Time elapsed: 01:20:44
                               ETA: 00:02:22

################################################################################
                     [1m Learning iteration 1944/2000 [0m                     

                       Computation: 42849 steps/s (collection: 2.185s, learning 0.109s)
             Mean action noise std: 3.70
          Mean value_function loss: 181.7528
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 69.6644
                       Mean reward: 804.57
               Mean episode length: 223.25
    Episode_Reward/reaching_object: 1.5228
     Episode_Reward/lifting_object: 166.4115
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0757
          Episode_Reward/joint_vel: -0.0920
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 2.29s
                      Time elapsed: 01:20:46
                               ETA: 00:02:19

################################################################################
                     [1m Learning iteration 1945/2000 [0m                     

                       Computation: 42677 steps/s (collection: 2.185s, learning 0.118s)
             Mean action noise std: 3.70
          Mean value_function loss: 218.3485
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 69.6693
                       Mean reward: 815.93
               Mean episode length: 219.73
    Episode_Reward/reaching_object: 1.5021
     Episode_Reward/lifting_object: 164.8940
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0746
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 2.30s
                      Time elapsed: 01:20:48
                               ETA: 00:02:17

################################################################################
                     [1m Learning iteration 1946/2000 [0m                     

                       Computation: 42422 steps/s (collection: 2.195s, learning 0.123s)
             Mean action noise std: 3.70
          Mean value_function loss: 210.6241
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 69.6738
                       Mean reward: 779.72
               Mean episode length: 211.79
    Episode_Reward/reaching_object: 1.4971
     Episode_Reward/lifting_object: 164.5530
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0745
          Episode_Reward/joint_vel: -0.0910
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 2.32s
                      Time elapsed: 01:20:51
                               ETA: 00:02:14

################################################################################
                     [1m Learning iteration 1947/2000 [0m                     

                       Computation: 42494 steps/s (collection: 2.216s, learning 0.098s)
             Mean action noise std: 3.70
          Mean value_function loss: 188.9872
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 69.6821
                       Mean reward: 847.90
               Mean episode length: 228.74
    Episode_Reward/reaching_object: 1.5054
     Episode_Reward/lifting_object: 164.5464
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0749
          Episode_Reward/joint_vel: -0.0913
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 2.31s
                      Time elapsed: 01:20:53
                               ETA: 00:02:12

################################################################################
                     [1m Learning iteration 1948/2000 [0m                     

                       Computation: 43746 steps/s (collection: 2.146s, learning 0.102s)
             Mean action noise std: 3.70
          Mean value_function loss: 245.8913
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 69.6977
                       Mean reward: 800.28
               Mean episode length: 216.84
    Episode_Reward/reaching_object: 1.4714
     Episode_Reward/lifting_object: 161.2047
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0898
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 2.25s
                      Time elapsed: 01:20:55
                               ETA: 00:02:09

################################################################################
                     [1m Learning iteration 1949/2000 [0m                     

                       Computation: 43551 steps/s (collection: 2.153s, learning 0.105s)
             Mean action noise std: 3.70
          Mean value_function loss: 213.9385
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 69.7093
                       Mean reward: 823.34
               Mean episode length: 222.24
    Episode_Reward/reaching_object: 1.5052
     Episode_Reward/lifting_object: 164.7970
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0753
          Episode_Reward/joint_vel: -0.0908
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 2.26s
                      Time elapsed: 01:20:58
                               ETA: 00:02:07

################################################################################
                     [1m Learning iteration 1950/2000 [0m                     

                       Computation: 42597 steps/s (collection: 2.178s, learning 0.130s)
             Mean action noise std: 3.71
          Mean value_function loss: 166.2815
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 69.7167
                       Mean reward: 806.85
               Mean episode length: 217.87
    Episode_Reward/reaching_object: 1.5167
     Episode_Reward/lifting_object: 165.5857
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0757
          Episode_Reward/joint_vel: -0.0910
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 2.31s
                      Time elapsed: 01:21:00
                               ETA: 00:02:04

################################################################################
                     [1m Learning iteration 1951/2000 [0m                     

                       Computation: 42876 steps/s (collection: 2.185s, learning 0.108s)
             Mean action noise std: 3.71
          Mean value_function loss: 189.2861
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 69.7257
                       Mean reward: 814.73
               Mean episode length: 221.44
    Episode_Reward/reaching_object: 1.4955
     Episode_Reward/lifting_object: 164.4233
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0752
          Episode_Reward/joint_vel: -0.0899
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 2.29s
                      Time elapsed: 01:21:02
                               ETA: 00:02:02

################################################################################
                     [1m Learning iteration 1952/2000 [0m                     

                       Computation: 43367 steps/s (collection: 2.155s, learning 0.112s)
             Mean action noise std: 3.71
          Mean value_function loss: 164.1262
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 69.7360
                       Mean reward: 866.35
               Mean episode length: 230.94
    Episode_Reward/reaching_object: 1.5726
     Episode_Reward/lifting_object: 172.6951
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0779
          Episode_Reward/joint_vel: -0.0927
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 2.27s
                      Time elapsed: 01:21:04
                               ETA: 00:01:59

################################################################################
                     [1m Learning iteration 1953/2000 [0m                     

                       Computation: 43588 steps/s (collection: 2.152s, learning 0.103s)
             Mean action noise std: 3.71
          Mean value_function loss: 164.1093
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 69.7420
                       Mean reward: 884.95
               Mean episode length: 238.08
    Episode_Reward/reaching_object: 1.5417
     Episode_Reward/lifting_object: 168.9608
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0771
          Episode_Reward/joint_vel: -0.0923
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 2.26s
                      Time elapsed: 01:21:07
                               ETA: 00:01:57

################################################################################
                     [1m Learning iteration 1954/2000 [0m                     

                       Computation: 43439 steps/s (collection: 2.157s, learning 0.106s)
             Mean action noise std: 3.71
          Mean value_function loss: 191.6627
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 69.7532
                       Mean reward: 821.95
               Mean episode length: 222.95
    Episode_Reward/reaching_object: 1.4932
     Episode_Reward/lifting_object: 163.4101
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0751
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 2.26s
                      Time elapsed: 01:21:09
                               ETA: 00:01:54

################################################################################
                     [1m Learning iteration 1955/2000 [0m                     

                       Computation: 42099 steps/s (collection: 2.226s, learning 0.109s)
             Mean action noise std: 3.71
          Mean value_function loss: 170.9400
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 69.7671
                       Mean reward: 861.81
               Mean episode length: 230.51
    Episode_Reward/reaching_object: 1.5077
     Episode_Reward/lifting_object: 166.2400
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0759
          Episode_Reward/joint_vel: -0.0911
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 2.34s
                      Time elapsed: 01:21:11
                               ETA: 00:01:52

################################################################################
                     [1m Learning iteration 1956/2000 [0m                     

                       Computation: 40493 steps/s (collection: 2.319s, learning 0.109s)
             Mean action noise std: 3.71
          Mean value_function loss: 204.4893
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 69.7767
                       Mean reward: 849.79
               Mean episode length: 228.09
    Episode_Reward/reaching_object: 1.5228
     Episode_Reward/lifting_object: 167.6478
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0765
          Episode_Reward/joint_vel: -0.0914
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 2.43s
                      Time elapsed: 01:21:14
                               ETA: 00:01:49

################################################################################
                     [1m Learning iteration 1957/2000 [0m                     

                       Computation: 39759 steps/s (collection: 2.351s, learning 0.122s)
             Mean action noise std: 3.72
          Mean value_function loss: 232.2507
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 69.7786
                       Mean reward: 885.95
               Mean episode length: 235.99
    Episode_Reward/reaching_object: 1.5062
     Episode_Reward/lifting_object: 166.1499
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0756
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 2.47s
                      Time elapsed: 01:21:16
                               ETA: 00:01:47

################################################################################
                     [1m Learning iteration 1958/2000 [0m                     

                       Computation: 42369 steps/s (collection: 2.199s, learning 0.122s)
             Mean action noise std: 3.72
          Mean value_function loss: 209.8497
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 69.7802
                       Mean reward: 828.70
               Mean episode length: 221.27
    Episode_Reward/reaching_object: 1.4850
     Episode_Reward/lifting_object: 163.8608
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0750
          Episode_Reward/joint_vel: -0.0888
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 2.32s
                      Time elapsed: 01:21:19
                               ETA: 00:01:44

################################################################################
                     [1m Learning iteration 1959/2000 [0m                     

                       Computation: 43188 steps/s (collection: 2.175s, learning 0.101s)
             Mean action noise std: 3.72
          Mean value_function loss: 266.2051
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 69.7813
                       Mean reward: 797.68
               Mean episode length: 216.77
    Episode_Reward/reaching_object: 1.4968
     Episode_Reward/lifting_object: 164.8014
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0759
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 2.28s
                      Time elapsed: 01:21:21
                               ETA: 00:01:42

################################################################################
                     [1m Learning iteration 1960/2000 [0m                     

                       Computation: 43104 steps/s (collection: 2.168s, learning 0.113s)
             Mean action noise std: 3.72
          Mean value_function loss: 231.9123
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 69.7852
                       Mean reward: 802.62
               Mean episode length: 217.94
    Episode_Reward/reaching_object: 1.5089
     Episode_Reward/lifting_object: 167.1324
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0762
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 2.28s
                      Time elapsed: 01:21:23
                               ETA: 00:01:39

################################################################################
                     [1m Learning iteration 1961/2000 [0m                     

                       Computation: 42930 steps/s (collection: 2.189s, learning 0.101s)
             Mean action noise std: 3.72
          Mean value_function loss: 185.5705
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 69.7994
                       Mean reward: 874.43
               Mean episode length: 233.45
    Episode_Reward/reaching_object: 1.4836
     Episode_Reward/lifting_object: 164.5732
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0752
          Episode_Reward/joint_vel: -0.0884
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 2.29s
                      Time elapsed: 01:21:25
                               ETA: 00:01:37

################################################################################
                     [1m Learning iteration 1962/2000 [0m                     

                       Computation: 43764 steps/s (collection: 2.148s, learning 0.099s)
             Mean action noise std: 3.72
          Mean value_function loss: 182.1484
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 69.8152
                       Mean reward: 824.77
               Mean episode length: 221.62
    Episode_Reward/reaching_object: 1.5027
     Episode_Reward/lifting_object: 166.7823
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0766
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 2.25s
                      Time elapsed: 01:21:28
                               ETA: 00:01:34

################################################################################
                     [1m Learning iteration 1963/2000 [0m                     

                       Computation: 43220 steps/s (collection: 2.165s, learning 0.109s)
             Mean action noise std: 3.72
          Mean value_function loss: 187.5089
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 69.8236
                       Mean reward: 848.60
               Mean episode length: 227.66
    Episode_Reward/reaching_object: 1.4888
     Episode_Reward/lifting_object: 165.3841
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0759
          Episode_Reward/joint_vel: -0.0889
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 2.27s
                      Time elapsed: 01:21:30
                               ETA: 00:01:32

################################################################################
                     [1m Learning iteration 1964/2000 [0m                     

                       Computation: 41748 steps/s (collection: 2.246s, learning 0.109s)
             Mean action noise std: 3.72
          Mean value_function loss: 194.3085
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 69.8270
                       Mean reward: 844.56
               Mean episode length: 226.97
    Episode_Reward/reaching_object: 1.5086
     Episode_Reward/lifting_object: 167.6782
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0770
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 2.35s
                      Time elapsed: 01:21:32
                               ETA: 00:01:29

################################################################################
                     [1m Learning iteration 1965/2000 [0m                     

                       Computation: 42068 steps/s (collection: 2.226s, learning 0.110s)
             Mean action noise std: 3.72
          Mean value_function loss: 207.5075
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 69.8288
                       Mean reward: 835.61
               Mean episode length: 224.73
    Episode_Reward/reaching_object: 1.4873
     Episode_Reward/lifting_object: 165.1310
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0762
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 2.34s
                      Time elapsed: 01:21:35
                               ETA: 00:01:27

################################################################################
                     [1m Learning iteration 1966/2000 [0m                     

                       Computation: 40465 steps/s (collection: 2.317s, learning 0.112s)
             Mean action noise std: 3.72
          Mean value_function loss: 168.3924
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 69.8302
                       Mean reward: 848.50
               Mean episode length: 226.88
    Episode_Reward/reaching_object: 1.5112
     Episode_Reward/lifting_object: 168.0889
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 2.43s
                      Time elapsed: 01:21:37
                               ETA: 00:01:24

################################################################################
                     [1m Learning iteration 1967/2000 [0m                     

                       Computation: 41164 steps/s (collection: 2.291s, learning 0.097s)
             Mean action noise std: 3.72
          Mean value_function loss: 192.7381
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 69.8339
                       Mean reward: 883.34
               Mean episode length: 235.48
    Episode_Reward/reaching_object: 1.5490
     Episode_Reward/lifting_object: 173.1086
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0792
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 2.39s
                      Time elapsed: 01:21:39
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 1968/2000 [0m                     

                       Computation: 43340 steps/s (collection: 2.159s, learning 0.110s)
             Mean action noise std: 3.72
          Mean value_function loss: 181.9443
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 69.8401
                       Mean reward: 887.62
               Mean episode length: 236.48
    Episode_Reward/reaching_object: 1.5318
     Episode_Reward/lifting_object: 171.2870
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0783
          Episode_Reward/joint_vel: -0.0910
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 2.27s
                      Time elapsed: 01:21:42
                               ETA: 00:01:19

################################################################################
                     [1m Learning iteration 1969/2000 [0m                     

                       Computation: 42825 steps/s (collection: 2.167s, learning 0.128s)
             Mean action noise std: 3.73
          Mean value_function loss: 211.8714
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 69.8445
                       Mean reward: 846.67
               Mean episode length: 228.64
    Episode_Reward/reaching_object: 1.5295
     Episode_Reward/lifting_object: 170.2033
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0784
          Episode_Reward/joint_vel: -0.0916
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 2.30s
                      Time elapsed: 01:21:44
                               ETA: 00:01:17

################################################################################
                     [1m Learning iteration 1970/2000 [0m                     

                       Computation: 42643 steps/s (collection: 2.180s, learning 0.125s)
             Mean action noise std: 3.73
          Mean value_function loss: 204.8811
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 69.8506
                       Mean reward: 832.62
               Mean episode length: 225.24
    Episode_Reward/reaching_object: 1.5044
     Episode_Reward/lifting_object: 167.0347
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0771
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 2.31s
                      Time elapsed: 01:21:46
                               ETA: 00:01:14

################################################################################
                     [1m Learning iteration 1971/2000 [0m                     

                       Computation: 43084 steps/s (collection: 2.164s, learning 0.118s)
             Mean action noise std: 3.73
          Mean value_function loss: 210.2321
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 69.8567
                       Mean reward: 814.70
               Mean episode length: 220.52
    Episode_Reward/reaching_object: 1.4803
     Episode_Reward/lifting_object: 165.0631
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0762
          Episode_Reward/joint_vel: -0.0898
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 2.28s
                      Time elapsed: 01:21:49
                               ETA: 00:01:12

################################################################################
                     [1m Learning iteration 1972/2000 [0m                     

                       Computation: 41406 steps/s (collection: 2.261s, learning 0.114s)
             Mean action noise std: 3.73
          Mean value_function loss: 202.6532
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 69.8620
                       Mean reward: 821.58
               Mean episode length: 222.96
    Episode_Reward/reaching_object: 1.5142
     Episode_Reward/lifting_object: 168.5071
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 2.37s
                      Time elapsed: 01:21:51
                               ETA: 00:01:09

################################################################################
                     [1m Learning iteration 1973/2000 [0m                     

                       Computation: 41018 steps/s (collection: 2.289s, learning 0.108s)
             Mean action noise std: 3.73
          Mean value_function loss: 195.6713
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 69.8724
                       Mean reward: 837.04
               Mean episode length: 224.98
    Episode_Reward/reaching_object: 1.4618
     Episode_Reward/lifting_object: 161.6719
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0747
          Episode_Reward/joint_vel: -0.0873
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 2.40s
                      Time elapsed: 01:21:53
                               ETA: 00:01:07

################################################################################
                     [1m Learning iteration 1974/2000 [0m                     

                       Computation: 43629 steps/s (collection: 2.160s, learning 0.094s)
             Mean action noise std: 3.73
          Mean value_function loss: 190.3632
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 69.8839
                       Mean reward: 852.32
               Mean episode length: 227.74
    Episode_Reward/reaching_object: 1.5411
     Episode_Reward/lifting_object: 171.8290
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0789
          Episode_Reward/joint_vel: -0.0915
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 2.25s
                      Time elapsed: 01:21:56
                               ETA: 00:01:04

################################################################################
                     [1m Learning iteration 1975/2000 [0m                     

                       Computation: 42383 steps/s (collection: 2.218s, learning 0.101s)
             Mean action noise std: 3.73
          Mean value_function loss: 203.5918
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 69.8931
                       Mean reward: 879.70
               Mean episode length: 234.57
    Episode_Reward/reaching_object: 1.4850
     Episode_Reward/lifting_object: 164.8537
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0760
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 2.32s
                      Time elapsed: 01:21:58
                               ETA: 00:01:02

################################################################################
                     [1m Learning iteration 1976/2000 [0m                     

                       Computation: 42030 steps/s (collection: 2.221s, learning 0.118s)
             Mean action noise std: 3.74
          Mean value_function loss: 217.1876
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 69.9089
                       Mean reward: 871.35
               Mean episode length: 234.39
    Episode_Reward/reaching_object: 1.4808
     Episode_Reward/lifting_object: 164.8343
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0762
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 2.34s
                      Time elapsed: 01:22:00
                               ETA: 00:00:59

################################################################################
                     [1m Learning iteration 1977/2000 [0m                     

                       Computation: 43054 steps/s (collection: 2.186s, learning 0.098s)
             Mean action noise std: 3.74
          Mean value_function loss: 212.7713
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 69.9238
                       Mean reward: 827.52
               Mean episode length: 222.14
    Episode_Reward/reaching_object: 1.4613
     Episode_Reward/lifting_object: 162.4279
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0755
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 2.28s
                      Time elapsed: 01:22:03
                               ETA: 00:00:57

################################################################################
                     [1m Learning iteration 1978/2000 [0m                     

                       Computation: 42568 steps/s (collection: 2.211s, learning 0.098s)
             Mean action noise std: 3.74
          Mean value_function loss: 241.4982
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 69.9322
                       Mean reward: 818.34
               Mean episode length: 221.20
    Episode_Reward/reaching_object: 1.4780
     Episode_Reward/lifting_object: 164.6224
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0765
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 2.31s
                      Time elapsed: 01:22:05
                               ETA: 00:00:54

################################################################################
                     [1m Learning iteration 1979/2000 [0m                     

                       Computation: 43458 steps/s (collection: 2.153s, learning 0.109s)
             Mean action noise std: 3.74
          Mean value_function loss: 227.3607
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 69.9434
                       Mean reward: 814.60
               Mean episode length: 220.08
    Episode_Reward/reaching_object: 1.4531
     Episode_Reward/lifting_object: 161.6512
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0757
          Episode_Reward/joint_vel: -0.0886
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 2.26s
                      Time elapsed: 01:22:07
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1980/2000 [0m                     

                       Computation: 39838 steps/s (collection: 2.351s, learning 0.117s)
             Mean action noise std: 3.74
          Mean value_function loss: 224.2368
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 69.9521
                       Mean reward: 828.54
               Mean episode length: 222.49
    Episode_Reward/reaching_object: 1.4770
     Episode_Reward/lifting_object: 164.1639
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0765
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 2.47s
                      Time elapsed: 01:22:10
                               ETA: 00:00:49

################################################################################
                     [1m Learning iteration 1981/2000 [0m                     

                       Computation: 40156 steps/s (collection: 2.328s, learning 0.120s)
             Mean action noise std: 3.74
          Mean value_function loss: 207.8909
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 69.9578
                       Mean reward: 819.96
               Mean episode length: 220.90
    Episode_Reward/reaching_object: 1.4665
     Episode_Reward/lifting_object: 163.3943
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0757
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 2.45s
                      Time elapsed: 01:22:12
                               ETA: 00:00:47

################################################################################
                     [1m Learning iteration 1982/2000 [0m                     

                       Computation: 41738 steps/s (collection: 2.239s, learning 0.116s)
             Mean action noise std: 3.74
          Mean value_function loss: 204.6564
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 69.9607
                       Mean reward: 825.12
               Mean episode length: 220.16
    Episode_Reward/reaching_object: 1.4875
     Episode_Reward/lifting_object: 166.1148
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0770
          Episode_Reward/joint_vel: -0.0885
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 2.36s
                      Time elapsed: 01:22:14
                               ETA: 00:00:44

################################################################################
                     [1m Learning iteration 1983/2000 [0m                     

                       Computation: 41553 steps/s (collection: 2.247s, learning 0.119s)
             Mean action noise std: 3.74
          Mean value_function loss: 246.6670
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 69.9617
                       Mean reward: 847.86
               Mean episode length: 227.25
    Episode_Reward/reaching_object: 1.4838
     Episode_Reward/lifting_object: 166.0519
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.0887
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 2.37s
                      Time elapsed: 01:22:17
                               ETA: 00:00:42

################################################################################
                     [1m Learning iteration 1984/2000 [0m                     

                       Computation: 42852 steps/s (collection: 2.171s, learning 0.123s)
             Mean action noise std: 3.75
          Mean value_function loss: 213.6393
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 69.9660
                       Mean reward: 822.97
               Mean episode length: 221.46
    Episode_Reward/reaching_object: 1.4900
     Episode_Reward/lifting_object: 166.9884
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0782
          Episode_Reward/joint_vel: -0.0899
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 2.29s
                      Time elapsed: 01:22:19
                               ETA: 00:00:39

################################################################################
                     [1m Learning iteration 1985/2000 [0m                     

                       Computation: 42883 steps/s (collection: 2.189s, learning 0.103s)
             Mean action noise std: 3.75
          Mean value_function loss: 245.3303
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 69.9738
                       Mean reward: 826.33
               Mean episode length: 221.37
    Episode_Reward/reaching_object: 1.4817
     Episode_Reward/lifting_object: 166.2562
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0774
          Episode_Reward/joint_vel: -0.0884
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 2.29s
                      Time elapsed: 01:22:21
                               ETA: 00:00:37

################################################################################
                     [1m Learning iteration 1986/2000 [0m                     

                       Computation: 43199 steps/s (collection: 2.180s, learning 0.096s)
             Mean action noise std: 3.75
          Mean value_function loss: 242.6190
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 69.9806
                       Mean reward: 835.66
               Mean episode length: 225.40
    Episode_Reward/reaching_object: 1.4251
     Episode_Reward/lifting_object: 159.4264
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0754
          Episode_Reward/joint_vel: -0.0887
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 2.28s
                      Time elapsed: 01:22:24
                               ETA: 00:00:34

################################################################################
                     [1m Learning iteration 1987/2000 [0m                     

                       Computation: 43105 steps/s (collection: 2.172s, learning 0.109s)
             Mean action noise std: 3.75
          Mean value_function loss: 241.0120
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 69.9906
                       Mean reward: 769.61
               Mean episode length: 208.98
    Episode_Reward/reaching_object: 1.4425
     Episode_Reward/lifting_object: 161.7235
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0763
          Episode_Reward/joint_vel: -0.0891
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 2.28s
                      Time elapsed: 01:22:26
                               ETA: 00:00:32

################################################################################
                     [1m Learning iteration 1988/2000 [0m                     

                       Computation: 43016 steps/s (collection: 2.167s, learning 0.118s)
             Mean action noise std: 3.75
          Mean value_function loss: 277.0060
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 69.9989
                       Mean reward: 813.72
               Mean episode length: 219.57
    Episode_Reward/reaching_object: 1.4381
     Episode_Reward/lifting_object: 161.0647
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0764
          Episode_Reward/joint_vel: -0.0889
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 2.29s
                      Time elapsed: 01:22:28
                               ETA: 00:00:29

################################################################################
                     [1m Learning iteration 1989/2000 [0m                     

                       Computation: 43342 steps/s (collection: 2.171s, learning 0.098s)
             Mean action noise std: 3.75
          Mean value_function loss: 314.3890
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 70.0083
                       Mean reward: 800.80
               Mean episode length: 216.93
    Episode_Reward/reaching_object: 1.3939
     Episode_Reward/lifting_object: 155.9157
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0740
          Episode_Reward/joint_vel: -0.0869
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 2.27s
                      Time elapsed: 01:22:30
                               ETA: 00:00:27

################################################################################
                     [1m Learning iteration 1990/2000 [0m                     

                       Computation: 43029 steps/s (collection: 2.179s, learning 0.106s)
             Mean action noise std: 3.76
          Mean value_function loss: 256.1260
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 70.0253
                       Mean reward: 806.99
               Mean episode length: 215.68
    Episode_Reward/reaching_object: 1.4167
     Episode_Reward/lifting_object: 159.0345
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0752
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 2.28s
                      Time elapsed: 01:22:33
                               ETA: 00:00:24

################################################################################
                     [1m Learning iteration 1991/2000 [0m                     

                       Computation: 40731 steps/s (collection: 2.273s, learning 0.140s)
             Mean action noise std: 3.76
          Mean value_function loss: 246.1392
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 70.0403
                       Mean reward: 766.81
               Mean episode length: 208.14
    Episode_Reward/reaching_object: 1.4572
     Episode_Reward/lifting_object: 164.3562
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0770
          Episode_Reward/joint_vel: -0.0887
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 2.41s
                      Time elapsed: 01:22:35
                               ETA: 00:00:22

################################################################################
                     [1m Learning iteration 1992/2000 [0m                     

                       Computation: 41046 steps/s (collection: 2.284s, learning 0.111s)
             Mean action noise std: 3.76
          Mean value_function loss: 308.2971
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 70.0546
                       Mean reward: 753.46
               Mean episode length: 205.39
    Episode_Reward/reaching_object: 1.4260
     Episode_Reward/lifting_object: 161.0148
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0760
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 2.39s
                      Time elapsed: 01:22:38
                               ETA: 00:00:19

################################################################################
                     [1m Learning iteration 1993/2000 [0m                     

                       Computation: 42649 steps/s (collection: 2.206s, learning 0.099s)
             Mean action noise std: 3.76
          Mean value_function loss: 321.0444
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 70.0646
                       Mean reward: 830.05
               Mean episode length: 223.26
    Episode_Reward/reaching_object: 1.4381
     Episode_Reward/lifting_object: 161.9801
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0763
          Episode_Reward/joint_vel: -0.0873
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 2.30s
                      Time elapsed: 01:22:40
                               ETA: 00:00:17

################################################################################
                     [1m Learning iteration 1994/2000 [0m                     

                       Computation: 43023 steps/s (collection: 2.183s, learning 0.102s)
             Mean action noise std: 3.76
          Mean value_function loss: 283.6978
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 70.0755
                       Mean reward: 769.26
               Mean episode length: 211.57
    Episode_Reward/reaching_object: 1.3857
     Episode_Reward/lifting_object: 155.3354
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0738
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 2.28s
                      Time elapsed: 01:22:42
                               ETA: 00:00:14

################################################################################
                     [1m Learning iteration 1995/2000 [0m                     

                       Computation: 42544 steps/s (collection: 2.187s, learning 0.124s)
             Mean action noise std: 3.77
          Mean value_function loss: 264.6455
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 70.0867
                       Mean reward: 820.70
               Mean episode length: 220.06
    Episode_Reward/reaching_object: 1.4540
     Episode_Reward/lifting_object: 163.6151
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.0885
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 2.31s
                      Time elapsed: 01:22:44
                               ETA: 00:00:12

################################################################################
                     [1m Learning iteration 1996/2000 [0m                     

                       Computation: 42470 steps/s (collection: 2.204s, learning 0.111s)
             Mean action noise std: 3.77
          Mean value_function loss: 263.7266
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 70.0908
                       Mean reward: 817.10
               Mean episode length: 222.45
    Episode_Reward/reaching_object: 1.4092
     Episode_Reward/lifting_object: 157.0516
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0752
          Episode_Reward/joint_vel: -0.0862
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 2.31s
                      Time elapsed: 01:22:47
                               ETA: 00:00:09

################################################################################
                     [1m Learning iteration 1997/2000 [0m                     

                       Computation: 43066 steps/s (collection: 2.171s, learning 0.112s)
             Mean action noise std: 3.77
          Mean value_function loss: 236.1066
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 70.0931
                       Mean reward: 781.69
               Mean episode length: 210.36
    Episode_Reward/reaching_object: 1.4293
     Episode_Reward/lifting_object: 160.1348
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0761
          Episode_Reward/joint_vel: -0.0872
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 2.28s
                      Time elapsed: 01:22:49
                               ETA: 00:00:07

################################################################################
                     [1m Learning iteration 1998/2000 [0m                     

                       Computation: 41935 steps/s (collection: 2.235s, learning 0.110s)
             Mean action noise std: 3.77
          Mean value_function loss: 279.7269
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 70.0971
                       Mean reward: 743.43
               Mean episode length: 202.49
    Episode_Reward/reaching_object: 1.4312
     Episode_Reward/lifting_object: 161.1636
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0762
          Episode_Reward/joint_vel: -0.0869
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 2.34s
                      Time elapsed: 01:22:51
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1999/2000 [0m                     

                       Computation: 42344 steps/s (collection: 2.222s, learning 0.100s)
             Mean action noise std: 3.77
          Mean value_function loss: 261.1467
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 70.1002
                       Mean reward: 800.55
               Mean episode length: 215.71
    Episode_Reward/reaching_object: 1.3793
     Episode_Reward/lifting_object: 154.8441
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0741
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 2.32s
                      Time elapsed: 01:22:54
                               ETA: 00:00:02

