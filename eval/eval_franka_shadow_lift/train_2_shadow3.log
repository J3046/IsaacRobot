################################################################################
                      [1m Learning iteration 0/2000 [0m                       

                       Computation: 10181 steps/s (collection: 9.400s, learning 0.255s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 31.2504
                       Mean reward: 0.00
               Mean episode length: 21.21
    Episode_Reward/reaching_object: 0.0007
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0002
          Episode_Reward/joint_vel: -0.0003
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 9.66s
                      Time elapsed: 00:00:09
                               ETA: 05:21:50

################################################################################
                      [1m Learning iteration 1/2000 [0m                       

                       Computation: 14276 steps/s (collection: 6.758s, learning 0.128s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 31.3680
                       Mean reward: 0.00
               Mean episode length: 45.83
    Episode_Reward/reaching_object: 0.0019
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0006
          Episode_Reward/joint_vel: -0.0008
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 6.89s
                      Time elapsed: 00:00:16
                               ETA: 04:35:33

################################################################################
                      [1m Learning iteration 2/2000 [0m                       

                       Computation: 14961 steps/s (collection: 6.420s, learning 0.150s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 31.4496
                       Mean reward: 0.01
               Mean episode length: 69.78
    Episode_Reward/reaching_object: 0.0030
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0011
          Episode_Reward/joint_vel: -0.0014
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 6.57s
                      Time elapsed: 00:00:23
                               ETA: 04:16:32

################################################################################
                      [1m Learning iteration 3/2000 [0m                       

                       Computation: 14500 steps/s (collection: 6.644s, learning 0.135s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 31.4505
                       Mean reward: 0.01
               Mean episode length: 93.42
    Episode_Reward/reaching_object: 0.0045
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0015
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 6.78s
                      Time elapsed: 00:00:29
                               ETA: 04:08:43

################################################################################
                      [1m Learning iteration 4/2000 [0m                       

                       Computation: 14377 steps/s (collection: 6.707s, learning 0.130s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 31.4573
                       Mean reward: 0.02
               Mean episode length: 117.74
    Episode_Reward/reaching_object: 0.0061
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 6.84s
                      Time elapsed: 00:00:36
                               ETA: 04:04:21

################################################################################
                      [1m Learning iteration 5/2000 [0m                       

                       Computation: 14582 steps/s (collection: 6.607s, learning 0.134s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 31.5069
                       Mean reward: 0.02
               Mean episode length: 141.96
    Episode_Reward/reaching_object: 0.0083
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0031
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 6.74s
                      Time elapsed: 00:00:43
                               ETA: 04:00:53

################################################################################
                      [1m Learning iteration 6/2000 [0m                       

                       Computation: 14604 steps/s (collection: 6.576s, learning 0.155s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 31.5458
                       Mean reward: 0.03
               Mean episode length: 165.77
    Episode_Reward/reaching_object: 0.0106
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0037
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 6.73s
                      Time elapsed: 00:00:50
                               ETA: 03:58:20

################################################################################
                      [1m Learning iteration 7/2000 [0m                       

                       Computation: 14249 steps/s (collection: 6.764s, learning 0.135s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 31.5908
                       Mean reward: 0.04
               Mean episode length: 189.36
    Episode_Reward/reaching_object: 0.0126
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 6.90s
                      Time elapsed: 00:00:57
                               ETA: 03:57:04

################################################################################
                      [1m Learning iteration 8/2000 [0m                       

                       Computation: 17874 steps/s (collection: 5.392s, learning 0.108s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 31.6221
                       Mean reward: 0.04
               Mean episode length: 213.10
    Episode_Reward/reaching_object: 0.0160
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0048
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 5.50s
                      Time elapsed: 00:01:02
                               ETA: 03:50:55

################################################################################
                      [1m Learning iteration 9/2000 [0m                       

                       Computation: 58154 steps/s (collection: 1.584s, learning 0.107s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 31.6605
                       Mean reward: 0.07
               Mean episode length: 237.13
    Episode_Reward/reaching_object: 0.0203
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0054
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 1.69s
                      Time elapsed: 00:01:04
                               ETA: 03:33:20

################################################################################
                      [1m Learning iteration 10/2000 [0m                      

                       Computation: 59125 steps/s (collection: 1.553s, learning 0.109s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 31.6721
                       Mean reward: 0.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0234
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0058
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 1.66s
                      Time elapsed: 00:01:05
                               ETA: 03:18:51

################################################################################
                      [1m Learning iteration 11/2000 [0m                      

                       Computation: 54207 steps/s (collection: 1.721s, learning 0.093s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 31.6836
                       Mean reward: 0.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0266
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0059
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 1.81s
                      Time elapsed: 00:01:07
                               ETA: 03:07:12

################################################################################
                      [1m Learning iteration 12/2000 [0m                      

                       Computation: 58937 steps/s (collection: 1.574s, learning 0.094s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 31.6958
                       Mean reward: 0.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0332
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0059
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 1.67s
                      Time elapsed: 00:01:09
                               ETA: 02:56:58

################################################################################
                      [1m Learning iteration 13/2000 [0m                      

                       Computation: 58034 steps/s (collection: 1.602s, learning 0.092s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 31.7335
                       Mean reward: 0.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0382
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0059
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 1.69s
                      Time elapsed: 00:01:11
                               ETA: 02:48:15

################################################################################
                      [1m Learning iteration 14/2000 [0m                      

                       Computation: 58571 steps/s (collection: 1.581s, learning 0.097s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 31.7287
                       Mean reward: 0.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0408
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0059
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 1.68s
                      Time elapsed: 00:01:12
                               ETA: 02:40:39

################################################################################
                      [1m Learning iteration 15/2000 [0m                      

                       Computation: 58285 steps/s (collection: 1.596s, learning 0.091s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 31.7386
                       Mean reward: 0.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0592
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0058
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 1.69s
                      Time elapsed: 00:01:14
                               ETA: 02:34:01

################################################################################
                      [1m Learning iteration 16/2000 [0m                      

                       Computation: 56765 steps/s (collection: 1.642s, learning 0.090s)
             Mean action noise std: 1.03
          Mean value_function loss: 1.6239
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 31.8154
                       Mean reward: -0.62
               Mean episode length: 249.87
    Episode_Reward/reaching_object: 0.0730
     Episode_Reward/lifting_object: -0.1324
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0059
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 1.73s
                      Time elapsed: 00:01:16
                               ETA: 02:28:15

################################################################################
                      [1m Learning iteration 17/2000 [0m                      

                       Computation: 55609 steps/s (collection: 1.674s, learning 0.094s)
             Mean action noise std: 1.04
          Mean value_function loss: 1.5498
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 31.8892
                       Mean reward: -0.51
               Mean episode length: 249.23
    Episode_Reward/reaching_object: 0.0965
     Episode_Reward/lifting_object: -0.1426
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0058
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 1.77s
                      Time elapsed: 00:01:17
                               ETA: 02:23:12

################################################################################
                      [1m Learning iteration 18/2000 [0m                      

                       Computation: 55357 steps/s (collection: 1.683s, learning 0.093s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.9281
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 32.1426
                       Mean reward: -0.48
               Mean episode length: 249.39
    Episode_Reward/reaching_object: 0.1180
     Episode_Reward/lifting_object: -0.2033
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0059
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 1.78s
                      Time elapsed: 00:01:19
                               ETA: 02:18:41

################################################################################
                      [1m Learning iteration 19/2000 [0m                      

                       Computation: 54180 steps/s (collection: 1.723s, learning 0.092s)
             Mean action noise std: 1.05
          Mean value_function loss: 1.4121
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 32.2719
                       Mean reward: 0.24
               Mean episode length: 247.57
    Episode_Reward/reaching_object: 0.1392
     Episode_Reward/lifting_object: -0.2024
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0059
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 1.81s
                      Time elapsed: 00:01:21
                               ETA: 02:14:40

################################################################################
                      [1m Learning iteration 20/2000 [0m                      

                       Computation: 55518 steps/s (collection: 1.681s, learning 0.090s)
             Mean action noise std: 1.06
          Mean value_function loss: 0.0802
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 32.4458
                       Mean reward: 0.56
               Mean episode length: 247.44
    Episode_Reward/reaching_object: 0.1535
     Episode_Reward/lifting_object: -0.0459
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0059
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 1.77s
                      Time elapsed: 00:01:23
                               ETA: 02:10:58

################################################################################
                      [1m Learning iteration 21/2000 [0m                      

                       Computation: 55275 steps/s (collection: 1.678s, learning 0.101s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.9599
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 32.5758
                       Mean reward: -0.53
               Mean episode length: 247.75
    Episode_Reward/reaching_object: 0.1725
     Episode_Reward/lifting_object: -0.1848
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0060
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 1.78s
                      Time elapsed: 00:01:25
                               ETA: 02:07:37

################################################################################
                      [1m Learning iteration 22/2000 [0m                      

                       Computation: 54518 steps/s (collection: 1.713s, learning 0.090s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.9186
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 32.7197
                       Mean reward: 0.54
               Mean episode length: 247.11
    Episode_Reward/reaching_object: 0.1849
     Episode_Reward/lifting_object: -0.1354
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0060
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 1.80s
                      Time elapsed: 00:01:26
                               ETA: 02:04:36

################################################################################
                      [1m Learning iteration 23/2000 [0m                      

                       Computation: 54588 steps/s (collection: 1.712s, learning 0.089s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 32.8550
                       Mean reward: 0.96
               Mean episode length: 249.76
    Episode_Reward/reaching_object: 0.1845
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 1.80s
                      Time elapsed: 00:01:28
                               ETA: 02:01:49

################################################################################
                      [1m Learning iteration 24/2000 [0m                      

                       Computation: 57038 steps/s (collection: 1.638s, learning 0.086s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.0010
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 32.9833
                       Mean reward: 0.82
               Mean episode length: 249.25
    Episode_Reward/reaching_object: 0.1846
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0062
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 1.72s
                      Time elapsed: 00:01:30
                               ETA: 01:59:09

################################################################################
                      [1m Learning iteration 25/2000 [0m                      

                       Computation: 55096 steps/s (collection: 1.669s, learning 0.116s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.1272
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 33.0311
                       Mean reward: 0.19
               Mean episode length: 248.70
    Episode_Reward/reaching_object: 0.1756
     Episode_Reward/lifting_object: -0.0441
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0063
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 1.78s
                      Time elapsed: 00:01:32
                               ETA: 01:56:46

################################################################################
                      [1m Learning iteration 26/2000 [0m                      

                       Computation: 53794 steps/s (collection: 1.714s, learning 0.113s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 33.0779
                       Mean reward: 0.66
               Mean episode length: 249.49
    Episode_Reward/reaching_object: 0.1604
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0064
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 1.83s
                      Time elapsed: 00:01:34
                               ETA: 01:54:37

################################################################################
                      [1m Learning iteration 27/2000 [0m                      

                       Computation: 53973 steps/s (collection: 1.709s, learning 0.113s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 33.2118
                       Mean reward: 0.64
               Mean episode length: 249.12
    Episode_Reward/reaching_object: 0.1466
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0065
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 1.82s
                      Time elapsed: 00:01:35
                               ETA: 01:52:36

################################################################################
                      [1m Learning iteration 28/2000 [0m                      

                       Computation: 56255 steps/s (collection: 1.661s, learning 0.086s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0505
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 33.2332
                       Mean reward: 0.41
               Mean episode length: 248.78
    Episode_Reward/reaching_object: 0.1352
     Episode_Reward/lifting_object: -0.0221
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0065
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 1.75s
                      Time elapsed: 00:01:37
                               ETA: 01:50:39

################################################################################
                      [1m Learning iteration 29/2000 [0m                      

                       Computation: 56833 steps/s (collection: 1.630s, learning 0.100s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.2503
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 33.2709
                       Mean reward: 0.47
               Mean episode length: 249.93
    Episode_Reward/reaching_object: 0.1207
     Episode_Reward/lifting_object: -0.0088
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0066
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 1.73s
                      Time elapsed: 00:01:39
                               ETA: 01:48:48

################################################################################
                      [1m Learning iteration 30/2000 [0m                      

                       Computation: 57437 steps/s (collection: 1.625s, learning 0.086s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.2399
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 33.3967
                       Mean reward: 0.57
               Mean episode length: 249.77
    Episode_Reward/reaching_object: 0.1293
     Episode_Reward/lifting_object: -0.0385
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0067
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 1.71s
                      Time elapsed: 00:01:41
                               ETA: 01:47:03

################################################################################
                      [1m Learning iteration 31/2000 [0m                      

                       Computation: 56466 steps/s (collection: 1.649s, learning 0.092s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 33.5058
                       Mean reward: 0.54
               Mean episode length: 249.64
    Episode_Reward/reaching_object: 0.1201
     Episode_Reward/lifting_object: -0.0357
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0067
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 1.74s
                      Time elapsed: 00:01:42
                               ETA: 01:45:26

################################################################################
                      [1m Learning iteration 32/2000 [0m                      

                       Computation: 55963 steps/s (collection: 1.671s, learning 0.086s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.1276
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 33.6291
                       Mean reward: 0.63
               Mean episode length: 249.33
    Episode_Reward/reaching_object: 0.1331
     Episode_Reward/lifting_object: -0.0088
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 1.76s
                      Time elapsed: 00:01:44
                               ETA: 01:43:56

################################################################################
                      [1m Learning iteration 33/2000 [0m                      

                       Computation: 55098 steps/s (collection: 1.698s, learning 0.087s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.3110
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 33.6931
                       Mean reward: 0.43
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.1417
     Episode_Reward/lifting_object: -0.0217
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 1.78s
                      Time elapsed: 00:01:46
                               ETA: 01:42:33

################################################################################
                      [1m Learning iteration 34/2000 [0m                      

                       Computation: 54755 steps/s (collection: 1.705s, learning 0.090s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.1156
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 33.8522
                       Mean reward: 0.73
               Mean episode length: 246.50
    Episode_Reward/reaching_object: 0.1473
     Episode_Reward/lifting_object: -0.1112
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 1.80s
                      Time elapsed: 00:01:48
                               ETA: 01:41:15

################################################################################
                      [1m Learning iteration 35/2000 [0m                      

                       Computation: 54432 steps/s (collection: 1.718s, learning 0.088s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 33.9486
                       Mean reward: 0.73
               Mean episode length: 246.04
    Episode_Reward/reaching_object: 0.1501
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 1.81s
                      Time elapsed: 00:01:49
                               ETA: 01:40:02

################################################################################
                      [1m Learning iteration 36/2000 [0m                      

                       Computation: 55865 steps/s (collection: 1.674s, learning 0.086s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.1095
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 34.0209
                       Mean reward: 0.79
               Mean episode length: 246.53
    Episode_Reward/reaching_object: 0.1590
     Episode_Reward/lifting_object: -0.0167
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 1.76s
                      Time elapsed: 00:01:51
                               ETA: 01:38:50

################################################################################
                      [1m Learning iteration 37/2000 [0m                      

                       Computation: 54906 steps/s (collection: 1.701s, learning 0.089s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.3782
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.0603
                       Mean reward: 0.84
               Mean episode length: 246.21
    Episode_Reward/reaching_object: 0.1722
     Episode_Reward/lifting_object: -0.0635
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0070
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 1.79s
                      Time elapsed: 00:01:53
                               ETA: 01:37:43

################################################################################
                      [1m Learning iteration 38/2000 [0m                      

                       Computation: 54226 steps/s (collection: 1.724s, learning 0.089s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.0577
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.1523
                       Mean reward: 0.93
               Mean episode length: 245.31
    Episode_Reward/reaching_object: 0.1805
     Episode_Reward/lifting_object: -0.0227
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0070
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 1.81s
                      Time elapsed: 00:01:55
                               ETA: 01:36:41

################################################################################
                      [1m Learning iteration 39/2000 [0m                      

                       Computation: 54794 steps/s (collection: 1.706s, learning 0.088s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.0546
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 34.2739
                       Mean reward: 0.82
               Mean episode length: 245.05
    Episode_Reward/reaching_object: 0.1759
     Episode_Reward/lifting_object: -0.0083
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0070
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 1.79s
                      Time elapsed: 00:01:57
                               ETA: 01:35:41

################################################################################
                      [1m Learning iteration 40/2000 [0m                      

                       Computation: 53358 steps/s (collection: 1.754s, learning 0.088s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.0614
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 34.3487
                       Mean reward: 0.85
               Mean episode length: 240.17
    Episode_Reward/reaching_object: 0.1751
     Episode_Reward/lifting_object: -0.0456
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0070
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 1.84s
                      Time elapsed: 00:01:58
                               ETA: 01:34:46

################################################################################
                      [1m Learning iteration 41/2000 [0m                      

                       Computation: 54254 steps/s (collection: 1.709s, learning 0.103s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.2495
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 34.5776
                       Mean reward: 0.28
               Mean episode length: 239.99
    Episode_Reward/reaching_object: 0.1849
     Episode_Reward/lifting_object: -0.0200
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0071
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 1.81s
                      Time elapsed: 00:02:00
                               ETA: 01:33:53

################################################################################
                      [1m Learning iteration 42/2000 [0m                      

                       Computation: 53969 steps/s (collection: 1.714s, learning 0.107s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.2669
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 34.6451
                       Mean reward: 0.90
               Mean episode length: 238.08
    Episode_Reward/reaching_object: 0.1915
     Episode_Reward/lifting_object: -0.0111
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0071
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 1.82s
                      Time elapsed: 00:02:02
                               ETA: 01:33:02

################################################################################
                      [1m Learning iteration 43/2000 [0m                      

                       Computation: 53997 steps/s (collection: 1.705s, learning 0.115s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.1334
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 34.7664
                       Mean reward: 0.77
               Mean episode length: 241.51
    Episode_Reward/reaching_object: 0.1769
     Episode_Reward/lifting_object: -0.0715
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0073
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 1.82s
                      Time elapsed: 00:02:04
                               ETA: 01:32:13

################################################################################
                      [1m Learning iteration 44/2000 [0m                      

                       Computation: 47758 steps/s (collection: 1.945s, learning 0.114s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 34.8788
                       Mean reward: 0.92
               Mean episode length: 237.62
    Episode_Reward/reaching_object: 0.1880
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0073
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 2.06s
                      Time elapsed: 00:02:06
                               ETA: 01:31:37

################################################################################
                      [1m Learning iteration 45/2000 [0m                      

                       Computation: 51165 steps/s (collection: 1.832s, learning 0.089s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0056
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 35.0320
                       Mean reward: 0.62
               Mean episode length: 235.42
    Episode_Reward/reaching_object: 0.1950
     Episode_Reward/lifting_object: -0.0119
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0072
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 1.92s
                      Time elapsed: 00:02:08
                               ETA: 01:30:56

################################################################################
                      [1m Learning iteration 46/2000 [0m                      

                       Computation: 53238 steps/s (collection: 1.751s, learning 0.095s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.2572
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.1733
                       Mean reward: 0.94
               Mean episode length: 230.74
    Episode_Reward/reaching_object: 0.1909
     Episode_Reward/lifting_object: -0.0492
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 1.85s
                      Time elapsed: 00:02:10
                               ETA: 01:30:14

################################################################################
                      [1m Learning iteration 47/2000 [0m                      

                       Computation: 52880 steps/s (collection: 1.764s, learning 0.095s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 35.2088
                       Mean reward: 0.85
               Mean episode length: 235.70
    Episode_Reward/reaching_object: 0.1888
     Episode_Reward/lifting_object: -0.0119
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 1.86s
                      Time elapsed: 00:02:12
                               ETA: 01:29:34

################################################################################
                      [1m Learning iteration 48/2000 [0m                      

                       Computation: 51118 steps/s (collection: 1.833s, learning 0.090s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.4184
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.2884
                       Mean reward: 0.66
               Mean episode length: 229.82
    Episode_Reward/reaching_object: 0.1873
     Episode_Reward/lifting_object: -0.0583
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 1.92s
                      Time elapsed: 00:02:14
                               ETA: 01:28:59

################################################################################
                      [1m Learning iteration 49/2000 [0m                      

                       Computation: 53678 steps/s (collection: 1.740s, learning 0.092s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0449
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.3282
                       Mean reward: 0.89
               Mean episode length: 228.54
    Episode_Reward/reaching_object: 0.1885
     Episode_Reward/lifting_object: -0.0431
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 1.83s
                      Time elapsed: 00:02:15
                               ETA: 01:28:21

################################################################################
                      [1m Learning iteration 50/2000 [0m                      

                       Computation: 53857 steps/s (collection: 1.737s, learning 0.088s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0415
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.4662
                       Mean reward: 0.92
               Mean episode length: 225.28
    Episode_Reward/reaching_object: 0.1856
     Episode_Reward/lifting_object: -0.0143
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 1.83s
                      Time elapsed: 00:02:17
                               ETA: 01:27:44

################################################################################
                      [1m Learning iteration 51/2000 [0m                      

                       Computation: 53024 steps/s (collection: 1.765s, learning 0.089s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.1812
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 35.5118
                       Mean reward: 0.37
               Mean episode length: 221.68
    Episode_Reward/reaching_object: 0.1939
     Episode_Reward/lifting_object: -0.0455
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 1.85s
                      Time elapsed: 00:02:19
                               ETA: 01:27:09

################################################################################
                      [1m Learning iteration 52/2000 [0m                      

                       Computation: 52940 steps/s (collection: 1.768s, learning 0.089s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 35.5566
                       Mean reward: 0.88
               Mean episode length: 220.35
    Episode_Reward/reaching_object: 0.1966
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 4.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 1.86s
                      Time elapsed: 00:02:21
                               ETA: 01:26:36

################################################################################
                      [1m Learning iteration 53/2000 [0m                      

                       Computation: 53358 steps/s (collection: 1.753s, learning 0.090s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 35.6649
                       Mean reward: 0.94
               Mean episode length: 212.78
    Episode_Reward/reaching_object: 0.2005
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 4.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 1.84s
                      Time elapsed: 00:02:23
                               ETA: 01:26:04

################################################################################
                      [1m Learning iteration 54/2000 [0m                      

                       Computation: 51252 steps/s (collection: 1.826s, learning 0.092s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0134
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.7342
                       Mean reward: 0.94
               Mean episode length: 218.83
    Episode_Reward/reaching_object: 0.2037
     Episode_Reward/lifting_object: -0.0119
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 1.92s
                      Time elapsed: 00:02:25
                               ETA: 01:25:35

################################################################################
                      [1m Learning iteration 55/2000 [0m                      

                       Computation: 48811 steps/s (collection: 1.927s, learning 0.087s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.0993
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.8278
                       Mean reward: 0.90
               Mean episode length: 214.11
    Episode_Reward/reaching_object: 0.2168
     Episode_Reward/lifting_object: -0.0358
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 2.01s
                      Time elapsed: 00:02:27
                               ETA: 01:25:11

################################################################################
                      [1m Learning iteration 56/2000 [0m                      

                       Computation: 51605 steps/s (collection: 1.818s, learning 0.087s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.0777
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.8854
                       Mean reward: 1.07
               Mean episode length: 213.85
    Episode_Reward/reaching_object: 0.2189
     Episode_Reward/lifting_object: -0.0198
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 1.90s
                      Time elapsed: 00:02:29
                               ETA: 01:24:44

################################################################################
                      [1m Learning iteration 57/2000 [0m                      

                       Computation: 51094 steps/s (collection: 1.831s, learning 0.093s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.2453
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 36.0150
                       Mean reward: 0.41
               Mean episode length: 209.68
    Episode_Reward/reaching_object: 0.2203
     Episode_Reward/lifting_object: -0.0682
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.7500
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 1.92s
                      Time elapsed: 00:02:30
                               ETA: 01:24:18

################################################################################
                      [1m Learning iteration 58/2000 [0m                      

                       Computation: 51458 steps/s (collection: 1.804s, learning 0.106s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.6395
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.0549
                       Mean reward: 1.01
               Mean episode length: 217.08
    Episode_Reward/reaching_object: 0.2225
     Episode_Reward/lifting_object: -0.0784
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.9167
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 1.91s
                      Time elapsed: 00:02:32
                               ETA: 01:23:52

################################################################################
                      [1m Learning iteration 59/2000 [0m                      

                       Computation: 52739 steps/s (collection: 1.766s, learning 0.098s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.1368
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 36.1321
                       Mean reward: 0.66
               Mean episode length: 214.37
    Episode_Reward/reaching_object: 0.2293
     Episode_Reward/lifting_object: -0.0194
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.0417
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 1.86s
                      Time elapsed: 00:02:34
                               ETA: 01:23:26

################################################################################
                      [1m Learning iteration 60/2000 [0m                      

                       Computation: 49936 steps/s (collection: 1.875s, learning 0.094s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0335
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.1841
                       Mean reward: 0.83
               Mean episode length: 210.48
    Episode_Reward/reaching_object: 0.2390
     Episode_Reward/lifting_object: -0.0186
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.1250
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 1.97s
                      Time elapsed: 00:02:36
                               ETA: 01:23:04

################################################################################
                      [1m Learning iteration 61/2000 [0m                      

                       Computation: 51603 steps/s (collection: 1.811s, learning 0.094s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0297
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.3140
                       Mean reward: 1.11
               Mean episode length: 208.60
    Episode_Reward/reaching_object: 0.2469
     Episode_Reward/lifting_object: -0.0079
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 4.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 1.91s
                      Time elapsed: 00:02:38
                               ETA: 01:22:41

################################################################################
                      [1m Learning iteration 62/2000 [0m                      

                       Computation: 52413 steps/s (collection: 1.785s, learning 0.091s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.1610
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.4353
                       Mean reward: 0.99
               Mean episode length: 214.00
    Episode_Reward/reaching_object: 0.2659
     Episode_Reward/lifting_object: -0.0311
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0081
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 1.88s
                      Time elapsed: 00:02:40
                               ETA: 01:22:17

################################################################################
                      [1m Learning iteration 63/2000 [0m                      

                       Computation: 52441 steps/s (collection: 1.780s, learning 0.095s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0374
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.4744
                       Mean reward: 1.29
               Mean episode length: 217.31
    Episode_Reward/reaching_object: 0.2661
     Episode_Reward/lifting_object: -0.0405
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0082
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 1.87s
                      Time elapsed: 00:02:42
                               ETA: 01:21:54

################################################################################
                      [1m Learning iteration 64/2000 [0m                      

                       Computation: 51479 steps/s (collection: 1.813s, learning 0.097s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.1334
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.5648
                       Mean reward: 0.82
               Mean episode length: 218.35
    Episode_Reward/reaching_object: 0.2926
     Episode_Reward/lifting_object: -0.0495
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0084
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 1.91s
                      Time elapsed: 00:02:44
                               ETA: 01:21:33

################################################################################
                      [1m Learning iteration 65/2000 [0m                      

                       Computation: 52766 steps/s (collection: 1.776s, learning 0.087s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.1313
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 36.5967
                       Mean reward: 1.39
               Mean episode length: 227.54
    Episode_Reward/reaching_object: 0.2927
     Episode_Reward/lifting_object: -0.0385
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 1.86s
                      Time elapsed: 00:02:46
                               ETA: 01:21:11

################################################################################
                      [1m Learning iteration 66/2000 [0m                      

                       Computation: 52705 steps/s (collection: 1.771s, learning 0.095s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.2125
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 36.6439
                       Mean reward: 0.87
               Mean episode length: 224.97
    Episode_Reward/reaching_object: 0.2976
     Episode_Reward/lifting_object: -0.0596
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 1.87s
                      Time elapsed: 00:02:48
                               ETA: 01:20:50

################################################################################
                      [1m Learning iteration 67/2000 [0m                      

                       Computation: 52738 steps/s (collection: 1.767s, learning 0.097s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0042
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 36.6698
                       Mean reward: 1.46
               Mean episode length: 230.51
    Episode_Reward/reaching_object: 0.2982
     Episode_Reward/lifting_object: -0.0105
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 1.86s
                      Time elapsed: 00:02:49
                               ETA: 01:20:29

################################################################################
                      [1m Learning iteration 68/2000 [0m                      

                       Computation: 52909 steps/s (collection: 1.771s, learning 0.087s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0251
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 36.7267
                       Mean reward: 1.46
               Mean episode length: 233.10
    Episode_Reward/reaching_object: 0.3046
     Episode_Reward/lifting_object: -0.0037
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 1.86s
                      Time elapsed: 00:02:51
                               ETA: 01:20:09

################################################################################
                      [1m Learning iteration 69/2000 [0m                      

                       Computation: 52304 steps/s (collection: 1.790s, learning 0.090s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.1103
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.8121
                       Mean reward: 1.25
               Mean episode length: 226.72
    Episode_Reward/reaching_object: 0.2980
     Episode_Reward/lifting_object: -0.0305
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 1.88s
                      Time elapsed: 00:02:53
                               ETA: 01:19:49

################################################################################
                      [1m Learning iteration 70/2000 [0m                      

                       Computation: 52064 steps/s (collection: 1.798s, learning 0.090s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0375
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.8398
                       Mean reward: 1.22
               Mean episode length: 224.41
    Episode_Reward/reaching_object: 0.2954
     Episode_Reward/lifting_object: -0.0054
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 1.89s
                      Time elapsed: 00:02:55
                               ETA: 01:19:31

################################################################################
                      [1m Learning iteration 71/2000 [0m                      

                       Computation: 51322 steps/s (collection: 1.824s, learning 0.091s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.1935
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 36.9169
                       Mean reward: 0.94
               Mean episode length: 225.99
    Episode_Reward/reaching_object: 0.3167
     Episode_Reward/lifting_object: -0.0294
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 1.92s
                      Time elapsed: 00:02:57
                               ETA: 01:19:13

################################################################################
                      [1m Learning iteration 72/2000 [0m                      

                       Computation: 51968 steps/s (collection: 1.801s, learning 0.091s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.1550
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.9456
                       Mean reward: 0.97
               Mean episode length: 215.79
    Episode_Reward/reaching_object: 0.3274
     Episode_Reward/lifting_object: -0.0528
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.1667
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 1.89s
                      Time elapsed: 00:02:59
                               ETA: 01:18:56

################################################################################
                      [1m Learning iteration 73/2000 [0m                      

                       Computation: 50867 steps/s (collection: 1.836s, learning 0.097s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.1976
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.0261
                       Mean reward: 1.08
               Mean episode length: 209.70
    Episode_Reward/reaching_object: 0.3166
     Episode_Reward/lifting_object: -0.0291
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 1.93s
                      Time elapsed: 00:03:01
                               ETA: 01:18:40

################################################################################
                      [1m Learning iteration 74/2000 [0m                      

                       Computation: 51416 steps/s (collection: 1.810s, learning 0.102s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.2924
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.0549
                       Mean reward: 1.57
               Mean episode length: 200.59
    Episode_Reward/reaching_object: 0.3199
     Episode_Reward/lifting_object: 0.0056
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 1.91s
                      Time elapsed: 00:03:03
                               ETA: 01:18:23

################################################################################
                      [1m Learning iteration 75/2000 [0m                      

                       Computation: 48299 steps/s (collection: 1.931s, learning 0.105s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.4511
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.1204
                       Mean reward: 1.19
               Mean episode length: 195.89
    Episode_Reward/reaching_object: 0.3261
     Episode_Reward/lifting_object: -0.1287
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 2.04s
                      Time elapsed: 00:03:05
                               ETA: 01:18:11

################################################################################
                      [1m Learning iteration 76/2000 [0m                      

                       Computation: 50970 steps/s (collection: 1.825s, learning 0.104s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.3573
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 37.1443
                       Mean reward: 1.30
               Mean episode length: 204.69
    Episode_Reward/reaching_object: 0.3464
     Episode_Reward/lifting_object: -0.0152
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.2917
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 1.93s
                      Time elapsed: 00:03:07
                               ETA: 01:17:55

################################################################################
                      [1m Learning iteration 77/2000 [0m                      

                       Computation: 50310 steps/s (collection: 1.858s, learning 0.096s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.7287
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.1739
                       Mean reward: 0.74
               Mean episode length: 208.35
    Episode_Reward/reaching_object: 0.3855
     Episode_Reward/lifting_object: -0.1862
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 1.95s
                      Time elapsed: 00:03:09
                               ETA: 01:17:41

################################################################################
                      [1m Learning iteration 78/2000 [0m                      

                       Computation: 51975 steps/s (collection: 1.801s, learning 0.091s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0060
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 37.2024
                       Mean reward: 2.02
               Mean episode length: 215.97
    Episode_Reward/reaching_object: 0.4103
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 1.89s
                      Time elapsed: 00:03:10
                               ETA: 01:17:26

################################################################################
                      [1m Learning iteration 79/2000 [0m                      

                       Computation: 51229 steps/s (collection: 1.829s, learning 0.090s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 37.2760
                       Mean reward: 2.33
               Mean episode length: 217.20
    Episode_Reward/reaching_object: 0.4462
     Episode_Reward/lifting_object: 0.0051
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 1.92s
                      Time elapsed: 00:03:12
                               ETA: 01:17:12

################################################################################
                      [1m Learning iteration 80/2000 [0m                      

                       Computation: 49337 steps/s (collection: 1.904s, learning 0.089s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.1117
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.3844
                       Mean reward: 2.24
               Mean episode length: 226.86
    Episode_Reward/reaching_object: 0.4580
     Episode_Reward/lifting_object: -0.0222
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 1.99s
                      Time elapsed: 00:03:14
                               ETA: 01:16:59

################################################################################
                      [1m Learning iteration 81/2000 [0m                      

                       Computation: 50330 steps/s (collection: 1.853s, learning 0.100s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0905
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 37.4126
                       Mean reward: 2.09
               Mean episode length: 214.92
    Episode_Reward/reaching_object: 0.4458
     Episode_Reward/lifting_object: -0.0063
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 1.95s
                      Time elapsed: 00:03:16
                               ETA: 01:16:46

################################################################################
                      [1m Learning iteration 82/2000 [0m                      

                       Computation: 49695 steps/s (collection: 1.863s, learning 0.116s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.1580
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 37.4641
                       Mean reward: 2.07
               Mean episode length: 219.76
    Episode_Reward/reaching_object: 0.4475
     Episode_Reward/lifting_object: 0.0074
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 1.98s
                      Time elapsed: 00:03:18
                               ETA: 01:16:34

################################################################################
                      [1m Learning iteration 83/2000 [0m                      

                       Computation: 50265 steps/s (collection: 1.854s, learning 0.102s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.5745
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.4932
                       Mean reward: 2.04
               Mean episode length: 214.66
    Episode_Reward/reaching_object: 0.4457
     Episode_Reward/lifting_object: -0.0625
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 1.96s
                      Time elapsed: 00:03:20
                               ETA: 01:16:22

################################################################################
                      [1m Learning iteration 84/2000 [0m                      

                       Computation: 51109 steps/s (collection: 1.836s, learning 0.087s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.2261
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.5128
                       Mean reward: 2.24
               Mean episode length: 218.11
    Episode_Reward/reaching_object: 0.4589
     Episode_Reward/lifting_object: -0.0732
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 1.92s
                      Time elapsed: 00:03:22
                               ETA: 01:16:09

################################################################################
                      [1m Learning iteration 85/2000 [0m                      

                       Computation: 50856 steps/s (collection: 1.845s, learning 0.088s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0258
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 37.5488
                       Mean reward: 2.30
               Mean episode length: 226.90
    Episode_Reward/reaching_object: 0.4763
     Episode_Reward/lifting_object: -0.0069
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 1.93s
                      Time elapsed: 00:03:24
                               ETA: 01:15:56

################################################################################
                      [1m Learning iteration 86/2000 [0m                      

                       Computation: 50788 steps/s (collection: 1.845s, learning 0.091s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0482
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 37.6116
                       Mean reward: 2.25
               Mean episode length: 219.26
    Episode_Reward/reaching_object: 0.4949
     Episode_Reward/lifting_object: -0.0313
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 1.94s
                      Time elapsed: 00:03:26
                               ETA: 01:15:44

################################################################################
                      [1m Learning iteration 87/2000 [0m                      

                       Computation: 49961 steps/s (collection: 1.875s, learning 0.093s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.3032
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.6631
                       Mean reward: 2.20
               Mean episode length: 223.12
    Episode_Reward/reaching_object: 0.5022
     Episode_Reward/lifting_object: -0.0296
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 1.97s
                      Time elapsed: 00:03:28
                               ETA: 01:15:33

################################################################################
                      [1m Learning iteration 88/2000 [0m                      

                       Computation: 50437 steps/s (collection: 1.859s, learning 0.090s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.2028
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 37.6825
                       Mean reward: 2.34
               Mean episode length: 216.51
    Episode_Reward/reaching_object: 0.4928
     Episode_Reward/lifting_object: -0.0408
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 1.95s
                      Time elapsed: 00:03:30
                               ETA: 01:15:21

################################################################################
                      [1m Learning iteration 89/2000 [0m                      

                       Computation: 49815 steps/s (collection: 1.868s, learning 0.106s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.5132
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.7143
                       Mean reward: 2.33
               Mean episode length: 204.47
    Episode_Reward/reaching_object: 0.4889
     Episode_Reward/lifting_object: -0.0188
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 1.97s
                      Time elapsed: 00:03:32
                               ETA: 01:15:11

################################################################################
                      [1m Learning iteration 90/2000 [0m                      

                       Computation: 50451 steps/s (collection: 1.850s, learning 0.099s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0756
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 37.7301
                       Mean reward: 2.91
               Mean episode length: 229.44
    Episode_Reward/reaching_object: 0.5674
     Episode_Reward/lifting_object: -0.0036
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 1.95s
                      Time elapsed: 00:03:34
                               ETA: 01:15:00

################################################################################
                      [1m Learning iteration 91/2000 [0m                      

                       Computation: 49346 steps/s (collection: 1.883s, learning 0.110s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 37.7751
                       Mean reward: 2.68
               Mean episode length: 237.18
    Episode_Reward/reaching_object: 0.5878
     Episode_Reward/lifting_object: -0.0450
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0119
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 1.99s
                      Time elapsed: 00:03:36
                               ETA: 01:14:50

################################################################################
                      [1m Learning iteration 92/2000 [0m                      

                       Computation: 50272 steps/s (collection: 1.860s, learning 0.096s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.2212
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.8155
                       Mean reward: 2.97
               Mean episode length: 239.36
    Episode_Reward/reaching_object: 0.5959
     Episode_Reward/lifting_object: -0.0125
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 1.96s
                      Time elapsed: 00:03:38
                               ETA: 01:14:39

################################################################################
                      [1m Learning iteration 93/2000 [0m                      

                       Computation: 49560 steps/s (collection: 1.894s, learning 0.090s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.1115
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.8289
                       Mean reward: 2.68
               Mean episode length: 237.44
    Episode_Reward/reaching_object: 0.6142
     Episode_Reward/lifting_object: -0.0446
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 1.98s
                      Time elapsed: 00:03:40
                               ETA: 01:14:30

################################################################################
                      [1m Learning iteration 94/2000 [0m                      

                       Computation: 49259 steps/s (collection: 1.898s, learning 0.098s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.1865
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.8667
                       Mean reward: 2.07
               Mean episode length: 238.93
    Episode_Reward/reaching_object: 0.6214
     Episode_Reward/lifting_object: -0.0630
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 2.00s
                      Time elapsed: 00:03:42
                               ETA: 01:14:20

################################################################################
                      [1m Learning iteration 95/2000 [0m                      

                       Computation: 48271 steps/s (collection: 1.944s, learning 0.092s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.2214
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 37.9115
                       Mean reward: 3.22
               Mean episode length: 236.56
    Episode_Reward/reaching_object: 0.6172
     Episode_Reward/lifting_object: -0.0198
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 2.04s
                      Time elapsed: 00:03:44
                               ETA: 01:14:12

################################################################################
                      [1m Learning iteration 96/2000 [0m                      

                       Computation: 43172 steps/s (collection: 2.170s, learning 0.107s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.2240
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 37.9589
                       Mean reward: 2.46
               Mean episode length: 238.93
    Episode_Reward/reaching_object: 0.6323
     Episode_Reward/lifting_object: -0.0192
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 2.28s
                      Time elapsed: 00:03:46
                               ETA: 01:14:08

################################################################################
                      [1m Learning iteration 97/2000 [0m                      

                       Computation: 49450 steps/s (collection: 1.874s, learning 0.114s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.3498
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.9913
                       Mean reward: 3.02
               Mean episode length: 236.48
    Episode_Reward/reaching_object: 0.6366
     Episode_Reward/lifting_object: -0.0116
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 1.99s
                      Time elapsed: 00:03:48
                               ETA: 01:13:59

################################################################################
                      [1m Learning iteration 98/2000 [0m                      

                       Computation: 47194 steps/s (collection: 1.988s, learning 0.095s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.1952
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.0095
                       Mean reward: 3.39
               Mean episode length: 227.60
    Episode_Reward/reaching_object: 0.6280
     Episode_Reward/lifting_object: 0.0180
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 2.08s
                      Time elapsed: 00:03:50
                               ETA: 01:13:52

################################################################################
                      [1m Learning iteration 99/2000 [0m                      

                       Computation: 48449 steps/s (collection: 1.943s, learning 0.086s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0753
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 38.0629
                       Mean reward: 2.73
               Mean episode length: 229.79
    Episode_Reward/reaching_object: 0.6224
     Episode_Reward/lifting_object: -0.0210
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 2.03s
                      Time elapsed: 00:03:52
                               ETA: 01:13:44

################################################################################
                     [1m Learning iteration 100/2000 [0m                      

                       Computation: 49768 steps/s (collection: 1.888s, learning 0.088s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.1868
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 38.1039
                       Mean reward: 3.09
               Mean episode length: 231.16
    Episode_Reward/reaching_object: 0.6199
     Episode_Reward/lifting_object: 0.0094
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 1.98s
                      Time elapsed: 00:03:54
                               ETA: 01:13:35

################################################################################
                     [1m Learning iteration 101/2000 [0m                      

                       Computation: 48873 steps/s (collection: 1.915s, learning 0.096s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.7472
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.1434
                       Mean reward: 2.96
               Mean episode length: 230.28
    Episode_Reward/reaching_object: 0.6589
     Episode_Reward/lifting_object: -0.0145
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 2.01s
                      Time elapsed: 00:03:56
                               ETA: 01:13:27

################################################################################
                     [1m Learning iteration 102/2000 [0m                      

                       Computation: 49220 steps/s (collection: 1.907s, learning 0.090s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.1043
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.1582
                       Mean reward: 3.18
               Mean episode length: 235.28
    Episode_Reward/reaching_object: 0.6875
     Episode_Reward/lifting_object: -0.0393
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 2.00s
                      Time elapsed: 00:03:58
                               ETA: 01:13:19

################################################################################
                     [1m Learning iteration 103/2000 [0m                      

                       Computation: 50206 steps/s (collection: 1.856s, learning 0.102s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.1056
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 38.2017
                       Mean reward: 3.22
               Mean episode length: 240.29
    Episode_Reward/reaching_object: 0.6764
     Episode_Reward/lifting_object: 0.0297
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 1.96s
                      Time elapsed: 00:04:00
                               ETA: 01:13:10

################################################################################
                     [1m Learning iteration 104/2000 [0m                      

                       Computation: 48847 steps/s (collection: 1.895s, learning 0.117s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.2273
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 38.2756
                       Mean reward: 2.88
               Mean episode length: 240.54
    Episode_Reward/reaching_object: 0.6737
     Episode_Reward/lifting_object: 0.0089
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 2.01s
                      Time elapsed: 00:04:02
                               ETA: 01:13:02

################################################################################
                     [1m Learning iteration 105/2000 [0m                      

                       Computation: 49092 steps/s (collection: 1.897s, learning 0.105s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.3986
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.3241
                       Mean reward: 3.57
               Mean episode length: 243.24
    Episode_Reward/reaching_object: 0.6946
     Episode_Reward/lifting_object: 0.0446
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 2.00s
                      Time elapsed: 00:04:04
                               ETA: 01:12:54

################################################################################
                     [1m Learning iteration 106/2000 [0m                      

                       Computation: 46960 steps/s (collection: 1.965s, learning 0.129s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.6026
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.3377
                       Mean reward: 3.30
               Mean episode length: 243.55
    Episode_Reward/reaching_object: 0.7217
     Episode_Reward/lifting_object: -0.0581
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 2.09s
                      Time elapsed: 00:04:06
                               ETA: 01:12:48

################################################################################
                     [1m Learning iteration 107/2000 [0m                      

                       Computation: 49018 steps/s (collection: 1.906s, learning 0.100s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.1568
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.3498
                       Mean reward: 3.59
               Mean episode length: 244.33
    Episode_Reward/reaching_object: 0.7298
     Episode_Reward/lifting_object: -0.0216
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 2.01s
                      Time elapsed: 00:04:08
                               ETA: 01:12:41

################################################################################
                     [1m Learning iteration 108/2000 [0m                      

                       Computation: 49224 steps/s (collection: 1.873s, learning 0.125s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.1095
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 38.3900
                       Mean reward: 3.70
               Mean episode length: 245.25
    Episode_Reward/reaching_object: 0.7114
     Episode_Reward/lifting_object: -0.0694
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 2.00s
                      Time elapsed: 00:04:10
                               ETA: 01:12:33

################################################################################
                     [1m Learning iteration 109/2000 [0m                      

                       Computation: 44660 steps/s (collection: 2.111s, learning 0.090s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0420
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.4644
                       Mean reward: 3.59
               Mean episode length: 233.86
    Episode_Reward/reaching_object: 0.7032
     Episode_Reward/lifting_object: 0.0345
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 2.20s
                      Time elapsed: 00:04:13
                               ETA: 01:12:29

################################################################################
                     [1m Learning iteration 110/2000 [0m                      

                       Computation: 46430 steps/s (collection: 2.009s, learning 0.108s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0626
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.5340
                       Mean reward: 3.77
               Mean episode length: 245.52
    Episode_Reward/reaching_object: 0.7378
     Episode_Reward/lifting_object: 0.0111
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 2.12s
                      Time elapsed: 00:04:15
                               ETA: 01:12:23

################################################################################
                     [1m Learning iteration 111/2000 [0m                      

                       Computation: 49609 steps/s (collection: 1.887s, learning 0.095s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.1612
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 38.5859
                       Mean reward: 3.46
               Mean episode length: 244.06
    Episode_Reward/reaching_object: 0.7251
     Episode_Reward/lifting_object: -0.0009
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 1.98s
                      Time elapsed: 00:04:17
                               ETA: 01:12:16

################################################################################
                     [1m Learning iteration 112/2000 [0m                      

                       Computation: 49507 steps/s (collection: 1.893s, learning 0.093s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.3181
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 38.6348
                       Mean reward: 3.64
               Mean episode length: 238.96
    Episode_Reward/reaching_object: 0.6861
     Episode_Reward/lifting_object: 0.0104
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 1.99s
                      Time elapsed: 00:04:19
                               ETA: 01:12:08

################################################################################
                     [1m Learning iteration 113/2000 [0m                      

                       Computation: 49407 steps/s (collection: 1.899s, learning 0.091s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.3200
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 38.6952
                       Mean reward: 3.62
               Mean episode length: 241.56
    Episode_Reward/reaching_object: 0.7002
     Episode_Reward/lifting_object: -0.0655
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 1.99s
                      Time elapsed: 00:04:21
                               ETA: 01:12:01

################################################################################
                     [1m Learning iteration 114/2000 [0m                      

                       Computation: 49472 steps/s (collection: 1.894s, learning 0.093s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.2581
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 38.7515
                       Mean reward: 3.45
               Mean episode length: 232.86
    Episode_Reward/reaching_object: 0.6938
     Episode_Reward/lifting_object: -0.0076
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 1.99s
                      Time elapsed: 00:04:23
                               ETA: 01:11:54

################################################################################
                     [1m Learning iteration 115/2000 [0m                      

                       Computation: 49810 steps/s (collection: 1.877s, learning 0.097s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.2604
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 38.7967
                       Mean reward: 3.82
               Mean episode length: 242.00
    Episode_Reward/reaching_object: 0.7038
     Episode_Reward/lifting_object: -0.0205
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 1.97s
                      Time elapsed: 00:04:25
                               ETA: 01:11:46

################################################################################
                     [1m Learning iteration 116/2000 [0m                      

                       Computation: 46017 steps/s (collection: 2.047s, learning 0.090s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.5111
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 38.8642
                       Mean reward: 3.49
               Mean episode length: 245.69
    Episode_Reward/reaching_object: 0.7074
     Episode_Reward/lifting_object: -0.0089
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 2.14s
                      Time elapsed: 00:04:27
                               ETA: 01:11:42

################################################################################
                     [1m Learning iteration 117/2000 [0m                      

                       Computation: 49988 steps/s (collection: 1.880s, learning 0.086s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.2439
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.8869
                       Mean reward: 3.44
               Mean episode length: 235.32
    Episode_Reward/reaching_object: 0.7014
     Episode_Reward/lifting_object: -0.0083
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 1.97s
                      Time elapsed: 00:04:29
                               ETA: 01:11:34

################################################################################
                     [1m Learning iteration 118/2000 [0m                      

                       Computation: 48860 steps/s (collection: 1.907s, learning 0.105s)
             Mean action noise std: 1.42
          Mean value_function loss: 1.0248
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.9362
                       Mean reward: 2.34
               Mean episode length: 243.52
    Episode_Reward/reaching_object: 0.7051
     Episode_Reward/lifting_object: -0.0563
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 2.01s
                      Time elapsed: 00:04:31
                               ETA: 01:11:28

################################################################################
                     [1m Learning iteration 119/2000 [0m                      

                       Computation: 49271 steps/s (collection: 1.883s, learning 0.113s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0614
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.9526
                       Mean reward: 3.51
               Mean episode length: 242.56
    Episode_Reward/reaching_object: 0.7246
     Episode_Reward/lifting_object: 0.0350
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 2.00s
                      Time elapsed: 00:04:33
                               ETA: 01:11:21

################################################################################
                     [1m Learning iteration 120/2000 [0m                      

                       Computation: 50451 steps/s (collection: 1.846s, learning 0.103s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.1148
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.9991
                       Mean reward: 3.41
               Mean episode length: 245.57
    Episode_Reward/reaching_object: 0.7103
     Episode_Reward/lifting_object: 0.0433
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 1.95s
                      Time elapsed: 00:04:35
                               ETA: 01:11:14

################################################################################
                     [1m Learning iteration 121/2000 [0m                      

                       Computation: 49583 steps/s (collection: 1.879s, learning 0.104s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.0948
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 39.0810
                       Mean reward: 3.58
               Mean episode length: 244.10
    Episode_Reward/reaching_object: 0.7180
     Episode_Reward/lifting_object: 0.0046
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 1.98s
                      Time elapsed: 00:04:37
                               ETA: 01:11:07

################################################################################
                     [1m Learning iteration 122/2000 [0m                      

                       Computation: 50883 steps/s (collection: 1.845s, learning 0.087s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.0770
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.1365
                       Mean reward: 3.10
               Mean episode length: 246.07
    Episode_Reward/reaching_object: 0.7163
     Episode_Reward/lifting_object: -0.0662
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 1.93s
                      Time elapsed: 00:04:39
                               ETA: 01:11:00

################################################################################
                     [1m Learning iteration 123/2000 [0m                      

                       Computation: 50595 steps/s (collection: 1.856s, learning 0.087s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.1292
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 39.1809
                       Mean reward: 4.05
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7154
     Episode_Reward/lifting_object: 0.0591
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 1.94s
                      Time elapsed: 00:04:40
                               ETA: 01:10:52

################################################################################
                     [1m Learning iteration 124/2000 [0m                      

                       Computation: 51013 steps/s (collection: 1.841s, learning 0.086s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.3019
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 39.2319
                       Mean reward: 3.40
               Mean episode length: 247.14
    Episode_Reward/reaching_object: 0.7271
     Episode_Reward/lifting_object: 0.0161
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 1.93s
                      Time elapsed: 00:04:42
                               ETA: 01:10:45

################################################################################
                     [1m Learning iteration 125/2000 [0m                      

                       Computation: 49357 steps/s (collection: 1.879s, learning 0.113s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.1303
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 39.2550
                       Mean reward: 3.80
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7280
     Episode_Reward/lifting_object: 0.0068
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 1.99s
                      Time elapsed: 00:04:44
                               ETA: 01:10:39

################################################################################
                     [1m Learning iteration 126/2000 [0m                      

                       Computation: 50178 steps/s (collection: 1.850s, learning 0.109s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.3106
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 39.3091
                       Mean reward: 3.59
               Mean episode length: 242.76
    Episode_Reward/reaching_object: 0.7201
     Episode_Reward/lifting_object: 0.0291
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 1.96s
                      Time elapsed: 00:04:46
                               ETA: 01:10:32

################################################################################
                     [1m Learning iteration 127/2000 [0m                      

                       Computation: 49810 steps/s (collection: 1.878s, learning 0.095s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.5609
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 39.3496
                       Mean reward: 3.92
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.7297
     Episode_Reward/lifting_object: -0.0353
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 1.97s
                      Time elapsed: 00:04:48
                               ETA: 01:10:26

################################################################################
                     [1m Learning iteration 128/2000 [0m                      

                       Computation: 49934 steps/s (collection: 1.882s, learning 0.087s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.1555
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 39.3884
                       Mean reward: 3.43
               Mean episode length: 242.00
    Episode_Reward/reaching_object: 0.7478
     Episode_Reward/lifting_object: 0.0489
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 1.97s
                      Time elapsed: 00:04:50
                               ETA: 01:10:19

################################################################################
                     [1m Learning iteration 129/2000 [0m                      

                       Computation: 50414 steps/s (collection: 1.862s, learning 0.088s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.2711
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.4429
                       Mean reward: 3.52
               Mean episode length: 246.00
    Episode_Reward/reaching_object: 0.7152
     Episode_Reward/lifting_object: 0.0383
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 1.95s
                      Time elapsed: 00:04:52
                               ETA: 01:10:12

################################################################################
                     [1m Learning iteration 130/2000 [0m                      

                       Computation: 50458 steps/s (collection: 1.862s, learning 0.086s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.4664
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 39.4686
                       Mean reward: 3.78
               Mean episode length: 240.45
    Episode_Reward/reaching_object: 0.7240
     Episode_Reward/lifting_object: 0.0220
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 1.95s
                      Time elapsed: 00:04:54
                               ETA: 01:10:06

################################################################################
                     [1m Learning iteration 131/2000 [0m                      

                       Computation: 50848 steps/s (collection: 1.847s, learning 0.086s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.3302
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 39.5093
                       Mean reward: 3.12
               Mean episode length: 244.53
    Episode_Reward/reaching_object: 0.7320
     Episode_Reward/lifting_object: 0.0021
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 1.93s
                      Time elapsed: 00:04:56
                               ETA: 01:09:59

################################################################################
                     [1m Learning iteration 132/2000 [0m                      

                       Computation: 50854 steps/s (collection: 1.838s, learning 0.095s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.1903
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 39.5849
                       Mean reward: 4.33
               Mean episode length: 245.79
    Episode_Reward/reaching_object: 0.7240
     Episode_Reward/lifting_object: 0.0188
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 1.93s
                      Time elapsed: 00:04:58
                               ETA: 01:09:53

################################################################################
                     [1m Learning iteration 133/2000 [0m                      

                       Computation: 48802 steps/s (collection: 1.922s, learning 0.093s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.3296
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 39.6455
                       Mean reward: 3.82
               Mean episode length: 242.74
    Episode_Reward/reaching_object: 0.7220
     Episode_Reward/lifting_object: 0.0844
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 2.01s
                      Time elapsed: 00:05:00
                               ETA: 01:09:47

################################################################################
                     [1m Learning iteration 134/2000 [0m                      

                       Computation: 48640 steps/s (collection: 1.913s, learning 0.108s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.1932
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 39.6748
                       Mean reward: 3.02
               Mean episode length: 245.22
    Episode_Reward/reaching_object: 0.7471
     Episode_Reward/lifting_object: -0.0560
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 2.02s
                      Time elapsed: 00:05:02
                               ETA: 01:09:42

################################################################################
                     [1m Learning iteration 135/2000 [0m                      

                       Computation: 49694 steps/s (collection: 1.873s, learning 0.106s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.0391
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 39.7310
                       Mean reward: 3.49
               Mean episode length: 243.08
    Episode_Reward/reaching_object: 0.7092
     Episode_Reward/lifting_object: 0.0290
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 1.98s
                      Time elapsed: 00:05:04
                               ETA: 01:09:36

################################################################################
                     [1m Learning iteration 136/2000 [0m                      

                       Computation: 47580 steps/s (collection: 1.958s, learning 0.108s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.6289
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.7821
                       Mean reward: 3.87
               Mean episode length: 244.43
    Episode_Reward/reaching_object: 0.7348
     Episode_Reward/lifting_object: 0.0587
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 2.07s
                      Time elapsed: 00:05:06
                               ETA: 01:09:31

################################################################################
                     [1m Learning iteration 137/2000 [0m                      

                       Computation: 49547 steps/s (collection: 1.883s, learning 0.101s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.3438
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.7931
                       Mean reward: 3.96
               Mean episode length: 239.62
    Episode_Reward/reaching_object: 0.7450
     Episode_Reward/lifting_object: 0.0454
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 1.98s
                      Time elapsed: 00:05:08
                               ETA: 01:09:26

################################################################################
                     [1m Learning iteration 138/2000 [0m                      

                       Computation: 49635 steps/s (collection: 1.882s, learning 0.099s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.8196
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.8237
                       Mean reward: 3.54
               Mean episode length: 242.33
    Episode_Reward/reaching_object: 0.7329
     Episode_Reward/lifting_object: 0.0409
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 1.98s
                      Time elapsed: 00:05:10
                               ETA: 01:09:20

################################################################################
                     [1m Learning iteration 139/2000 [0m                      

                       Computation: 50179 steps/s (collection: 1.867s, learning 0.092s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.1019
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 39.8615
                       Mean reward: 4.29
               Mean episode length: 241.96
    Episode_Reward/reaching_object: 0.7296
     Episode_Reward/lifting_object: 0.0611
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 1.96s
                      Time elapsed: 00:05:12
                               ETA: 01:09:14

################################################################################
                     [1m Learning iteration 140/2000 [0m                      

                       Computation: 49579 steps/s (collection: 1.874s, learning 0.109s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.2436
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 39.9189
                       Mean reward: 3.30
               Mean episode length: 233.37
    Episode_Reward/reaching_object: 0.7197
     Episode_Reward/lifting_object: 0.0453
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 1.98s
                      Time elapsed: 00:05:14
                               ETA: 01:09:09

################################################################################
                     [1m Learning iteration 141/2000 [0m                      

                       Computation: 50567 steps/s (collection: 1.857s, learning 0.087s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.2920
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 39.9599
                       Mean reward: 3.56
               Mean episode length: 238.72
    Episode_Reward/reaching_object: 0.7257
     Episode_Reward/lifting_object: 0.0085
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 1.94s
                      Time elapsed: 00:05:16
                               ETA: 01:09:03

################################################################################
                     [1m Learning iteration 142/2000 [0m                      

                       Computation: 50348 steps/s (collection: 1.858s, learning 0.094s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.2966
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 40.0232
                       Mean reward: 3.93
               Mean episode length: 232.89
    Episode_Reward/reaching_object: 0.6994
     Episode_Reward/lifting_object: 0.0582
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 1.95s
                      Time elapsed: 00:05:18
                               ETA: 01:08:57

################################################################################
                     [1m Learning iteration 143/2000 [0m                      

                       Computation: 49991 steps/s (collection: 1.874s, learning 0.092s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.4477
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 40.0738
                       Mean reward: 3.31
               Mean episode length: 239.63
    Episode_Reward/reaching_object: 0.7025
     Episode_Reward/lifting_object: 0.0628
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 1.97s
                      Time elapsed: 00:05:20
                               ETA: 01:08:51

################################################################################
                     [1m Learning iteration 144/2000 [0m                      

                       Computation: 49878 steps/s (collection: 1.881s, learning 0.090s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.5091
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 40.1054
                       Mean reward: 3.71
               Mean episode length: 239.48
    Episode_Reward/reaching_object: 0.7033
     Episode_Reward/lifting_object: 0.0676
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 1.97s
                      Time elapsed: 00:05:22
                               ETA: 01:08:46

################################################################################
                     [1m Learning iteration 145/2000 [0m                      

                       Computation: 49850 steps/s (collection: 1.883s, learning 0.089s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.3090
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.1364
                       Mean reward: 2.93
               Mean episode length: 235.92
    Episode_Reward/reaching_object: 0.7031
     Episode_Reward/lifting_object: -0.0393
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 1.97s
                      Time elapsed: 00:05:24
                               ETA: 01:08:40

################################################################################
                     [1m Learning iteration 146/2000 [0m                      

                       Computation: 48785 steps/s (collection: 1.927s, learning 0.088s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.1257
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 40.1628
                       Mean reward: 3.66
               Mean episode length: 238.20
    Episode_Reward/reaching_object: 0.6897
     Episode_Reward/lifting_object: 0.0690
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 2.02s
                      Time elapsed: 00:05:26
                               ETA: 01:08:35

################################################################################
                     [1m Learning iteration 147/2000 [0m                      

                       Computation: 49985 steps/s (collection: 1.881s, learning 0.086s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.3965
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 40.2142
                       Mean reward: 4.21
               Mean episode length: 241.01
    Episode_Reward/reaching_object: 0.6910
     Episode_Reward/lifting_object: 0.0690
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 1.97s
                      Time elapsed: 00:05:28
                               ETA: 01:08:30

################################################################################
                     [1m Learning iteration 148/2000 [0m                      

                       Computation: 49035 steps/s (collection: 1.896s, learning 0.109s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.6228
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 40.2728
                       Mean reward: 3.38
               Mean episode length: 244.44
    Episode_Reward/reaching_object: 0.6914
     Episode_Reward/lifting_object: 0.0299
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 2.00s
                      Time elapsed: 00:05:30
                               ETA: 01:08:25

################################################################################
                     [1m Learning iteration 149/2000 [0m                      

                       Computation: 47605 steps/s (collection: 1.949s, learning 0.116s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.5385
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 40.2918
                       Mean reward: 3.37
               Mean episode length: 234.12
    Episode_Reward/reaching_object: 0.6559
     Episode_Reward/lifting_object: 0.0433
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 2.06s
                      Time elapsed: 00:05:32
                               ETA: 01:08:21

################################################################################
                     [1m Learning iteration 150/2000 [0m                      

                       Computation: 49000 steps/s (collection: 1.886s, learning 0.121s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.3205
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 40.3397
                       Mean reward: 3.44
               Mean episode length: 234.62
    Episode_Reward/reaching_object: 0.6669
     Episode_Reward/lifting_object: -0.0136
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 2.01s
                      Time elapsed: 00:05:34
                               ETA: 01:08:16

################################################################################
                     [1m Learning iteration 151/2000 [0m                      

                       Computation: 47883 steps/s (collection: 1.938s, learning 0.115s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.1510
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 40.4210
                       Mean reward: 3.48
               Mean episode length: 233.29
    Episode_Reward/reaching_object: 0.6683
     Episode_Reward/lifting_object: 0.0832
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 2.05s
                      Time elapsed: 00:05:36
                               ETA: 01:08:12

################################################################################
                     [1m Learning iteration 152/2000 [0m                      

                       Computation: 49932 steps/s (collection: 1.883s, learning 0.086s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.8753
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.4805
                       Mean reward: 3.97
               Mean episode length: 233.02
    Episode_Reward/reaching_object: 0.6874
     Episode_Reward/lifting_object: 0.0762
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 1.97s
                      Time elapsed: 00:05:38
                               ETA: 01:08:07

################################################################################
                     [1m Learning iteration 153/2000 [0m                      

                       Computation: 50273 steps/s (collection: 1.869s, learning 0.086s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.2588
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 40.4954
                       Mean reward: 3.22
               Mean episode length: 232.76
    Episode_Reward/reaching_object: 0.6262
     Episode_Reward/lifting_object: 0.0272
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 1.96s
                      Time elapsed: 00:05:40
                               ETA: 01:08:02

################################################################################
                     [1m Learning iteration 154/2000 [0m                      

                       Computation: 49409 steps/s (collection: 1.899s, learning 0.091s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.4480
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 40.5310
                       Mean reward: 3.38
               Mean episode length: 239.65
    Episode_Reward/reaching_object: 0.6589
     Episode_Reward/lifting_object: 0.0706
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 1.99s
                      Time elapsed: 00:05:42
                               ETA: 01:07:57

################################################################################
                     [1m Learning iteration 155/2000 [0m                      

                       Computation: 49434 steps/s (collection: 1.903s, learning 0.086s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.2992
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.5472
                       Mean reward: 3.74
               Mean episode length: 239.98
    Episode_Reward/reaching_object: 0.6635
     Episode_Reward/lifting_object: 0.1324
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 1.99s
                      Time elapsed: 00:05:44
                               ETA: 01:07:52

################################################################################
                     [1m Learning iteration 156/2000 [0m                      

                       Computation: 49634 steps/s (collection: 1.890s, learning 0.091s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.3084
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 40.5939
                       Mean reward: 3.19
               Mean episode length: 240.00
    Episode_Reward/reaching_object: 0.6643
     Episode_Reward/lifting_object: 0.0441
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 1.98s
                      Time elapsed: 00:05:46
                               ETA: 01:07:47

################################################################################
                     [1m Learning iteration 157/2000 [0m                      

                       Computation: 48939 steps/s (collection: 1.902s, learning 0.107s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.2434
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 40.6655
                       Mean reward: 3.44
               Mean episode length: 234.97
    Episode_Reward/reaching_object: 0.6592
     Episode_Reward/lifting_object: 0.0128
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 2.01s
                      Time elapsed: 00:05:48
                               ETA: 01:07:43

################################################################################
                     [1m Learning iteration 158/2000 [0m                      

                       Computation: 49692 steps/s (collection: 1.879s, learning 0.099s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.1555
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 40.7326
                       Mean reward: 3.27
               Mean episode length: 233.39
    Episode_Reward/reaching_object: 0.6547
     Episode_Reward/lifting_object: -0.0109
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 1.98s
                      Time elapsed: 00:05:50
                               ETA: 01:07:38

################################################################################
                     [1m Learning iteration 159/2000 [0m                      

                       Computation: 49243 steps/s (collection: 1.908s, learning 0.088s)
             Mean action noise std: 1.55
          Mean value_function loss: 1.1067
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 40.7826
                       Mean reward: 3.91
               Mean episode length: 243.98
    Episode_Reward/reaching_object: 0.6848
     Episode_Reward/lifting_object: 0.0537
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 2.00s
                      Time elapsed: 00:05:52
                               ETA: 01:07:33

################################################################################
                     [1m Learning iteration 160/2000 [0m                      

                       Computation: 49850 steps/s (collection: 1.875s, learning 0.097s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.1742
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 40.8401
                       Mean reward: 3.31
               Mean episode length: 242.85
    Episode_Reward/reaching_object: 0.6505
     Episode_Reward/lifting_object: 0.0903
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 1.97s
                      Time elapsed: 00:05:54
                               ETA: 01:07:28

################################################################################
                     [1m Learning iteration 161/2000 [0m                      

                       Computation: 49591 steps/s (collection: 1.881s, learning 0.101s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.1344
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 40.9068
                       Mean reward: 3.44
               Mean episode length: 240.59
    Episode_Reward/reaching_object: 0.6587
     Episode_Reward/lifting_object: 0.0616
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 1.98s
                      Time elapsed: 00:05:56
                               ETA: 01:07:24

################################################################################
                     [1m Learning iteration 162/2000 [0m                      

                       Computation: 48459 steps/s (collection: 1.940s, learning 0.089s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.3659
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 40.9557
                       Mean reward: 3.51
               Mean episode length: 239.12
    Episode_Reward/reaching_object: 0.6592
     Episode_Reward/lifting_object: 0.0870
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 2.03s
                      Time elapsed: 00:05:58
                               ETA: 01:07:20

################################################################################
                     [1m Learning iteration 163/2000 [0m                      

                       Computation: 47528 steps/s (collection: 1.961s, learning 0.108s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.3010
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 40.9828
                       Mean reward: 3.76
               Mean episode length: 241.76
    Episode_Reward/reaching_object: 0.6811
     Episode_Reward/lifting_object: 0.0714
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 2.07s
                      Time elapsed: 00:06:00
                               ETA: 01:07:16

################################################################################
                     [1m Learning iteration 164/2000 [0m                      

                       Computation: 49397 steps/s (collection: 1.876s, learning 0.115s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.4851
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 41.0411
                       Mean reward: 3.71
               Mean episode length: 244.26
    Episode_Reward/reaching_object: 0.6773
     Episode_Reward/lifting_object: 0.0435
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 1.99s
                      Time elapsed: 00:06:02
                               ETA: 01:07:11

################################################################################
                     [1m Learning iteration 165/2000 [0m                      

                       Computation: 49121 steps/s (collection: 1.882s, learning 0.120s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.4389
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.0926
                       Mean reward: 3.63
               Mean episode length: 241.46
    Episode_Reward/reaching_object: 0.6613
     Episode_Reward/lifting_object: 0.1068
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 2.00s
                      Time elapsed: 00:06:04
                               ETA: 01:07:07

################################################################################
                     [1m Learning iteration 166/2000 [0m                      

                       Computation: 47905 steps/s (collection: 1.938s, learning 0.114s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.3508
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 41.1180
                       Mean reward: 3.26
               Mean episode length: 244.14
    Episode_Reward/reaching_object: 0.6615
     Episode_Reward/lifting_object: 0.0979
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 2.05s
                      Time elapsed: 00:06:06
                               ETA: 01:07:03

################################################################################
                     [1m Learning iteration 167/2000 [0m                      

                       Computation: 49322 steps/s (collection: 1.901s, learning 0.093s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.5297
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 41.1592
                       Mean reward: 4.30
               Mean episode length: 244.73
    Episode_Reward/reaching_object: 0.6799
     Episode_Reward/lifting_object: 0.1124
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 1.99s
                      Time elapsed: 00:06:08
                               ETA: 01:06:59

################################################################################
                     [1m Learning iteration 168/2000 [0m                      

                       Computation: 49252 steps/s (collection: 1.908s, learning 0.088s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.5762
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 41.2075
                       Mean reward: 3.82
               Mean episode length: 241.38
    Episode_Reward/reaching_object: 0.6612
     Episode_Reward/lifting_object: 0.0719
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 2.00s
                      Time elapsed: 00:06:10
                               ETA: 01:06:55

################################################################################
                     [1m Learning iteration 169/2000 [0m                      

                       Computation: 50299 steps/s (collection: 1.865s, learning 0.090s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.2649
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 41.2660
                       Mean reward: 3.73
               Mean episode length: 243.70
    Episode_Reward/reaching_object: 0.6561
     Episode_Reward/lifting_object: 0.0578
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 1.95s
                      Time elapsed: 00:06:12
                               ETA: 01:06:50

################################################################################
                     [1m Learning iteration 170/2000 [0m                      

                       Computation: 47306 steps/s (collection: 1.986s, learning 0.093s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.2525
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 41.3182
                       Mean reward: 4.31
               Mean episode length: 245.35
    Episode_Reward/reaching_object: 0.6460
     Episode_Reward/lifting_object: 0.1138
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 2.08s
                      Time elapsed: 00:06:14
                               ETA: 01:06:46

################################################################################
                     [1m Learning iteration 171/2000 [0m                      

                       Computation: 49985 steps/s (collection: 1.879s, learning 0.088s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.6105
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.3579
                       Mean reward: 4.30
               Mean episode length: 239.69
    Episode_Reward/reaching_object: 0.6619
     Episode_Reward/lifting_object: 0.0806
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 1.97s
                      Time elapsed: 00:06:16
                               ETA: 01:06:42

################################################################################
                     [1m Learning iteration 172/2000 [0m                      

                       Computation: 50141 steps/s (collection: 1.857s, learning 0.103s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.2190
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 41.3932
                       Mean reward: 3.54
               Mean episode length: 249.01
    Episode_Reward/reaching_object: 0.6835
     Episode_Reward/lifting_object: 0.0170
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 1.96s
                      Time elapsed: 00:06:18
                               ETA: 01:06:37

################################################################################
                     [1m Learning iteration 173/2000 [0m                      

                       Computation: 49124 steps/s (collection: 1.911s, learning 0.091s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.7292
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 41.4382
                       Mean reward: 3.82
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.6469
     Episode_Reward/lifting_object: 0.0927
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 2.00s
                      Time elapsed: 00:06:20
                               ETA: 01:06:33

################################################################################
                     [1m Learning iteration 174/2000 [0m                      

                       Computation: 48164 steps/s (collection: 1.939s, learning 0.102s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.3125
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 41.4510
                       Mean reward: 2.67
               Mean episode length: 243.00
    Episode_Reward/reaching_object: 0.6595
     Episode_Reward/lifting_object: 0.1249
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 2.04s
                      Time elapsed: 00:06:22
                               ETA: 01:06:29

################################################################################
                     [1m Learning iteration 175/2000 [0m                      

                       Computation: 49132 steps/s (collection: 1.899s, learning 0.102s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.2289
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 41.4918
                       Mean reward: 4.21
               Mean episode length: 247.00
    Episode_Reward/reaching_object: 0.6443
     Episode_Reward/lifting_object: 0.1453
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 2.00s
                      Time elapsed: 00:06:24
                               ETA: 01:06:25

################################################################################
                     [1m Learning iteration 176/2000 [0m                      

                       Computation: 48198 steps/s (collection: 1.944s, learning 0.096s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.4464
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.5511
                       Mean reward: 3.84
               Mean episode length: 245.27
    Episode_Reward/reaching_object: 0.6545
     Episode_Reward/lifting_object: 0.0789
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 2.04s
                      Time elapsed: 00:06:26
                               ETA: 01:06:22

################################################################################
                     [1m Learning iteration 177/2000 [0m                      

                       Computation: 47281 steps/s (collection: 1.985s, learning 0.094s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.3236
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 41.5758
                       Mean reward: 3.67
               Mean episode length: 244.70
    Episode_Reward/reaching_object: 0.6646
     Episode_Reward/lifting_object: 0.1142
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 2.08s
                      Time elapsed: 00:06:28
                               ETA: 01:06:18

################################################################################
                     [1m Learning iteration 178/2000 [0m                      

                       Computation: 46712 steps/s (collection: 2.008s, learning 0.096s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.4298
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 41.6279
                       Mean reward: 3.32
               Mean episode length: 244.42
    Episode_Reward/reaching_object: 0.6829
     Episode_Reward/lifting_object: 0.0889
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 2.10s
                      Time elapsed: 00:06:30
                               ETA: 01:06:15

################################################################################
                     [1m Learning iteration 179/2000 [0m                      

                       Computation: 44286 steps/s (collection: 2.069s, learning 0.151s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.3918
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 41.6832
                       Mean reward: 3.55
               Mean episode length: 246.79
    Episode_Reward/reaching_object: 0.6716
     Episode_Reward/lifting_object: 0.0916
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 2.22s
                      Time elapsed: 00:06:32
                               ETA: 01:06:14

################################################################################
                     [1m Learning iteration 180/2000 [0m                      

                       Computation: 33860 steps/s (collection: 2.671s, learning 0.232s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.4126
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 41.7468
                       Mean reward: 3.76
               Mean episode length: 242.27
    Episode_Reward/reaching_object: 0.6536
     Episode_Reward/lifting_object: 0.1163
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 2.90s
                      Time elapsed: 00:06:35
                               ETA: 01:06:19

################################################################################
                     [1m Learning iteration 181/2000 [0m                      

                       Computation: 37587 steps/s (collection: 2.479s, learning 0.137s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.7851
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 41.8215
                       Mean reward: 3.98
               Mean episode length: 246.58
    Episode_Reward/reaching_object: 0.6592
     Episode_Reward/lifting_object: 0.1168
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 2.62s
                      Time elapsed: 00:06:38
                               ETA: 01:06:21

################################################################################
                     [1m Learning iteration 182/2000 [0m                      

                       Computation: 43593 steps/s (collection: 2.137s, learning 0.118s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.6350
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 41.8752
                       Mean reward: 3.91
               Mean episode length: 245.89
    Episode_Reward/reaching_object: 0.6687
     Episode_Reward/lifting_object: 0.0874
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 2.26s
                      Time elapsed: 00:06:40
                               ETA: 01:06:19

################################################################################
                     [1m Learning iteration 183/2000 [0m                      

                       Computation: 43590 steps/s (collection: 2.108s, learning 0.147s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.3013
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 41.9372
                       Mean reward: 3.84
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.6843
     Episode_Reward/lifting_object: 0.1752
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 2.26s
                      Time elapsed: 00:06:42
                               ETA: 01:06:18

################################################################################
                     [1m Learning iteration 184/2000 [0m                      

                       Computation: 43541 steps/s (collection: 2.108s, learning 0.150s)
             Mean action noise std: 1.64
          Mean value_function loss: 1.2202
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 42.0107
                       Mean reward: 4.38
               Mean episode length: 245.56
    Episode_Reward/reaching_object: 0.6773
     Episode_Reward/lifting_object: 0.1511
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 2.26s
                      Time elapsed: 00:06:45
                               ETA: 01:06:16

################################################################################
                     [1m Learning iteration 185/2000 [0m                      

                       Computation: 44699 steps/s (collection: 2.070s, learning 0.129s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.4929
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 42.0250
                       Mean reward: 3.69
               Mean episode length: 248.52
    Episode_Reward/reaching_object: 0.6700
     Episode_Reward/lifting_object: 0.0957
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 2.20s
                      Time elapsed: 00:06:47
                               ETA: 01:06:14

################################################################################
                     [1m Learning iteration 186/2000 [0m                      

                       Computation: 45817 steps/s (collection: 1.998s, learning 0.147s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.3589
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 42.0569
                       Mean reward: 3.78
               Mean episode length: 246.16
    Episode_Reward/reaching_object: 0.6840
     Episode_Reward/lifting_object: 0.1783
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 2.15s
                      Time elapsed: 00:06:49
                               ETA: 01:06:12

################################################################################
                     [1m Learning iteration 187/2000 [0m                      

                       Computation: 44974 steps/s (collection: 2.027s, learning 0.159s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.4782
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 42.1112
                       Mean reward: 3.60
               Mean episode length: 246.48
    Episode_Reward/reaching_object: 0.6743
     Episode_Reward/lifting_object: 0.1192
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 18.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 2.19s
                      Time elapsed: 00:06:51
                               ETA: 01:06:09

################################################################################
                     [1m Learning iteration 188/2000 [0m                      

                       Computation: 45442 steps/s (collection: 2.026s, learning 0.138s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.6609
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 42.1684
                       Mean reward: 3.87
               Mean episode length: 244.86
    Episode_Reward/reaching_object: 0.7038
     Episode_Reward/lifting_object: 0.1040
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 2.16s
                      Time elapsed: 00:06:53
                               ETA: 01:06:07

################################################################################
                     [1m Learning iteration 189/2000 [0m                      

                       Computation: 47130 steps/s (collection: 1.976s, learning 0.110s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.9760
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 42.2123
                       Mean reward: 4.07
               Mean episode length: 245.00
    Episode_Reward/reaching_object: 0.6895
     Episode_Reward/lifting_object: 0.1631
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 2.09s
                      Time elapsed: 00:06:55
                               ETA: 01:06:04

################################################################################
                     [1m Learning iteration 190/2000 [0m                      

                       Computation: 45507 steps/s (collection: 2.024s, learning 0.136s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.8894
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 42.2571
                       Mean reward: 3.09
               Mean episode length: 245.86
    Episode_Reward/reaching_object: 0.7100
     Episode_Reward/lifting_object: 0.1181
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 2.16s
                      Time elapsed: 00:06:58
                               ETA: 01:06:01

################################################################################
                     [1m Learning iteration 191/2000 [0m                      

                       Computation: 42540 steps/s (collection: 2.179s, learning 0.132s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.4943
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 42.2931
                       Mean reward: 4.71
               Mean episode length: 245.89
    Episode_Reward/reaching_object: 0.7241
     Episode_Reward/lifting_object: 0.1772
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 2.31s
                      Time elapsed: 00:07:00
                               ETA: 01:06:00

################################################################################
                     [1m Learning iteration 192/2000 [0m                      

                       Computation: 33880 steps/s (collection: 2.717s, learning 0.185s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.4523
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 42.3475
                       Mean reward: 4.28
               Mean episode length: 244.15
    Episode_Reward/reaching_object: 0.6949
     Episode_Reward/lifting_object: 0.1276
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 2.90s
                      Time elapsed: 00:07:03
                               ETA: 01:06:05

################################################################################
                     [1m Learning iteration 193/2000 [0m                      

                       Computation: 39135 steps/s (collection: 2.411s, learning 0.101s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.6397
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 42.3997
                       Mean reward: 4.20
               Mean episode length: 245.97
    Episode_Reward/reaching_object: 0.6943
     Episode_Reward/lifting_object: 0.1468
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 2.51s
                      Time elapsed: 00:07:05
                               ETA: 01:06:05

################################################################################
                     [1m Learning iteration 194/2000 [0m                      

                       Computation: 47998 steps/s (collection: 1.902s, learning 0.146s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.4715
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 42.4404
                       Mean reward: 3.77
               Mean episode length: 245.96
    Episode_Reward/reaching_object: 0.7159
     Episode_Reward/lifting_object: 0.1452
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 2.05s
                      Time elapsed: 00:07:07
                               ETA: 01:06:02

################################################################################
                     [1m Learning iteration 195/2000 [0m                      

                       Computation: 41121 steps/s (collection: 2.238s, learning 0.153s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.6544
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 42.4925
                       Mean reward: 3.44
               Mean episode length: 241.48
    Episode_Reward/reaching_object: 0.6974
     Episode_Reward/lifting_object: 0.1578
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 2.39s
                      Time elapsed: 00:07:10
                               ETA: 01:06:02

################################################################################
                     [1m Learning iteration 196/2000 [0m                      

                       Computation: 45283 steps/s (collection: 2.041s, learning 0.130s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.7676
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 42.5356
                       Mean reward: 4.86
               Mean episode length: 246.38
    Episode_Reward/reaching_object: 0.7003
     Episode_Reward/lifting_object: 0.1768
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 2.17s
                      Time elapsed: 00:07:12
                               ETA: 01:05:59

################################################################################
                     [1m Learning iteration 197/2000 [0m                      

                       Computation: 42442 steps/s (collection: 2.177s, learning 0.140s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.5960
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 42.5927
                       Mean reward: 4.29
               Mean episode length: 246.07
    Episode_Reward/reaching_object: 0.6696
     Episode_Reward/lifting_object: 0.1652
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 2.32s
                      Time elapsed: 00:07:14
                               ETA: 01:05:58

################################################################################
                     [1m Learning iteration 198/2000 [0m                      

                       Computation: 45115 steps/s (collection: 2.058s, learning 0.121s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.8317
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 42.6662
                       Mean reward: 4.08
               Mean episode length: 246.84
    Episode_Reward/reaching_object: 0.6751
     Episode_Reward/lifting_object: 0.0916
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 2.18s
                      Time elapsed: 00:07:16
                               ETA: 01:05:56

################################################################################
                     [1m Learning iteration 199/2000 [0m                      

                       Computation: 47817 steps/s (collection: 1.944s, learning 0.112s)
             Mean action noise std: 1.69
          Mean value_function loss: 1.8044
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 42.7044
                       Mean reward: 4.69
               Mean episode length: 248.63
    Episode_Reward/reaching_object: 0.6698
     Episode_Reward/lifting_object: 0.1819
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 18.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 2.06s
                      Time elapsed: 00:07:18
                               ETA: 01:05:52

################################################################################
                     [1m Learning iteration 200/2000 [0m                      

                       Computation: 49995 steps/s (collection: 1.862s, learning 0.104s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.5857
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 42.7534
                       Mean reward: 3.92
               Mean episode length: 244.93
    Episode_Reward/reaching_object: 0.6504
     Episode_Reward/lifting_object: 0.1167
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 1.97s
                      Time elapsed: 00:07:20
                               ETA: 01:05:48

################################################################################
                     [1m Learning iteration 201/2000 [0m                      

                       Computation: 49138 steps/s (collection: 1.895s, learning 0.106s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.9149
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 42.8164
                       Mean reward: 3.51
               Mean episode length: 240.53
    Episode_Reward/reaching_object: 0.6777
     Episode_Reward/lifting_object: 0.1311
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 2.00s
                      Time elapsed: 00:07:22
                               ETA: 01:05:44

################################################################################
                     [1m Learning iteration 202/2000 [0m                      

                       Computation: 49525 steps/s (collection: 1.870s, learning 0.115s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.5842
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 42.9033
                       Mean reward: 4.36
               Mean episode length: 247.27
    Episode_Reward/reaching_object: 0.6504
     Episode_Reward/lifting_object: 0.2468
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 1.98s
                      Time elapsed: 00:07:24
                               ETA: 01:05:40

################################################################################
                     [1m Learning iteration 203/2000 [0m                      

                       Computation: 48618 steps/s (collection: 1.911s, learning 0.111s)
             Mean action noise std: 1.71
          Mean value_function loss: 1.4894
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 42.9452
                       Mean reward: 1.05
               Mean episode length: 246.62
    Episode_Reward/reaching_object: 0.6590
     Episode_Reward/lifting_object: -0.0401
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 2.02s
                      Time elapsed: 00:07:26
                               ETA: 01:05:36

################################################################################
                     [1m Learning iteration 204/2000 [0m                      

                       Computation: 49926 steps/s (collection: 1.862s, learning 0.107s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.5623
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 42.9782
                       Mean reward: 4.31
               Mean episode length: 246.37
    Episode_Reward/reaching_object: 0.6733
     Episode_Reward/lifting_object: 0.2609
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 1.97s
                      Time elapsed: 00:07:28
                               ETA: 01:05:32

################################################################################
                     [1m Learning iteration 205/2000 [0m                      

                       Computation: 49451 steps/s (collection: 1.884s, learning 0.104s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.8652
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 43.0325
                       Mean reward: 3.70
               Mean episode length: 244.76
    Episode_Reward/reaching_object: 0.6676
     Episode_Reward/lifting_object: 0.0998
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 1.99s
                      Time elapsed: 00:07:30
                               ETA: 01:05:28

################################################################################
                     [1m Learning iteration 206/2000 [0m                      

                       Computation: 48711 steps/s (collection: 1.904s, learning 0.114s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.3462
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 43.1143
                       Mean reward: 5.48
               Mean episode length: 244.36
    Episode_Reward/reaching_object: 0.6646
     Episode_Reward/lifting_object: 0.1910
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 2.02s
                      Time elapsed: 00:07:32
                               ETA: 01:05:25

################################################################################
                     [1m Learning iteration 207/2000 [0m                      

                       Computation: 49150 steps/s (collection: 1.880s, learning 0.120s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.7685
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 43.1889
                       Mean reward: 3.46
               Mean episode length: 245.71
    Episode_Reward/reaching_object: 0.6800
     Episode_Reward/lifting_object: 0.1619
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 2.00s
                      Time elapsed: 00:07:34
                               ETA: 01:05:21

################################################################################
                     [1m Learning iteration 208/2000 [0m                      

                       Computation: 49090 steps/s (collection: 1.886s, learning 0.116s)
             Mean action noise std: 1.73
          Mean value_function loss: 1.0287
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 43.2318
                       Mean reward: 4.42
               Mean episode length: 247.29
    Episode_Reward/reaching_object: 0.6845
     Episode_Reward/lifting_object: 0.2081
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 2.00s
                      Time elapsed: 00:07:36
                               ETA: 01:05:17

################################################################################
                     [1m Learning iteration 209/2000 [0m                      

                       Computation: 48803 steps/s (collection: 1.901s, learning 0.114s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.5636
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 43.3000
                       Mean reward: 4.48
               Mean episode length: 247.43
    Episode_Reward/reaching_object: 0.7002
     Episode_Reward/lifting_object: 0.2191
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 2.01s
                      Time elapsed: 00:07:38
                               ETA: 01:05:13

################################################################################
                     [1m Learning iteration 210/2000 [0m                      

                       Computation: 49020 steps/s (collection: 1.894s, learning 0.112s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.6186
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 43.3822
                       Mean reward: 3.66
               Mean episode length: 242.45
    Episode_Reward/reaching_object: 0.6762
     Episode_Reward/lifting_object: 0.2314
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 2.01s
                      Time elapsed: 00:07:40
                               ETA: 01:05:10

################################################################################
                     [1m Learning iteration 211/2000 [0m                      

                       Computation: 50030 steps/s (collection: 1.861s, learning 0.104s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.9687
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 43.4401
                       Mean reward: 3.92
               Mean episode length: 244.46
    Episode_Reward/reaching_object: 0.6939
     Episode_Reward/lifting_object: 0.2028
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 1.96s
                      Time elapsed: 00:07:42
                               ETA: 01:05:06

################################################################################
                     [1m Learning iteration 212/2000 [0m                      

                       Computation: 49107 steps/s (collection: 1.899s, learning 0.103s)
             Mean action noise std: 1.75
          Mean value_function loss: 1.7187
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 43.4697
                       Mean reward: 3.68
               Mean episode length: 247.73
    Episode_Reward/reaching_object: 0.6933
     Episode_Reward/lifting_object: 0.2529
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 2.00s
                      Time elapsed: 00:07:44
                               ETA: 01:05:02

################################################################################
                     [1m Learning iteration 213/2000 [0m                      

                       Computation: 49117 steps/s (collection: 1.900s, learning 0.102s)
             Mean action noise std: 1.75
          Mean value_function loss: 1.0661
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 43.4931
                       Mean reward: 4.33
               Mean episode length: 244.60
    Episode_Reward/reaching_object: 0.6864
     Episode_Reward/lifting_object: 0.1877
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 2.00s
                      Time elapsed: 00:07:46
                               ETA: 01:04:58

################################################################################
                     [1m Learning iteration 214/2000 [0m                      

                       Computation: 49357 steps/s (collection: 1.897s, learning 0.095s)
             Mean action noise std: 1.75
          Mean value_function loss: 2.2706
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 43.5425
                       Mean reward: 3.99
               Mean episode length: 244.98
    Episode_Reward/reaching_object: 0.6657
     Episode_Reward/lifting_object: 0.2671
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 1.99s
                      Time elapsed: 00:07:48
                               ETA: 01:04:54

################################################################################
                     [1m Learning iteration 215/2000 [0m                      

                       Computation: 49742 steps/s (collection: 1.867s, learning 0.109s)
             Mean action noise std: 1.76
          Mean value_function loss: 1.4100
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 43.5866
                       Mean reward: 4.56
               Mean episode length: 244.84
    Episode_Reward/reaching_object: 0.6229
     Episode_Reward/lifting_object: 0.3107
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 1.98s
                      Time elapsed: 00:07:50
                               ETA: 01:04:51

################################################################################
                     [1m Learning iteration 216/2000 [0m                      

                       Computation: 49267 steps/s (collection: 1.893s, learning 0.103s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.9002
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 43.6342
                       Mean reward: 3.45
               Mean episode length: 240.99
    Episode_Reward/reaching_object: 0.6314
     Episode_Reward/lifting_object: 0.2235
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 2.00s
                      Time elapsed: 00:07:52
                               ETA: 01:04:47

################################################################################
                     [1m Learning iteration 217/2000 [0m                      

                       Computation: 49201 steps/s (collection: 1.885s, learning 0.113s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.8939
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 43.6939
                       Mean reward: 3.93
               Mean episode length: 243.30
    Episode_Reward/reaching_object: 0.5953
     Episode_Reward/lifting_object: 0.2372
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 2.00s
                      Time elapsed: 00:07:54
                               ETA: 01:04:43

################################################################################
                     [1m Learning iteration 218/2000 [0m                      

                       Computation: 49103 steps/s (collection: 1.884s, learning 0.118s)
             Mean action noise std: 1.77
          Mean value_function loss: 1.2904
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 43.7618
                       Mean reward: 5.60
               Mean episode length: 246.51
    Episode_Reward/reaching_object: 0.6164
     Episode_Reward/lifting_object: 0.3805
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 2.00s
                      Time elapsed: 00:07:56
                               ETA: 01:04:40

################################################################################
                     [1m Learning iteration 219/2000 [0m                      

                       Computation: 49644 steps/s (collection: 1.861s, learning 0.119s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.7560
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 43.8189
                       Mean reward: 3.76
               Mean episode length: 247.31
    Episode_Reward/reaching_object: 0.6269
     Episode_Reward/lifting_object: 0.4011
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 1.98s
                      Time elapsed: 00:07:58
                               ETA: 01:04:36

################################################################################
                     [1m Learning iteration 220/2000 [0m                      

                       Computation: 49419 steps/s (collection: 1.872s, learning 0.117s)
             Mean action noise std: 1.78
          Mean value_function loss: 1.8330
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 43.8620
                       Mean reward: 3.42
               Mean episode length: 246.14
    Episode_Reward/reaching_object: 0.5890
     Episode_Reward/lifting_object: 0.2298
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 1.99s
                      Time elapsed: 00:08:00
                               ETA: 01:04:32

################################################################################
                     [1m Learning iteration 221/2000 [0m                      

                       Computation: 49117 steps/s (collection: 1.878s, learning 0.123s)
             Mean action noise std: 1.78
          Mean value_function loss: 1.0278
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 43.9027
                       Mean reward: 4.54
               Mean episode length: 244.25
    Episode_Reward/reaching_object: 0.6020
     Episode_Reward/lifting_object: 0.3146
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 2.00s
                      Time elapsed: 00:08:02
                               ETA: 01:04:29

################################################################################
                     [1m Learning iteration 222/2000 [0m                      

                       Computation: 46799 steps/s (collection: 1.977s, learning 0.124s)
             Mean action noise std: 1.79
          Mean value_function loss: 2.4619
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 43.9534
                       Mean reward: 4.31
               Mean episode length: 238.98
    Episode_Reward/reaching_object: 0.6146
     Episode_Reward/lifting_object: 0.3235
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 2.10s
                      Time elapsed: 00:08:04
                               ETA: 01:04:26

################################################################################
                     [1m Learning iteration 223/2000 [0m                      

                       Computation: 48452 steps/s (collection: 1.916s, learning 0.113s)
             Mean action noise std: 1.80
          Mean value_function loss: 1.5060
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 44.0335
                       Mean reward: 5.09
               Mean episode length: 238.24
    Episode_Reward/reaching_object: 0.6454
     Episode_Reward/lifting_object: 0.2687
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 2.03s
                      Time elapsed: 00:08:06
                               ETA: 01:04:22

################################################################################
                     [1m Learning iteration 224/2000 [0m                      

                       Computation: 48638 steps/s (collection: 1.906s, learning 0.115s)
             Mean action noise std: 1.80
          Mean value_function loss: 1.7325
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 44.1006
                       Mean reward: 4.41
               Mean episode length: 242.54
    Episode_Reward/reaching_object: 0.5786
     Episode_Reward/lifting_object: 0.3285
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 2.02s
                      Time elapsed: 00:08:08
                               ETA: 01:04:19

################################################################################
                     [1m Learning iteration 225/2000 [0m                      

                       Computation: 49439 steps/s (collection: 1.874s, learning 0.115s)
             Mean action noise std: 1.80
          Mean value_function loss: 1.0751
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 44.1467
                       Mean reward: 4.58
               Mean episode length: 241.02
    Episode_Reward/reaching_object: 0.5905
     Episode_Reward/lifting_object: 0.4072
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 1.99s
                      Time elapsed: 00:08:10
                               ETA: 01:04:15

################################################################################
                     [1m Learning iteration 226/2000 [0m                      

                       Computation: 48634 steps/s (collection: 1.911s, learning 0.110s)
             Mean action noise std: 1.81
          Mean value_function loss: 1.0525
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 44.1910
                       Mean reward: 5.97
               Mean episode length: 244.05
    Episode_Reward/reaching_object: 0.6286
     Episode_Reward/lifting_object: 0.4687
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 2.02s
                      Time elapsed: 00:08:12
                               ETA: 01:04:12

################################################################################
                     [1m Learning iteration 227/2000 [0m                      

                       Computation: 47445 steps/s (collection: 1.959s, learning 0.113s)
             Mean action noise std: 1.81
          Mean value_function loss: 2.1347
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 44.2476
                       Mean reward: 5.74
               Mean episode length: 242.71
    Episode_Reward/reaching_object: 0.6425
     Episode_Reward/lifting_object: 0.3584
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 2.07s
                      Time elapsed: 00:08:15
                               ETA: 01:04:09

################################################################################
                     [1m Learning iteration 228/2000 [0m                      

                       Computation: 48567 steps/s (collection: 1.914s, learning 0.110s)
             Mean action noise std: 1.81
          Mean value_function loss: 1.6611
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 44.2777
                       Mean reward: 5.71
               Mean episode length: 243.28
    Episode_Reward/reaching_object: 0.6087
     Episode_Reward/lifting_object: 0.4604
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 2.02s
                      Time elapsed: 00:08:17
                               ETA: 01:04:06

################################################################################
                     [1m Learning iteration 229/2000 [0m                      

                       Computation: 49495 steps/s (collection: 1.872s, learning 0.115s)
             Mean action noise std: 1.82
          Mean value_function loss: 2.0624
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 44.3082
                       Mean reward: 5.17
               Mean episode length: 232.56
    Episode_Reward/reaching_object: 0.6233
     Episode_Reward/lifting_object: 0.5356
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 1.99s
                      Time elapsed: 00:08:19
                               ETA: 01:04:02

################################################################################
                     [1m Learning iteration 230/2000 [0m                      

                       Computation: 48946 steps/s (collection: 1.892s, learning 0.117s)
             Mean action noise std: 1.82
          Mean value_function loss: 2.1213
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 44.3343
                       Mean reward: 5.11
               Mean episode length: 243.00
    Episode_Reward/reaching_object: 0.6217
     Episode_Reward/lifting_object: 0.4661
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 2.01s
                      Time elapsed: 00:08:21
                               ETA: 01:03:59

################################################################################
                     [1m Learning iteration 231/2000 [0m                      

                       Computation: 48846 steps/s (collection: 1.898s, learning 0.115s)
             Mean action noise std: 1.82
          Mean value_function loss: 1.5423
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 44.3794
                       Mean reward: 6.44
               Mean episode length: 239.11
    Episode_Reward/reaching_object: 0.5974
     Episode_Reward/lifting_object: 0.5604
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 2.01s
                      Time elapsed: 00:08:23
                               ETA: 01:03:56

################################################################################
                     [1m Learning iteration 232/2000 [0m                      

                       Computation: 44390 steps/s (collection: 2.085s, learning 0.130s)
             Mean action noise std: 1.83
          Mean value_function loss: 1.5829
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 44.4422
                       Mean reward: 4.77
               Mean episode length: 222.32
    Episode_Reward/reaching_object: 0.5843
     Episode_Reward/lifting_object: 0.5400
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 2.21s
                      Time elapsed: 00:08:25
                               ETA: 01:03:54

################################################################################
                     [1m Learning iteration 233/2000 [0m                      

                       Computation: 49473 steps/s (collection: 1.881s, learning 0.106s)
             Mean action noise std: 1.83
          Mean value_function loss: 3.0330
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 44.4857
                       Mean reward: 4.84
               Mean episode length: 232.92
    Episode_Reward/reaching_object: 0.5807
     Episode_Reward/lifting_object: 0.5541
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 1.99s
                      Time elapsed: 00:08:27
                               ETA: 01:03:50

################################################################################
                     [1m Learning iteration 234/2000 [0m                      

                       Computation: 47327 steps/s (collection: 1.960s, learning 0.117s)
             Mean action noise std: 1.83
          Mean value_function loss: 1.4665
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 44.5119
                       Mean reward: 6.07
               Mean episode length: 232.04
    Episode_Reward/reaching_object: 0.5727
     Episode_Reward/lifting_object: 0.5886
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 2.08s
                      Time elapsed: 00:08:29
                               ETA: 01:03:47

################################################################################
                     [1m Learning iteration 235/2000 [0m                      

                       Computation: 49282 steps/s (collection: 1.888s, learning 0.107s)
             Mean action noise std: 1.84
          Mean value_function loss: 1.5077
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 44.5415
                       Mean reward: 6.77
               Mean episode length: 231.14
    Episode_Reward/reaching_object: 0.5854
     Episode_Reward/lifting_object: 0.6029
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 1.99s
                      Time elapsed: 00:08:31
                               ETA: 01:03:44

################################################################################
                     [1m Learning iteration 236/2000 [0m                      

                       Computation: 49054 steps/s (collection: 1.892s, learning 0.112s)
             Mean action noise std: 1.84
          Mean value_function loss: 2.7653
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 44.5830
                       Mean reward: 5.90
               Mean episode length: 236.61
    Episode_Reward/reaching_object: 0.5452
     Episode_Reward/lifting_object: 0.5094
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 2.00s
                      Time elapsed: 00:08:33
                               ETA: 01:03:40

################################################################################
                     [1m Learning iteration 237/2000 [0m                      

                       Computation: 49230 steps/s (collection: 1.882s, learning 0.115s)
             Mean action noise std: 1.84
          Mean value_function loss: 2.1411
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 44.6453
                       Mean reward: 6.63
               Mean episode length: 228.71
    Episode_Reward/reaching_object: 0.5536
     Episode_Reward/lifting_object: 0.6159
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 2.00s
                      Time elapsed: 00:08:35
                               ETA: 01:03:37

################################################################################
                     [1m Learning iteration 238/2000 [0m                      

                       Computation: 49004 steps/s (collection: 1.890s, learning 0.116s)
             Mean action noise std: 1.85
          Mean value_function loss: 2.7551
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 44.6986
                       Mean reward: 7.37
               Mean episode length: 222.76
    Episode_Reward/reaching_object: 0.5464
     Episode_Reward/lifting_object: 0.7055
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 2.01s
                      Time elapsed: 00:08:37
                               ETA: 01:03:34

################################################################################
                     [1m Learning iteration 239/2000 [0m                      

                       Computation: 49104 steps/s (collection: 1.895s, learning 0.107s)
             Mean action noise std: 1.85
          Mean value_function loss: 3.2586
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 44.7540
                       Mean reward: 6.73
               Mean episode length: 225.51
    Episode_Reward/reaching_object: 0.5426
     Episode_Reward/lifting_object: 0.6910
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0271
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 2.00s
                      Time elapsed: 00:08:39
                               ETA: 01:03:30

################################################################################
                     [1m Learning iteration 240/2000 [0m                      

                       Computation: 49002 steps/s (collection: 1.896s, learning 0.111s)
             Mean action noise std: 1.86
          Mean value_function loss: 2.9650
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 44.8155
                       Mean reward: 3.21
               Mean episode length: 211.77
    Episode_Reward/reaching_object: 0.5272
     Episode_Reward/lifting_object: 0.6916
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 2.01s
                      Time elapsed: 00:08:41
                               ETA: 01:03:27

################################################################################
                     [1m Learning iteration 241/2000 [0m                      

                       Computation: 49302 steps/s (collection: 1.887s, learning 0.107s)
             Mean action noise std: 1.86
          Mean value_function loss: 2.8952
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 44.8697
                       Mean reward: 4.37
               Mean episode length: 214.84
    Episode_Reward/reaching_object: 0.5220
     Episode_Reward/lifting_object: 0.6637
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 1.99s
                      Time elapsed: 00:08:43
                               ETA: 01:03:24

################################################################################
                     [1m Learning iteration 242/2000 [0m                      

                       Computation: 49079 steps/s (collection: 1.904s, learning 0.099s)
             Mean action noise std: 1.87
          Mean value_function loss: 3.8676
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 44.9194
                       Mean reward: 3.08
               Mean episode length: 212.59
    Episode_Reward/reaching_object: 0.5045
     Episode_Reward/lifting_object: 0.5584
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 2.00s
                      Time elapsed: 00:08:45
                               ETA: 01:03:20

################################################################################
                     [1m Learning iteration 243/2000 [0m                      

                       Computation: 49667 steps/s (collection: 1.890s, learning 0.089s)
             Mean action noise std: 1.87
          Mean value_function loss: 3.0558
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 44.9592
                       Mean reward: 7.23
               Mean episode length: 202.10
    Episode_Reward/reaching_object: 0.4910
     Episode_Reward/lifting_object: 0.8359
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 1.98s
                      Time elapsed: 00:08:47
                               ETA: 01:03:17

################################################################################
                     [1m Learning iteration 244/2000 [0m                      

                       Computation: 49655 steps/s (collection: 1.870s, learning 0.110s)
             Mean action noise std: 1.88
          Mean value_function loss: 2.3147
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 45.0023
                       Mean reward: 5.36
               Mean episode length: 186.66
    Episode_Reward/reaching_object: 0.4863
     Episode_Reward/lifting_object: 0.6964
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 1.98s
                      Time elapsed: 00:08:49
                               ETA: 01:03:13

################################################################################
                     [1m Learning iteration 245/2000 [0m                      

                       Computation: 49644 steps/s (collection: 1.875s, learning 0.105s)
             Mean action noise std: 1.88
          Mean value_function loss: 2.7033
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 45.0538
                       Mean reward: 5.02
               Mean episode length: 195.32
    Episode_Reward/reaching_object: 0.4826
     Episode_Reward/lifting_object: 0.5441
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 1.98s
                      Time elapsed: 00:08:51
                               ETA: 01:03:10

################################################################################
                     [1m Learning iteration 246/2000 [0m                      

                       Computation: 49163 steps/s (collection: 1.891s, learning 0.108s)
             Mean action noise std: 1.89
          Mean value_function loss: 3.2768
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 45.1164
                       Mean reward: 7.43
               Mean episode length: 195.39
    Episode_Reward/reaching_object: 0.5044
     Episode_Reward/lifting_object: 1.0550
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 2.00s
                      Time elapsed: 00:08:53
                               ETA: 01:03:07

################################################################################
                     [1m Learning iteration 247/2000 [0m                      

                       Computation: 49709 steps/s (collection: 1.871s, learning 0.106s)
             Mean action noise std: 1.89
          Mean value_function loss: 2.5178
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 45.1866
                       Mean reward: 7.10
               Mean episode length: 203.22
    Episode_Reward/reaching_object: 0.4872
     Episode_Reward/lifting_object: 0.7310
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 1.98s
                      Time elapsed: 00:08:55
                               ETA: 01:03:03

################################################################################
                     [1m Learning iteration 248/2000 [0m                      

                       Computation: 49689 steps/s (collection: 1.864s, learning 0.114s)
             Mean action noise std: 1.89
          Mean value_function loss: 3.7031
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 45.2286
                       Mean reward: 7.26
               Mean episode length: 198.47
    Episode_Reward/reaching_object: 0.4909
     Episode_Reward/lifting_object: 1.0641
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 1.98s
                      Time elapsed: 00:08:57
                               ETA: 01:03:00

################################################################################
                     [1m Learning iteration 249/2000 [0m                      

                       Computation: 46519 steps/s (collection: 1.997s, learning 0.117s)
             Mean action noise std: 1.90
          Mean value_function loss: 2.1607
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 45.2804
                       Mean reward: 7.14
               Mean episode length: 225.33
    Episode_Reward/reaching_object: 0.4980
     Episode_Reward/lifting_object: 0.8740
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 2.11s
                      Time elapsed: 00:08:59
                               ETA: 01:02:57

################################################################################
                     [1m Learning iteration 250/2000 [0m                      

                       Computation: 44985 steps/s (collection: 2.094s, learning 0.091s)
             Mean action noise std: 1.90
          Mean value_function loss: 2.7681
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 45.3261
                       Mean reward: 6.37
               Mean episode length: 212.80
    Episode_Reward/reaching_object: 0.4940
     Episode_Reward/lifting_object: 1.0247
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 2.19s
                      Time elapsed: 00:09:01
                               ETA: 01:02:55

################################################################################
                     [1m Learning iteration 251/2000 [0m                      

                       Computation: 48861 steps/s (collection: 1.895s, learning 0.117s)
             Mean action noise std: 1.91
          Mean value_function loss: 3.2320
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 45.3633
                       Mean reward: 5.86
               Mean episode length: 213.79
    Episode_Reward/reaching_object: 0.4851
     Episode_Reward/lifting_object: 0.7534
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 2.01s
                      Time elapsed: 00:09:03
                               ETA: 01:02:52

################################################################################
                     [1m Learning iteration 252/2000 [0m                      

                       Computation: 45112 steps/s (collection: 2.091s, learning 0.089s)
             Mean action noise std: 1.91
          Mean value_function loss: 3.9234
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 45.4099
                       Mean reward: 7.72
               Mean episode length: 214.52
    Episode_Reward/reaching_object: 0.4933
     Episode_Reward/lifting_object: 0.9627
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 2.18s
                      Time elapsed: 00:09:05
                               ETA: 01:02:50

################################################################################
                     [1m Learning iteration 253/2000 [0m                      

                       Computation: 50149 steps/s (collection: 1.848s, learning 0.112s)
             Mean action noise std: 1.91
          Mean value_function loss: 5.7718
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 45.4425
                       Mean reward: 6.64
               Mean episode length: 216.55
    Episode_Reward/reaching_object: 0.4773
     Episode_Reward/lifting_object: 0.9492
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 1.96s
                      Time elapsed: 00:09:07
                               ETA: 01:02:47

################################################################################
                     [1m Learning iteration 254/2000 [0m                      

                       Computation: 50239 steps/s (collection: 1.838s, learning 0.119s)
             Mean action noise std: 1.92
          Mean value_function loss: 3.4689
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 45.4726
                       Mean reward: 8.37
               Mean episode length: 208.22
    Episode_Reward/reaching_object: 0.4721
     Episode_Reward/lifting_object: 0.8968
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 1.96s
                      Time elapsed: 00:09:09
                               ETA: 01:02:43

################################################################################
                     [1m Learning iteration 255/2000 [0m                      

                       Computation: 50105 steps/s (collection: 1.845s, learning 0.117s)
             Mean action noise std: 1.92
          Mean value_function loss: 2.6373
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 45.5070
                       Mean reward: 8.04
               Mean episode length: 214.47
    Episode_Reward/reaching_object: 0.4692
     Episode_Reward/lifting_object: 0.8243
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 1.96s
                      Time elapsed: 00:09:11
                               ETA: 01:02:40

################################################################################
                     [1m Learning iteration 256/2000 [0m                      

                       Computation: 46485 steps/s (collection: 1.988s, learning 0.127s)
             Mean action noise std: 1.92
          Mean value_function loss: 2.6376
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 45.5550
                       Mean reward: 6.32
               Mean episode length: 203.20
    Episode_Reward/reaching_object: 0.4373
     Episode_Reward/lifting_object: 0.8845
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 2.11s
                      Time elapsed: 00:09:13
                               ETA: 01:02:37

################################################################################
                     [1m Learning iteration 257/2000 [0m                      

                       Computation: 49974 steps/s (collection: 1.857s, learning 0.110s)
             Mean action noise std: 1.93
          Mean value_function loss: 3.4839
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 45.5992
                       Mean reward: 4.96
               Mean episode length: 207.24
    Episode_Reward/reaching_object: 0.4207
     Episode_Reward/lifting_object: 0.9007
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 1.97s
                      Time elapsed: 00:09:15
                               ETA: 01:02:34

################################################################################
                     [1m Learning iteration 258/2000 [0m                      

                       Computation: 50567 steps/s (collection: 1.831s, learning 0.113s)
             Mean action noise std: 1.93
          Mean value_function loss: 8.1278
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 45.6336
                       Mean reward: 7.63
               Mean episode length: 210.75
    Episode_Reward/reaching_object: 0.4421
     Episode_Reward/lifting_object: 1.0682
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 1.94s
                      Time elapsed: 00:09:17
                               ETA: 01:02:30

################################################################################
                     [1m Learning iteration 259/2000 [0m                      

                       Computation: 50271 steps/s (collection: 1.847s, learning 0.109s)
             Mean action noise std: 1.93
          Mean value_function loss: 2.9259
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 45.6501
                       Mean reward: 4.82
               Mean episode length: 213.25
    Episode_Reward/reaching_object: 0.4246
     Episode_Reward/lifting_object: 0.7776
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 1.96s
                      Time elapsed: 00:09:19
                               ETA: 01:02:27

################################################################################
                     [1m Learning iteration 260/2000 [0m                      

                       Computation: 49785 steps/s (collection: 1.863s, learning 0.111s)
             Mean action noise std: 1.93
          Mean value_function loss: 3.7138
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 45.6881
                       Mean reward: 5.68
               Mean episode length: 211.10
    Episode_Reward/reaching_object: 0.4188
     Episode_Reward/lifting_object: 0.8640
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 1.97s
                      Time elapsed: 00:09:21
                               ETA: 01:02:23

################################################################################
                     [1m Learning iteration 261/2000 [0m                      

                       Computation: 49890 steps/s (collection: 1.860s, learning 0.110s)
             Mean action noise std: 1.94
          Mean value_function loss: 4.0896
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 45.7364
                       Mean reward: 7.47
               Mean episode length: 195.00
    Episode_Reward/reaching_object: 0.4364
     Episode_Reward/lifting_object: 1.0243
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 1.97s
                      Time elapsed: 00:09:23
                               ETA: 01:02:20

################################################################################
                     [1m Learning iteration 262/2000 [0m                      

                       Computation: 47684 steps/s (collection: 1.945s, learning 0.117s)
             Mean action noise std: 1.94
          Mean value_function loss: 3.4599
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 45.7951
                       Mean reward: 6.56
               Mean episode length: 211.94
    Episode_Reward/reaching_object: 0.4259
     Episode_Reward/lifting_object: 1.0304
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 2.06s
                      Time elapsed: 00:09:25
                               ETA: 01:02:17

################################################################################
                     [1m Learning iteration 263/2000 [0m                      

                       Computation: 49068 steps/s (collection: 1.884s, learning 0.119s)
             Mean action noise std: 1.95
          Mean value_function loss: 3.0448
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 45.8347
                       Mean reward: 8.41
               Mean episode length: 211.66
    Episode_Reward/reaching_object: 0.4554
     Episode_Reward/lifting_object: 1.0809
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 2.00s
                      Time elapsed: 00:09:27
                               ETA: 01:02:14

################################################################################
                     [1m Learning iteration 264/2000 [0m                      

                       Computation: 48645 steps/s (collection: 1.898s, learning 0.123s)
             Mean action noise std: 1.95
          Mean value_function loss: 2.8646
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 45.8681
                       Mean reward: 4.40
               Mean episode length: 212.60
    Episode_Reward/reaching_object: 0.4568
     Episode_Reward/lifting_object: 0.9986
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 2.02s
                      Time elapsed: 00:09:29
                               ETA: 01:02:11

################################################################################
                     [1m Learning iteration 265/2000 [0m                      

                       Computation: 48929 steps/s (collection: 1.886s, learning 0.123s)
             Mean action noise std: 1.95
          Mean value_function loss: 4.7760
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 45.8986
                       Mean reward: 6.48
               Mean episode length: 200.81
    Episode_Reward/reaching_object: 0.4502
     Episode_Reward/lifting_object: 1.0229
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 2.01s
                      Time elapsed: 00:09:31
                               ETA: 01:02:08

################################################################################
                     [1m Learning iteration 266/2000 [0m                      

                       Computation: 49464 steps/s (collection: 1.894s, learning 0.094s)
             Mean action noise std: 1.96
          Mean value_function loss: 7.3031
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 45.9570
                       Mean reward: 8.33
               Mean episode length: 206.23
    Episode_Reward/reaching_object: 0.4695
     Episode_Reward/lifting_object: 1.1721
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 1.99s
                      Time elapsed: 00:09:33
                               ETA: 01:02:05

################################################################################
                     [1m Learning iteration 267/2000 [0m                      

                       Computation: 48334 steps/s (collection: 1.914s, learning 0.120s)
             Mean action noise std: 1.96
          Mean value_function loss: 5.1781
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 45.9944
                       Mean reward: 9.16
               Mean episode length: 203.94
    Episode_Reward/reaching_object: 0.4597
     Episode_Reward/lifting_object: 1.1423
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 2.03s
                      Time elapsed: 00:09:35
                               ETA: 01:02:02

################################################################################
                     [1m Learning iteration 268/2000 [0m                      

                       Computation: 48327 steps/s (collection: 1.908s, learning 0.126s)
             Mean action noise std: 1.97
          Mean value_function loss: 5.1795
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 46.0334
                       Mean reward: 9.57
               Mean episode length: 210.46
    Episode_Reward/reaching_object: 0.4718
     Episode_Reward/lifting_object: 1.4091
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 2.03s
                      Time elapsed: 00:09:37
                               ETA: 01:01:59

################################################################################
                     [1m Learning iteration 269/2000 [0m                      

                       Computation: 48749 steps/s (collection: 1.901s, learning 0.115s)
             Mean action noise std: 1.97
          Mean value_function loss: 3.6824
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 46.0807
                       Mean reward: 6.12
               Mean episode length: 209.14
    Episode_Reward/reaching_object: 0.4604
     Episode_Reward/lifting_object: 0.9323
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 2.02s
                      Time elapsed: 00:09:39
                               ETA: 01:01:56

################################################################################
                     [1m Learning iteration 270/2000 [0m                      

                       Computation: 49061 steps/s (collection: 1.890s, learning 0.114s)
             Mean action noise std: 1.97
          Mean value_function loss: 4.1394
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 46.1220
                       Mean reward: 7.93
               Mean episode length: 196.46
    Episode_Reward/reaching_object: 0.4655
     Episode_Reward/lifting_object: 1.0814
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 2.00s
                      Time elapsed: 00:09:41
                               ETA: 01:01:53

################################################################################
                     [1m Learning iteration 271/2000 [0m                      

                       Computation: 48800 steps/s (collection: 1.900s, learning 0.115s)
             Mean action noise std: 1.98
          Mean value_function loss: 4.7920
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 46.1593
                       Mean reward: 6.81
               Mean episode length: 201.49
    Episode_Reward/reaching_object: 0.4544
     Episode_Reward/lifting_object: 1.1177
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 2.01s
                      Time elapsed: 00:09:43
                               ETA: 01:01:50

################################################################################
                     [1m Learning iteration 272/2000 [0m                      

                       Computation: 49028 steps/s (collection: 1.891s, learning 0.114s)
             Mean action noise std: 1.98
          Mean value_function loss: 3.8079
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 46.1760
                       Mean reward: 8.81
               Mean episode length: 195.74
    Episode_Reward/reaching_object: 0.4407
     Episode_Reward/lifting_object: 1.1275
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 2.01s
                      Time elapsed: 00:09:45
                               ETA: 01:01:47

################################################################################
                     [1m Learning iteration 273/2000 [0m                      

                       Computation: 49320 steps/s (collection: 1.872s, learning 0.121s)
             Mean action noise std: 1.98
          Mean value_function loss: 3.9845
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 46.2102
                       Mean reward: 7.50
               Mean episode length: 196.43
    Episode_Reward/reaching_object: 0.4420
     Episode_Reward/lifting_object: 1.2457
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 1.99s
                      Time elapsed: 00:09:47
                               ETA: 01:01:44

################################################################################
                     [1m Learning iteration 274/2000 [0m                      

                       Computation: 48143 steps/s (collection: 1.931s, learning 0.111s)
             Mean action noise std: 1.98
          Mean value_function loss: 4.0148
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 46.2428
                       Mean reward: 6.95
               Mean episode length: 190.65
    Episode_Reward/reaching_object: 0.4380
     Episode_Reward/lifting_object: 1.0257
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 2.04s
                      Time elapsed: 00:09:49
                               ETA: 01:01:41

################################################################################
                     [1m Learning iteration 275/2000 [0m                      

                       Computation: 49126 steps/s (collection: 1.909s, learning 0.092s)
             Mean action noise std: 1.99
          Mean value_function loss: 4.1278
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 46.2675
                       Mean reward: 8.40
               Mean episode length: 201.29
    Episode_Reward/reaching_object: 0.4531
     Episode_Reward/lifting_object: 1.4291
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 2.00s
                      Time elapsed: 00:09:51
                               ETA: 01:01:38

################################################################################
                     [1m Learning iteration 276/2000 [0m                      

                       Computation: 49283 steps/s (collection: 1.904s, learning 0.091s)
             Mean action noise std: 1.99
          Mean value_function loss: 4.1484
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 46.2829
                       Mean reward: 8.36
               Mean episode length: 196.91
    Episode_Reward/reaching_object: 0.4534
     Episode_Reward/lifting_object: 1.4096
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 1.99s
                      Time elapsed: 00:09:53
                               ETA: 01:01:35

################################################################################
                     [1m Learning iteration 277/2000 [0m                      

                       Computation: 49355 steps/s (collection: 1.902s, learning 0.090s)
             Mean action noise std: 1.99
          Mean value_function loss: 5.0556
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 46.3072
                       Mean reward: 7.30
               Mean episode length: 201.49
    Episode_Reward/reaching_object: 0.4325
     Episode_Reward/lifting_object: 1.2637
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 1.99s
                      Time elapsed: 00:09:55
                               ETA: 01:01:32

################################################################################
                     [1m Learning iteration 278/2000 [0m                      

                       Computation: 48023 steps/s (collection: 1.921s, learning 0.126s)
             Mean action noise std: 1.99
          Mean value_function loss: 14.5456
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 46.3329
                       Mean reward: 8.47
               Mean episode length: 187.11
    Episode_Reward/reaching_object: 0.4077
     Episode_Reward/lifting_object: 1.1678
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 2.05s
                      Time elapsed: 00:09:57
                               ETA: 01:01:29

################################################################################
                     [1m Learning iteration 279/2000 [0m                      

                       Computation: 47659 steps/s (collection: 1.944s, learning 0.119s)
             Mean action noise std: 1.99
          Mean value_function loss: 5.1538
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 46.3601
                       Mean reward: 7.93
               Mean episode length: 165.40
    Episode_Reward/reaching_object: 0.4290
     Episode_Reward/lifting_object: 1.2629
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.4583
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 2.06s
                      Time elapsed: 00:09:59
                               ETA: 01:01:27

################################################################################
                     [1m Learning iteration 280/2000 [0m                      

                       Computation: 48105 steps/s (collection: 1.946s, learning 0.098s)
             Mean action noise std: 2.00
          Mean value_function loss: 5.7562
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 46.4051
                       Mean reward: 8.41
               Mean episode length: 187.78
    Episode_Reward/reaching_object: 0.4485
     Episode_Reward/lifting_object: 1.3784
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.0417
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 2.04s
                      Time elapsed: 00:10:01
                               ETA: 01:01:24

################################################################################
                     [1m Learning iteration 281/2000 [0m                      

                       Computation: 47534 steps/s (collection: 1.945s, learning 0.123s)
             Mean action noise std: 2.00
          Mean value_function loss: 12.3294
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 46.4393
                       Mean reward: 2.34
               Mean episode length: 180.13
    Episode_Reward/reaching_object: 0.4282
     Episode_Reward/lifting_object: 1.1169
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 2.07s
                      Time elapsed: 00:10:03
                               ETA: 01:01:21

################################################################################
                     [1m Learning iteration 282/2000 [0m                      

                       Computation: 46803 steps/s (collection: 1.989s, learning 0.111s)
             Mean action noise std: 2.00
          Mean value_function loss: 5.0667
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 46.4743
                       Mean reward: 10.51
               Mean episode length: 170.90
    Episode_Reward/reaching_object: 0.4224
     Episode_Reward/lifting_object: 1.4111
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 14.2917
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 2.10s
                      Time elapsed: 00:10:06
                               ETA: 01:01:19

################################################################################
                     [1m Learning iteration 283/2000 [0m                      

                       Computation: 47642 steps/s (collection: 1.940s, learning 0.124s)
             Mean action noise std: 2.01
          Mean value_function loss: 6.1166
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 46.4967
                       Mean reward: 6.30
               Mean episode length: 174.57
    Episode_Reward/reaching_object: 0.4352
     Episode_Reward/lifting_object: 1.3076
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 2.06s
                      Time elapsed: 00:10:08
                               ETA: 01:01:16

################################################################################
                     [1m Learning iteration 284/2000 [0m                      

                       Computation: 47127 steps/s (collection: 1.955s, learning 0.131s)
             Mean action noise std: 2.01
          Mean value_function loss: 5.9418
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 46.5295
                       Mean reward: 9.28
               Mean episode length: 188.54
    Episode_Reward/reaching_object: 0.4474
     Episode_Reward/lifting_object: 1.4630
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 2.09s
                      Time elapsed: 00:10:10
                               ETA: 01:01:14

################################################################################
                     [1m Learning iteration 285/2000 [0m                      

                       Computation: 47135 steps/s (collection: 1.954s, learning 0.132s)
             Mean action noise std: 2.01
          Mean value_function loss: 7.3750
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 46.5572
                       Mean reward: 10.75
               Mean episode length: 174.57
    Episode_Reward/reaching_object: 0.4368
     Episode_Reward/lifting_object: 1.6059
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 2.09s
                      Time elapsed: 00:10:12
                               ETA: 01:01:11

################################################################################
                     [1m Learning iteration 286/2000 [0m                      

                       Computation: 48055 steps/s (collection: 1.932s, learning 0.114s)
             Mean action noise std: 2.02
          Mean value_function loss: 9.2851
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 46.5932
                       Mean reward: 9.64
               Mean episode length: 166.96
    Episode_Reward/reaching_object: 0.4444
     Episode_Reward/lifting_object: 1.4556
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 2.05s
                      Time elapsed: 00:10:14
                               ETA: 01:01:09

################################################################################
                     [1m Learning iteration 287/2000 [0m                      

                       Computation: 47833 steps/s (collection: 1.934s, learning 0.121s)
             Mean action noise std: 2.02
          Mean value_function loss: 8.9796
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 46.6329
                       Mean reward: 10.25
               Mean episode length: 185.10
    Episode_Reward/reaching_object: 0.4742
     Episode_Reward/lifting_object: 1.5339
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 2.06s
                      Time elapsed: 00:10:16
                               ETA: 01:01:06

################################################################################
                     [1m Learning iteration 288/2000 [0m                      

                       Computation: 48057 steps/s (collection: 1.931s, learning 0.115s)
             Mean action noise std: 2.02
          Mean value_function loss: 9.2783
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 46.6590
                       Mean reward: 10.74
               Mean episode length: 193.68
    Episode_Reward/reaching_object: 0.4904
     Episode_Reward/lifting_object: 1.6143
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 2.05s
                      Time elapsed: 00:10:18
                               ETA: 01:01:03

################################################################################
                     [1m Learning iteration 289/2000 [0m                      

                       Computation: 48208 steps/s (collection: 1.920s, learning 0.120s)
             Mean action noise std: 2.02
          Mean value_function loss: 8.0525
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 46.6822
                       Mean reward: 11.21
               Mean episode length: 197.80
    Episode_Reward/reaching_object: 0.5235
     Episode_Reward/lifting_object: 1.7774
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 2.04s
                      Time elapsed: 00:10:20
                               ETA: 01:01:01

################################################################################
                     [1m Learning iteration 290/2000 [0m                      

                       Computation: 47326 steps/s (collection: 1.956s, learning 0.121s)
             Mean action noise std: 2.03
          Mean value_function loss: 11.2249
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 46.7185
                       Mean reward: 12.94
               Mean episode length: 200.75
    Episode_Reward/reaching_object: 0.5090
     Episode_Reward/lifting_object: 2.0429
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 2.08s
                      Time elapsed: 00:10:22
                               ETA: 01:00:58

################################################################################
                     [1m Learning iteration 291/2000 [0m                      

                       Computation: 47660 steps/s (collection: 1.938s, learning 0.125s)
             Mean action noise std: 2.03
          Mean value_function loss: 11.2658
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 46.7518
                       Mean reward: 13.64
               Mean episode length: 204.84
    Episode_Reward/reaching_object: 0.5015
     Episode_Reward/lifting_object: 2.1877
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 2.06s
                      Time elapsed: 00:10:24
                               ETA: 01:00:55

################################################################################
                     [1m Learning iteration 292/2000 [0m                      

                       Computation: 47729 steps/s (collection: 1.933s, learning 0.126s)
             Mean action noise std: 2.03
          Mean value_function loss: 7.6544
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 46.7809
                       Mean reward: 9.77
               Mean episode length: 190.90
    Episode_Reward/reaching_object: 0.4744
     Episode_Reward/lifting_object: 1.6288
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 2.06s
                      Time elapsed: 00:10:26
                               ETA: 01:00:53

################################################################################
                     [1m Learning iteration 293/2000 [0m                      

                       Computation: 47525 steps/s (collection: 1.955s, learning 0.114s)
             Mean action noise std: 2.04
          Mean value_function loss: 9.1494
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 46.8074
                       Mean reward: 11.84
               Mean episode length: 194.35
    Episode_Reward/reaching_object: 0.4838
     Episode_Reward/lifting_object: 1.8148
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 2.07s
                      Time elapsed: 00:10:28
                               ETA: 01:00:50

################################################################################
                     [1m Learning iteration 294/2000 [0m                      

                       Computation: 46390 steps/s (collection: 2.003s, learning 0.116s)
             Mean action noise std: 2.04
          Mean value_function loss: 8.7820
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 46.8294
                       Mean reward: 13.66
               Mean episode length: 186.96
    Episode_Reward/reaching_object: 0.4913
     Episode_Reward/lifting_object: 2.0382
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 2.12s
                      Time elapsed: 00:10:30
                               ETA: 01:00:48

################################################################################
                     [1m Learning iteration 295/2000 [0m                      

                       Computation: 47657 steps/s (collection: 1.945s, learning 0.118s)
             Mean action noise std: 2.04
          Mean value_function loss: 13.2153
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 46.8501
                       Mean reward: 9.75
               Mean episode length: 190.94
    Episode_Reward/reaching_object: 0.4801
     Episode_Reward/lifting_object: 1.8668
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 2.06s
                      Time elapsed: 00:10:32
                               ETA: 01:00:45

################################################################################
                     [1m Learning iteration 296/2000 [0m                      

                       Computation: 45527 steps/s (collection: 2.034s, learning 0.126s)
             Mean action noise std: 2.04
          Mean value_function loss: 11.6098
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 46.8787
                       Mean reward: 13.84
               Mean episode length: 181.34
    Episode_Reward/reaching_object: 0.4914
     Episode_Reward/lifting_object: 2.0602
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 2.16s
                      Time elapsed: 00:10:35
                               ETA: 01:00:43

################################################################################
                     [1m Learning iteration 297/2000 [0m                      

                       Computation: 47744 steps/s (collection: 1.927s, learning 0.132s)
             Mean action noise std: 2.04
          Mean value_function loss: 9.1751
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 46.9019
                       Mean reward: 10.73
               Mean episode length: 179.11
    Episode_Reward/reaching_object: 0.4848
     Episode_Reward/lifting_object: 1.9800
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 9.9583
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 2.06s
                      Time elapsed: 00:10:37
                               ETA: 01:00:41

################################################################################
                     [1m Learning iteration 298/2000 [0m                      

                       Computation: 47444 steps/s (collection: 1.951s, learning 0.121s)
             Mean action noise std: 2.05
          Mean value_function loss: 13.4180
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 46.9270
                       Mean reward: 15.12
               Mean episode length: 208.58
    Episode_Reward/reaching_object: 0.4611
     Episode_Reward/lifting_object: 1.8003
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 2.07s
                      Time elapsed: 00:10:39
                               ETA: 01:00:38

################################################################################
                     [1m Learning iteration 299/2000 [0m                      

                       Computation: 47452 steps/s (collection: 1.943s, learning 0.129s)
             Mean action noise std: 2.05
          Mean value_function loss: 10.4320
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 46.9564
                       Mean reward: 13.92
               Mean episode length: 190.29
    Episode_Reward/reaching_object: 0.4801
     Episode_Reward/lifting_object: 1.8895
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 2.07s
                      Time elapsed: 00:10:41
                               ETA: 01:00:36

################################################################################
                     [1m Learning iteration 300/2000 [0m                      

                       Computation: 47738 steps/s (collection: 1.942s, learning 0.118s)
             Mean action noise std: 2.05
          Mean value_function loss: 10.8230
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 46.9787
                       Mean reward: 18.46
               Mean episode length: 209.36
    Episode_Reward/reaching_object: 0.5042
     Episode_Reward/lifting_object: 2.3049
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 2.06s
                      Time elapsed: 00:10:43
                               ETA: 01:00:33

################################################################################
                     [1m Learning iteration 301/2000 [0m                      

                       Computation: 47474 steps/s (collection: 1.948s, learning 0.123s)
             Mean action noise std: 2.05
          Mean value_function loss: 11.0449
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 46.9938
                       Mean reward: 12.97
               Mean episode length: 201.99
    Episode_Reward/reaching_object: 0.5156
     Episode_Reward/lifting_object: 2.3210
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 2.07s
                      Time elapsed: 00:10:45
                               ETA: 01:00:31

################################################################################
                     [1m Learning iteration 302/2000 [0m                      

                       Computation: 47417 steps/s (collection: 1.948s, learning 0.125s)
             Mean action noise std: 2.06
          Mean value_function loss: 9.4686
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 47.0196
                       Mean reward: 16.27
               Mean episode length: 211.95
    Episode_Reward/reaching_object: 0.5065
     Episode_Reward/lifting_object: 2.1591
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 2.07s
                      Time elapsed: 00:10:47
                               ETA: 01:00:28

################################################################################
                     [1m Learning iteration 303/2000 [0m                      

                       Computation: 46669 steps/s (collection: 1.984s, learning 0.123s)
             Mean action noise std: 2.06
          Mean value_function loss: 16.0771
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.0433
                       Mean reward: 13.21
               Mean episode length: 190.88
    Episode_Reward/reaching_object: 0.4959
     Episode_Reward/lifting_object: 2.3107
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 2.11s
                      Time elapsed: 00:10:49
                               ETA: 01:00:26

################################################################################
                     [1m Learning iteration 304/2000 [0m                      

                       Computation: 47241 steps/s (collection: 1.954s, learning 0.127s)
             Mean action noise std: 2.06
          Mean value_function loss: 17.8993
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 47.0665
                       Mean reward: 8.08
               Mean episode length: 191.90
    Episode_Reward/reaching_object: 0.5013
     Episode_Reward/lifting_object: 2.1236
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 2.08s
                      Time elapsed: 00:10:51
                               ETA: 01:00:23

################################################################################
                     [1m Learning iteration 305/2000 [0m                      

                       Computation: 47330 steps/s (collection: 1.951s, learning 0.126s)
             Mean action noise std: 2.06
          Mean value_function loss: 13.0340
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 47.0879
                       Mean reward: 10.98
               Mean episode length: 190.80
    Episode_Reward/reaching_object: 0.5254
     Episode_Reward/lifting_object: 2.2724
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 2.08s
                      Time elapsed: 00:10:53
                               ETA: 01:00:21

################################################################################
                     [1m Learning iteration 306/2000 [0m                      

                       Computation: 45449 steps/s (collection: 2.031s, learning 0.132s)
             Mean action noise std: 2.06
          Mean value_function loss: 14.4877
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 47.1075
                       Mean reward: 10.48
               Mean episode length: 203.93
    Episode_Reward/reaching_object: 0.5094
     Episode_Reward/lifting_object: 2.3722
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 2.16s
                      Time elapsed: 00:10:55
                               ETA: 01:00:19

################################################################################
                     [1m Learning iteration 307/2000 [0m                      

                       Computation: 45291 steps/s (collection: 2.048s, learning 0.122s)
             Mean action noise std: 2.07
          Mean value_function loss: 13.1081
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.1272
                       Mean reward: 13.35
               Mean episode length: 198.46
    Episode_Reward/reaching_object: 0.5211
     Episode_Reward/lifting_object: 2.9893
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 2.17s
                      Time elapsed: 00:10:58
                               ETA: 01:00:17

################################################################################
                     [1m Learning iteration 308/2000 [0m                      

                       Computation: 42869 steps/s (collection: 2.072s, learning 0.222s)
             Mean action noise std: 2.07
          Mean value_function loss: 9.0660
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.1444
                       Mean reward: 18.56
               Mean episode length: 210.19
    Episode_Reward/reaching_object: 0.5069
     Episode_Reward/lifting_object: 2.2413
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 2.29s
                      Time elapsed: 00:11:00
                               ETA: 01:00:16

################################################################################
                     [1m Learning iteration 309/2000 [0m                      

                       Computation: 32664 steps/s (collection: 2.778s, learning 0.231s)
             Mean action noise std: 2.07
          Mean value_function loss: 11.9354
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.1690
                       Mean reward: 17.04
               Mean episode length: 173.33
    Episode_Reward/reaching_object: 0.5111
     Episode_Reward/lifting_object: 2.9228
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 3.01s
                      Time elapsed: 00:11:03
                               ETA: 01:00:18

################################################################################
                     [1m Learning iteration 310/2000 [0m                      

                       Computation: 35508 steps/s (collection: 2.607s, learning 0.162s)
             Mean action noise std: 2.07
          Mean value_function loss: 12.3872
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 47.1982
                       Mean reward: 16.32
               Mean episode length: 194.81
    Episode_Reward/reaching_object: 0.5047
     Episode_Reward/lifting_object: 2.8304
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 2.77s
                      Time elapsed: 00:11:06
                               ETA: 01:00:20

################################################################################
                     [1m Learning iteration 311/2000 [0m                      

                       Computation: 36274 steps/s (collection: 2.529s, learning 0.181s)
             Mean action noise std: 2.07
          Mean value_function loss: 9.1621
               Mean surrogate loss: 0.0106
                 Mean entropy loss: 47.2115
                       Mean reward: 14.25
               Mean episode length: 186.83
    Episode_Reward/reaching_object: 0.5015
     Episode_Reward/lifting_object: 2.6039
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 2.71s
                      Time elapsed: 00:11:08
                               ETA: 01:00:21

################################################################################
                     [1m Learning iteration 312/2000 [0m                      

                       Computation: 40369 steps/s (collection: 2.259s, learning 0.176s)
             Mean action noise std: 2.07
          Mean value_function loss: 10.0324
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 47.2127
                       Mean reward: 15.90
               Mean episode length: 198.21
    Episode_Reward/reaching_object: 0.5178
     Episode_Reward/lifting_object: 2.8538
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 2.44s
                      Time elapsed: 00:11:11
                               ETA: 01:00:20

################################################################################
                     [1m Learning iteration 313/2000 [0m                      

                       Computation: 42579 steps/s (collection: 2.166s, learning 0.143s)
             Mean action noise std: 2.07
          Mean value_function loss: 10.7781
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 47.2130
                       Mean reward: 13.23
               Mean episode length: 199.23
    Episode_Reward/reaching_object: 0.5118
     Episode_Reward/lifting_object: 2.9124
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 2.31s
                      Time elapsed: 00:11:13
                               ETA: 01:00:19

################################################################################
                     [1m Learning iteration 314/2000 [0m                      

                       Computation: 45084 steps/s (collection: 2.052s, learning 0.128s)
             Mean action noise std: 2.07
          Mean value_function loss: 9.6278
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 47.2137
                       Mean reward: 20.05
               Mean episode length: 186.10
    Episode_Reward/reaching_object: 0.4970
     Episode_Reward/lifting_object: 2.6917
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 2.18s
                      Time elapsed: 00:11:15
                               ETA: 01:00:17

################################################################################
                     [1m Learning iteration 315/2000 [0m                      

                       Computation: 45341 steps/s (collection: 2.053s, learning 0.115s)
             Mean action noise std: 2.07
          Mean value_function loss: 13.5349
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.2189
                       Mean reward: 19.73
               Mean episode length: 192.02
    Episode_Reward/reaching_object: 0.5057
     Episode_Reward/lifting_object: 2.8603
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 2.17s
                      Time elapsed: 00:11:17
                               ETA: 01:00:15

################################################################################
                     [1m Learning iteration 316/2000 [0m                      

                       Computation: 43614 steps/s (collection: 2.127s, learning 0.127s)
             Mean action noise std: 2.08
          Mean value_function loss: 14.2748
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 47.2339
                       Mean reward: 17.99
               Mean episode length: 203.07
    Episode_Reward/reaching_object: 0.5020
     Episode_Reward/lifting_object: 2.7271
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 2.25s
                      Time elapsed: 00:11:20
                               ETA: 01:00:13

################################################################################
                     [1m Learning iteration 317/2000 [0m                      

                       Computation: 43595 steps/s (collection: 2.097s, learning 0.158s)
             Mean action noise std: 2.08
          Mean value_function loss: 11.9438
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 47.2523
                       Mean reward: 15.09
               Mean episode length: 192.62
    Episode_Reward/reaching_object: 0.5319
     Episode_Reward/lifting_object: 2.9231
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 2.25s
                      Time elapsed: 00:11:22
                               ETA: 01:00:12

################################################################################
                     [1m Learning iteration 318/2000 [0m                      

                       Computation: 44162 steps/s (collection: 2.120s, learning 0.106s)
             Mean action noise std: 2.08
          Mean value_function loss: 17.9567
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 47.2738
                       Mean reward: 16.09
               Mean episode length: 203.88
    Episode_Reward/reaching_object: 0.5246
     Episode_Reward/lifting_object: 2.6768
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 2.23s
                      Time elapsed: 00:11:24
                               ETA: 01:00:10

################################################################################
                     [1m Learning iteration 319/2000 [0m                      

                       Computation: 47871 steps/s (collection: 1.961s, learning 0.093s)
             Mean action noise std: 2.08
          Mean value_function loss: 20.7176
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 47.2931
                       Mean reward: 18.46
               Mean episode length: 201.36
    Episode_Reward/reaching_object: 0.5187
     Episode_Reward/lifting_object: 3.0422
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 2.05s
                      Time elapsed: 00:11:26
                               ETA: 01:00:07

################################################################################
                     [1m Learning iteration 320/2000 [0m                      

                       Computation: 48415 steps/s (collection: 1.930s, learning 0.100s)
             Mean action noise std: 2.08
          Mean value_function loss: 13.5367
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 47.3149
                       Mean reward: 17.33
               Mean episode length: 204.38
    Episode_Reward/reaching_object: 0.5578
     Episode_Reward/lifting_object: 3.4722
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 2.03s
                      Time elapsed: 00:11:28
                               ETA: 01:00:05

################################################################################
                     [1m Learning iteration 321/2000 [0m                      

                       Computation: 48190 steps/s (collection: 1.944s, learning 0.096s)
             Mean action noise std: 2.09
          Mean value_function loss: 12.1237
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 47.3359
                       Mean reward: 18.31
               Mean episode length: 206.47
    Episode_Reward/reaching_object: 0.5409
     Episode_Reward/lifting_object: 3.2964
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 2.04s
                      Time elapsed: 00:11:30
                               ETA: 01:00:02

################################################################################
                     [1m Learning iteration 322/2000 [0m                      

                       Computation: 47839 steps/s (collection: 1.953s, learning 0.102s)
             Mean action noise std: 2.09
          Mean value_function loss: 16.0750
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.3542
                       Mean reward: 20.23
               Mean episode length: 205.69
    Episode_Reward/reaching_object: 0.5610
     Episode_Reward/lifting_object: 3.1516
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 2.05s
                      Time elapsed: 00:11:32
                               ETA: 00:59:59

################################################################################
                     [1m Learning iteration 323/2000 [0m                      

                       Computation: 47140 steps/s (collection: 1.970s, learning 0.116s)
             Mean action noise std: 2.09
          Mean value_function loss: 15.0306
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 47.3665
                       Mean reward: 17.94
               Mean episode length: 195.54
    Episode_Reward/reaching_object: 0.5413
     Episode_Reward/lifting_object: 2.9249
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 2.09s
                      Time elapsed: 00:11:34
                               ETA: 00:59:57

################################################################################
                     [1m Learning iteration 324/2000 [0m                      

                       Computation: 47504 steps/s (collection: 1.969s, learning 0.100s)
             Mean action noise std: 2.09
          Mean value_function loss: 13.6154
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 47.3753
                       Mean reward: 20.99
               Mean episode length: 207.65
    Episode_Reward/reaching_object: 0.5615
     Episode_Reward/lifting_object: 3.5861
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 2.07s
                      Time elapsed: 00:11:37
                               ETA: 00:59:54

################################################################################
                     [1m Learning iteration 325/2000 [0m                      

                       Computation: 43368 steps/s (collection: 2.132s, learning 0.135s)
             Mean action noise std: 2.09
          Mean value_function loss: 15.0531
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 47.3860
                       Mean reward: 17.74
               Mean episode length: 199.92
    Episode_Reward/reaching_object: 0.5544
     Episode_Reward/lifting_object: 3.5838
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 2.27s
                      Time elapsed: 00:11:39
                               ETA: 00:59:53

################################################################################
                     [1m Learning iteration 326/2000 [0m                      

                       Computation: 45676 steps/s (collection: 2.036s, learning 0.117s)
             Mean action noise std: 2.09
          Mean value_function loss: 20.4357
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 47.3992
                       Mean reward: 21.28
               Mean episode length: 192.26
    Episode_Reward/reaching_object: 0.5420
     Episode_Reward/lifting_object: 3.6850
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 2.15s
                      Time elapsed: 00:11:41
                               ETA: 00:59:51

################################################################################
                     [1m Learning iteration 327/2000 [0m                      

                       Computation: 44101 steps/s (collection: 2.094s, learning 0.135s)
             Mean action noise std: 2.09
          Mean value_function loss: 12.7255
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 47.4096
                       Mean reward: 18.83
               Mean episode length: 183.11
    Episode_Reward/reaching_object: 0.5293
     Episode_Reward/lifting_object: 3.5593
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 2.23s
                      Time elapsed: 00:11:43
                               ETA: 00:59:49

################################################################################
                     [1m Learning iteration 328/2000 [0m                      

                       Computation: 44909 steps/s (collection: 2.069s, learning 0.120s)
             Mean action noise std: 2.09
          Mean value_function loss: 21.8988
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 47.4162
                       Mean reward: 15.73
               Mean episode length: 183.66
    Episode_Reward/reaching_object: 0.5311
     Episode_Reward/lifting_object: 3.5335
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 2.19s
                      Time elapsed: 00:11:45
                               ETA: 00:59:47

################################################################################
                     [1m Learning iteration 329/2000 [0m                      

                       Computation: 44281 steps/s (collection: 2.069s, learning 0.151s)
             Mean action noise std: 2.09
          Mean value_function loss: 17.0727
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 47.4277
                       Mean reward: 19.20
               Mean episode length: 189.97
    Episode_Reward/reaching_object: 0.5264
     Episode_Reward/lifting_object: 3.4212
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 2.22s
                      Time elapsed: 00:11:48
                               ETA: 00:59:45

################################################################################
                     [1m Learning iteration 330/2000 [0m                      

                       Computation: 42609 steps/s (collection: 2.192s, learning 0.116s)
             Mean action noise std: 2.09
          Mean value_function loss: 16.8573
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 47.4341
                       Mean reward: 23.78
               Mean episode length: 192.49
    Episode_Reward/reaching_object: 0.5436
     Episode_Reward/lifting_object: 3.9029
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 2.31s
                      Time elapsed: 00:11:50
                               ETA: 00:59:44

################################################################################
                     [1m Learning iteration 331/2000 [0m                      

                       Computation: 43725 steps/s (collection: 2.076s, learning 0.173s)
             Mean action noise std: 2.10
          Mean value_function loss: 18.6014
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.4394
                       Mean reward: 16.89
               Mean episode length: 185.06
    Episode_Reward/reaching_object: 0.5319
     Episode_Reward/lifting_object: 4.0987
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 2.25s
                      Time elapsed: 00:11:52
                               ETA: 00:59:42

################################################################################
                     [1m Learning iteration 332/2000 [0m                      

                       Computation: 30954 steps/s (collection: 2.927s, learning 0.249s)
             Mean action noise std: 2.10
          Mean value_function loss: 22.2224
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 47.4499
                       Mean reward: 21.25
               Mean episode length: 178.58
    Episode_Reward/reaching_object: 0.5347
     Episode_Reward/lifting_object: 4.2301
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 3.18s
                      Time elapsed: 00:11:55
                               ETA: 00:59:45

################################################################################
                     [1m Learning iteration 333/2000 [0m                      

                       Computation: 17192 steps/s (collection: 5.588s, learning 0.130s)
             Mean action noise std: 2.10
          Mean value_function loss: 24.8557
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 47.4591
                       Mean reward: 26.11
               Mean episode length: 196.76
    Episode_Reward/reaching_object: 0.5386
     Episode_Reward/lifting_object: 4.3783
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 9.9583
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 5.72s
                      Time elapsed: 00:12:01
                               ETA: 01:00:01

################################################################################
                     [1m Learning iteration 334/2000 [0m                      

                       Computation: 14284 steps/s (collection: 6.750s, learning 0.132s)
             Mean action noise std: 2.10
          Mean value_function loss: 22.5150
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 47.4713
                       Mean reward: 26.74
               Mean episode length: 190.84
    Episode_Reward/reaching_object: 0.5348
     Episode_Reward/lifting_object: 3.7564
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 6.88s
                      Time elapsed: 00:12:08
                               ETA: 01:00:22

################################################################################
                     [1m Learning iteration 335/2000 [0m                      

                       Computation: 14300 steps/s (collection: 6.749s, learning 0.125s)
             Mean action noise std: 2.10
          Mean value_function loss: 28.9370
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 47.4905
                       Mean reward: 21.87
               Mean episode length: 176.99
    Episode_Reward/reaching_object: 0.5256
     Episode_Reward/lifting_object: 4.5148
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 6.87s
                      Time elapsed: 00:12:15
                               ETA: 01:00:43

################################################################################
                     [1m Learning iteration 336/2000 [0m                      

                       Computation: 14011 steps/s (collection: 6.903s, learning 0.113s)
             Mean action noise std: 2.10
          Mean value_function loss: 26.3401
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 47.5085
                       Mean reward: 27.27
               Mean episode length: 177.29
    Episode_Reward/reaching_object: 0.5268
     Episode_Reward/lifting_object: 4.1928
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 7.02s
                      Time elapsed: 00:12:22
                               ETA: 01:01:05

################################################################################
                     [1m Learning iteration 337/2000 [0m                      

                       Computation: 14844 steps/s (collection: 6.507s, learning 0.115s)
             Mean action noise std: 2.10
          Mean value_function loss: 24.4774
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 47.5259
                       Mean reward: 22.82
               Mean episode length: 172.34
    Episode_Reward/reaching_object: 0.5161
     Episode_Reward/lifting_object: 4.2303
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 6.62s
                      Time elapsed: 00:12:28
                               ETA: 01:01:24

################################################################################
                     [1m Learning iteration 338/2000 [0m                      

                       Computation: 14330 steps/s (collection: 6.740s, learning 0.120s)
             Mean action noise std: 2.11
          Mean value_function loss: 24.2214
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 47.5421
                       Mean reward: 20.61
               Mean episode length: 166.57
    Episode_Reward/reaching_object: 0.5047
     Episode_Reward/lifting_object: 4.0873
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 6.86s
                      Time elapsed: 00:12:35
                               ETA: 01:01:45

################################################################################
                     [1m Learning iteration 339/2000 [0m                      

                       Computation: 14360 steps/s (collection: 6.728s, learning 0.117s)
             Mean action noise std: 2.11
          Mean value_function loss: 25.5506
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 47.5514
                       Mean reward: 28.06
               Mean episode length: 189.98
    Episode_Reward/reaching_object: 0.5214
     Episode_Reward/lifting_object: 4.2114
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 6.85s
                      Time elapsed: 00:12:42
                               ETA: 01:02:05

################################################################################
                     [1m Learning iteration 340/2000 [0m                      

                       Computation: 15608 steps/s (collection: 6.150s, learning 0.149s)
             Mean action noise std: 2.11
          Mean value_function loss: 30.1854
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 47.5657
                       Mean reward: 20.04
               Mean episode length: 184.57
    Episode_Reward/reaching_object: 0.5345
     Episode_Reward/lifting_object: 4.4622
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 6.30s
                      Time elapsed: 00:12:48
                               ETA: 01:02:23

################################################################################
                     [1m Learning iteration 341/2000 [0m                      

                       Computation: 13119 steps/s (collection: 7.359s, learning 0.134s)
             Mean action noise std: 2.11
          Mean value_function loss: 20.1201
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 47.5844
                       Mean reward: 31.99
               Mean episode length: 191.14
    Episode_Reward/reaching_object: 0.5181
     Episode_Reward/lifting_object: 4.8524
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 7.49s
                      Time elapsed: 00:12:56
                               ETA: 01:02:46

################################################################################
                     [1m Learning iteration 342/2000 [0m                      

                       Computation: 46446 steps/s (collection: 2.010s, learning 0.107s)
             Mean action noise std: 2.11
          Mean value_function loss: 21.9446
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 47.5982
                       Mean reward: 25.37
               Mean episode length: 194.41
    Episode_Reward/reaching_object: 0.5338
     Episode_Reward/lifting_object: 4.8782
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 2.12s
                      Time elapsed: 00:12:58
                               ETA: 01:02:43

################################################################################
                     [1m Learning iteration 343/2000 [0m                      

                       Computation: 50427 steps/s (collection: 1.850s, learning 0.100s)
             Mean action noise std: 2.11
          Mean value_function loss: 18.9215
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 47.6139
                       Mean reward: 24.83
               Mean episode length: 183.58
    Episode_Reward/reaching_object: 0.5242
     Episode_Reward/lifting_object: 4.8262
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 1.95s
                      Time elapsed: 00:13:00
                               ETA: 01:02:39

################################################################################
                     [1m Learning iteration 344/2000 [0m                      

                       Computation: 49673 steps/s (collection: 1.885s, learning 0.094s)
             Mean action noise std: 2.11
          Mean value_function loss: 26.1644
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 47.6307
                       Mean reward: 29.00
               Mean episode length: 197.12
    Episode_Reward/reaching_object: 0.5408
     Episode_Reward/lifting_object: 5.4453
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 1.98s
                      Time elapsed: 00:13:02
                               ETA: 01:02:36

################################################################################
                     [1m Learning iteration 345/2000 [0m                      

                       Computation: 48424 steps/s (collection: 1.938s, learning 0.092s)
             Mean action noise std: 2.12
          Mean value_function loss: 21.7629
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 47.6403
                       Mean reward: 26.83
               Mean episode length: 181.21
    Episode_Reward/reaching_object: 0.5311
     Episode_Reward/lifting_object: 5.1462
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 2.03s
                      Time elapsed: 00:13:04
                               ETA: 01:02:32

################################################################################
                     [1m Learning iteration 346/2000 [0m                      

                       Computation: 49423 steps/s (collection: 1.890s, learning 0.099s)
             Mean action noise std: 2.12
          Mean value_function loss: 21.0401
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 47.6526
                       Mean reward: 29.49
               Mean episode length: 190.36
    Episode_Reward/reaching_object: 0.5448
     Episode_Reward/lifting_object: 5.2616
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 1.99s
                      Time elapsed: 00:13:06
                               ETA: 01:02:29

################################################################################
                     [1m Learning iteration 347/2000 [0m                      

                       Computation: 50518 steps/s (collection: 1.855s, learning 0.091s)
             Mean action noise std: 2.12
          Mean value_function loss: 27.0367
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 47.6698
                       Mean reward: 32.47
               Mean episode length: 195.00
    Episode_Reward/reaching_object: 0.5459
     Episode_Reward/lifting_object: 5.8971
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 1.95s
                      Time elapsed: 00:13:08
                               ETA: 01:02:25

################################################################################
                     [1m Learning iteration 348/2000 [0m                      

                       Computation: 49342 steps/s (collection: 1.899s, learning 0.093s)
             Mean action noise std: 2.12
          Mean value_function loss: 27.0317
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 47.6868
                       Mean reward: 31.31
               Mean episode length: 186.40
    Episode_Reward/reaching_object: 0.5466
     Episode_Reward/lifting_object: 5.5651
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 1.99s
                      Time elapsed: 00:13:10
                               ETA: 01:02:21

################################################################################
                     [1m Learning iteration 349/2000 [0m                      

                       Computation: 48413 steps/s (collection: 1.922s, learning 0.109s)
             Mean action noise std: 2.12
          Mean value_function loss: 30.3398
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 47.6976
                       Mean reward: 35.99
               Mean episode length: 182.34
    Episode_Reward/reaching_object: 0.5413
     Episode_Reward/lifting_object: 5.3923
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 2.03s
                      Time elapsed: 00:13:12
                               ETA: 01:02:18

################################################################################
                     [1m Learning iteration 350/2000 [0m                      

                       Computation: 50434 steps/s (collection: 1.858s, learning 0.091s)
             Mean action noise std: 2.12
          Mean value_function loss: 28.1863
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 47.7098
                       Mean reward: 28.05
               Mean episode length: 194.12
    Episode_Reward/reaching_object: 0.5630
     Episode_Reward/lifting_object: 5.6380
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 12.0417
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 1.95s
                      Time elapsed: 00:13:14
                               ETA: 01:02:14

################################################################################
                     [1m Learning iteration 351/2000 [0m                      

                       Computation: 49836 steps/s (collection: 1.882s, learning 0.090s)
             Mean action noise std: 2.12
          Mean value_function loss: 33.1078
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 47.7187
                       Mean reward: 34.10
               Mean episode length: 186.31
    Episode_Reward/reaching_object: 0.5329
     Episode_Reward/lifting_object: 5.5998
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 1.97s
                      Time elapsed: 00:13:16
                               ETA: 01:02:10

################################################################################
                     [1m Learning iteration 352/2000 [0m                      

                       Computation: 47885 steps/s (collection: 1.953s, learning 0.100s)
             Mean action noise std: 2.12
          Mean value_function loss: 22.8147
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 47.7300
                       Mean reward: 23.55
               Mean episode length: 183.13
    Episode_Reward/reaching_object: 0.5475
     Episode_Reward/lifting_object: 5.9863
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 2.05s
                      Time elapsed: 00:13:18
                               ETA: 01:02:07

################################################################################
                     [1m Learning iteration 353/2000 [0m                      

                       Computation: 49551 steps/s (collection: 1.882s, learning 0.102s)
             Mean action noise std: 2.12
          Mean value_function loss: 24.5463
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 47.7371
                       Mean reward: 35.88
               Mean episode length: 176.81
    Episode_Reward/reaching_object: 0.5458
     Episode_Reward/lifting_object: 6.5949
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 1.98s
                      Time elapsed: 00:13:20
                               ETA: 01:02:04

################################################################################
                     [1m Learning iteration 354/2000 [0m                      

                       Computation: 50258 steps/s (collection: 1.854s, learning 0.102s)
             Mean action noise std: 2.13
          Mean value_function loss: 27.8728
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 47.7423
                       Mean reward: 33.39
               Mean episode length: 186.57
    Episode_Reward/reaching_object: 0.5195
     Episode_Reward/lifting_object: 5.5946
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 15.9583
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 1.96s
                      Time elapsed: 00:13:22
                               ETA: 01:02:00

################################################################################
                     [1m Learning iteration 355/2000 [0m                      

                       Computation: 48760 steps/s (collection: 1.922s, learning 0.094s)
             Mean action noise std: 2.13
          Mean value_function loss: 34.6141
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.7545
                       Mean reward: 37.90
               Mean episode length: 174.04
    Episode_Reward/reaching_object: 0.5025
     Episode_Reward/lifting_object: 5.5694
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 2.02s
                      Time elapsed: 00:13:24
                               ETA: 01:01:57

################################################################################
                     [1m Learning iteration 356/2000 [0m                      

                       Computation: 49817 steps/s (collection: 1.877s, learning 0.096s)
             Mean action noise std: 2.13
          Mean value_function loss: 42.9090
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 47.7689
                       Mean reward: 32.80
               Mean episode length: 182.42
    Episode_Reward/reaching_object: 0.5228
     Episode_Reward/lifting_object: 6.2161
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 1.97s
                      Time elapsed: 00:13:26
                               ETA: 01:01:53

################################################################################
                     [1m Learning iteration 357/2000 [0m                      

                       Computation: 48665 steps/s (collection: 1.920s, learning 0.100s)
             Mean action noise std: 2.13
          Mean value_function loss: 32.9586
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 47.7764
                       Mean reward: 35.38
               Mean episode length: 181.59
    Episode_Reward/reaching_object: 0.5102
     Episode_Reward/lifting_object: 5.7065
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0438
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 14.1667
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 2.02s
                      Time elapsed: 00:13:28
                               ETA: 01:01:50

################################################################################
                     [1m Learning iteration 358/2000 [0m                      

                       Computation: 48706 steps/s (collection: 1.928s, learning 0.091s)
             Mean action noise std: 2.13
          Mean value_function loss: 28.9637
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 47.7847
                       Mean reward: 36.45
               Mean episode length: 175.31
    Episode_Reward/reaching_object: 0.5239
     Episode_Reward/lifting_object: 5.9429
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 2.02s
                      Time elapsed: 00:13:30
                               ETA: 01:01:46

################################################################################
                     [1m Learning iteration 359/2000 [0m                      

                       Computation: 47867 steps/s (collection: 1.955s, learning 0.099s)
             Mean action noise std: 2.13
          Mean value_function loss: 27.4505
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 47.7925
                       Mean reward: 28.98
               Mean episode length: 177.42
    Episode_Reward/reaching_object: 0.4942
     Episode_Reward/lifting_object: 5.6345
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 2.05s
                      Time elapsed: 00:13:32
                               ETA: 01:01:43

################################################################################
                     [1m Learning iteration 360/2000 [0m                      

                       Computation: 47461 steps/s (collection: 1.981s, learning 0.091s)
             Mean action noise std: 2.13
          Mean value_function loss: 30.1000
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 47.7964
                       Mean reward: 31.20
               Mean episode length: 178.72
    Episode_Reward/reaching_object: 0.5250
     Episode_Reward/lifting_object: 6.2348
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 2.07s
                      Time elapsed: 00:13:34
                               ETA: 01:01:40

################################################################################
                     [1m Learning iteration 361/2000 [0m                      

                       Computation: 50353 steps/s (collection: 1.861s, learning 0.092s)
             Mean action noise std: 2.13
          Mean value_function loss: 32.5701
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 47.8007
                       Mean reward: 28.76
               Mean episode length: 170.89
    Episode_Reward/reaching_object: 0.4984
     Episode_Reward/lifting_object: 6.0713
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 1.95s
                      Time elapsed: 00:13:36
                               ETA: 01:01:36

################################################################################
                     [1m Learning iteration 362/2000 [0m                      

                       Computation: 48636 steps/s (collection: 1.925s, learning 0.097s)
             Mean action noise std: 2.13
          Mean value_function loss: 32.4856
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 47.8076
                       Mean reward: 34.35
               Mean episode length: 162.78
    Episode_Reward/reaching_object: 0.5052
     Episode_Reward/lifting_object: 6.6404
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 2.02s
                      Time elapsed: 00:13:38
                               ETA: 01:01:33

################################################################################
                     [1m Learning iteration 363/2000 [0m                      

                       Computation: 48555 steps/s (collection: 1.936s, learning 0.089s)
             Mean action noise std: 2.13
          Mean value_function loss: 29.6356
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 47.8154
                       Mean reward: 30.71
               Mean episode length: 173.18
    Episode_Reward/reaching_object: 0.5046
     Episode_Reward/lifting_object: 5.8214
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 2.02s
                      Time elapsed: 00:13:40
                               ETA: 01:01:30

################################################################################
                     [1m Learning iteration 364/2000 [0m                      

                       Computation: 49592 steps/s (collection: 1.887s, learning 0.095s)
             Mean action noise std: 2.13
          Mean value_function loss: 33.0708
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 47.8174
                       Mean reward: 34.73
               Mean episode length: 178.43
    Episode_Reward/reaching_object: 0.5287
     Episode_Reward/lifting_object: 6.5084
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 1.98s
                      Time elapsed: 00:13:42
                               ETA: 01:01:26

################################################################################
                     [1m Learning iteration 365/2000 [0m                      

                       Computation: 50143 steps/s (collection: 1.863s, learning 0.097s)
             Mean action noise std: 2.13
          Mean value_function loss: 31.7499
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 47.8206
                       Mean reward: 32.40
               Mean episode length: 173.93
    Episode_Reward/reaching_object: 0.5485
     Episode_Reward/lifting_object: 7.3537
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 13.7917
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 1.96s
                      Time elapsed: 00:13:44
                               ETA: 01:01:23

################################################################################
                     [1m Learning iteration 366/2000 [0m                      

                       Computation: 49189 steps/s (collection: 1.907s, learning 0.091s)
             Mean action noise std: 2.13
          Mean value_function loss: 30.4342
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 47.8252
                       Mean reward: 39.24
               Mean episode length: 163.42
    Episode_Reward/reaching_object: 0.5505
     Episode_Reward/lifting_object: 8.0463
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 15.7083
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 2.00s
                      Time elapsed: 00:13:46
                               ETA: 01:01:19

################################################################################
                     [1m Learning iteration 367/2000 [0m                      

                       Computation: 45255 steps/s (collection: 2.054s, learning 0.119s)
             Mean action noise std: 2.13
          Mean value_function loss: 37.1181
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 47.8303
                       Mean reward: 36.46
               Mean episode length: 176.63
    Episode_Reward/reaching_object: 0.5501
     Episode_Reward/lifting_object: 7.9393
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 17.1667
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 2.17s
                      Time elapsed: 00:13:48
                               ETA: 01:01:17

################################################################################
                     [1m Learning iteration 368/2000 [0m                      

                       Computation: 48585 steps/s (collection: 1.915s, learning 0.109s)
             Mean action noise std: 2.14
          Mean value_function loss: 32.2095
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 47.8363
                       Mean reward: 46.50
               Mean episode length: 163.59
    Episode_Reward/reaching_object: 0.5194
     Episode_Reward/lifting_object: 7.9895
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 17.7500
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 2.02s
                      Time elapsed: 00:13:50
                               ETA: 01:01:13

################################################################################
                     [1m Learning iteration 369/2000 [0m                      

                       Computation: 47722 steps/s (collection: 1.925s, learning 0.134s)
             Mean action noise std: 2.14
          Mean value_function loss: 34.8338
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 47.8426
                       Mean reward: 40.29
               Mean episode length: 168.74
    Episode_Reward/reaching_object: 0.5213
     Episode_Reward/lifting_object: 7.7447
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 17.5833
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 2.06s
                      Time elapsed: 00:13:52
                               ETA: 01:01:10

################################################################################
                     [1m Learning iteration 370/2000 [0m                      

                       Computation: 42033 steps/s (collection: 2.246s, learning 0.092s)
             Mean action noise std: 2.14
          Mean value_function loss: 39.6340
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 47.8477
                       Mean reward: 36.38
               Mean episode length: 160.58
    Episode_Reward/reaching_object: 0.5325
     Episode_Reward/lifting_object: 7.8729
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 17.3333
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 2.34s
                      Time elapsed: 00:13:55
                               ETA: 01:01:09

################################################################################
                     [1m Learning iteration 371/2000 [0m                      

                       Computation: 48179 steps/s (collection: 1.919s, learning 0.121s)
             Mean action noise std: 2.14
          Mean value_function loss: 50.7972
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 47.8543
                       Mean reward: 38.22
               Mean episode length: 148.97
    Episode_Reward/reaching_object: 0.5250
     Episode_Reward/lifting_object: 7.4426
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 19.0833
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 2.04s
                      Time elapsed: 00:13:57
                               ETA: 01:01:05

################################################################################
                     [1m Learning iteration 372/2000 [0m                      

                       Computation: 47168 steps/s (collection: 1.986s, learning 0.098s)
             Mean action noise std: 2.14
          Mean value_function loss: 39.9194
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 47.8571
                       Mean reward: 35.72
               Mean episode length: 149.17
    Episode_Reward/reaching_object: 0.5296
     Episode_Reward/lifting_object: 8.3393
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 17.7917
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 2.08s
                      Time elapsed: 00:13:59
                               ETA: 01:01:02

################################################################################
                     [1m Learning iteration 373/2000 [0m                      

                       Computation: 45556 steps/s (collection: 2.055s, learning 0.103s)
             Mean action noise std: 2.14
          Mean value_function loss: 41.8071
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 47.8624
                       Mean reward: 38.90
               Mean episode length: 164.82
    Episode_Reward/reaching_object: 0.5221
     Episode_Reward/lifting_object: 8.0068
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 19.0833
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 2.16s
                      Time elapsed: 00:14:01
                               ETA: 01:01:00

################################################################################
                     [1m Learning iteration 374/2000 [0m                      

                       Computation: 46158 steps/s (collection: 2.025s, learning 0.105s)
             Mean action noise std: 2.14
          Mean value_function loss: 61.4335
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 47.8686
                       Mean reward: 55.09
               Mean episode length: 163.19
    Episode_Reward/reaching_object: 0.5399
     Episode_Reward/lifting_object: 8.8907
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 19.4583
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 2.13s
                      Time elapsed: 00:14:03
                               ETA: 01:00:57

################################################################################
                     [1m Learning iteration 375/2000 [0m                      

                       Computation: 48703 steps/s (collection: 1.930s, learning 0.089s)
             Mean action noise std: 2.14
          Mean value_function loss: 43.6384
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 47.8755
                       Mean reward: 46.36
               Mean episode length: 164.48
    Episode_Reward/reaching_object: 0.5547
     Episode_Reward/lifting_object: 8.7078
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 17.3333
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 2.02s
                      Time elapsed: 00:14:05
                               ETA: 01:00:54

################################################################################
                     [1m Learning iteration 376/2000 [0m                      

                       Computation: 49028 steps/s (collection: 1.914s, learning 0.091s)
             Mean action noise std: 2.14
          Mean value_function loss: 49.2338
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 47.8854
                       Mean reward: 43.65
               Mean episode length: 145.37
    Episode_Reward/reaching_object: 0.5445
     Episode_Reward/lifting_object: 9.1081
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 15.7083
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 2.01s
                      Time elapsed: 00:14:07
                               ETA: 01:00:50

################################################################################
                     [1m Learning iteration 377/2000 [0m                      

                       Computation: 45693 steps/s (collection: 2.058s, learning 0.093s)
             Mean action noise std: 2.14
          Mean value_function loss: 44.6949
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 47.8925
                       Mean reward: 53.31
               Mean episode length: 165.46
    Episode_Reward/reaching_object: 0.5587
     Episode_Reward/lifting_object: 9.0965
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 16.7917
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 2.15s
                      Time elapsed: 00:14:09
                               ETA: 01:00:48

################################################################################
                     [1m Learning iteration 378/2000 [0m                      

                       Computation: 49478 steps/s (collection: 1.892s, learning 0.095s)
             Mean action noise std: 2.14
          Mean value_function loss: 40.6751
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 47.8971
                       Mean reward: 49.65
               Mean episode length: 163.14
    Episode_Reward/reaching_object: 0.5579
     Episode_Reward/lifting_object: 9.5077
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 18.0417
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 1.99s
                      Time elapsed: 00:14:11
                               ETA: 01:00:44

################################################################################
                     [1m Learning iteration 379/2000 [0m                      

                       Computation: 48918 steps/s (collection: 1.917s, learning 0.092s)
             Mean action noise std: 2.14
          Mean value_function loss: 58.4950
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 47.9007
                       Mean reward: 50.84
               Mean episode length: 161.02
    Episode_Reward/reaching_object: 0.5436
     Episode_Reward/lifting_object: 8.8007
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 18.1250
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 2.01s
                      Time elapsed: 00:14:13
                               ETA: 01:00:41

################################################################################
                     [1m Learning iteration 380/2000 [0m                      

                       Computation: 48899 steps/s (collection: 1.913s, learning 0.098s)
             Mean action noise std: 2.14
          Mean value_function loss: 61.5095
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 47.9056
                       Mean reward: 43.76
               Mean episode length: 153.61
    Episode_Reward/reaching_object: 0.5576
     Episode_Reward/lifting_object: 9.4377
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 18.1250
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 2.01s
                      Time elapsed: 00:14:15
                               ETA: 01:00:38

################################################################################
                     [1m Learning iteration 381/2000 [0m                      

                       Computation: 49073 steps/s (collection: 1.899s, learning 0.105s)
             Mean action noise std: 2.14
          Mean value_function loss: 51.2216
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 47.9091
                       Mean reward: 46.55
               Mean episode length: 168.09
    Episode_Reward/reaching_object: 0.5762
     Episode_Reward/lifting_object: 10.2924
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 18.3333
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 2.00s
                      Time elapsed: 00:14:17
                               ETA: 01:00:35

################################################################################
                     [1m Learning iteration 382/2000 [0m                      

                       Computation: 47609 steps/s (collection: 1.957s, learning 0.108s)
             Mean action noise std: 2.14
          Mean value_function loss: 43.1927
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 47.9115
                       Mean reward: 53.29
               Mean episode length: 166.79
    Episode_Reward/reaching_object: 0.5667
     Episode_Reward/lifting_object: 9.9429
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 16.7500
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 2.06s
                      Time elapsed: 00:14:19
                               ETA: 01:00:32

################################################################################
                     [1m Learning iteration 383/2000 [0m                      

                       Computation: 49147 steps/s (collection: 1.891s, learning 0.110s)
             Mean action noise std: 2.14
          Mean value_function loss: 47.3654
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 47.9126
                       Mean reward: 53.44
               Mean episode length: 167.29
    Episode_Reward/reaching_object: 0.5856
     Episode_Reward/lifting_object: 10.7843
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 17.2917
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 2.00s
                      Time elapsed: 00:14:21
                               ETA: 01:00:28

################################################################################
                     [1m Learning iteration 384/2000 [0m                      

                       Computation: 45266 steps/s (collection: 2.061s, learning 0.111s)
             Mean action noise std: 2.14
          Mean value_function loss: 48.3953
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 47.9139
                       Mean reward: 53.50
               Mean episode length: 164.84
    Episode_Reward/reaching_object: 0.5529
     Episode_Reward/lifting_object: 9.7070
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 17.2500
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 2.17s
                      Time elapsed: 00:14:23
                               ETA: 01:00:26

################################################################################
                     [1m Learning iteration 385/2000 [0m                      

                       Computation: 46803 steps/s (collection: 2.009s, learning 0.092s)
             Mean action noise std: 2.14
          Mean value_function loss: 61.7020
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 47.9170
                       Mean reward: 46.86
               Mean episode length: 157.93
    Episode_Reward/reaching_object: 0.5560
     Episode_Reward/lifting_object: 9.6475
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 18.2500
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 2.10s
                      Time elapsed: 00:14:26
                               ETA: 01:00:23

################################################################################
                     [1m Learning iteration 386/2000 [0m                      

                       Computation: 46540 steps/s (collection: 1.978s, learning 0.135s)
             Mean action noise std: 2.14
          Mean value_function loss: 41.7472
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 47.9192
                       Mean reward: 50.97
               Mean episode length: 167.32
    Episode_Reward/reaching_object: 0.5619
     Episode_Reward/lifting_object: 10.2984
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 17.0833
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 2.11s
                      Time elapsed: 00:14:28
                               ETA: 01:00:20

################################################################################
                     [1m Learning iteration 387/2000 [0m                      

                       Computation: 42831 steps/s (collection: 2.188s, learning 0.108s)
             Mean action noise std: 2.14
          Mean value_function loss: 55.3676
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 47.9206
                       Mean reward: 59.59
               Mean episode length: 172.71
    Episode_Reward/reaching_object: 0.5763
     Episode_Reward/lifting_object: 10.7426
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 19.5000
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 2.30s
                      Time elapsed: 00:14:30
                               ETA: 01:00:18

################################################################################
                     [1m Learning iteration 388/2000 [0m                      

                       Computation: 41501 steps/s (collection: 2.218s, learning 0.151s)
             Mean action noise std: 2.14
          Mean value_function loss: 49.1759
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 47.9214
                       Mean reward: 65.84
               Mean episode length: 159.58
    Episode_Reward/reaching_object: 0.5869
     Episode_Reward/lifting_object: 10.8250
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 16.9167
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 2.37s
                      Time elapsed: 00:14:32
                               ETA: 01:00:16

################################################################################
                     [1m Learning iteration 389/2000 [0m                      

                       Computation: 43332 steps/s (collection: 2.101s, learning 0.168s)
             Mean action noise std: 2.14
          Mean value_function loss: 55.1483
               Mean surrogate loss: 0.0134
                 Mean entropy loss: 47.9233
                       Mean reward: 45.78
               Mean episode length: 153.32
    Episode_Reward/reaching_object: 0.5580
     Episode_Reward/lifting_object: 10.1223
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 19.2500
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 2.27s
                      Time elapsed: 00:14:35
                               ETA: 01:00:14

################################################################################
                     [1m Learning iteration 390/2000 [0m                      

                       Computation: 47856 steps/s (collection: 1.967s, learning 0.087s)
             Mean action noise std: 2.14
          Mean value_function loss: 50.7243
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 47.9237
                       Mean reward: 56.82
               Mean episode length: 169.94
    Episode_Reward/reaching_object: 0.5912
     Episode_Reward/lifting_object: 11.2018
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 16.9583
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 2.05s
                      Time elapsed: 00:14:37
                               ETA: 01:00:11

################################################################################
                     [1m Learning iteration 391/2000 [0m                      

                       Computation: 43850 steps/s (collection: 2.133s, learning 0.109s)
             Mean action noise std: 2.14
          Mean value_function loss: 55.8802
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 47.9242
                       Mean reward: 58.34
               Mean episode length: 169.56
    Episode_Reward/reaching_object: 0.5729
     Episode_Reward/lifting_object: 10.9316
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 18.5417
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 2.24s
                      Time elapsed: 00:14:39
                               ETA: 01:00:09

################################################################################
                     [1m Learning iteration 392/2000 [0m                      

                       Computation: 48459 steps/s (collection: 1.936s, learning 0.093s)
             Mean action noise std: 2.14
          Mean value_function loss: 56.6286
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 47.9264
                       Mean reward: 61.47
               Mean episode length: 172.43
    Episode_Reward/reaching_object: 0.5730
     Episode_Reward/lifting_object: 10.6751
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 17.0417
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 2.03s
                      Time elapsed: 00:14:41
                               ETA: 01:00:06

################################################################################
                     [1m Learning iteration 393/2000 [0m                      

                       Computation: 49510 steps/s (collection: 1.891s, learning 0.094s)
             Mean action noise std: 2.14
          Mean value_function loss: 54.5615
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 47.9280
                       Mean reward: 61.43
               Mean episode length: 159.43
    Episode_Reward/reaching_object: 0.5573
     Episode_Reward/lifting_object: 10.9373
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 19.9167
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 1.99s
                      Time elapsed: 00:14:43
                               ETA: 01:00:03

################################################################################
                     [1m Learning iteration 394/2000 [0m                      

                       Computation: 47695 steps/s (collection: 1.972s, learning 0.089s)
             Mean action noise std: 2.14
          Mean value_function loss: 54.0226
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 47.9287
                       Mean reward: 51.02
               Mean episode length: 149.84
    Episode_Reward/reaching_object: 0.5534
     Episode_Reward/lifting_object: 11.1672
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 19.0417
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 2.06s
                      Time elapsed: 00:14:45
                               ETA: 01:00:00

################################################################################
                     [1m Learning iteration 395/2000 [0m                      

                       Computation: 48700 steps/s (collection: 1.928s, learning 0.091s)
             Mean action noise std: 2.14
          Mean value_function loss: 58.4308
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.9310
                       Mean reward: 60.39
               Mean episode length: 164.12
    Episode_Reward/reaching_object: 0.5779
     Episode_Reward/lifting_object: 12.0782
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 16.2917
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 2.02s
                      Time elapsed: 00:14:47
                               ETA: 00:59:56

################################################################################
                     [1m Learning iteration 396/2000 [0m                      

                       Computation: 47085 steps/s (collection: 1.990s, learning 0.098s)
             Mean action noise std: 2.15
          Mean value_function loss: 52.4305
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 47.9353
                       Mean reward: 60.18
               Mean episode length: 157.90
    Episode_Reward/reaching_object: 0.5584
     Episode_Reward/lifting_object: 11.4307
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 17.5833
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 2.09s
                      Time elapsed: 00:14:49
                               ETA: 00:59:54

################################################################################
                     [1m Learning iteration 397/2000 [0m                      

                       Computation: 48107 steps/s (collection: 1.938s, learning 0.106s)
             Mean action noise std: 2.15
          Mean value_function loss: 63.3229
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 47.9383
                       Mean reward: 60.19
               Mean episode length: 167.20
    Episode_Reward/reaching_object: 0.5652
     Episode_Reward/lifting_object: 12.3579
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 18.5000
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 2.04s
                      Time elapsed: 00:14:51
                               ETA: 00:59:51

################################################################################
                     [1m Learning iteration 398/2000 [0m                      

                       Computation: 48679 steps/s (collection: 1.913s, learning 0.107s)
             Mean action noise std: 2.15
          Mean value_function loss: 56.2049
               Mean surrogate loss: 0.0137
                 Mean entropy loss: 47.9398
                       Mean reward: 61.07
               Mean episode length: 173.09
    Episode_Reward/reaching_object: 0.5765
     Episode_Reward/lifting_object: 11.9398
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 16.7917
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 2.02s
                      Time elapsed: 00:14:53
                               ETA: 00:59:47

################################################################################
                     [1m Learning iteration 399/2000 [0m                      

                       Computation: 49352 steps/s (collection: 1.900s, learning 0.092s)
             Mean action noise std: 2.15
          Mean value_function loss: 48.2779
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 47.9402
                       Mean reward: 69.78
               Mean episode length: 163.82
    Episode_Reward/reaching_object: 0.5600
     Episode_Reward/lifting_object: 11.8962
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 17.8750
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 1.99s
                      Time elapsed: 00:14:55
                               ETA: 00:59:44

################################################################################
                     [1m Learning iteration 400/2000 [0m                      

                       Computation: 47825 steps/s (collection: 1.964s, learning 0.092s)
             Mean action noise std: 2.15
          Mean value_function loss: 51.7214
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 47.9407
                       Mean reward: 65.43
               Mean episode length: 159.64
    Episode_Reward/reaching_object: 0.5713
     Episode_Reward/lifting_object: 12.7273
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 17.8333
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 2.06s
                      Time elapsed: 00:14:57
                               ETA: 00:59:41

################################################################################
                     [1m Learning iteration 401/2000 [0m                      

                       Computation: 45682 steps/s (collection: 2.043s, learning 0.109s)
             Mean action noise std: 2.15
          Mean value_function loss: 58.3446
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 47.9419
                       Mean reward: 60.19
               Mean episode length: 165.26
    Episode_Reward/reaching_object: 0.5390
     Episode_Reward/lifting_object: 11.3201
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 17.8333
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 2.15s
                      Time elapsed: 00:14:59
                               ETA: 00:59:39

################################################################################
                     [1m Learning iteration 402/2000 [0m                      

                       Computation: 48694 steps/s (collection: 1.918s, learning 0.101s)
             Mean action noise std: 2.15
          Mean value_function loss: 51.3729
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 47.9455
                       Mean reward: 63.55
               Mean episode length: 163.88
    Episode_Reward/reaching_object: 0.5380
     Episode_Reward/lifting_object: 11.7195
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 19.5417
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 2.02s
                      Time elapsed: 00:15:01
                               ETA: 00:59:35

################################################################################
                     [1m Learning iteration 403/2000 [0m                      

                       Computation: 48791 steps/s (collection: 1.926s, learning 0.089s)
             Mean action noise std: 2.15
          Mean value_function loss: 46.1793
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 47.9506
                       Mean reward: 60.25
               Mean episode length: 169.01
    Episode_Reward/reaching_object: 0.5874
     Episode_Reward/lifting_object: 12.8533
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 15.9583
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 2.01s
                      Time elapsed: 00:15:03
                               ETA: 00:59:32

################################################################################
                     [1m Learning iteration 404/2000 [0m                      

                       Computation: 49752 steps/s (collection: 1.884s, learning 0.092s)
             Mean action noise std: 2.15
          Mean value_function loss: 55.2703
               Mean surrogate loss: 0.0096
                 Mean entropy loss: 47.9534
                       Mean reward: 69.84
               Mean episode length: 170.75
    Episode_Reward/reaching_object: 0.5902
     Episode_Reward/lifting_object: 13.4351
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 1.98s
                      Time elapsed: 00:15:05
                               ETA: 00:59:29

################################################################################
                     [1m Learning iteration 405/2000 [0m                      

                       Computation: 48939 steps/s (collection: 1.922s, learning 0.087s)
             Mean action noise std: 2.15
          Mean value_function loss: 57.3303
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 47.9534
                       Mean reward: 60.02
               Mean episode length: 157.48
    Episode_Reward/reaching_object: 0.5713
     Episode_Reward/lifting_object: 12.4303
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 17.2500
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 2.01s
                      Time elapsed: 00:15:07
                               ETA: 00:59:26

################################################################################
                     [1m Learning iteration 406/2000 [0m                      

                       Computation: 48354 steps/s (collection: 1.940s, learning 0.093s)
             Mean action noise std: 2.15
          Mean value_function loss: 56.8668
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 47.9535
                       Mean reward: 63.50
               Mean episode length: 158.37
    Episode_Reward/reaching_object: 0.5676
     Episode_Reward/lifting_object: 13.7859
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 18.5000
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 2.03s
                      Time elapsed: 00:15:09
                               ETA: 00:59:23

################################################################################
                     [1m Learning iteration 407/2000 [0m                      

                       Computation: 49117 steps/s (collection: 1.909s, learning 0.092s)
             Mean action noise std: 2.15
          Mean value_function loss: 52.6951
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 47.9548
                       Mean reward: 68.17
               Mean episode length: 164.60
    Episode_Reward/reaching_object: 0.5703
     Episode_Reward/lifting_object: 13.1732
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 18.8750
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 2.00s
                      Time elapsed: 00:15:11
                               ETA: 00:59:20

################################################################################
                     [1m Learning iteration 408/2000 [0m                      

                       Computation: 48713 steps/s (collection: 1.921s, learning 0.097s)
             Mean action noise std: 2.15
          Mean value_function loss: 57.2390
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 47.9559
                       Mean reward: 65.30
               Mean episode length: 157.05
    Episode_Reward/reaching_object: 0.5401
     Episode_Reward/lifting_object: 12.3225
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 20.9583
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 2.02s
                      Time elapsed: 00:15:13
                               ETA: 00:59:17

################################################################################
                     [1m Learning iteration 409/2000 [0m                      

                       Computation: 49058 steps/s (collection: 1.906s, learning 0.098s)
             Mean action noise std: 2.15
          Mean value_function loss: 59.1102
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 47.9575
                       Mean reward: 58.08
               Mean episode length: 137.75
    Episode_Reward/reaching_object: 0.5410
     Episode_Reward/lifting_object: 13.0037
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 21.2917
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 2.00s
                      Time elapsed: 00:15:15
                               ETA: 00:59:14

################################################################################
                     [1m Learning iteration 410/2000 [0m                      

                       Computation: 45744 steps/s (collection: 2.061s, learning 0.088s)
             Mean action noise std: 2.15
          Mean value_function loss: 59.2874
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 47.9595
                       Mean reward: 76.97
               Mean episode length: 169.39
    Episode_Reward/reaching_object: 0.5649
     Episode_Reward/lifting_object: 14.0350
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 20.2500
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 2.15s
                      Time elapsed: 00:15:18
                               ETA: 00:59:11

################################################################################
                     [1m Learning iteration 411/2000 [0m                      

                       Computation: 48568 steps/s (collection: 1.922s, learning 0.102s)
             Mean action noise std: 2.15
          Mean value_function loss: 60.9197
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 47.9605
                       Mean reward: 67.18
               Mean episode length: 141.08
    Episode_Reward/reaching_object: 0.5505
     Episode_Reward/lifting_object: 13.6969
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 21.2500
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 2.02s
                      Time elapsed: 00:15:20
                               ETA: 00:59:08

################################################################################
                     [1m Learning iteration 412/2000 [0m                      

                       Computation: 49336 steps/s (collection: 1.890s, learning 0.103s)
             Mean action noise std: 2.15
          Mean value_function loss: 64.3626
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 47.9618
                       Mean reward: 60.94
               Mean episode length: 143.43
    Episode_Reward/reaching_object: 0.5207
     Episode_Reward/lifting_object: 13.0393
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 22.6250
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 1.99s
                      Time elapsed: 00:15:22
                               ETA: 00:59:05

################################################################################
                     [1m Learning iteration 413/2000 [0m                      

                       Computation: 47662 steps/s (collection: 1.954s, learning 0.108s)
             Mean action noise std: 2.15
          Mean value_function loss: 62.8224
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 47.9628
                       Mean reward: 52.01
               Mean episode length: 127.22
    Episode_Reward/reaching_object: 0.5224
     Episode_Reward/lifting_object: 13.1838
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 22.5833
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 2.06s
                      Time elapsed: 00:15:24
                               ETA: 00:59:02

################################################################################
                     [1m Learning iteration 414/2000 [0m                      

                       Computation: 47167 steps/s (collection: 1.993s, learning 0.092s)
             Mean action noise std: 2.15
          Mean value_function loss: 63.5882
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 47.9651
                       Mean reward: 66.31
               Mean episode length: 131.93
    Episode_Reward/reaching_object: 0.4997
     Episode_Reward/lifting_object: 12.7552
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 4.1250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 22.4583
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 2.08s
                      Time elapsed: 00:15:26
                               ETA: 00:58:59

################################################################################
                     [1m Learning iteration 415/2000 [0m                      

                       Computation: 47560 steps/s (collection: 1.977s, learning 0.090s)
             Mean action noise std: 2.15
          Mean value_function loss: 72.9694
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 47.9689
                       Mean reward: 72.38
               Mean episode length: 141.42
    Episode_Reward/reaching_object: 0.5163
     Episode_Reward/lifting_object: 13.7756
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 22.2500
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 2.07s
                      Time elapsed: 00:15:28
                               ETA: 00:58:56

################################################################################
                     [1m Learning iteration 416/2000 [0m                      

                       Computation: 48627 steps/s (collection: 1.934s, learning 0.088s)
             Mean action noise std: 2.15
          Mean value_function loss: 82.9286
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 47.9702
                       Mean reward: 72.82
               Mean episode length: 145.52
    Episode_Reward/reaching_object: 0.4948
     Episode_Reward/lifting_object: 13.0272
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 26.2500
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 2.02s
                      Time elapsed: 00:15:30
                               ETA: 00:58:53

################################################################################
                     [1m Learning iteration 417/2000 [0m                      

                       Computation: 48448 steps/s (collection: 1.943s, learning 0.086s)
             Mean action noise std: 2.15
          Mean value_function loss: 78.1104
               Mean surrogate loss: 0.0106
                 Mean entropy loss: 47.9718
                       Mean reward: 79.52
               Mean episode length: 153.00
    Episode_Reward/reaching_object: 0.5047
     Episode_Reward/lifting_object: 13.1456
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 24.5417
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 2.03s
                      Time elapsed: 00:15:32
                               ETA: 00:58:50

################################################################################
                     [1m Learning iteration 418/2000 [0m                      

                       Computation: 48881 steps/s (collection: 1.923s, learning 0.089s)
             Mean action noise std: 2.15
          Mean value_function loss: 79.5997
               Mean surrogate loss: 0.0144
                 Mean entropy loss: 47.9721
                       Mean reward: 74.08
               Mean episode length: 138.60
    Episode_Reward/reaching_object: 0.5192
     Episode_Reward/lifting_object: 14.0324
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 24.8333
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 2.01s
                      Time elapsed: 00:15:34
                               ETA: 00:58:47

################################################################################
                     [1m Learning iteration 419/2000 [0m                      

                       Computation: 48683 steps/s (collection: 1.933s, learning 0.086s)
             Mean action noise std: 2.15
          Mean value_function loss: 70.7571
               Mean surrogate loss: 0.0100
                 Mean entropy loss: 47.9721
                       Mean reward: 72.30
               Mean episode length: 139.49
    Episode_Reward/reaching_object: 0.5181
     Episode_Reward/lifting_object: 14.1128
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 24.9167
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 2.02s
                      Time elapsed: 00:15:36
                               ETA: 00:58:44

################################################################################
                     [1m Learning iteration 420/2000 [0m                      

                       Computation: 48691 steps/s (collection: 1.933s, learning 0.086s)
             Mean action noise std: 2.15
          Mean value_function loss: 71.0953
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 47.9722
                       Mean reward: 69.62
               Mean episode length: 129.08
    Episode_Reward/reaching_object: 0.5059
     Episode_Reward/lifting_object: 13.5406
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 4.2917
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 24.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 2.02s
                      Time elapsed: 00:15:38
                               ETA: 00:58:41

################################################################################
                     [1m Learning iteration 421/2000 [0m                      

                       Computation: 49183 steps/s (collection: 1.910s, learning 0.089s)
             Mean action noise std: 2.15
          Mean value_function loss: 72.1567
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 47.9722
                       Mean reward: 76.26
               Mean episode length: 146.55
    Episode_Reward/reaching_object: 0.5051
     Episode_Reward/lifting_object: 13.9707
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 25.2500
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 2.00s
                      Time elapsed: 00:15:40
                               ETA: 00:58:38

################################################################################
                     [1m Learning iteration 422/2000 [0m                      

                       Computation: 47987 steps/s (collection: 1.955s, learning 0.093s)
             Mean action noise std: 2.15
          Mean value_function loss: 79.8821
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 47.9731
                       Mean reward: 78.96
               Mean episode length: 143.61
    Episode_Reward/reaching_object: 0.5176
     Episode_Reward/lifting_object: 14.9759
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 21.5000
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 2.05s
                      Time elapsed: 00:15:42
                               ETA: 00:58:35

################################################################################
                     [1m Learning iteration 423/2000 [0m                      

                       Computation: 47712 steps/s (collection: 1.971s, learning 0.090s)
             Mean action noise std: 2.15
          Mean value_function loss: 65.4009
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 47.9747
                       Mean reward: 66.87
               Mean episode length: 133.93
    Episode_Reward/reaching_object: 0.5157
     Episode_Reward/lifting_object: 14.2995
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 25.7917
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 2.06s
                      Time elapsed: 00:15:44
                               ETA: 00:58:32

################################################################################
                     [1m Learning iteration 424/2000 [0m                      

                       Computation: 46912 steps/s (collection: 1.998s, learning 0.097s)
             Mean action noise std: 2.15
          Mean value_function loss: 78.5830
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 47.9754
                       Mean reward: 75.86
               Mean episode length: 143.15
    Episode_Reward/reaching_object: 0.5182
     Episode_Reward/lifting_object: 15.1041
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 21.7917
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 2.10s
                      Time elapsed: 00:15:46
                               ETA: 00:58:30

################################################################################
                     [1m Learning iteration 425/2000 [0m                      

                       Computation: 43051 steps/s (collection: 2.176s, learning 0.107s)
             Mean action noise std: 2.15
          Mean value_function loss: 66.9151
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 47.9755
                       Mean reward: 74.23
               Mean episode length: 128.02
    Episode_Reward/reaching_object: 0.5135
     Episode_Reward/lifting_object: 15.4538
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 24.3750
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 2.28s
                      Time elapsed: 00:15:48
                               ETA: 00:58:28

################################################################################
                     [1m Learning iteration 426/2000 [0m                      

                       Computation: 47235 steps/s (collection: 1.970s, learning 0.112s)
             Mean action noise std: 2.15
          Mean value_function loss: 70.0809
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 47.9761
                       Mean reward: 80.46
               Mean episode length: 145.15
    Episode_Reward/reaching_object: 0.5200
     Episode_Reward/lifting_object: 15.4596
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 23.1667
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 2.08s
                      Time elapsed: 00:15:50
                               ETA: 00:58:25

################################################################################
                     [1m Learning iteration 427/2000 [0m                      

                       Computation: 47488 steps/s (collection: 1.962s, learning 0.109s)
             Mean action noise std: 2.15
          Mean value_function loss: 94.0974
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 47.9772
                       Mean reward: 88.19
               Mean episode length: 157.04
    Episode_Reward/reaching_object: 0.5560
     Episode_Reward/lifting_object: 16.5691
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 22.1250
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 2.07s
                      Time elapsed: 00:15:53
                               ETA: 00:58:22

################################################################################
                     [1m Learning iteration 428/2000 [0m                      

                       Computation: 48127 steps/s (collection: 1.936s, learning 0.106s)
             Mean action noise std: 2.15
          Mean value_function loss: 100.8785
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 47.9815
                       Mean reward: 72.63
               Mean episode length: 137.22
    Episode_Reward/reaching_object: 0.5167
     Episode_Reward/lifting_object: 15.5642
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 23.8333
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 2.04s
                      Time elapsed: 00:15:55
                               ETA: 00:58:19

################################################################################
                     [1m Learning iteration 429/2000 [0m                      

                       Computation: 47434 steps/s (collection: 1.974s, learning 0.098s)
             Mean action noise std: 2.15
          Mean value_function loss: 75.3558
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.9844
                       Mean reward: 81.73
               Mean episode length: 141.04
    Episode_Reward/reaching_object: 0.5377
     Episode_Reward/lifting_object: 16.0910
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 22.9167
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 2.07s
                      Time elapsed: 00:15:57
                               ETA: 00:58:16

################################################################################
                     [1m Learning iteration 430/2000 [0m                      

                       Computation: 48388 steps/s (collection: 1.944s, learning 0.088s)
             Mean action noise std: 2.15
          Mean value_function loss: 104.2539
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 47.9895
                       Mean reward: 74.83
               Mean episode length: 133.77
    Episode_Reward/reaching_object: 0.5418
     Episode_Reward/lifting_object: 16.4518
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 21.7500
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 2.03s
                      Time elapsed: 00:15:59
                               ETA: 00:58:13

################################################################################
                     [1m Learning iteration 431/2000 [0m                      

                       Computation: 48603 steps/s (collection: 1.934s, learning 0.088s)
             Mean action noise std: 2.15
          Mean value_function loss: 70.6491
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 47.9938
                       Mean reward: 74.16
               Mean episode length: 145.79
    Episode_Reward/reaching_object: 0.5408
     Episode_Reward/lifting_object: 15.7460
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 21.9167
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 2.02s
                      Time elapsed: 00:16:01
                               ETA: 00:58:10

################################################################################
                     [1m Learning iteration 432/2000 [0m                      

                       Computation: 48891 steps/s (collection: 1.911s, learning 0.100s)
             Mean action noise std: 2.15
          Mean value_function loss: 79.9919
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 47.9959
                       Mean reward: 77.13
               Mean episode length: 143.33
    Episode_Reward/reaching_object: 0.5218
     Episode_Reward/lifting_object: 16.1270
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 22.8750
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 2.01s
                      Time elapsed: 00:16:03
                               ETA: 00:58:07

################################################################################
                     [1m Learning iteration 433/2000 [0m                      

                       Computation: 48281 steps/s (collection: 1.939s, learning 0.097s)
             Mean action noise std: 2.15
          Mean value_function loss: 98.2092
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 47.9986
                       Mean reward: 84.01
               Mean episode length: 137.51
    Episode_Reward/reaching_object: 0.5375
     Episode_Reward/lifting_object: 17.1315
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 22.4583
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 2.04s
                      Time elapsed: 00:16:05
                               ETA: 00:58:05

################################################################################
                     [1m Learning iteration 434/2000 [0m                      

                       Computation: 47515 steps/s (collection: 1.979s, learning 0.090s)
             Mean action noise std: 2.15
          Mean value_function loss: 90.1073
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 48.0025
                       Mean reward: 81.37
               Mean episode length: 138.25
    Episode_Reward/reaching_object: 0.5145
     Episode_Reward/lifting_object: 15.8301
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 4.2917
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 23.0833
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 2.07s
                      Time elapsed: 00:16:07
                               ETA: 00:58:02

################################################################################
                     [1m Learning iteration 435/2000 [0m                      

                       Computation: 48701 steps/s (collection: 1.929s, learning 0.089s)
             Mean action noise std: 2.15
          Mean value_function loss: 84.3712
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 48.0033
                       Mean reward: 85.19
               Mean episode length: 142.65
    Episode_Reward/reaching_object: 0.5481
     Episode_Reward/lifting_object: 17.7054
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 22.5417
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 2.02s
                      Time elapsed: 00:16:09
                               ETA: 00:57:59

################################################################################
                     [1m Learning iteration 436/2000 [0m                      

                       Computation: 48858 steps/s (collection: 1.923s, learning 0.089s)
             Mean action noise std: 2.15
          Mean value_function loss: 94.5142
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.0042
                       Mean reward: 91.08
               Mean episode length: 148.62
    Episode_Reward/reaching_object: 0.5421
     Episode_Reward/lifting_object: 17.8106
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 23.5833
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 2.01s
                      Time elapsed: 00:16:11
                               ETA: 00:57:56

################################################################################
                     [1m Learning iteration 437/2000 [0m                      

                       Computation: 47598 steps/s (collection: 1.966s, learning 0.100s)
             Mean action noise std: 2.15
          Mean value_function loss: 91.7006
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 48.0067
                       Mean reward: 83.11
               Mean episode length: 141.68
    Episode_Reward/reaching_object: 0.5272
     Episode_Reward/lifting_object: 16.9943
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 24.6250
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 2.07s
                      Time elapsed: 00:16:13
                               ETA: 00:57:53

################################################################################
                     [1m Learning iteration 438/2000 [0m                      

                       Computation: 47124 steps/s (collection: 1.994s, learning 0.092s)
             Mean action noise std: 2.15
          Mean value_function loss: 91.8559
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 48.0082
                       Mean reward: 102.50
               Mean episode length: 150.25
    Episode_Reward/reaching_object: 0.5276
     Episode_Reward/lifting_object: 17.2299
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 23.7917
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 2.09s
                      Time elapsed: 00:16:15
                               ETA: 00:57:50

################################################################################
                     [1m Learning iteration 439/2000 [0m                      

                       Computation: 48167 steps/s (collection: 1.947s, learning 0.094s)
             Mean action noise std: 2.15
          Mean value_function loss: 96.0412
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 48.0094
                       Mean reward: 97.66
               Mean episode length: 155.87
    Episode_Reward/reaching_object: 0.5512
     Episode_Reward/lifting_object: 18.5330
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0438
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 21.6667
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 2.04s
                      Time elapsed: 00:16:17
                               ETA: 00:57:47

################################################################################
                     [1m Learning iteration 440/2000 [0m                      

                       Computation: 47839 steps/s (collection: 1.946s, learning 0.109s)
             Mean action noise std: 2.15
          Mean value_function loss: 99.8671
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 48.0113
                       Mean reward: 84.46
               Mean episode length: 134.14
    Episode_Reward/reaching_object: 0.5062
     Episode_Reward/lifting_object: 16.1530
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 23.1667
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 2.05s
                      Time elapsed: 00:16:19
                               ETA: 00:57:45

################################################################################
                     [1m Learning iteration 441/2000 [0m                      

                       Computation: 48108 steps/s (collection: 1.942s, learning 0.102s)
             Mean action noise std: 2.15
          Mean value_function loss: 89.3649
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 48.0134
                       Mean reward: 91.85
               Mean episode length: 142.68
    Episode_Reward/reaching_object: 0.5044
     Episode_Reward/lifting_object: 17.2148
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 25.2500
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 2.04s
                      Time elapsed: 00:16:21
                               ETA: 00:57:42

################################################################################
                     [1m Learning iteration 442/2000 [0m                      

                       Computation: 45800 steps/s (collection: 2.033s, learning 0.113s)
             Mean action noise std: 2.15
          Mean value_function loss: 78.3770
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 48.0144
                       Mean reward: 91.21
               Mean episode length: 145.73
    Episode_Reward/reaching_object: 0.5347
     Episode_Reward/lifting_object: 18.0996
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 24.0833
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 2.15s
                      Time elapsed: 00:16:23
                               ETA: 00:57:39

################################################################################
                     [1m Learning iteration 443/2000 [0m                      

                       Computation: 47059 steps/s (collection: 2.000s, learning 0.089s)
             Mean action noise std: 2.15
          Mean value_function loss: 96.8402
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 48.0148
                       Mean reward: 80.99
               Mean episode length: 142.62
    Episode_Reward/reaching_object: 0.5235
     Episode_Reward/lifting_object: 18.1191
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 24.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 2.09s
                      Time elapsed: 00:16:25
                               ETA: 00:57:37

################################################################################
                     [1m Learning iteration 444/2000 [0m                      

                       Computation: 48266 steps/s (collection: 1.943s, learning 0.094s)
             Mean action noise std: 2.15
          Mean value_function loss: 93.1978
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 48.0149
                       Mean reward: 82.55
               Mean episode length: 135.76
    Episode_Reward/reaching_object: 0.5292
     Episode_Reward/lifting_object: 18.4037
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 24.7917
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 2.04s
                      Time elapsed: 00:16:27
                               ETA: 00:57:34

################################################################################
                     [1m Learning iteration 445/2000 [0m                      

                       Computation: 48067 steps/s (collection: 1.957s, learning 0.089s)
             Mean action noise std: 2.15
          Mean value_function loss: 82.4500
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 48.0153
                       Mean reward: 84.23
               Mean episode length: 141.45
    Episode_Reward/reaching_object: 0.5107
     Episode_Reward/lifting_object: 17.4197
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 26.8333
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 2.05s
                      Time elapsed: 00:16:29
                               ETA: 00:57:31

################################################################################
                     [1m Learning iteration 446/2000 [0m                      

                       Computation: 47357 steps/s (collection: 1.978s, learning 0.098s)
             Mean action noise std: 2.15
          Mean value_function loss: 90.8976
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 48.0155
                       Mean reward: 77.92
               Mean episode length: 127.53
    Episode_Reward/reaching_object: 0.4933
     Episode_Reward/lifting_object: 16.7081
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 25.2500
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 2.08s
                      Time elapsed: 00:16:32
                               ETA: 00:57:28

################################################################################
                     [1m Learning iteration 447/2000 [0m                      

                       Computation: 44721 steps/s (collection: 2.101s, learning 0.097s)
             Mean action noise std: 2.15
          Mean value_function loss: 93.8214
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 48.0156
                       Mean reward: 82.45
               Mean episode length: 129.26
    Episode_Reward/reaching_object: 0.4888
     Episode_Reward/lifting_object: 16.7529
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 25.5000
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 2.20s
                      Time elapsed: 00:16:34
                               ETA: 00:57:26

################################################################################
                     [1m Learning iteration 448/2000 [0m                      

                       Computation: 46382 steps/s (collection: 2.022s, learning 0.098s)
             Mean action noise std: 2.15
          Mean value_function loss: 83.8943
               Mean surrogate loss: 0.0105
                 Mean entropy loss: 48.0158
                       Mean reward: 89.45
               Mean episode length: 135.50
    Episode_Reward/reaching_object: 0.5010
     Episode_Reward/lifting_object: 17.7832
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 27.4167
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 2.12s
                      Time elapsed: 00:16:36
                               ETA: 00:57:23

################################################################################
                     [1m Learning iteration 449/2000 [0m                      

                       Computation: 42281 steps/s (collection: 2.135s, learning 0.190s)
             Mean action noise std: 2.15
          Mean value_function loss: 79.6244
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 48.0160
                       Mean reward: 87.25
               Mean episode length: 135.86
    Episode_Reward/reaching_object: 0.4861
     Episode_Reward/lifting_object: 16.8375
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 24.1667
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 2.33s
                      Time elapsed: 00:16:38
                               ETA: 00:57:21

################################################################################
                     [1m Learning iteration 450/2000 [0m                      

                       Computation: 36468 steps/s (collection: 2.531s, learning 0.164s)
             Mean action noise std: 2.15
          Mean value_function loss: 95.9017
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 48.0163
                       Mean reward: 80.16
               Mean episode length: 128.38
    Episode_Reward/reaching_object: 0.4801
     Episode_Reward/lifting_object: 17.1739
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 26.3750
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 2.70s
                      Time elapsed: 00:16:41
                               ETA: 00:57:21

################################################################################
                     [1m Learning iteration 451/2000 [0m                      

                       Computation: 41809 steps/s (collection: 2.202s, learning 0.150s)
             Mean action noise std: 2.15
          Mean value_function loss: 116.0220
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 48.0165
                       Mean reward: 77.10
               Mean episode length: 128.24
    Episode_Reward/reaching_object: 0.4809
     Episode_Reward/lifting_object: 16.1839
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 27.4583
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 2.35s
                      Time elapsed: 00:16:43
                               ETA: 00:57:19

################################################################################
                     [1m Learning iteration 452/2000 [0m                      

                       Computation: 44565 steps/s (collection: 2.050s, learning 0.156s)
             Mean action noise std: 2.15
          Mean value_function loss: 87.3179
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 48.0167
                       Mean reward: 97.48
               Mean episode length: 135.91
    Episode_Reward/reaching_object: 0.5097
     Episode_Reward/lifting_object: 18.2869
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 26.6667
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 2.21s
                      Time elapsed: 00:16:45
                               ETA: 00:57:17

################################################################################
                     [1m Learning iteration 453/2000 [0m                      

                       Computation: 44387 steps/s (collection: 2.092s, learning 0.123s)
             Mean action noise std: 2.15
          Mean value_function loss: 96.9127
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 48.0168
                       Mean reward: 78.43
               Mean episode length: 118.02
    Episode_Reward/reaching_object: 0.4735
     Episode_Reward/lifting_object: 16.6325
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 26.3750
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 2.21s
                      Time elapsed: 00:16:48
                               ETA: 00:57:15

################################################################################
                     [1m Learning iteration 454/2000 [0m                      

                       Computation: 42942 steps/s (collection: 2.157s, learning 0.133s)
             Mean action noise std: 2.15
          Mean value_function loss: 83.7711
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 48.0171
                       Mean reward: 91.08
               Mean episode length: 127.75
    Episode_Reward/reaching_object: 0.5058
     Episode_Reward/lifting_object: 17.7279
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 26.7500
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 2.29s
                      Time elapsed: 00:16:50
                               ETA: 00:57:13

################################################################################
                     [1m Learning iteration 455/2000 [0m                      

                       Computation: 42644 steps/s (collection: 2.114s, learning 0.191s)
             Mean action noise std: 2.15
          Mean value_function loss: 86.4262
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 48.0175
                       Mean reward: 85.04
               Mean episode length: 141.29
    Episode_Reward/reaching_object: 0.4757
     Episode_Reward/lifting_object: 16.6766
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 28.2500
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 2.31s
                      Time elapsed: 00:16:52
                               ETA: 00:57:11

################################################################################
                     [1m Learning iteration 456/2000 [0m                      

                       Computation: 39694 steps/s (collection: 2.332s, learning 0.144s)
             Mean action noise std: 2.15
          Mean value_function loss: 92.2116
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 48.0178
                       Mean reward: 98.41
               Mean episode length: 142.64
    Episode_Reward/reaching_object: 0.4712
     Episode_Reward/lifting_object: 16.0814
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 25.0417
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 2.48s
                      Time elapsed: 00:16:55
                               ETA: 00:57:09

################################################################################
                     [1m Learning iteration 457/2000 [0m                      

                       Computation: 44484 steps/s (collection: 2.094s, learning 0.116s)
             Mean action noise std: 2.16
          Mean value_function loss: 94.8898
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 48.0181
                       Mean reward: 95.13
               Mean episode length: 132.97
    Episode_Reward/reaching_object: 0.4833
     Episode_Reward/lifting_object: 16.2549
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 25.9583
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 2.21s
                      Time elapsed: 00:16:57
                               ETA: 00:57:07

################################################################################
                     [1m Learning iteration 458/2000 [0m                      

                       Computation: 48679 steps/s (collection: 1.931s, learning 0.089s)
             Mean action noise std: 2.16
          Mean value_function loss: 99.9017
               Mean surrogate loss: 0.0101
                 Mean entropy loss: 48.0189
                       Mean reward: 84.27
               Mean episode length: 122.79
    Episode_Reward/reaching_object: 0.4777
     Episode_Reward/lifting_object: 17.0387
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 27.7500
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 2.02s
                      Time elapsed: 00:16:59
                               ETA: 00:57:04

################################################################################
                     [1m Learning iteration 459/2000 [0m                      

                       Computation: 44863 steps/s (collection: 2.101s, learning 0.090s)
             Mean action noise std: 2.16
          Mean value_function loss: 95.4746
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 48.0192
                       Mean reward: 85.34
               Mean episode length: 123.06
    Episode_Reward/reaching_object: 0.4965
     Episode_Reward/lifting_object: 18.0723
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 24.7500
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 2.19s
                      Time elapsed: 00:17:01
                               ETA: 00:57:02

################################################################################
                     [1m Learning iteration 460/2000 [0m                      

                       Computation: 47648 steps/s (collection: 1.974s, learning 0.090s)
             Mean action noise std: 2.16
          Mean value_function loss: 115.8858
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 48.0197
                       Mean reward: 85.25
               Mean episode length: 124.40
    Episode_Reward/reaching_object: 0.4928
     Episode_Reward/lifting_object: 17.7024
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 28.5417
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 2.06s
                      Time elapsed: 00:17:03
                               ETA: 00:56:59

################################################################################
                     [1m Learning iteration 461/2000 [0m                      

                       Computation: 43886 steps/s (collection: 2.116s, learning 0.124s)
             Mean action noise std: 2.16
          Mean value_function loss: 104.2910
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 48.0208
                       Mean reward: 100.09
               Mean episode length: 134.18
    Episode_Reward/reaching_object: 0.5124
     Episode_Reward/lifting_object: 17.9977
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 25.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 2.24s
                      Time elapsed: 00:17:05
                               ETA: 00:56:57

################################################################################
                     [1m Learning iteration 462/2000 [0m                      

                       Computation: 45786 steps/s (collection: 2.043s, learning 0.104s)
             Mean action noise std: 2.16
          Mean value_function loss: 108.0357
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 48.0230
                       Mean reward: 98.01
               Mean episode length: 137.33
    Episode_Reward/reaching_object: 0.4920
     Episode_Reward/lifting_object: 17.5215
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 26.2917
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 2.15s
                      Time elapsed: 00:17:08
                               ETA: 00:56:55

################################################################################
                     [1m Learning iteration 463/2000 [0m                      

                       Computation: 46956 steps/s (collection: 2.002s, learning 0.091s)
             Mean action noise std: 2.16
          Mean value_function loss: 124.0122
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 48.0239
                       Mean reward: 92.64
               Mean episode length: 132.91
    Episode_Reward/reaching_object: 0.5087
     Episode_Reward/lifting_object: 17.5756
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 25.7083
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 2.09s
                      Time elapsed: 00:17:10
                               ETA: 00:56:52

################################################################################
                     [1m Learning iteration 464/2000 [0m                      

                       Computation: 42704 steps/s (collection: 2.198s, learning 0.104s)
             Mean action noise std: 2.16
          Mean value_function loss: 94.6595
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 48.0246
                       Mean reward: 87.62
               Mean episode length: 128.96
    Episode_Reward/reaching_object: 0.4918
     Episode_Reward/lifting_object: 17.8749
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 26.2083
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 2.30s
                      Time elapsed: 00:17:12
                               ETA: 00:56:50

################################################################################
                     [1m Learning iteration 465/2000 [0m                      

                       Computation: 45476 steps/s (collection: 2.071s, learning 0.091s)
             Mean action noise std: 2.16
          Mean value_function loss: 102.7943
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 48.0251
                       Mean reward: 80.52
               Mean episode length: 125.24
    Episode_Reward/reaching_object: 0.5162
     Episode_Reward/lifting_object: 18.7667
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 25.7917
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 2.16s
                      Time elapsed: 00:17:14
                               ETA: 00:56:47

################################################################################
                     [1m Learning iteration 466/2000 [0m                      

                       Computation: 48435 steps/s (collection: 1.941s, learning 0.089s)
             Mean action noise std: 2.16
          Mean value_function loss: 111.3327
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 48.0254
                       Mean reward: 108.68
               Mean episode length: 145.38
    Episode_Reward/reaching_object: 0.5029
     Episode_Reward/lifting_object: 18.1607
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 24.7917
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 2.03s
                      Time elapsed: 00:17:16
                               ETA: 00:56:45

################################################################################
                     [1m Learning iteration 467/2000 [0m                      

                       Computation: 48243 steps/s (collection: 1.932s, learning 0.106s)
             Mean action noise std: 2.16
          Mean value_function loss: 93.9113
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 48.0256
                       Mean reward: 107.21
               Mean episode length: 140.29
    Episode_Reward/reaching_object: 0.5005
     Episode_Reward/lifting_object: 18.4518
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 26.4167
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 2.04s
                      Time elapsed: 00:17:18
                               ETA: 00:56:42

################################################################################
                     [1m Learning iteration 468/2000 [0m                      

                       Computation: 47593 steps/s (collection: 1.954s, learning 0.112s)
             Mean action noise std: 2.16
          Mean value_function loss: 93.1289
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 48.0259
                       Mean reward: 95.62
               Mean episode length: 133.61
    Episode_Reward/reaching_object: 0.5153
     Episode_Reward/lifting_object: 19.4896
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 27.1667
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 2.07s
                      Time elapsed: 00:17:20
                               ETA: 00:56:39

################################################################################
                     [1m Learning iteration 469/2000 [0m                      

                       Computation: 47756 steps/s (collection: 1.955s, learning 0.103s)
             Mean action noise std: 2.16
          Mean value_function loss: 97.1981
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 48.0261
                       Mean reward: 95.03
               Mean episode length: 132.62
    Episode_Reward/reaching_object: 0.5274
     Episode_Reward/lifting_object: 20.3208
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 25.8750
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 2.06s
                      Time elapsed: 00:17:22
                               ETA: 00:56:36

################################################################################
                     [1m Learning iteration 470/2000 [0m                      

                       Computation: 47767 steps/s (collection: 1.960s, learning 0.098s)
             Mean action noise std: 2.16
          Mean value_function loss: 113.2799
               Mean surrogate loss: 0.0164
                 Mean entropy loss: 48.0262
                       Mean reward: 94.52
               Mean episode length: 125.60
    Episode_Reward/reaching_object: 0.5037
     Episode_Reward/lifting_object: 18.9658
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 26.7500
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 2.06s
                      Time elapsed: 00:17:24
                               ETA: 00:56:34

################################################################################
                     [1m Learning iteration 471/2000 [0m                      

                       Computation: 47990 steps/s (collection: 1.953s, learning 0.095s)
             Mean action noise std: 2.16
          Mean value_function loss: 111.1891
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 48.0262
                       Mean reward: 96.04
               Mean episode length: 125.56
    Episode_Reward/reaching_object: 0.5035
     Episode_Reward/lifting_object: 19.3031
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 26.5000
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 2.05s
                      Time elapsed: 00:17:26
                               ETA: 00:56:31

################################################################################
                     [1m Learning iteration 472/2000 [0m                      

                       Computation: 47573 steps/s (collection: 1.974s, learning 0.092s)
             Mean action noise std: 2.16
          Mean value_function loss: 99.4760
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 48.0262
                       Mean reward: 109.13
               Mean episode length: 144.02
    Episode_Reward/reaching_object: 0.5224
     Episode_Reward/lifting_object: 20.3729
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 25.5417
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 2.07s
                      Time elapsed: 00:17:28
                               ETA: 00:56:28

################################################################################
                     [1m Learning iteration 473/2000 [0m                      

                       Computation: 47687 steps/s (collection: 1.969s, learning 0.092s)
             Mean action noise std: 2.16
          Mean value_function loss: 115.1217
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.0263
                       Mean reward: 104.09
               Mean episode length: 138.16
    Episode_Reward/reaching_object: 0.5398
     Episode_Reward/lifting_object: 20.3947
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 26.3333
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 2.06s
                      Time elapsed: 00:17:31
                               ETA: 00:56:25

################################################################################
                     [1m Learning iteration 474/2000 [0m                      

                       Computation: 46912 steps/s (collection: 1.992s, learning 0.103s)
             Mean action noise std: 2.16
          Mean value_function loss: 110.5088
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 48.0273
                       Mean reward: 101.92
               Mean episode length: 135.71
    Episode_Reward/reaching_object: 0.5158
     Episode_Reward/lifting_object: 20.0539
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 24.5000
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 2.10s
                      Time elapsed: 00:17:33
                               ETA: 00:56:23

################################################################################
                     [1m Learning iteration 475/2000 [0m                      

                       Computation: 47728 steps/s (collection: 1.967s, learning 0.093s)
             Mean action noise std: 2.16
          Mean value_function loss: 136.8061
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 48.0291
                       Mean reward: 110.16
               Mean episode length: 133.16
    Episode_Reward/reaching_object: 0.5169
     Episode_Reward/lifting_object: 20.7705
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 25.4583
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 2.06s
                      Time elapsed: 00:17:35
                               ETA: 00:56:20

################################################################################
                     [1m Learning iteration 476/2000 [0m                      

                       Computation: 45383 steps/s (collection: 2.073s, learning 0.093s)
             Mean action noise std: 2.16
          Mean value_function loss: 131.6947
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.0308
                       Mean reward: 102.02
               Mean episode length: 137.18
    Episode_Reward/reaching_object: 0.5283
     Episode_Reward/lifting_object: 20.4963
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 26.6667
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 2.17s
                      Time elapsed: 00:17:37
                               ETA: 00:56:18

################################################################################
                     [1m Learning iteration 477/2000 [0m                      

                       Computation: 47166 steps/s (collection: 1.990s, learning 0.094s)
             Mean action noise std: 2.16
          Mean value_function loss: 131.8697
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 48.0311
                       Mean reward: 116.97
               Mean episode length: 144.52
    Episode_Reward/reaching_object: 0.5280
     Episode_Reward/lifting_object: 21.2258
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 27.5000
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 2.08s
                      Time elapsed: 00:17:39
                               ETA: 00:56:15

################################################################################
                     [1m Learning iteration 478/2000 [0m                      

                       Computation: 47895 steps/s (collection: 1.956s, learning 0.097s)
             Mean action noise std: 2.16
          Mean value_function loss: 151.3088
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 48.0321
                       Mean reward: 97.26
               Mean episode length: 120.81
    Episode_Reward/reaching_object: 0.5074
     Episode_Reward/lifting_object: 19.2919
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 26.7083
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 2.05s
                      Time elapsed: 00:17:41
                               ETA: 00:56:12

################################################################################
                     [1m Learning iteration 479/2000 [0m                      

                       Computation: 48032 steps/s (collection: 1.957s, learning 0.090s)
             Mean action noise std: 2.16
          Mean value_function loss: 120.7705
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 48.0333
                       Mean reward: 107.84
               Mean episode length: 135.43
    Episode_Reward/reaching_object: 0.5137
     Episode_Reward/lifting_object: 20.6533
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 26.4167
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 2.05s
                      Time elapsed: 00:17:43
                               ETA: 00:56:10

################################################################################
                     [1m Learning iteration 480/2000 [0m                      

                       Computation: 48167 steps/s (collection: 1.943s, learning 0.098s)
             Mean action noise std: 2.16
          Mean value_function loss: 110.4408
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 48.0337
                       Mean reward: 106.83
               Mean episode length: 129.64
    Episode_Reward/reaching_object: 0.5327
     Episode_Reward/lifting_object: 22.3321
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 26.8750
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 2.04s
                      Time elapsed: 00:17:45
                               ETA: 00:56:07

################################################################################
                     [1m Learning iteration 481/2000 [0m                      

                       Computation: 47340 steps/s (collection: 1.989s, learning 0.088s)
             Mean action noise std: 2.16
          Mean value_function loss: 112.1374
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 48.0343
                       Mean reward: 110.40
               Mean episode length: 128.49
    Episode_Reward/reaching_object: 0.5060
     Episode_Reward/lifting_object: 21.0796
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 26.2917
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 2.08s
                      Time elapsed: 00:17:47
                               ETA: 00:56:04

################################################################################
                     [1m Learning iteration 482/2000 [0m                      

                       Computation: 47894 steps/s (collection: 1.950s, learning 0.103s)
             Mean action noise std: 2.16
          Mean value_function loss: 114.2703
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 48.0345
                       Mean reward: 111.62
               Mean episode length: 129.38
    Episode_Reward/reaching_object: 0.5164
     Episode_Reward/lifting_object: 21.8902
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 28.7083
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 2.05s
                      Time elapsed: 00:17:49
                               ETA: 00:56:01

################################################################################
                     [1m Learning iteration 483/2000 [0m                      

                       Computation: 47081 steps/s (collection: 1.977s, learning 0.111s)
             Mean action noise std: 2.16
          Mean value_function loss: 120.4328
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 48.0347
                       Mean reward: 114.12
               Mean episode length: 124.46
    Episode_Reward/reaching_object: 0.5142
     Episode_Reward/lifting_object: 21.6801
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 26.7500
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 2.09s
                      Time elapsed: 00:17:51
                               ETA: 00:55:59

################################################################################
                     [1m Learning iteration 484/2000 [0m                      

                       Computation: 46738 steps/s (collection: 1.984s, learning 0.119s)
             Mean action noise std: 2.16
          Mean value_function loss: 115.8559
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 48.0352
                       Mean reward: 104.55
               Mean episode length: 126.42
    Episode_Reward/reaching_object: 0.5108
     Episode_Reward/lifting_object: 21.2370
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 28.7500
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 2.10s
                      Time elapsed: 00:17:53
                               ETA: 00:55:56

################################################################################
                     [1m Learning iteration 485/2000 [0m                      

                       Computation: 45574 steps/s (collection: 2.049s, learning 0.108s)
             Mean action noise std: 2.16
          Mean value_function loss: 130.3956
               Mean surrogate loss: 0.0184
                 Mean entropy loss: 48.0355
                       Mean reward: 94.71
               Mean episode length: 123.18
    Episode_Reward/reaching_object: 0.4972
     Episode_Reward/lifting_object: 20.1940
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 26.0833
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 2.16s
                      Time elapsed: 00:17:56
                               ETA: 00:55:54

################################################################################
                     [1m Learning iteration 486/2000 [0m                      

                       Computation: 44432 steps/s (collection: 2.093s, learning 0.120s)
             Mean action noise std: 2.16
          Mean value_function loss: 114.9392
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 48.0356
                       Mean reward: 105.91
               Mean episode length: 122.86
    Episode_Reward/reaching_object: 0.4984
     Episode_Reward/lifting_object: 20.6103
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 27.8333
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 2.21s
                      Time elapsed: 00:17:58
                               ETA: 00:55:52

################################################################################
                     [1m Learning iteration 487/2000 [0m                      

                       Computation: 40217 steps/s (collection: 2.286s, learning 0.158s)
             Mean action noise std: 2.16
          Mean value_function loss: 123.4106
               Mean surrogate loss: 0.0101
                 Mean entropy loss: 48.0356
                       Mean reward: 118.34
               Mean episode length: 132.76
    Episode_Reward/reaching_object: 0.5164
     Episode_Reward/lifting_object: 22.5380
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 26.3333
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 2.44s
                      Time elapsed: 00:18:00
                               ETA: 00:55:50

################################################################################
                     [1m Learning iteration 488/2000 [0m                      

                       Computation: 38556 steps/s (collection: 2.368s, learning 0.182s)
             Mean action noise std: 2.16
          Mean value_function loss: 129.1382
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 48.0356
                       Mean reward: 99.89
               Mean episode length: 115.40
    Episode_Reward/reaching_object: 0.5020
     Episode_Reward/lifting_object: 21.5401
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 28.3750
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 2.55s
                      Time elapsed: 00:18:03
                               ETA: 00:55:49

################################################################################
                     [1m Learning iteration 489/2000 [0m                      

                       Computation: 45040 steps/s (collection: 2.088s, learning 0.094s)
             Mean action noise std: 2.16
          Mean value_function loss: 119.7997
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 48.0358
                       Mean reward: 107.72
               Mean episode length: 123.91
    Episode_Reward/reaching_object: 0.5145
     Episode_Reward/lifting_object: 22.5396
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 29.9583
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 2.18s
                      Time elapsed: 00:18:05
                               ETA: 00:55:47

################################################################################
                     [1m Learning iteration 490/2000 [0m                      

                       Computation: 47823 steps/s (collection: 1.958s, learning 0.098s)
             Mean action noise std: 2.16
          Mean value_function loss: 119.9848
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 48.0367
                       Mean reward: 119.51
               Mean episode length: 138.38
    Episode_Reward/reaching_object: 0.5191
     Episode_Reward/lifting_object: 22.2309
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 27.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 2.06s
                      Time elapsed: 00:18:07
                               ETA: 00:55:44

################################################################################
                     [1m Learning iteration 491/2000 [0m                      

                       Computation: 44958 steps/s (collection: 2.096s, learning 0.090s)
             Mean action noise std: 2.16
          Mean value_function loss: 127.3963
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 48.0373
                       Mean reward: 114.02
               Mean episode length: 125.57
    Episode_Reward/reaching_object: 0.5403
     Episode_Reward/lifting_object: 23.0601
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 25.7083
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 2.19s
                      Time elapsed: 00:18:09
                               ETA: 00:55:42

################################################################################
                     [1m Learning iteration 492/2000 [0m                      

                       Computation: 48044 steps/s (collection: 1.949s, learning 0.097s)
             Mean action noise std: 2.16
          Mean value_function loss: 122.6887
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.0373
                       Mean reward: 120.62
               Mean episode length: 135.00
    Episode_Reward/reaching_object: 0.5263
     Episode_Reward/lifting_object: 23.0395
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 25.1667
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 2.05s
                      Time elapsed: 00:18:11
                               ETA: 00:55:39

################################################################################
                     [1m Learning iteration 493/2000 [0m                      

                       Computation: 48172 steps/s (collection: 1.947s, learning 0.094s)
             Mean action noise std: 2.16
          Mean value_function loss: 124.4872
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 48.0371
                       Mean reward: 120.12
               Mean episode length: 128.78
    Episode_Reward/reaching_object: 0.5346
     Episode_Reward/lifting_object: 23.7554
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 24.9583
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 2.04s
                      Time elapsed: 00:18:13
                               ETA: 00:55:36

################################################################################
                     [1m Learning iteration 494/2000 [0m                      

                       Computation: 46685 steps/s (collection: 2.002s, learning 0.104s)
             Mean action noise std: 2.16
          Mean value_function loss: 133.1780
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 48.0365
                       Mean reward: 133.20
               Mean episode length: 148.37
    Episode_Reward/reaching_object: 0.5710
     Episode_Reward/lifting_object: 24.9868
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 23.4167
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 2.11s
                      Time elapsed: 00:18:15
                               ETA: 00:55:34

################################################################################
                     [1m Learning iteration 495/2000 [0m                      

                       Computation: 46793 steps/s (collection: 1.999s, learning 0.102s)
             Mean action noise std: 2.16
          Mean value_function loss: 127.7470
               Mean surrogate loss: 0.0129
                 Mean entropy loss: 48.0367
                       Mean reward: 110.72
               Mean episode length: 120.54
    Episode_Reward/reaching_object: 0.5653
     Episode_Reward/lifting_object: 25.7111
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 23.8750
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 2.10s
                      Time elapsed: 00:18:17
                               ETA: 00:55:31

################################################################################
                     [1m Learning iteration 496/2000 [0m                      

                       Computation: 46638 steps/s (collection: 1.995s, learning 0.113s)
             Mean action noise std: 2.16
          Mean value_function loss: 133.5218
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 48.0368
                       Mean reward: 131.06
               Mean episode length: 135.22
    Episode_Reward/reaching_object: 0.5699
     Episode_Reward/lifting_object: 25.5413
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 26.6250
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 2.11s
                      Time elapsed: 00:18:20
                               ETA: 00:55:29

################################################################################
                     [1m Learning iteration 497/2000 [0m                      

                       Computation: 45020 steps/s (collection: 2.064s, learning 0.119s)
             Mean action noise std: 2.16
          Mean value_function loss: 125.1188
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 48.0368
                       Mean reward: 124.45
               Mean episode length: 140.19
    Episode_Reward/reaching_object: 0.5843
     Episode_Reward/lifting_object: 25.9130
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 4.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 23.1667
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 2.18s
                      Time elapsed: 00:18:22
                               ETA: 00:55:26

################################################################################
                     [1m Learning iteration 498/2000 [0m                      

                       Computation: 46918 steps/s (collection: 1.981s, learning 0.115s)
             Mean action noise std: 2.16
          Mean value_function loss: 133.6188
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 48.0368
                       Mean reward: 127.19
               Mean episode length: 133.78
    Episode_Reward/reaching_object: 0.5951
     Episode_Reward/lifting_object: 26.3830
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 22.9167
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 2.10s
                      Time elapsed: 00:18:24
                               ETA: 00:55:24

################################################################################
                     [1m Learning iteration 499/2000 [0m                      

                       Computation: 46412 steps/s (collection: 1.996s, learning 0.122s)
             Mean action noise std: 2.16
          Mean value_function loss: 121.2842
               Mean surrogate loss: 0.0107
                 Mean entropy loss: 48.0368
                       Mean reward: 142.26
               Mean episode length: 150.83
    Episode_Reward/reaching_object: 0.6029
     Episode_Reward/lifting_object: 27.2098
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 22.9583
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 2.12s
                      Time elapsed: 00:18:26
                               ETA: 00:55:21

################################################################################
                     [1m Learning iteration 500/2000 [0m                      

                       Computation: 46409 steps/s (collection: 2.006s, learning 0.112s)
             Mean action noise std: 2.16
          Mean value_function loss: 141.1638
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 48.0368
                       Mean reward: 132.59
               Mean episode length: 132.61
    Episode_Reward/reaching_object: 0.5943
     Episode_Reward/lifting_object: 27.7704
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 26.0417
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 2.12s
                      Time elapsed: 00:18:28
                               ETA: 00:55:19

################################################################################
                     [1m Learning iteration 501/2000 [0m                      

                       Computation: 47330 steps/s (collection: 1.963s, learning 0.114s)
             Mean action noise std: 2.16
          Mean value_function loss: 128.6752
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 48.0369
                       Mean reward: 126.76
               Mean episode length: 133.29
    Episode_Reward/reaching_object: 0.5854
     Episode_Reward/lifting_object: 27.1030
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 25.1667
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 2.08s
                      Time elapsed: 00:18:30
                               ETA: 00:55:16

################################################################################
                     [1m Learning iteration 502/2000 [0m                      

                       Computation: 45258 steps/s (collection: 2.048s, learning 0.124s)
             Mean action noise std: 2.16
          Mean value_function loss: 114.2158
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 48.0369
                       Mean reward: 130.45
               Mean episode length: 131.01
    Episode_Reward/reaching_object: 0.5830
     Episode_Reward/lifting_object: 27.3206
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 24.0833
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 2.17s
                      Time elapsed: 00:18:32
                               ETA: 00:55:14

################################################################################
                     [1m Learning iteration 503/2000 [0m                      

                       Computation: 46888 steps/s (collection: 2.000s, learning 0.097s)
             Mean action noise std: 2.16
          Mean value_function loss: 136.1151
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 48.0367
                       Mean reward: 141.19
               Mean episode length: 142.97
    Episode_Reward/reaching_object: 0.5859
     Episode_Reward/lifting_object: 27.4217
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 22.0833
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 2.10s
                      Time elapsed: 00:18:34
                               ETA: 00:55:11

################################################################################
                     [1m Learning iteration 504/2000 [0m                      

                       Computation: 45399 steps/s (collection: 2.048s, learning 0.117s)
             Mean action noise std: 2.16
          Mean value_function loss: 142.5519
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 48.0365
                       Mean reward: 141.48
               Mean episode length: 144.40
    Episode_Reward/reaching_object: 0.5938
     Episode_Reward/lifting_object: 28.0444
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 25.5417
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 2.17s
                      Time elapsed: 00:18:37
                               ETA: 00:55:09

################################################################################
                     [1m Learning iteration 505/2000 [0m                      

                       Computation: 45371 steps/s (collection: 2.056s, learning 0.111s)
             Mean action noise std: 2.16
          Mean value_function loss: 137.5483
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 48.0366
                       Mean reward: 124.99
               Mean episode length: 140.92
    Episode_Reward/reaching_object: 0.5855
     Episode_Reward/lifting_object: 26.2208
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 23.8750
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 2.17s
                      Time elapsed: 00:18:39
                               ETA: 00:55:06

################################################################################
                     [1m Learning iteration 506/2000 [0m                      

                       Computation: 46521 steps/s (collection: 2.014s, learning 0.099s)
             Mean action noise std: 2.16
          Mean value_function loss: 140.4429
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 48.0371
                       Mean reward: 118.14
               Mean episode length: 134.69
    Episode_Reward/reaching_object: 0.5809
     Episode_Reward/lifting_object: 27.2760
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 4.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 24.1250
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 2.11s
                      Time elapsed: 00:18:41
                               ETA: 00:55:04

################################################################################
                     [1m Learning iteration 507/2000 [0m                      

                       Computation: 46687 steps/s (collection: 2.014s, learning 0.092s)
             Mean action noise std: 2.16
          Mean value_function loss: 144.8905
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 48.0383
                       Mean reward: 145.03
               Mean episode length: 142.54
    Episode_Reward/reaching_object: 0.5808
     Episode_Reward/lifting_object: 27.8885
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 26.2500
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 2.11s
                      Time elapsed: 00:18:43
                               ETA: 00:55:01

################################################################################
                     [1m Learning iteration 508/2000 [0m                      

                       Computation: 46821 steps/s (collection: 2.008s, learning 0.091s)
             Mean action noise std: 2.16
          Mean value_function loss: 126.6982
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 48.0387
                       Mean reward: 148.27
               Mean episode length: 146.90
    Episode_Reward/reaching_object: 0.5832
     Episode_Reward/lifting_object: 26.9526
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 23.5833
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 2.10s
                      Time elapsed: 00:18:45
                               ETA: 00:54:59

################################################################################
                     [1m Learning iteration 509/2000 [0m                      

                       Computation: 48268 steps/s (collection: 1.947s, learning 0.090s)
             Mean action noise std: 2.16
          Mean value_function loss: 136.8032
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 48.0382
                       Mean reward: 147.40
               Mean episode length: 143.69
    Episode_Reward/reaching_object: 0.5735
     Episode_Reward/lifting_object: 27.1980
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 24.7083
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 2.04s
                      Time elapsed: 00:18:47
                               ETA: 00:54:56

################################################################################
                     [1m Learning iteration 510/2000 [0m                      

                       Computation: 47182 steps/s (collection: 1.970s, learning 0.113s)
             Mean action noise std: 2.16
          Mean value_function loss: 145.4352
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 48.0383
                       Mean reward: 138.20
               Mean episode length: 139.63
    Episode_Reward/reaching_object: 0.5971
     Episode_Reward/lifting_object: 28.2676
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 4.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 26.1667
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 2.08s
                      Time elapsed: 00:18:49
                               ETA: 00:54:54

################################################################################
                     [1m Learning iteration 511/2000 [0m                      

                       Computation: 44774 steps/s (collection: 2.083s, learning 0.113s)
             Mean action noise std: 2.16
          Mean value_function loss: 134.0017
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 48.0383
                       Mean reward: 159.07
               Mean episode length: 144.75
    Episode_Reward/reaching_object: 0.5876
     Episode_Reward/lifting_object: 28.7876
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 24.7917
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 2.20s
                      Time elapsed: 00:18:51
                               ETA: 00:54:51

################################################################################
                     [1m Learning iteration 512/2000 [0m                      

                       Computation: 45297 steps/s (collection: 2.062s, learning 0.109s)
             Mean action noise std: 2.16
          Mean value_function loss: 136.5499
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 48.0386
                       Mean reward: 160.71
               Mean episode length: 149.29
    Episode_Reward/reaching_object: 0.5851
     Episode_Reward/lifting_object: 29.0563
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 24.0417
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 2.17s
                      Time elapsed: 00:18:54
                               ETA: 00:54:49

################################################################################
                     [1m Learning iteration 513/2000 [0m                      

                       Computation: 47869 steps/s (collection: 1.960s, learning 0.094s)
             Mean action noise std: 2.16
          Mean value_function loss: 144.9995
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 48.0416
                       Mean reward: 158.55
               Mean episode length: 138.75
    Episode_Reward/reaching_object: 0.5785
     Episode_Reward/lifting_object: 29.2587
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 26.1667
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 2.05s
                      Time elapsed: 00:18:56
                               ETA: 00:54:46

################################################################################
                     [1m Learning iteration 514/2000 [0m                      

                       Computation: 47444 steps/s (collection: 1.976s, learning 0.096s)
             Mean action noise std: 2.16
          Mean value_function loss: 171.9531
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 48.0450
                       Mean reward: 153.86
               Mean episode length: 140.56
    Episode_Reward/reaching_object: 0.5815
     Episode_Reward/lifting_object: 28.6630
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 26.2917
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 2.07s
                      Time elapsed: 00:18:58
                               ETA: 00:54:44

################################################################################
                     [1m Learning iteration 515/2000 [0m                      

                       Computation: 47640 steps/s (collection: 1.973s, learning 0.090s)
             Mean action noise std: 2.16
          Mean value_function loss: 160.4481
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 48.0468
                       Mean reward: 149.04
               Mean episode length: 139.03
    Episode_Reward/reaching_object: 0.5589
     Episode_Reward/lifting_object: 27.9848
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 28.2083
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 2.06s
                      Time elapsed: 00:19:00
                               ETA: 00:54:41

################################################################################
                     [1m Learning iteration 516/2000 [0m                      

                       Computation: 47689 steps/s (collection: 1.963s, learning 0.099s)
             Mean action noise std: 2.16
          Mean value_function loss: 148.3539
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 48.0473
                       Mean reward: 147.72
               Mean episode length: 137.79
    Episode_Reward/reaching_object: 0.5736
     Episode_Reward/lifting_object: 28.7249
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 3.8750
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 27.0417
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 2.06s
                      Time elapsed: 00:19:02
                               ETA: 00:54:38

################################################################################
                     [1m Learning iteration 517/2000 [0m                      

                       Computation: 47811 steps/s (collection: 1.964s, learning 0.092s)
             Mean action noise std: 2.16
          Mean value_function loss: 153.1146
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.0479
                       Mean reward: 134.68
               Mean episode length: 121.82
    Episode_Reward/reaching_object: 0.5675
     Episode_Reward/lifting_object: 28.5959
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 27.7917
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 2.06s
                      Time elapsed: 00:19:04
                               ETA: 00:54:36

################################################################################
                     [1m Learning iteration 518/2000 [0m                      

                       Computation: 47614 steps/s (collection: 1.965s, learning 0.100s)
             Mean action noise std: 2.16
          Mean value_function loss: 167.7548
               Mean surrogate loss: 0.0102
                 Mean entropy loss: 48.0490
                       Mean reward: 128.31
               Mean episode length: 123.71
    Episode_Reward/reaching_object: 0.5525
     Episode_Reward/lifting_object: 28.6254
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 27.2500
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 2.06s
                      Time elapsed: 00:19:06
                               ETA: 00:54:33

################################################################################
                     [1m Learning iteration 519/2000 [0m                      

                       Computation: 47169 steps/s (collection: 1.989s, learning 0.095s)
             Mean action noise std: 2.16
          Mean value_function loss: 230.7394
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 48.0499
                       Mean reward: 130.66
               Mean episode length: 125.90
    Episode_Reward/reaching_object: 0.5592
     Episode_Reward/lifting_object: 29.0071
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 26.7083
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 2.08s
                      Time elapsed: 00:19:08
                               ETA: 00:54:31

################################################################################
                     [1m Learning iteration 520/2000 [0m                      

                       Computation: 47521 steps/s (collection: 1.978s, learning 0.091s)
             Mean action noise std: 2.16
          Mean value_function loss: 207.8905
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.0506
                       Mean reward: 129.98
               Mean episode length: 119.47
    Episode_Reward/reaching_object: 0.5448
     Episode_Reward/lifting_object: 28.1007
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 28.0833
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 2.07s
                      Time elapsed: 00:19:10
                               ETA: 00:54:28

################################################################################
                     [1m Learning iteration 521/2000 [0m                      

                       Computation: 47467 steps/s (collection: 1.983s, learning 0.088s)
             Mean action noise std: 2.16
          Mean value_function loss: 190.0658
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 48.0514
                       Mean reward: 139.34
               Mean episode length: 128.22
    Episode_Reward/reaching_object: 0.5337
     Episode_Reward/lifting_object: 27.8548
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 28.5417
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 2.07s
                      Time elapsed: 00:19:12
                               ETA: 00:54:25

################################################################################
                     [1m Learning iteration 522/2000 [0m                      

                       Computation: 47258 steps/s (collection: 1.988s, learning 0.093s)
             Mean action noise std: 2.16
          Mean value_function loss: 157.2484
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 48.0528
                       Mean reward: 143.03
               Mean episode length: 128.22
    Episode_Reward/reaching_object: 0.5399
     Episode_Reward/lifting_object: 28.3154
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 27.1667
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 2.08s
                      Time elapsed: 00:19:14
                               ETA: 00:54:23

################################################################################
                     [1m Learning iteration 523/2000 [0m                      

                       Computation: 47264 steps/s (collection: 1.979s, learning 0.101s)
             Mean action noise std: 2.16
          Mean value_function loss: 162.3142
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 48.0530
                       Mean reward: 136.68
               Mean episode length: 122.87
    Episode_Reward/reaching_object: 0.5491
     Episode_Reward/lifting_object: 28.5427
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 27.7083
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 2.08s
                      Time elapsed: 00:19:16
                               ETA: 00:54:20

################################################################################
                     [1m Learning iteration 524/2000 [0m                      

                       Computation: 47154 steps/s (collection: 1.995s, learning 0.090s)
             Mean action noise std: 2.16
          Mean value_function loss: 170.4918
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 48.0530
                       Mean reward: 164.18
               Mean episode length: 135.32
    Episode_Reward/reaching_object: 0.5506
     Episode_Reward/lifting_object: 29.3589
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 26.5833
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 2.08s
                      Time elapsed: 00:19:18
                               ETA: 00:54:18

################################################################################
                     [1m Learning iteration 525/2000 [0m                      

                       Computation: 47600 steps/s (collection: 1.966s, learning 0.099s)
             Mean action noise std: 2.16
          Mean value_function loss: 160.9943
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 48.0538
                       Mean reward: 157.76
               Mean episode length: 133.63
    Episode_Reward/reaching_object: 0.5621
     Episode_Reward/lifting_object: 30.0008
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 27.8333
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 2.07s
                      Time elapsed: 00:19:20
                               ETA: 00:54:15

################################################################################
                     [1m Learning iteration 526/2000 [0m                      

                       Computation: 46492 steps/s (collection: 2.007s, learning 0.107s)
             Mean action noise std: 2.16
          Mean value_function loss: 161.1992
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 48.0542
                       Mean reward: 147.31
               Mean episode length: 125.86
    Episode_Reward/reaching_object: 0.5547
     Episode_Reward/lifting_object: 30.0469
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 26.9583
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 2.11s
                      Time elapsed: 00:19:23
                               ETA: 00:54:13

################################################################################
                     [1m Learning iteration 527/2000 [0m                      

                       Computation: 47066 steps/s (collection: 1.983s, learning 0.106s)
             Mean action noise std: 2.16
          Mean value_function loss: 157.7260
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 48.0542
                       Mean reward: 175.87
               Mean episode length: 140.89
    Episode_Reward/reaching_object: 0.5750
     Episode_Reward/lifting_object: 32.5419
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 26.5000
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 2.09s
                      Time elapsed: 00:19:25
                               ETA: 00:54:10

################################################################################
                     [1m Learning iteration 528/2000 [0m                      

                       Computation: 45899 steps/s (collection: 2.015s, learning 0.127s)
             Mean action noise std: 2.16
          Mean value_function loss: 171.9484
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.0531
                       Mean reward: 161.20
               Mean episode length: 134.10
    Episode_Reward/reaching_object: 0.5501
     Episode_Reward/lifting_object: 30.6631
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.9583
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 2.14s
                      Time elapsed: 00:19:27
                               ETA: 00:54:08

################################################################################
                     [1m Learning iteration 529/2000 [0m                      

                       Computation: 45950 steps/s (collection: 2.032s, learning 0.107s)
             Mean action noise std: 2.16
          Mean value_function loss: 197.8928
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 48.0524
                       Mean reward: 167.14
               Mean episode length: 137.41
    Episode_Reward/reaching_object: 0.5963
     Episode_Reward/lifting_object: 33.2477
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 26.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 2.14s
                      Time elapsed: 00:19:29
                               ETA: 00:54:05

################################################################################
                     [1m Learning iteration 530/2000 [0m                      

                       Computation: 46922 steps/s (collection: 1.998s, learning 0.097s)
             Mean action noise std: 2.16
          Mean value_function loss: 168.3211
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 48.0524
                       Mean reward: 177.60
               Mean episode length: 138.46
    Episode_Reward/reaching_object: 0.5905
     Episode_Reward/lifting_object: 33.9467
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 25.0833
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 2.10s
                      Time elapsed: 00:19:31
                               ETA: 00:54:03

################################################################################
                     [1m Learning iteration 531/2000 [0m                      

                       Computation: 47289 steps/s (collection: 1.990s, learning 0.089s)
             Mean action noise std: 2.16
          Mean value_function loss: 173.5042
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 48.0523
                       Mean reward: 187.48
               Mean episode length: 144.71
    Episode_Reward/reaching_object: 0.6121
     Episode_Reward/lifting_object: 35.4170
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 25.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 2.08s
                      Time elapsed: 00:19:33
                               ETA: 00:54:00

################################################################################
                     [1m Learning iteration 532/2000 [0m                      

                       Computation: 46684 steps/s (collection: 2.013s, learning 0.092s)
             Mean action noise std: 2.16
          Mean value_function loss: 164.0746
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 48.0523
                       Mean reward: 172.71
               Mean episode length: 137.22
    Episode_Reward/reaching_object: 0.5958
     Episode_Reward/lifting_object: 35.0313
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 26.8750
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 2.11s
                      Time elapsed: 00:19:35
                               ETA: 00:53:58

################################################################################
                     [1m Learning iteration 533/2000 [0m                      

                       Computation: 47513 steps/s (collection: 1.981s, learning 0.088s)
             Mean action noise std: 2.16
          Mean value_function loss: 175.7399
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 48.0528
                       Mean reward: 170.22
               Mean episode length: 131.09
    Episode_Reward/reaching_object: 0.5731
     Episode_Reward/lifting_object: 33.6004
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 25.3333
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 2.07s
                      Time elapsed: 00:19:37
                               ETA: 00:53:55

################################################################################
                     [1m Learning iteration 534/2000 [0m                      

                       Computation: 46539 steps/s (collection: 2.008s, learning 0.105s)
             Mean action noise std: 2.16
          Mean value_function loss: 180.0181
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 48.0526
                       Mean reward: 186.89
               Mean episode length: 145.37
    Episode_Reward/reaching_object: 0.6073
     Episode_Reward/lifting_object: 35.9622
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 27.6250
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 2.11s
                      Time elapsed: 00:19:39
                               ETA: 00:53:53

################################################################################
                     [1m Learning iteration 535/2000 [0m                      

                       Computation: 47704 steps/s (collection: 1.968s, learning 0.093s)
             Mean action noise std: 2.16
          Mean value_function loss: 170.8605
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 48.0523
                       Mean reward: 167.76
               Mean episode length: 128.09
    Episode_Reward/reaching_object: 0.5924
     Episode_Reward/lifting_object: 35.5330
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 26.0833
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 2.06s
                      Time elapsed: 00:19:41
                               ETA: 00:53:50

################################################################################
                     [1m Learning iteration 536/2000 [0m                      

                       Computation: 46612 steps/s (collection: 2.010s, learning 0.099s)
             Mean action noise std: 2.16
          Mean value_function loss: 172.0565
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 48.0521
                       Mean reward: 191.38
               Mean episode length: 143.47
    Episode_Reward/reaching_object: 0.5830
     Episode_Reward/lifting_object: 34.4703
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 27.3750
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 2.11s
                      Time elapsed: 00:19:44
                               ETA: 00:53:48

################################################################################
                     [1m Learning iteration 537/2000 [0m                      

                       Computation: 46609 steps/s (collection: 2.012s, learning 0.098s)
             Mean action noise std: 2.16
          Mean value_function loss: 195.5887
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 48.0523
                       Mean reward: 179.16
               Mean episode length: 138.53
    Episode_Reward/reaching_object: 0.6094
     Episode_Reward/lifting_object: 36.4006
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 25.6250
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 2.11s
                      Time elapsed: 00:19:46
                               ETA: 00:53:45

################################################################################
                     [1m Learning iteration 538/2000 [0m                      

                       Computation: 45689 steps/s (collection: 2.056s, learning 0.096s)
             Mean action noise std: 2.16
          Mean value_function loss: 206.0819
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 48.0524
                       Mean reward: 186.71
               Mean episode length: 140.65
    Episode_Reward/reaching_object: 0.5773
     Episode_Reward/lifting_object: 34.1466
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 26.8333
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 2.15s
                      Time elapsed: 00:19:48
                               ETA: 00:53:43

################################################################################
                     [1m Learning iteration 539/2000 [0m                      

                       Computation: 45310 steps/s (collection: 2.049s, learning 0.121s)
             Mean action noise std: 2.16
          Mean value_function loss: 178.5818
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 48.0523
                       Mean reward: 184.07
               Mean episode length: 140.96
    Episode_Reward/reaching_object: 0.5893
     Episode_Reward/lifting_object: 36.0473
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 26.0417
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 2.17s
                      Time elapsed: 00:19:50
                               ETA: 00:53:41

################################################################################
                     [1m Learning iteration 540/2000 [0m                      

                       Computation: 42513 steps/s (collection: 2.151s, learning 0.161s)
             Mean action noise std: 2.16
          Mean value_function loss: 175.2298
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 48.0517
                       Mean reward: 200.03
               Mean episode length: 142.98
    Episode_Reward/reaching_object: 0.5645
     Episode_Reward/lifting_object: 34.3941
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 27.4167
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 2.31s
                      Time elapsed: 00:19:52
                               ETA: 00:53:39

################################################################################
                     [1m Learning iteration 541/2000 [0m                      

                       Computation: 34192 steps/s (collection: 2.735s, learning 0.140s)
             Mean action noise std: 2.16
          Mean value_function loss: 177.1985
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 48.0513
                       Mean reward: 180.11
               Mean episode length: 133.75
    Episode_Reward/reaching_object: 0.5995
     Episode_Reward/lifting_object: 36.3402
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 25.7500
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 2.88s
                      Time elapsed: 00:19:55
                               ETA: 00:53:38

################################################################################
                     [1m Learning iteration 542/2000 [0m                      

                       Computation: 41879 steps/s (collection: 2.226s, learning 0.121s)
             Mean action noise std: 2.16
          Mean value_function loss: 178.0036
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.0502
                       Mean reward: 194.97
               Mean episode length: 141.68
    Episode_Reward/reaching_object: 0.6041
     Episode_Reward/lifting_object: 37.8322
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 26.5833
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 2.35s
                      Time elapsed: 00:19:58
                               ETA: 00:53:36

################################################################################
                     [1m Learning iteration 543/2000 [0m                      

                       Computation: 37838 steps/s (collection: 2.489s, learning 0.109s)
             Mean action noise std: 2.16
          Mean value_function loss: 174.8320
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 48.0492
                       Mean reward: 184.59
               Mean episode length: 132.27
    Episode_Reward/reaching_object: 0.6003
     Episode_Reward/lifting_object: 37.4241
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 26.5833
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 2.60s
                      Time elapsed: 00:20:00
                               ETA: 00:53:35

################################################################################
                     [1m Learning iteration 544/2000 [0m                      

                       Computation: 46625 steps/s (collection: 2.014s, learning 0.095s)
             Mean action noise std: 2.16
          Mean value_function loss: 178.9060
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 48.0493
                       Mean reward: 179.03
               Mean episode length: 130.51
    Episode_Reward/reaching_object: 0.5922
     Episode_Reward/lifting_object: 36.4718
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 24.2917
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 2.11s
                      Time elapsed: 00:20:02
                               ETA: 00:53:33

################################################################################
                     [1m Learning iteration 545/2000 [0m                      

                       Computation: 44776 steps/s (collection: 2.095s, learning 0.101s)
             Mean action noise std: 2.16
          Mean value_function loss: 209.5065
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 48.0493
                       Mean reward: 195.20
               Mean episode length: 131.95
    Episode_Reward/reaching_object: 0.6062
     Episode_Reward/lifting_object: 38.6591
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 27.9583
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 2.20s
                      Time elapsed: 00:20:04
                               ETA: 00:53:31

################################################################################
                     [1m Learning iteration 546/2000 [0m                      

                       Computation: 44809 steps/s (collection: 2.097s, learning 0.097s)
             Mean action noise std: 2.16
          Mean value_function loss: 194.3639
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 48.0493
                       Mean reward: 194.91
               Mean episode length: 140.20
    Episode_Reward/reaching_object: 0.6094
     Episode_Reward/lifting_object: 38.1434
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 29.0417
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 2.19s
                      Time elapsed: 00:20:07
                               ETA: 00:53:28

################################################################################
                     [1m Learning iteration 547/2000 [0m                      

                       Computation: 46657 steps/s (collection: 2.004s, learning 0.103s)
             Mean action noise std: 2.16
          Mean value_function loss: 198.0141
               Mean surrogate loss: 0.0124
                 Mean entropy loss: 48.0494
                       Mean reward: 211.44
               Mean episode length: 153.45
    Episode_Reward/reaching_object: 0.5933
     Episode_Reward/lifting_object: 37.8113
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 30.5833
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 2.11s
                      Time elapsed: 00:20:09
                               ETA: 00:53:26

################################################################################
                     [1m Learning iteration 548/2000 [0m                      

                       Computation: 47028 steps/s (collection: 1.987s, learning 0.103s)
             Mean action noise std: 2.16
          Mean value_function loss: 191.3753
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 48.0495
                       Mean reward: 192.35
               Mean episode length: 127.70
    Episode_Reward/reaching_object: 0.5711
     Episode_Reward/lifting_object: 37.0388
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 30.0833
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 2.09s
                      Time elapsed: 00:20:11
                               ETA: 00:53:23

################################################################################
                     [1m Learning iteration 549/2000 [0m                      

                       Computation: 47237 steps/s (collection: 1.980s, learning 0.101s)
             Mean action noise std: 2.16
          Mean value_function loss: 190.1403
               Mean surrogate loss: 0.0115
                 Mean entropy loss: 48.0495
                       Mean reward: 196.56
               Mean episode length: 133.75
    Episode_Reward/reaching_object: 0.5749
     Episode_Reward/lifting_object: 37.0380
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 28.5833
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 2.08s
                      Time elapsed: 00:20:13
                               ETA: 00:53:21

################################################################################
                     [1m Learning iteration 550/2000 [0m                      

                       Computation: 47132 steps/s (collection: 1.996s, learning 0.090s)
             Mean action noise std: 2.16
          Mean value_function loss: 196.1273
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 48.0496
                       Mean reward: 181.75
               Mean episode length: 123.30
    Episode_Reward/reaching_object: 0.5558
     Episode_Reward/lifting_object: 36.3288
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 29.8750
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 2.09s
                      Time elapsed: 00:20:15
                               ETA: 00:53:18

################################################################################
                     [1m Learning iteration 551/2000 [0m                      

                       Computation: 45580 steps/s (collection: 2.042s, learning 0.115s)
             Mean action noise std: 2.16
          Mean value_function loss: 225.5644
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 48.0496
                       Mean reward: 174.41
               Mean episode length: 120.94
    Episode_Reward/reaching_object: 0.5628
     Episode_Reward/lifting_object: 36.7706
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 29.1667
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 2.16s
                      Time elapsed: 00:20:17
                               ETA: 00:53:16

################################################################################
                     [1m Learning iteration 552/2000 [0m                      

                       Computation: 46021 steps/s (collection: 2.025s, learning 0.111s)
             Mean action noise std: 2.16
          Mean value_function loss: 224.9336
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 48.0497
                       Mean reward: 198.12
               Mean episode length: 128.84
    Episode_Reward/reaching_object: 0.5619
     Episode_Reward/lifting_object: 36.7211
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 29.1250
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 2.14s
                      Time elapsed: 00:20:19
                               ETA: 00:53:14

################################################################################
                     [1m Learning iteration 553/2000 [0m                      

                       Computation: 46212 steps/s (collection: 2.025s, learning 0.102s)
             Mean action noise std: 2.16
          Mean value_function loss: 192.2255
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 48.0498
                       Mean reward: 178.90
               Mean episode length: 122.87
    Episode_Reward/reaching_object: 0.5476
     Episode_Reward/lifting_object: 36.4909
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 29.2917
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 2.13s
                      Time elapsed: 00:20:21
                               ETA: 00:53:11

################################################################################
                     [1m Learning iteration 554/2000 [0m                      

                       Computation: 45914 steps/s (collection: 2.027s, learning 0.114s)
             Mean action noise std: 2.16
          Mean value_function loss: 191.6386
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 48.0500
                       Mean reward: 177.66
               Mean episode length: 125.95
    Episode_Reward/reaching_object: 0.5479
     Episode_Reward/lifting_object: 35.4728
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 29.4583
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 2.14s
                      Time elapsed: 00:20:24
                               ETA: 00:53:09

################################################################################
                     [1m Learning iteration 555/2000 [0m                      

                       Computation: 46157 steps/s (collection: 2.038s, learning 0.092s)
             Mean action noise std: 2.16
          Mean value_function loss: 266.6045
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 48.0501
                       Mean reward: 185.16
               Mean episode length: 131.90
    Episode_Reward/reaching_object: 0.5531
     Episode_Reward/lifting_object: 35.6740
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 30.6667
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 2.13s
                      Time elapsed: 00:20:26
                               ETA: 00:53:06

################################################################################
                     [1m Learning iteration 556/2000 [0m                      

                       Computation: 46216 steps/s (collection: 2.032s, learning 0.095s)
             Mean action noise std: 2.16
          Mean value_function loss: 328.9890
               Mean surrogate loss: 0.0097
                 Mean entropy loss: 48.0501
                       Mean reward: 170.71
               Mean episode length: 120.33
    Episode_Reward/reaching_object: 0.5631
     Episode_Reward/lifting_object: 36.5535
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 30.6667
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 2.13s
                      Time elapsed: 00:20:28
                               ETA: 00:53:04

################################################################################
                     [1m Learning iteration 557/2000 [0m                      

                       Computation: 45372 steps/s (collection: 2.068s, learning 0.099s)
             Mean action noise std: 2.16
          Mean value_function loss: 196.7043
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 48.0502
                       Mean reward: 159.51
               Mean episode length: 112.26
    Episode_Reward/reaching_object: 0.5422
     Episode_Reward/lifting_object: 35.1935
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 29.0833
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 2.17s
                      Time elapsed: 00:20:30
                               ETA: 00:53:02

################################################################################
                     [1m Learning iteration 558/2000 [0m                      

                       Computation: 46910 steps/s (collection: 1.990s, learning 0.106s)
             Mean action noise std: 2.16
          Mean value_function loss: 212.6499
               Mean surrogate loss: 0.0104
                 Mean entropy loss: 48.0502
                       Mean reward: 176.73
               Mean episode length: 123.99
    Episode_Reward/reaching_object: 0.5605
     Episode_Reward/lifting_object: 36.5959
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 28.5417
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 2.10s
                      Time elapsed: 00:20:32
                               ETA: 00:52:59

################################################################################
                     [1m Learning iteration 559/2000 [0m                      

                       Computation: 46575 steps/s (collection: 2.009s, learning 0.102s)
             Mean action noise std: 2.16
          Mean value_function loss: 199.9821
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 48.0502
                       Mean reward: 172.44
               Mean episode length: 125.09
    Episode_Reward/reaching_object: 0.5705
     Episode_Reward/lifting_object: 36.6279
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 29.1667
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 2.11s
                      Time elapsed: 00:20:34
                               ETA: 00:52:57

################################################################################
                     [1m Learning iteration 560/2000 [0m                      

                       Computation: 45497 steps/s (collection: 2.065s, learning 0.096s)
             Mean action noise std: 2.16
          Mean value_function loss: 201.5968
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 48.0503
                       Mean reward: 169.28
               Mean episode length: 117.88
    Episode_Reward/reaching_object: 0.5399
     Episode_Reward/lifting_object: 34.4097
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 29.9583
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 2.16s
                      Time elapsed: 00:20:36
                               ETA: 00:52:54

################################################################################
                     [1m Learning iteration 561/2000 [0m                      

                       Computation: 46183 steps/s (collection: 2.034s, learning 0.095s)
             Mean action noise std: 2.16
          Mean value_function loss: 199.3388
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 48.0502
                       Mean reward: 187.82
               Mean episode length: 126.45
    Episode_Reward/reaching_object: 0.5569
     Episode_Reward/lifting_object: 35.9387
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 30.6667
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 2.13s
                      Time elapsed: 00:20:39
                               ETA: 00:52:52

################################################################################
                     [1m Learning iteration 562/2000 [0m                      

                       Computation: 45476 steps/s (collection: 2.061s, learning 0.101s)
             Mean action noise std: 2.16
          Mean value_function loss: 231.8035
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 48.0494
                       Mean reward: 199.63
               Mean episode length: 136.71
    Episode_Reward/reaching_object: 0.5757
     Episode_Reward/lifting_object: 37.6121
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 28.8333
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 2.16s
                      Time elapsed: 00:20:41
                               ETA: 00:52:50

################################################################################
                     [1m Learning iteration 563/2000 [0m                      

                       Computation: 45140 steps/s (collection: 2.068s, learning 0.110s)
             Mean action noise std: 2.16
          Mean value_function loss: 208.5090
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 48.0493
                       Mean reward: 189.49
               Mean episode length: 129.42
    Episode_Reward/reaching_object: 0.5685
     Episode_Reward/lifting_object: 37.1285
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 27.5417
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 2.18s
                      Time elapsed: 00:20:43
                               ETA: 00:52:47

################################################################################
                     [1m Learning iteration 564/2000 [0m                      

                       Computation: 45699 steps/s (collection: 2.053s, learning 0.099s)
             Mean action noise std: 2.16
          Mean value_function loss: 206.7143
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 48.0490
                       Mean reward: 199.60
               Mean episode length: 129.97
    Episode_Reward/reaching_object: 0.5502
     Episode_Reward/lifting_object: 36.3253
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 29.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 2.15s
                      Time elapsed: 00:20:45
                               ETA: 00:52:45

################################################################################
                     [1m Learning iteration 565/2000 [0m                      

                       Computation: 44751 steps/s (collection: 2.093s, learning 0.104s)
             Mean action noise std: 2.16
          Mean value_function loss: 206.4285
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.0490
                       Mean reward: 173.93
               Mean episode length: 124.10
    Episode_Reward/reaching_object: 0.5536
     Episode_Reward/lifting_object: 36.6577
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 31.6250
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 2.20s
                      Time elapsed: 00:20:47
                               ETA: 00:52:43

################################################################################
                     [1m Learning iteration 566/2000 [0m                      

                       Computation: 46126 steps/s (collection: 2.023s, learning 0.108s)
             Mean action noise std: 2.16
          Mean value_function loss: 223.9212
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 48.0487
                       Mean reward: 183.79
               Mean episode length: 126.91
    Episode_Reward/reaching_object: 0.5706
     Episode_Reward/lifting_object: 37.5122
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 28.9167
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 2.13s
                      Time elapsed: 00:20:49
                               ETA: 00:52:40

################################################################################
                     [1m Learning iteration 567/2000 [0m                      

                       Computation: 46143 steps/s (collection: 2.021s, learning 0.110s)
             Mean action noise std: 2.16
          Mean value_function loss: 225.5059
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 48.0474
                       Mean reward: 190.69
               Mean episode length: 128.62
    Episode_Reward/reaching_object: 0.5610
     Episode_Reward/lifting_object: 36.9933
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 27.2500
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 2.13s
                      Time elapsed: 00:20:51
                               ETA: 00:52:38

################################################################################
                     [1m Learning iteration 568/2000 [0m                      

                       Computation: 45166 steps/s (collection: 2.060s, learning 0.117s)
             Mean action noise std: 2.16
          Mean value_function loss: 209.0022
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 48.0467
                       Mean reward: 198.72
               Mean episode length: 130.92
    Episode_Reward/reaching_object: 0.5887
     Episode_Reward/lifting_object: 38.5624
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 2.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 27.8333
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 2.18s
                      Time elapsed: 00:20:54
                               ETA: 00:52:36

################################################################################
                     [1m Learning iteration 569/2000 [0m                      

                       Computation: 46403 steps/s (collection: 2.019s, learning 0.099s)
             Mean action noise std: 2.16
          Mean value_function loss: 244.0544
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 48.0465
                       Mean reward: 182.09
               Mean episode length: 117.35
    Episode_Reward/reaching_object: 0.5600
     Episode_Reward/lifting_object: 37.1376
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 30.5833
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 2.12s
                      Time elapsed: 00:20:56
                               ETA: 00:52:33

################################################################################
                     [1m Learning iteration 570/2000 [0m                      

                       Computation: 45328 steps/s (collection: 2.059s, learning 0.110s)
             Mean action noise std: 2.16
          Mean value_function loss: 230.4961
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 48.0463
                       Mean reward: 192.60
               Mean episode length: 129.45
    Episode_Reward/reaching_object: 0.5605
     Episode_Reward/lifting_object: 36.8474
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 32.5417
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 2.17s
                      Time elapsed: 00:20:58
                               ETA: 00:52:31

################################################################################
                     [1m Learning iteration 571/2000 [0m                      

                       Computation: 45143 steps/s (collection: 2.074s, learning 0.104s)
             Mean action noise std: 2.16
          Mean value_function loss: 228.4998
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 48.0453
                       Mean reward: 194.03
               Mean episode length: 131.50
    Episode_Reward/reaching_object: 0.5605
     Episode_Reward/lifting_object: 36.3345
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 28.7500
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 2.18s
                      Time elapsed: 00:21:00
                               ETA: 00:52:29

################################################################################
                     [1m Learning iteration 572/2000 [0m                      

                       Computation: 45444 steps/s (collection: 2.068s, learning 0.096s)
             Mean action noise std: 2.16
          Mean value_function loss: 238.9507
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 48.0439
                       Mean reward: 194.34
               Mean episode length: 122.53
    Episode_Reward/reaching_object: 0.5577
     Episode_Reward/lifting_object: 37.2328
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 27.7917
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 2.16s
                      Time elapsed: 00:21:02
                               ETA: 00:52:26

################################################################################
                     [1m Learning iteration 573/2000 [0m                      

                       Computation: 44824 steps/s (collection: 2.092s, learning 0.102s)
             Mean action noise std: 2.16
          Mean value_function loss: 252.3054
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 48.0415
                       Mean reward: 192.37
               Mean episode length: 125.51
    Episode_Reward/reaching_object: 0.5712
     Episode_Reward/lifting_object: 38.3441
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 28.7500
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 2.19s
                      Time elapsed: 00:21:04
                               ETA: 00:52:24

################################################################################
                     [1m Learning iteration 574/2000 [0m                      

                       Computation: 45090 steps/s (collection: 2.072s, learning 0.109s)
             Mean action noise std: 2.16
          Mean value_function loss: 227.8512
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 48.0396
                       Mean reward: 241.92
               Mean episode length: 141.80
    Episode_Reward/reaching_object: 0.5951
     Episode_Reward/lifting_object: 40.9111
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 25.6250
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 2.18s
                      Time elapsed: 00:21:07
                               ETA: 00:52:22

################################################################################
                     [1m Learning iteration 575/2000 [0m                      

                       Computation: 45088 steps/s (collection: 2.063s, learning 0.118s)
             Mean action noise std: 2.16
          Mean value_function loss: 227.9127
               Mean surrogate loss: 0.0098
                 Mean entropy loss: 48.0391
                       Mean reward: 231.06
               Mean episode length: 140.80
    Episode_Reward/reaching_object: 0.5930
     Episode_Reward/lifting_object: 40.7117
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 29.2083
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 2.18s
                      Time elapsed: 00:21:09
                               ETA: 00:52:20

################################################################################
                     [1m Learning iteration 576/2000 [0m                      

                       Computation: 44217 steps/s (collection: 2.107s, learning 0.117s)
             Mean action noise std: 2.16
          Mean value_function loss: 237.4572
               Mean surrogate loss: 0.0118
                 Mean entropy loss: 48.0389
                       Mean reward: 216.42
               Mean episode length: 129.35
    Episode_Reward/reaching_object: 0.5972
     Episode_Reward/lifting_object: 42.0068
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 24.0833
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 2.22s
                      Time elapsed: 00:21:11
                               ETA: 00:52:18

################################################################################
                     [1m Learning iteration 577/2000 [0m                      

                       Computation: 43552 steps/s (collection: 2.140s, learning 0.118s)
             Mean action noise std: 2.16
          Mean value_function loss: 236.3035
               Mean surrogate loss: 0.0102
                 Mean entropy loss: 48.0389
                       Mean reward: 226.56
               Mean episode length: 134.25
    Episode_Reward/reaching_object: 0.6311
     Episode_Reward/lifting_object: 44.7860
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 25.9583
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 2.26s
                      Time elapsed: 00:21:13
                               ETA: 00:52:16

################################################################################
                     [1m Learning iteration 578/2000 [0m                      

                       Computation: 44490 steps/s (collection: 2.107s, learning 0.103s)
             Mean action noise std: 2.16
          Mean value_function loss: 230.8289
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 48.0389
                       Mean reward: 264.67
               Mean episode length: 144.58
    Episode_Reward/reaching_object: 0.6404
     Episode_Reward/lifting_object: 47.1952
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 25.9583
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 2.21s
                      Time elapsed: 00:21:16
                               ETA: 00:52:13

################################################################################
                     [1m Learning iteration 579/2000 [0m                      

                       Computation: 45191 steps/s (collection: 2.079s, learning 0.096s)
             Mean action noise std: 2.16
          Mean value_function loss: 251.9638
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 48.0385
                       Mean reward: 247.34
               Mean episode length: 139.98
    Episode_Reward/reaching_object: 0.6591
     Episode_Reward/lifting_object: 48.6681
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 27.3750
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 2.18s
                      Time elapsed: 00:21:18
                               ETA: 00:52:11

################################################################################
                     [1m Learning iteration 580/2000 [0m                      

                       Computation: 45497 steps/s (collection: 2.048s, learning 0.112s)
             Mean action noise std: 2.16
          Mean value_function loss: 250.0279
               Mean surrogate loss: 0.0110
                 Mean entropy loss: 48.0383
                       Mean reward: 263.07
               Mean episode length: 143.65
    Episode_Reward/reaching_object: 0.6856
     Episode_Reward/lifting_object: 52.0701
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 27.1250
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 2.16s
                      Time elapsed: 00:21:20
                               ETA: 00:52:09

################################################################################
                     [1m Learning iteration 581/2000 [0m                      

                       Computation: 45720 steps/s (collection: 2.037s, learning 0.114s)
             Mean action noise std: 2.16
          Mean value_function loss: 280.6116
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 48.0382
                       Mean reward: 249.57
               Mean episode length: 137.72
    Episode_Reward/reaching_object: 0.6510
     Episode_Reward/lifting_object: 50.0675
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 30.9583
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 2.15s
                      Time elapsed: 00:21:22
                               ETA: 00:52:06

################################################################################
                     [1m Learning iteration 582/2000 [0m                      

                       Computation: 46506 steps/s (collection: 2.008s, learning 0.106s)
             Mean action noise std: 2.16
          Mean value_function loss: 259.0784
               Mean surrogate loss: 0.0157
                 Mean entropy loss: 48.0383
                       Mean reward: 246.36
               Mean episode length: 132.83
    Episode_Reward/reaching_object: 0.6112
     Episode_Reward/lifting_object: 47.8900
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 33.9167
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 2.11s
                      Time elapsed: 00:21:24
                               ETA: 00:52:04

################################################################################
                     [1m Learning iteration 583/2000 [0m                      

                       Computation: 45506 steps/s (collection: 2.056s, learning 0.104s)
             Mean action noise std: 2.16
          Mean value_function loss: 244.9833
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 48.0383
                       Mean reward: 244.98
               Mean episode length: 136.23
    Episode_Reward/reaching_object: 0.5886
     Episode_Reward/lifting_object: 46.0413
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 32.7083
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 2.16s
                      Time elapsed: 00:21:26
                               ETA: 00:52:02

################################################################################
                     [1m Learning iteration 584/2000 [0m                      

                       Computation: 45348 steps/s (collection: 2.037s, learning 0.131s)
             Mean action noise std: 2.16
          Mean value_function loss: 259.3515
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.0381
                       Mean reward: 199.45
               Mean episode length: 116.77
    Episode_Reward/reaching_object: 0.5535
     Episode_Reward/lifting_object: 43.6958
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 34.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 2.17s
                      Time elapsed: 00:21:28
                               ETA: 00:51:59

################################################################################
                     [1m Learning iteration 585/2000 [0m                      

                       Computation: 46237 steps/s (collection: 2.020s, learning 0.106s)
             Mean action noise std: 2.16
          Mean value_function loss: 272.2931
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 48.0385
                       Mean reward: 197.53
               Mean episode length: 116.31
    Episode_Reward/reaching_object: 0.5380
     Episode_Reward/lifting_object: 41.9152
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 36.0417
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 2.13s
                      Time elapsed: 00:21:31
                               ETA: 00:51:57

################################################################################
                     [1m Learning iteration 586/2000 [0m                      

                       Computation: 44470 steps/s (collection: 2.109s, learning 0.102s)
             Mean action noise std: 2.16
          Mean value_function loss: 266.5049
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 48.0387
                       Mean reward: 198.56
               Mean episode length: 109.07
    Episode_Reward/reaching_object: 0.5051
     Episode_Reward/lifting_object: 39.1994
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 37.8333
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 2.21s
                      Time elapsed: 00:21:33
                               ETA: 00:51:55

################################################################################
                     [1m Learning iteration 587/2000 [0m                      

                       Computation: 39785 steps/s (collection: 2.377s, learning 0.094s)
             Mean action noise std: 2.16
          Mean value_function loss: 262.4730
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 48.0389
                       Mean reward: 198.62
               Mean episode length: 112.04
    Episode_Reward/reaching_object: 0.4971
     Episode_Reward/lifting_object: 38.8718
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 34.9167
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 2.47s
                      Time elapsed: 00:21:35
                               ETA: 00:51:53

################################################################################
                     [1m Learning iteration 588/2000 [0m                      

                       Computation: 46698 steps/s (collection: 2.003s, learning 0.102s)
             Mean action noise std: 2.16
          Mean value_function loss: 273.4219
               Mean surrogate loss: 0.0097
                 Mean entropy loss: 48.0389
                       Mean reward: 193.54
               Mean episode length: 114.58
    Episode_Reward/reaching_object: 0.4872
     Episode_Reward/lifting_object: 37.2321
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 36.8750
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 2.11s
                      Time elapsed: 00:21:37
                               ETA: 00:51:51

################################################################################
                     [1m Learning iteration 589/2000 [0m                      

                       Computation: 45852 steps/s (collection: 2.042s, learning 0.102s)
             Mean action noise std: 2.16
          Mean value_function loss: 277.4521
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 48.0389
                       Mean reward: 187.23
               Mean episode length: 107.20
    Episode_Reward/reaching_object: 0.4790
     Episode_Reward/lifting_object: 36.9027
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 37.1250
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 2.14s
                      Time elapsed: 00:21:39
                               ETA: 00:51:48

################################################################################
                     [1m Learning iteration 590/2000 [0m                      

                       Computation: 46671 steps/s (collection: 2.010s, learning 0.096s)
             Mean action noise std: 2.16
          Mean value_function loss: 276.1877
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 48.0387
                       Mean reward: 211.89
               Mean episode length: 113.96
    Episode_Reward/reaching_object: 0.4768
     Episode_Reward/lifting_object: 38.0546
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 39.4167
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 2.11s
                      Time elapsed: 00:21:42
                               ETA: 00:51:46

################################################################################
                     [1m Learning iteration 591/2000 [0m                      

                       Computation: 45343 steps/s (collection: 2.073s, learning 0.095s)
             Mean action noise std: 2.16
          Mean value_function loss: 279.4522
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 48.0393
                       Mean reward: 185.62
               Mean episode length: 102.80
    Episode_Reward/reaching_object: 0.4634
     Episode_Reward/lifting_object: 36.1594
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 39.4167
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 2.17s
                      Time elapsed: 00:21:44
                               ETA: 00:51:44

################################################################################
                     [1m Learning iteration 592/2000 [0m                      

                       Computation: 44199 steps/s (collection: 2.121s, learning 0.103s)
             Mean action noise std: 2.16
          Mean value_function loss: 298.4251
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 48.0395
                       Mean reward: 183.23
               Mean episode length: 105.89
    Episode_Reward/reaching_object: 0.4609
     Episode_Reward/lifting_object: 36.2840
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 39.4167
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 2.22s
                      Time elapsed: 00:21:46
                               ETA: 00:51:42

################################################################################
                     [1m Learning iteration 593/2000 [0m                      

                       Computation: 45645 steps/s (collection: 2.036s, learning 0.118s)
             Mean action noise std: 2.16
          Mean value_function loss: 286.0485
               Mean surrogate loss: 0.0119
                 Mean entropy loss: 48.0396
                       Mean reward: 190.86
               Mean episode length: 109.01
    Episode_Reward/reaching_object: 0.4525
     Episode_Reward/lifting_object: 35.6391
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 36.4167
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 2.15s
                      Time elapsed: 00:21:48
                               ETA: 00:51:39

################################################################################
                     [1m Learning iteration 594/2000 [0m                      

                       Computation: 45763 steps/s (collection: 2.041s, learning 0.107s)
             Mean action noise std: 2.16
          Mean value_function loss: 354.6266
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 48.0396
                       Mean reward: 211.74
               Mean episode length: 111.19
    Episode_Reward/reaching_object: 0.4639
     Episode_Reward/lifting_object: 38.7715
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 37.2917
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 2.15s
                      Time elapsed: 00:21:50
                               ETA: 00:51:37

################################################################################
                     [1m Learning iteration 595/2000 [0m                      

                       Computation: 45371 steps/s (collection: 2.053s, learning 0.114s)
             Mean action noise std: 2.16
          Mean value_function loss: 322.5016
               Mean surrogate loss: 0.0104
                 Mean entropy loss: 48.0397
                       Mean reward: 204.62
               Mean episode length: 101.70
    Episode_Reward/reaching_object: 0.4671
     Episode_Reward/lifting_object: 38.7231
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 35.4583
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 2.17s
                      Time elapsed: 00:21:52
                               ETA: 00:51:35

################################################################################
                     [1m Learning iteration 596/2000 [0m                      

                       Computation: 44791 steps/s (collection: 2.061s, learning 0.134s)
             Mean action noise std: 2.16
          Mean value_function loss: 286.8271
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 48.0398
                       Mean reward: 202.24
               Mean episode length: 108.47
    Episode_Reward/reaching_object: 0.4646
     Episode_Reward/lifting_object: 38.8352
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 36.1667
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 2.19s
                      Time elapsed: 00:21:55
                               ETA: 00:51:32

################################################################################
                     [1m Learning iteration 597/2000 [0m                      

                       Computation: 46312 steps/s (collection: 2.028s, learning 0.095s)
             Mean action noise std: 2.16
          Mean value_function loss: 303.2383
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 48.0394
                       Mean reward: 185.60
               Mean episode length: 98.89
    Episode_Reward/reaching_object: 0.4716
     Episode_Reward/lifting_object: 39.9235
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 37.9583
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 2.12s
                      Time elapsed: 00:21:57
                               ETA: 00:51:30

################################################################################
                     [1m Learning iteration 598/2000 [0m                      

                       Computation: 45235 steps/s (collection: 2.078s, learning 0.095s)
             Mean action noise std: 2.16
          Mean value_function loss: 294.8942
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 48.0395
                       Mean reward: 187.00
               Mean episode length: 110.28
    Episode_Reward/reaching_object: 0.4876
     Episode_Reward/lifting_object: 40.8050
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 37.7917
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 2.17s
                      Time elapsed: 00:21:59
                               ETA: 00:51:28

################################################################################
                     [1m Learning iteration 599/2000 [0m                      

                       Computation: 44037 steps/s (collection: 2.131s, learning 0.102s)
             Mean action noise std: 2.16
          Mean value_function loss: 315.5321
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 48.0396
                       Mean reward: 191.82
               Mean episode length: 105.73
    Episode_Reward/reaching_object: 0.4748
     Episode_Reward/lifting_object: 39.4979
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 35.7083
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 2.23s
                      Time elapsed: 00:22:01
                               ETA: 00:51:26

################################################################################
                     [1m Learning iteration 600/2000 [0m                      

                       Computation: 44760 steps/s (collection: 2.071s, learning 0.125s)
             Mean action noise std: 2.16
          Mean value_function loss: 308.5017
               Mean surrogate loss: 0.0155
                 Mean entropy loss: 48.0397
                       Mean reward: 219.55
               Mean episode length: 112.89
    Episode_Reward/reaching_object: 0.4874
     Episode_Reward/lifting_object: 40.8868
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 34.7500
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 2.20s
                      Time elapsed: 00:22:03
                               ETA: 00:51:23

################################################################################
                     [1m Learning iteration 601/2000 [0m                      

                       Computation: 38705 steps/s (collection: 2.448s, learning 0.092s)
             Mean action noise std: 2.16
          Mean value_function loss: 321.2545
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 48.0398
                       Mean reward: 210.60
               Mean episode length: 114.22
    Episode_Reward/reaching_object: 0.5057
     Episode_Reward/lifting_object: 42.4336
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 36.4583
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 2.54s
                      Time elapsed: 00:22:06
                               ETA: 00:51:22

################################################################################
                     [1m Learning iteration 602/2000 [0m                      

                       Computation: 42591 steps/s (collection: 2.193s, learning 0.115s)
             Mean action noise std: 2.16
          Mean value_function loss: 311.2123
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 48.0398
                       Mean reward: 222.43
               Mean episode length: 114.54
    Episode_Reward/reaching_object: 0.4987
     Episode_Reward/lifting_object: 42.9062
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 34.2917
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 2.31s
                      Time elapsed: 00:22:08
                               ETA: 00:51:20

################################################################################
                     [1m Learning iteration 603/2000 [0m                      

                       Computation: 32755 steps/s (collection: 2.715s, learning 0.286s)
             Mean action noise std: 2.16
          Mean value_function loss: 324.4649
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 48.0396
                       Mean reward: 223.84
               Mean episode length: 113.62
    Episode_Reward/reaching_object: 0.5061
     Episode_Reward/lifting_object: 44.5476
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 33.5417
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 3.00s
                      Time elapsed: 00:22:11
                               ETA: 00:51:20

################################################################################
                     [1m Learning iteration 604/2000 [0m                      

                       Computation: 32883 steps/s (collection: 2.794s, learning 0.196s)
             Mean action noise std: 2.16
          Mean value_function loss: 333.8948
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 48.0383
                       Mean reward: 220.85
               Mean episode length: 108.65
    Episode_Reward/reaching_object: 0.4976
     Episode_Reward/lifting_object: 42.9299
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 33.1667
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 2.99s
                      Time elapsed: 00:22:14
                               ETA: 00:51:19

################################################################################
                     [1m Learning iteration 605/2000 [0m                      

                       Computation: 43616 steps/s (collection: 2.142s, learning 0.112s)
             Mean action noise std: 2.16
          Mean value_function loss: 337.5784
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.0380
                       Mean reward: 228.13
               Mean episode length: 111.81
    Episode_Reward/reaching_object: 0.5134
     Episode_Reward/lifting_object: 46.0078
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 33.2917
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 2.25s
                      Time elapsed: 00:22:16
                               ETA: 00:51:17

################################################################################
                     [1m Learning iteration 606/2000 [0m                      

                       Computation: 45516 steps/s (collection: 2.060s, learning 0.100s)
             Mean action noise std: 2.16
          Mean value_function loss: 356.8181
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 48.0374
                       Mean reward: 258.47
               Mean episode length: 121.43
    Episode_Reward/reaching_object: 0.5298
     Episode_Reward/lifting_object: 48.2897
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 35.3333
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 2.16s
                      Time elapsed: 00:22:19
                               ETA: 00:51:15

################################################################################
                     [1m Learning iteration 607/2000 [0m                      

                       Computation: 41081 steps/s (collection: 2.292s, learning 0.101s)
             Mean action noise std: 2.16
          Mean value_function loss: 346.8942
               Mean surrogate loss: 0.0107
                 Mean entropy loss: 48.0371
                       Mean reward: 284.09
               Mean episode length: 127.49
    Episode_Reward/reaching_object: 0.5366
     Episode_Reward/lifting_object: 49.4909
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 34.2500
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 2.39s
                      Time elapsed: 00:22:21
                               ETA: 00:51:13

################################################################################
                     [1m Learning iteration 608/2000 [0m                      

                       Computation: 46620 steps/s (collection: 1.997s, learning 0.112s)
             Mean action noise std: 2.16
          Mean value_function loss: 357.0032
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 48.0369
                       Mean reward: 267.11
               Mean episode length: 127.94
    Episode_Reward/reaching_object: 0.5499
     Episode_Reward/lifting_object: 50.5227
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 33.8333
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 2.11s
                      Time elapsed: 00:22:23
                               ETA: 00:51:11

################################################################################
                     [1m Learning iteration 609/2000 [0m                      

                       Computation: 45289 steps/s (collection: 2.070s, learning 0.101s)
             Mean action noise std: 2.16
          Mean value_function loss: 341.3226
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 48.0368
                       Mean reward: 248.09
               Mean episode length: 114.62
    Episode_Reward/reaching_object: 0.5272
     Episode_Reward/lifting_object: 47.6999
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 34.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 2.17s
                      Time elapsed: 00:22:25
                               ETA: 00:51:08

################################################################################
                     [1m Learning iteration 610/2000 [0m                      

                       Computation: 45766 steps/s (collection: 2.050s, learning 0.098s)
             Mean action noise std: 2.16
          Mean value_function loss: 332.7195
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 48.0368
                       Mean reward: 240.71
               Mean episode length: 115.90
    Episode_Reward/reaching_object: 0.5221
     Episode_Reward/lifting_object: 48.0780
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 37.5833
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 2.15s
                      Time elapsed: 00:22:27
                               ETA: 00:51:06

################################################################################
                     [1m Learning iteration 611/2000 [0m                      

                       Computation: 43719 steps/s (collection: 2.138s, learning 0.110s)
             Mean action noise std: 2.16
          Mean value_function loss: 349.9112
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 48.0365
                       Mean reward: 236.60
               Mean episode length: 111.48
    Episode_Reward/reaching_object: 0.5059
     Episode_Reward/lifting_object: 46.7556
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 39.7083
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 2.25s
                      Time elapsed: 00:22:30
                               ETA: 00:51:04

################################################################################
                     [1m Learning iteration 612/2000 [0m                      

                       Computation: 43542 steps/s (collection: 2.138s, learning 0.120s)
             Mean action noise std: 2.16
          Mean value_function loss: 328.7627
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 48.0365
                       Mean reward: 208.16
               Mean episode length: 101.42
    Episode_Reward/reaching_object: 0.4966
     Episode_Reward/lifting_object: 45.3399
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 40.3750
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 2.26s
                      Time elapsed: 00:22:32
                               ETA: 00:51:02

################################################################################
                     [1m Learning iteration 613/2000 [0m                      

                       Computation: 44934 steps/s (collection: 2.074s, learning 0.114s)
             Mean action noise std: 2.16
          Mean value_function loss: 349.8721
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 48.0365
                       Mean reward: 206.76
               Mean episode length: 102.50
    Episode_Reward/reaching_object: 0.4575
     Episode_Reward/lifting_object: 39.8693
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 41.5417
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 2.19s
                      Time elapsed: 00:22:34
                               ETA: 00:51:00

################################################################################
                     [1m Learning iteration 614/2000 [0m                      

                       Computation: 42086 steps/s (collection: 2.237s, learning 0.099s)
             Mean action noise std: 2.16
          Mean value_function loss: 365.7947
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 48.0366
                       Mean reward: 193.20
               Mean episode length: 103.06
    Episode_Reward/reaching_object: 0.4672
     Episode_Reward/lifting_object: 40.4928
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 38.1667
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 2.34s
                      Time elapsed: 00:22:36
                               ETA: 00:50:58

################################################################################
                     [1m Learning iteration 615/2000 [0m                      

                       Computation: 45237 steps/s (collection: 2.066s, learning 0.107s)
             Mean action noise std: 2.16
          Mean value_function loss: 347.5692
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 48.0366
                       Mean reward: 189.72
               Mean episode length: 99.59
    Episode_Reward/reaching_object: 0.4522
     Episode_Reward/lifting_object: 38.7676
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 41.6667
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 2.17s
                      Time elapsed: 00:22:39
                               ETA: 00:50:55

################################################################################
                     [1m Learning iteration 616/2000 [0m                      

                       Computation: 45529 steps/s (collection: 2.052s, learning 0.107s)
             Mean action noise std: 2.16
          Mean value_function loss: 367.2162
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 48.0365
                       Mean reward: 177.55
               Mean episode length: 93.74
    Episode_Reward/reaching_object: 0.4504
     Episode_Reward/lifting_object: 39.1001
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 42.4583
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 2.16s
                      Time elapsed: 00:22:41
                               ETA: 00:50:53

################################################################################
                     [1m Learning iteration 617/2000 [0m                      

                       Computation: 45098 steps/s (collection: 2.076s, learning 0.104s)
             Mean action noise std: 2.16
          Mean value_function loss: 398.8381
               Mean surrogate loss: 0.0120
                 Mean entropy loss: 48.0364
                       Mean reward: 175.30
               Mean episode length: 97.47
    Episode_Reward/reaching_object: 0.4593
     Episode_Reward/lifting_object: 39.1071
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 37.5000
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 2.18s
                      Time elapsed: 00:22:43
                               ETA: 00:50:51

################################################################################
                     [1m Learning iteration 618/2000 [0m                      

                       Computation: 44919 steps/s (collection: 2.070s, learning 0.119s)
             Mean action noise std: 2.16
          Mean value_function loss: 369.3966
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 48.0365
                       Mean reward: 203.71
               Mean episode length: 97.56
    Episode_Reward/reaching_object: 0.4464
     Episode_Reward/lifting_object: 39.0469
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 38.3333
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 2.19s
                      Time elapsed: 00:22:45
                               ETA: 00:50:49

################################################################################
                     [1m Learning iteration 619/2000 [0m                      

                       Computation: 44922 steps/s (collection: 2.076s, learning 0.112s)
             Mean action noise std: 2.16
          Mean value_function loss: 389.6160
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 48.0359
                       Mean reward: 230.43
               Mean episode length: 107.20
    Episode_Reward/reaching_object: 0.4576
     Episode_Reward/lifting_object: 41.8159
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 42.4167
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 2.19s
                      Time elapsed: 00:22:47
                               ETA: 00:50:46

################################################################################
                     [1m Learning iteration 620/2000 [0m                      

                       Computation: 45487 steps/s (collection: 2.048s, learning 0.114s)
             Mean action noise std: 2.16
          Mean value_function loss: 383.6076
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 48.0350
                       Mean reward: 236.49
               Mean episode length: 112.94
    Episode_Reward/reaching_object: 0.4697
     Episode_Reward/lifting_object: 43.7295
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 38.8750
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 2.16s
                      Time elapsed: 00:22:50
                               ETA: 00:50:44

################################################################################
                     [1m Learning iteration 621/2000 [0m                      

                       Computation: 44306 steps/s (collection: 2.116s, learning 0.103s)
             Mean action noise std: 2.16
          Mean value_function loss: 389.4978
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.0337
                       Mean reward: 220.52
               Mean episode length: 101.64
    Episode_Reward/reaching_object: 0.4667
     Episode_Reward/lifting_object: 43.6252
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 40.1250
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 2.22s
                      Time elapsed: 00:22:52
                               ETA: 00:50:42

################################################################################
                     [1m Learning iteration 622/2000 [0m                      

                       Computation: 45444 steps/s (collection: 2.055s, learning 0.109s)
             Mean action noise std: 2.16
          Mean value_function loss: 360.4936
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 48.0325
                       Mean reward: 209.34
               Mean episode length: 101.76
    Episode_Reward/reaching_object: 0.4585
     Episode_Reward/lifting_object: 43.2025
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 40.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 2.16s
                      Time elapsed: 00:22:54
                               ETA: 00:50:40

################################################################################
                     [1m Learning iteration 623/2000 [0m                      

                       Computation: 44578 steps/s (collection: 2.102s, learning 0.103s)
             Mean action noise std: 2.16
          Mean value_function loss: 355.4670
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 48.0321
                       Mean reward: 202.75
               Mean episode length: 99.88
    Episode_Reward/reaching_object: 0.4509
     Episode_Reward/lifting_object: 42.1245
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 40.9167
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 2.21s
                      Time elapsed: 00:22:56
                               ETA: 00:50:37

################################################################################
                     [1m Learning iteration 624/2000 [0m                      

                       Computation: 45580 steps/s (collection: 2.061s, learning 0.096s)
             Mean action noise std: 2.16
          Mean value_function loss: 372.5904
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 48.0317
                       Mean reward: 211.77
               Mean episode length: 97.72
    Episode_Reward/reaching_object: 0.4574
     Episode_Reward/lifting_object: 42.0212
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 39.6250
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 2.16s
                      Time elapsed: 00:22:58
                               ETA: 00:50:35

################################################################################
                     [1m Learning iteration 625/2000 [0m                      

                       Computation: 44591 steps/s (collection: 2.101s, learning 0.104s)
             Mean action noise std: 2.16
          Mean value_function loss: 358.0674
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.0315
                       Mean reward: 187.98
               Mean episode length: 94.63
    Episode_Reward/reaching_object: 0.4501
     Episode_Reward/lifting_object: 40.4622
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 39.8333
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 2.20s
                      Time elapsed: 00:23:00
                               ETA: 00:50:33

################################################################################
                     [1m Learning iteration 626/2000 [0m                      

                       Computation: 45363 steps/s (collection: 2.053s, learning 0.115s)
             Mean action noise std: 2.16
          Mean value_function loss: 366.9730
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 48.0318
                       Mean reward: 200.02
               Mean episode length: 103.50
    Episode_Reward/reaching_object: 0.4514
     Episode_Reward/lifting_object: 40.2945
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 40.5417
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 2.17s
                      Time elapsed: 00:23:03
                               ETA: 00:50:31

################################################################################
                     [1m Learning iteration 627/2000 [0m                      

                       Computation: 44773 steps/s (collection: 2.078s, learning 0.118s)
             Mean action noise std: 2.16
          Mean value_function loss: 373.9545
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 48.0324
                       Mean reward: 193.13
               Mean episode length: 97.37
    Episode_Reward/reaching_object: 0.4501
     Episode_Reward/lifting_object: 39.6241
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 39.6250
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 2.20s
                      Time elapsed: 00:23:05
                               ETA: 00:50:28

################################################################################
                     [1m Learning iteration 628/2000 [0m                      

                       Computation: 45224 steps/s (collection: 2.070s, learning 0.104s)
             Mean action noise std: 2.16
          Mean value_function loss: 369.5730
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 48.0329
                       Mean reward: 194.84
               Mean episode length: 102.69
    Episode_Reward/reaching_object: 0.4577
     Episode_Reward/lifting_object: 40.9414
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 38.7917
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 2.17s
                      Time elapsed: 00:23:07
                               ETA: 00:50:26

################################################################################
                     [1m Learning iteration 629/2000 [0m                      

                       Computation: 45446 steps/s (collection: 2.062s, learning 0.102s)
             Mean action noise std: 2.16
          Mean value_function loss: 381.0000
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 48.0328
                       Mean reward: 196.29
               Mean episode length: 95.57
    Episode_Reward/reaching_object: 0.4426
     Episode_Reward/lifting_object: 38.7612
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 39.7083
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 2.16s
                      Time elapsed: 00:23:09
                               ETA: 00:50:24

################################################################################
                     [1m Learning iteration 630/2000 [0m                      

                       Computation: 43876 steps/s (collection: 2.129s, learning 0.112s)
             Mean action noise std: 2.16
          Mean value_function loss: 387.2740
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 48.0328
                       Mean reward: 201.75
               Mean episode length: 98.06
    Episode_Reward/reaching_object: 0.4422
     Episode_Reward/lifting_object: 40.1751
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 39.9167
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 2.24s
                      Time elapsed: 00:23:11
                               ETA: 00:50:22

################################################################################
                     [1m Learning iteration 631/2000 [0m                      

                       Computation: 45637 steps/s (collection: 2.058s, learning 0.096s)
             Mean action noise std: 2.16
          Mean value_function loss: 424.8640
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 48.0330
                       Mean reward: 198.97
               Mean episode length: 105.38
    Episode_Reward/reaching_object: 0.4481
     Episode_Reward/lifting_object: 40.5625
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 40.7083
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 2.15s
                      Time elapsed: 00:23:14
                               ETA: 00:50:19

################################################################################
                     [1m Learning iteration 632/2000 [0m                      

                       Computation: 44914 steps/s (collection: 2.094s, learning 0.095s)
             Mean action noise std: 2.16
          Mean value_function loss: 426.4273
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 48.0331
                       Mean reward: 236.36
               Mean episode length: 108.86
    Episode_Reward/reaching_object: 0.4567
     Episode_Reward/lifting_object: 42.6903
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 39.3333
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 2.19s
                      Time elapsed: 00:23:16
                               ETA: 00:50:17

################################################################################
                     [1m Learning iteration 633/2000 [0m                      

                       Computation: 45404 steps/s (collection: 2.060s, learning 0.106s)
             Mean action noise std: 2.16
          Mean value_function loss: 475.9212
               Mean surrogate loss: 0.0098
                 Mean entropy loss: 48.0334
                       Mean reward: 235.02
               Mean episode length: 100.00
    Episode_Reward/reaching_object: 0.4637
     Episode_Reward/lifting_object: 44.0209
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 40.5417
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 2.17s
                      Time elapsed: 00:23:18
                               ETA: 00:50:15

################################################################################
                     [1m Learning iteration 634/2000 [0m                      

                       Computation: 41524 steps/s (collection: 2.251s, learning 0.116s)
             Mean action noise std: 2.16
          Mean value_function loss: 475.4384
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 48.0334
                       Mean reward: 237.43
               Mean episode length: 97.29
    Episode_Reward/reaching_object: 0.4673
     Episode_Reward/lifting_object: 46.8895
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 38.7500
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 2.37s
                      Time elapsed: 00:23:20
                               ETA: 00:50:13

################################################################################
                     [1m Learning iteration 635/2000 [0m                      

                       Computation: 45093 steps/s (collection: 2.072s, learning 0.108s)
             Mean action noise std: 2.16
          Mean value_function loss: 480.8578
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 48.0329
                       Mean reward: 231.11
               Mean episode length: 98.49
    Episode_Reward/reaching_object: 0.4701
     Episode_Reward/lifting_object: 47.6078
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 40.8333
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 2.18s
                      Time elapsed: 00:23:22
                               ETA: 00:50:11

################################################################################
                     [1m Learning iteration 636/2000 [0m                      

                       Computation: 44558 steps/s (collection: 2.102s, learning 0.105s)
             Mean action noise std: 2.16
          Mean value_function loss: 475.3942
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 48.0323
                       Mean reward: 226.67
               Mean episode length: 95.64
    Episode_Reward/reaching_object: 0.4668
     Episode_Reward/lifting_object: 47.9588
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 39.6250
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 2.21s
                      Time elapsed: 00:23:25
                               ETA: 00:50:08

################################################################################
                     [1m Learning iteration 637/2000 [0m                      

                       Computation: 45274 steps/s (collection: 2.064s, learning 0.107s)
             Mean action noise std: 2.16
          Mean value_function loss: 466.6602
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 48.0323
                       Mean reward: 240.83
               Mean episode length: 99.56
    Episode_Reward/reaching_object: 0.4617
     Episode_Reward/lifting_object: 46.9720
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 38.7917
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 2.17s
                      Time elapsed: 00:23:27
                               ETA: 00:50:06

################################################################################
                     [1m Learning iteration 638/2000 [0m                      

                       Computation: 43630 steps/s (collection: 2.138s, learning 0.116s)
             Mean action noise std: 2.16
          Mean value_function loss: 422.0387
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 48.0322
                       Mean reward: 251.18
               Mean episode length: 107.87
    Episode_Reward/reaching_object: 0.4703
     Episode_Reward/lifting_object: 48.3932
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 32.2917
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 2.25s
                      Time elapsed: 00:23:29
                               ETA: 00:50:04

################################################################################
                     [1m Learning iteration 639/2000 [0m                      

                       Computation: 42851 steps/s (collection: 2.190s, learning 0.105s)
             Mean action noise std: 2.16
          Mean value_function loss: 436.9532
               Mean surrogate loss: 0.0137
                 Mean entropy loss: 48.0321
                       Mean reward: 267.63
               Mean episode length: 106.54
    Episode_Reward/reaching_object: 0.4861
     Episode_Reward/lifting_object: 49.8433
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 32.2083
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 2.29s
                      Time elapsed: 00:23:31
                               ETA: 00:50:02

################################################################################
                     [1m Learning iteration 640/2000 [0m                      

                       Computation: 44719 steps/s (collection: 2.094s, learning 0.105s)
             Mean action noise std: 2.16
          Mean value_function loss: 417.5839
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 48.0320
                       Mean reward: 234.23
               Mean episode length: 100.69
    Episode_Reward/reaching_object: 0.4708
     Episode_Reward/lifting_object: 46.9252
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 32.6667
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 2.20s
                      Time elapsed: 00:23:34
                               ETA: 00:50:00

################################################################################
                     [1m Learning iteration 641/2000 [0m                      

                       Computation: 44576 steps/s (collection: 2.111s, learning 0.094s)
             Mean action noise std: 2.16
          Mean value_function loss: 441.8313
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 48.0319
                       Mean reward: 245.27
               Mean episode length: 106.06
    Episode_Reward/reaching_object: 0.4635
     Episode_Reward/lifting_object: 44.3809
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 37.2500
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 2.21s
                      Time elapsed: 00:23:36
                               ETA: 00:49:58

################################################################################
                     [1m Learning iteration 642/2000 [0m                      

                       Computation: 45446 steps/s (collection: 2.063s, learning 0.100s)
             Mean action noise std: 2.16
          Mean value_function loss: 435.7113
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 48.0309
                       Mean reward: 225.68
               Mean episode length: 103.12
    Episode_Reward/reaching_object: 0.5169
     Episode_Reward/lifting_object: 50.4533
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 31.6250
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 2.16s
                      Time elapsed: 00:23:38
                               ETA: 00:49:55

################################################################################
                     [1m Learning iteration 643/2000 [0m                      

                       Computation: 45403 steps/s (collection: 2.072s, learning 0.094s)
             Mean action noise std: 2.16
          Mean value_function loss: 430.2399
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 48.0292
                       Mean reward: 324.33
               Mean episode length: 131.12
    Episode_Reward/reaching_object: 0.5575
     Episode_Reward/lifting_object: 55.3597
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 29.5000
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 2.17s
                      Time elapsed: 00:23:40
                               ETA: 00:49:53

################################################################################
                     [1m Learning iteration 644/2000 [0m                      

                       Computation: 45297 steps/s (collection: 2.071s, learning 0.100s)
             Mean action noise std: 2.16
          Mean value_function loss: 411.3317
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.0285
                       Mean reward: 306.20
               Mean episode length: 124.41
    Episode_Reward/reaching_object: 0.6051
     Episode_Reward/lifting_object: 61.7851
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 24.7917
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 2.17s
                      Time elapsed: 00:23:42
                               ETA: 00:49:51

################################################################################
                     [1m Learning iteration 645/2000 [0m                      

                       Computation: 45790 steps/s (collection: 2.050s, learning 0.097s)
             Mean action noise std: 2.16
          Mean value_function loss: 428.5757
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 48.0270
                       Mean reward: 369.57
               Mean episode length: 138.93
    Episode_Reward/reaching_object: 0.6449
     Episode_Reward/lifting_object: 67.1299
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 26.1667
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 2.15s
                      Time elapsed: 00:23:44
                               ETA: 00:49:48

################################################################################
                     [1m Learning iteration 646/2000 [0m                      

                       Computation: 45340 steps/s (collection: 2.071s, learning 0.098s)
             Mean action noise std: 2.16
          Mean value_function loss: 434.7941
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 48.0254
                       Mean reward: 369.60
               Mean episode length: 144.19
    Episode_Reward/reaching_object: 0.7056
     Episode_Reward/lifting_object: 73.7842
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.6667
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 2.17s
                      Time elapsed: 00:23:47
                               ETA: 00:49:46

################################################################################
                     [1m Learning iteration 647/2000 [0m                      

                       Computation: 44628 steps/s (collection: 2.099s, learning 0.104s)
             Mean action noise std: 2.16
          Mean value_function loss: 432.2908
               Mean surrogate loss: 0.0170
                 Mean entropy loss: 48.0249
                       Mean reward: 405.75
               Mean episode length: 150.21
    Episode_Reward/reaching_object: 0.7038
     Episode_Reward/lifting_object: 74.7596
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 27.2500
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 2.20s
                      Time elapsed: 00:23:49
                               ETA: 00:49:44

################################################################################
                     [1m Learning iteration 648/2000 [0m                      

                       Computation: 45012 steps/s (collection: 2.069s, learning 0.115s)
             Mean action noise std: 2.16
          Mean value_function loss: 439.2146
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 48.0248
                       Mean reward: 342.21
               Mean episode length: 129.87
    Episode_Reward/reaching_object: 0.7144
     Episode_Reward/lifting_object: 76.8355
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.3333
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 2.18s
                      Time elapsed: 00:23:51
                               ETA: 00:49:42

################################################################################
                     [1m Learning iteration 649/2000 [0m                      

                       Computation: 45028 steps/s (collection: 2.073s, learning 0.110s)
             Mean action noise std: 2.16
          Mean value_function loss: 451.9718
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 48.0248
                       Mean reward: 390.41
               Mean episode length: 145.76
    Episode_Reward/reaching_object: 0.6771
     Episode_Reward/lifting_object: 72.1191
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 32.4167
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 2.18s
                      Time elapsed: 00:23:53
                               ETA: 00:49:39

################################################################################
                     [1m Learning iteration 650/2000 [0m                      

                       Computation: 44579 steps/s (collection: 2.082s, learning 0.123s)
             Mean action noise std: 2.16
          Mean value_function loss: 450.1695
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 48.0248
                       Mean reward: 343.39
               Mean episode length: 131.52
    Episode_Reward/reaching_object: 0.6813
     Episode_Reward/lifting_object: 72.3513
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 32.3333
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 2.21s
                      Time elapsed: 00:23:55
                               ETA: 00:49:37

################################################################################
                     [1m Learning iteration 651/2000 [0m                      

                       Computation: 44873 steps/s (collection: 2.094s, learning 0.097s)
             Mean action noise std: 2.16
          Mean value_function loss: 477.7296
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 48.0248
                       Mean reward: 349.61
               Mean episode length: 134.96
    Episode_Reward/reaching_object: 0.6446
     Episode_Reward/lifting_object: 68.2019
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 30.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 2.19s
                      Time elapsed: 00:23:58
                               ETA: 00:49:35

################################################################################
                     [1m Learning iteration 652/2000 [0m                      

                       Computation: 45235 steps/s (collection: 2.079s, learning 0.094s)
             Mean action noise std: 2.16
          Mean value_function loss: 450.6547
               Mean surrogate loss: 0.0142
                 Mean entropy loss: 48.0246
                       Mean reward: 340.88
               Mean episode length: 130.37
    Episode_Reward/reaching_object: 0.6340
     Episode_Reward/lifting_object: 66.1932
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.2917
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 2.17s
                      Time elapsed: 00:24:00
                               ETA: 00:49:33

################################################################################
                     [1m Learning iteration 653/2000 [0m                      

                       Computation: 43854 steps/s (collection: 2.112s, learning 0.130s)
             Mean action noise std: 2.16
          Mean value_function loss: 461.0956
               Mean surrogate loss: 0.0112
                 Mean entropy loss: 48.0245
                       Mean reward: 354.18
               Mean episode length: 132.81
    Episode_Reward/reaching_object: 0.6324
     Episode_Reward/lifting_object: 68.1333
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 28.4167
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 2.24s
                      Time elapsed: 00:24:02
                               ETA: 00:49:31

################################################################################
                     [1m Learning iteration 654/2000 [0m                      

                       Computation: 44361 steps/s (collection: 2.120s, learning 0.096s)
             Mean action noise std: 2.16
          Mean value_function loss: 469.6255
               Mean surrogate loss: 0.0155
                 Mean entropy loss: 48.0245
                       Mean reward: 379.60
               Mean episode length: 136.24
    Episode_Reward/reaching_object: 0.6398
     Episode_Reward/lifting_object: 69.6071
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.7917
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 2.22s
                      Time elapsed: 00:24:04
                               ETA: 00:49:28

################################################################################
                     [1m Learning iteration 655/2000 [0m                      

                       Computation: 42297 steps/s (collection: 2.203s, learning 0.121s)
             Mean action noise std: 2.16
          Mean value_function loss: 480.6528
               Mean surrogate loss: 0.0208
                 Mean entropy loss: 48.0245
                       Mean reward: 348.93
               Mean episode length: 127.77
    Episode_Reward/reaching_object: 0.6684
     Episode_Reward/lifting_object: 74.5201
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.2917
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 2.32s
                      Time elapsed: 00:24:07
                               ETA: 00:49:26

################################################################################
                     [1m Learning iteration 656/2000 [0m                      

                       Computation: 42035 steps/s (collection: 2.233s, learning 0.106s)
             Mean action noise std: 2.16
          Mean value_function loss: 464.2455
               Mean surrogate loss: 0.0101
                 Mean entropy loss: 48.0245
                       Mean reward: 413.73
               Mean episode length: 143.39
    Episode_Reward/reaching_object: 0.6927
     Episode_Reward/lifting_object: 78.0758
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 28.3333
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 2.34s
                      Time elapsed: 00:24:09
                               ETA: 00:49:24

################################################################################
                     [1m Learning iteration 657/2000 [0m                      

                       Computation: 45007 steps/s (collection: 2.085s, learning 0.100s)
             Mean action noise std: 2.16
          Mean value_function loss: 464.1091
               Mean surrogate loss: 0.0156
                 Mean entropy loss: 48.0245
                       Mean reward: 387.68
               Mean episode length: 133.07
    Episode_Reward/reaching_object: 0.6989
     Episode_Reward/lifting_object: 79.8968
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.0417
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 2.18s
                      Time elapsed: 00:24:11
                               ETA: 00:49:22

################################################################################
                     [1m Learning iteration 658/2000 [0m                      

                       Computation: 37575 steps/s (collection: 2.390s, learning 0.227s)
             Mean action noise std: 2.16
          Mean value_function loss: 475.5025
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 48.0245
                       Mean reward: 361.41
               Mean episode length: 130.30
    Episode_Reward/reaching_object: 0.6779
     Episode_Reward/lifting_object: 77.7653
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 29.6250
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 2.62s
                      Time elapsed: 00:24:14
                               ETA: 00:49:21

################################################################################
                     [1m Learning iteration 659/2000 [0m                      

                       Computation: 34232 steps/s (collection: 2.649s, learning 0.223s)
             Mean action noise std: 2.16
          Mean value_function loss: 451.8483
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 48.0245
                       Mean reward: 358.85
               Mean episode length: 126.47
    Episode_Reward/reaching_object: 0.6925
     Episode_Reward/lifting_object: 79.0978
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 25.9167
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 2.87s
                      Time elapsed: 00:24:17
                               ETA: 00:49:20

################################################################################
                     [1m Learning iteration 660/2000 [0m                      

                       Computation: 39412 steps/s (collection: 2.321s, learning 0.173s)
             Mean action noise std: 2.16
          Mean value_function loss: 465.8329
               Mean surrogate loss: 0.0197
                 Mean entropy loss: 48.0246
                       Mean reward: 411.19
               Mean episode length: 140.41
    Episode_Reward/reaching_object: 0.7092
     Episode_Reward/lifting_object: 81.7385
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 27.5833
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 2.49s
                      Time elapsed: 00:24:19
                               ETA: 00:49:18

################################################################################
                     [1m Learning iteration 661/2000 [0m                      

                       Computation: 42724 steps/s (collection: 2.197s, learning 0.104s)
             Mean action noise std: 2.16
          Mean value_function loss: 436.7913
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 48.0246
                       Mean reward: 357.11
               Mean episode length: 129.91
    Episode_Reward/reaching_object: 0.6809
     Episode_Reward/lifting_object: 78.0462
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 25.7917
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 2.30s
                      Time elapsed: 00:24:21
                               ETA: 00:49:16

################################################################################
                     [1m Learning iteration 662/2000 [0m                      

                       Computation: 45527 steps/s (collection: 2.055s, learning 0.104s)
             Mean action noise std: 2.16
          Mean value_function loss: 467.4040
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 48.0243
                       Mean reward: 366.21
               Mean episode length: 128.33
    Episode_Reward/reaching_object: 0.6891
     Episode_Reward/lifting_object: 79.3465
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 29.0833
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 2.16s
                      Time elapsed: 00:24:23
                               ETA: 00:49:14

################################################################################
                     [1m Learning iteration 663/2000 [0m                      

                       Computation: 45643 steps/s (collection: 2.057s, learning 0.097s)
             Mean action noise std: 2.16
          Mean value_function loss: 462.6921
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 48.0243
                       Mean reward: 408.93
               Mean episode length: 144.33
    Episode_Reward/reaching_object: 0.7008
     Episode_Reward/lifting_object: 80.3947
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 26.3750
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 2.15s
                      Time elapsed: 00:24:26
                               ETA: 00:49:12

################################################################################
                     [1m Learning iteration 664/2000 [0m                      

                       Computation: 45558 steps/s (collection: 2.049s, learning 0.109s)
             Mean action noise std: 2.16
          Mean value_function loss: 462.3770
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 48.0242
                       Mean reward: 370.85
               Mean episode length: 133.94
    Episode_Reward/reaching_object: 0.7097
     Episode_Reward/lifting_object: 80.4512
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 29.1667
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 2.16s
                      Time elapsed: 00:24:28
                               ETA: 00:49:09

################################################################################
                     [1m Learning iteration 665/2000 [0m                      

                       Computation: 43541 steps/s (collection: 2.150s, learning 0.108s)
             Mean action noise std: 2.16
          Mean value_function loss: 464.9046
               Mean surrogate loss: 0.0104
                 Mean entropy loss: 48.0242
                       Mean reward: 390.90
               Mean episode length: 136.13
    Episode_Reward/reaching_object: 0.6964
     Episode_Reward/lifting_object: 79.1317
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 26.9167
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 2.26s
                      Time elapsed: 00:24:30
                               ETA: 00:49:07

################################################################################
                     [1m Learning iteration 666/2000 [0m                      

                       Computation: 26188 steps/s (collection: 3.638s, learning 0.116s)
             Mean action noise std: 2.16
          Mean value_function loss: 451.0942
               Mean surrogate loss: 0.0108
                 Mean entropy loss: 48.0243
                       Mean reward: 410.80
               Mean episode length: 139.31
    Episode_Reward/reaching_object: 0.6997
     Episode_Reward/lifting_object: 79.1839
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.1667
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 3.75s
                      Time elapsed: 00:24:34
                               ETA: 00:49:08

################################################################################
                     [1m Learning iteration 667/2000 [0m                      

                       Computation: 14410 steps/s (collection: 6.689s, learning 0.133s)
             Mean action noise std: 2.16
          Mean value_function loss: 436.0952
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 48.0244
                       Mean reward: 414.11
               Mean episode length: 140.67
    Episode_Reward/reaching_object: 0.7270
     Episode_Reward/lifting_object: 81.9536
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 24.0833
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 6.82s
                      Time elapsed: 00:24:41
                               ETA: 00:49:15

################################################################################
                     [1m Learning iteration 668/2000 [0m                      

                       Computation: 14246 steps/s (collection: 6.779s, learning 0.121s)
             Mean action noise std: 2.16
          Mean value_function loss: 471.5903
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 48.0238
                       Mean reward: 391.73
               Mean episode length: 140.93
    Episode_Reward/reaching_object: 0.7337
     Episode_Reward/lifting_object: 83.6416
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 26.3750
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 6.90s
                      Time elapsed: 00:24:48
                               ETA: 00:49:22

################################################################################
                     [1m Learning iteration 669/2000 [0m                      

                       Computation: 14165 steps/s (collection: 6.802s, learning 0.138s)
             Mean action noise std: 2.16
          Mean value_function loss: 466.0348
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 48.0222
                       Mean reward: 434.90
               Mean episode length: 148.49
    Episode_Reward/reaching_object: 0.7380
     Episode_Reward/lifting_object: 84.3916
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 24.2500
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 6.94s
                      Time elapsed: 00:24:54
                               ETA: 00:49:29

################################################################################
                     [1m Learning iteration 670/2000 [0m                      

                       Computation: 14625 steps/s (collection: 6.590s, learning 0.132s)
             Mean action noise std: 2.16
          Mean value_function loss: 461.1902
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 48.0212
                       Mean reward: 419.48
               Mean episode length: 147.81
    Episode_Reward/reaching_object: 0.7466
     Episode_Reward/lifting_object: 84.8827
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 25.6667
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 6.72s
                      Time elapsed: 00:25:01
                               ETA: 00:49:36

################################################################################
                     [1m Learning iteration 671/2000 [0m                      

                       Computation: 14259 steps/s (collection: 6.776s, learning 0.118s)
             Mean action noise std: 2.16
          Mean value_function loss: 466.1166
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 48.0211
                       Mean reward: 406.70
               Mean episode length: 144.24
    Episode_Reward/reaching_object: 0.7355
     Episode_Reward/lifting_object: 83.0205
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 27.0417
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 6.89s
                      Time elapsed: 00:25:08
                               ETA: 00:49:43

################################################################################
                     [1m Learning iteration 672/2000 [0m                      

                       Computation: 14820 steps/s (collection: 6.510s, learning 0.123s)
             Mean action noise std: 2.16
          Mean value_function loss: 502.5357
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 48.0211
                       Mean reward: 455.01
               Mean episode length: 156.00
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 88.0793
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 27.9583
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 6.63s
                      Time elapsed: 00:25:15
                               ETA: 00:49:49

################################################################################
                     [1m Learning iteration 673/2000 [0m                      

                       Computation: 14651 steps/s (collection: 6.590s, learning 0.119s)
             Mean action noise std: 2.16
          Mean value_function loss: 487.2331
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 48.0214
                       Mean reward: 451.98
               Mean episode length: 151.75
    Episode_Reward/reaching_object: 0.7540
     Episode_Reward/lifting_object: 86.1975
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 26.0833
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 6.71s
                      Time elapsed: 00:25:21
                               ETA: 00:49:56

################################################################################
                     [1m Learning iteration 674/2000 [0m                      

                       Computation: 14211 steps/s (collection: 6.776s, learning 0.141s)
             Mean action noise std: 2.16
          Mean value_function loss: 477.3871
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 48.0218
                       Mean reward: 441.95
               Mean episode length: 148.62
    Episode_Reward/reaching_object: 0.7502
     Episode_Reward/lifting_object: 86.6709
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 25.2083
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 6.92s
                      Time elapsed: 00:25:28
                               ETA: 00:50:03

################################################################################
                     [1m Learning iteration 675/2000 [0m                      

                       Computation: 23888 steps/s (collection: 4.027s, learning 0.088s)
             Mean action noise std: 2.16
          Mean value_function loss: 495.6331
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 48.0216
                       Mean reward: 411.34
               Mean episode length: 141.05
    Episode_Reward/reaching_object: 0.7271
     Episode_Reward/lifting_object: 84.2698
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 28.5417
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 4.12s
                      Time elapsed: 00:25:32
                               ETA: 00:50:04

################################################################################
                     [1m Learning iteration 676/2000 [0m                      

                       Computation: 45223 steps/s (collection: 2.060s, learning 0.114s)
             Mean action noise std: 2.16
          Mean value_function loss: 508.6580
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 48.0210
                       Mean reward: 414.10
               Mean episode length: 141.80
    Episode_Reward/reaching_object: 0.7208
     Episode_Reward/lifting_object: 83.7519
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.2083
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 2.17s
                      Time elapsed: 00:25:35
                               ETA: 00:50:02

################################################################################
                     [1m Learning iteration 677/2000 [0m                      

                       Computation: 48254 steps/s (collection: 1.940s, learning 0.097s)
             Mean action noise std: 2.16
          Mean value_function loss: 500.3406
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 48.0209
                       Mean reward: 440.34
               Mean episode length: 145.46
    Episode_Reward/reaching_object: 0.7170
     Episode_Reward/lifting_object: 83.3486
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 27.8750
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 2.04s
                      Time elapsed: 00:25:37
                               ETA: 00:49:59

################################################################################
                     [1m Learning iteration 678/2000 [0m                      

                       Computation: 48647 steps/s (collection: 1.918s, learning 0.103s)
             Mean action noise std: 2.16
          Mean value_function loss: 489.4229
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 48.0209
                       Mean reward: 406.97
               Mean episode length: 140.69
    Episode_Reward/reaching_object: 0.6951
     Episode_Reward/lifting_object: 80.1435
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 28.2917
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 2.02s
                      Time elapsed: 00:25:39
                               ETA: 00:49:56

################################################################################
                     [1m Learning iteration 679/2000 [0m                      

                       Computation: 47425 steps/s (collection: 1.984s, learning 0.089s)
             Mean action noise std: 2.16
          Mean value_function loss: 484.3790
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 48.0208
                       Mean reward: 405.76
               Mean episode length: 134.30
    Episode_Reward/reaching_object: 0.6686
     Episode_Reward/lifting_object: 77.2617
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 27.5833
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 2.07s
                      Time elapsed: 00:25:41
                               ETA: 00:49:54

################################################################################
                     [1m Learning iteration 680/2000 [0m                      

                       Computation: 47667 steps/s (collection: 1.956s, learning 0.107s)
             Mean action noise std: 2.16
          Mean value_function loss: 491.4217
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 48.0204
                       Mean reward: 410.26
               Mean episode length: 140.21
    Episode_Reward/reaching_object: 0.7078
     Episode_Reward/lifting_object: 80.3624
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 25.9583
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 2.06s
                      Time elapsed: 00:25:43
                               ETA: 00:49:51

################################################################################
                     [1m Learning iteration 681/2000 [0m                      

                       Computation: 47962 steps/s (collection: 1.954s, learning 0.096s)
             Mean action noise std: 2.16
          Mean value_function loss: 581.1930
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 48.0204
                       Mean reward: 383.59
               Mean episode length: 130.34
    Episode_Reward/reaching_object: 0.6864
     Episode_Reward/lifting_object: 78.3845
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 34.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 2.05s
                      Time elapsed: 00:25:45
                               ETA: 00:49:48

################################################################################
                     [1m Learning iteration 682/2000 [0m                      

                       Computation: 48486 steps/s (collection: 1.937s, learning 0.091s)
             Mean action noise std: 2.16
          Mean value_function loss: 698.8984
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 48.0219
                       Mean reward: 366.69
               Mean episode length: 119.31
    Episode_Reward/reaching_object: 0.6016
     Episode_Reward/lifting_object: 67.6454
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 44.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 2.03s
                      Time elapsed: 00:25:47
                               ETA: 00:49:46

################################################################################
                     [1m Learning iteration 683/2000 [0m                      

                       Computation: 47860 steps/s (collection: 1.963s, learning 0.091s)
             Mean action noise std: 2.16
          Mean value_function loss: 685.5703
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 48.0239
                       Mean reward: 332.25
               Mean episode length: 110.34
    Episode_Reward/reaching_object: 0.5623
     Episode_Reward/lifting_object: 64.1476
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 46.0417
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 2.05s
                      Time elapsed: 00:25:49
                               ETA: 00:49:43

################################################################################
                     [1m Learning iteration 684/2000 [0m                      

                       Computation: 48439 steps/s (collection: 1.933s, learning 0.096s)
             Mean action noise std: 2.16
          Mean value_function loss: 653.9124
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 48.0261
                       Mean reward: 340.35
               Mean episode length: 112.08
    Episode_Reward/reaching_object: 0.5532
     Episode_Reward/lifting_object: 64.1853
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 36.5833
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 2.03s
                      Time elapsed: 00:25:51
                               ETA: 00:49:40

################################################################################
                     [1m Learning iteration 685/2000 [0m                      

                       Computation: 47723 steps/s (collection: 1.952s, learning 0.108s)
             Mean action noise std: 2.16
          Mean value_function loss: 573.3037
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 48.0272
                       Mean reward: 287.49
               Mean episode length: 101.87
    Episode_Reward/reaching_object: 0.5869
     Episode_Reward/lifting_object: 68.2419
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 31.7917
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 2.06s
                      Time elapsed: 00:25:53
                               ETA: 00:49:38

################################################################################
                     [1m Learning iteration 686/2000 [0m                      

                       Computation: 47578 steps/s (collection: 1.975s, learning 0.092s)
             Mean action noise std: 2.16
          Mean value_function loss: 546.4157
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 48.0276
                       Mean reward: 381.10
               Mean episode length: 129.77
    Episode_Reward/reaching_object: 0.5907
     Episode_Reward/lifting_object: 67.8460
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 28.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 2.07s
                      Time elapsed: 00:25:55
                               ETA: 00:49:35

################################################################################
                     [1m Learning iteration 687/2000 [0m                      

                       Computation: 48141 steps/s (collection: 1.947s, learning 0.095s)
             Mean action noise std: 2.16
          Mean value_function loss: 521.6958
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 48.0277
                       Mean reward: 332.72
               Mean episode length: 115.98
    Episode_Reward/reaching_object: 0.5924
     Episode_Reward/lifting_object: 67.8727
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.7500
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 2.04s
                      Time elapsed: 00:25:57
                               ETA: 00:49:32

################################################################################
                     [1m Learning iteration 688/2000 [0m                      

                       Computation: 47533 steps/s (collection: 1.967s, learning 0.101s)
             Mean action noise std: 2.16
          Mean value_function loss: 574.3214
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 48.0277
                       Mean reward: 366.20
               Mean episode length: 122.89
    Episode_Reward/reaching_object: 0.6270
     Episode_Reward/lifting_object: 72.7234
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.7083
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 2.07s
                      Time elapsed: 00:25:59
                               ETA: 00:49:30

################################################################################
                     [1m Learning iteration 689/2000 [0m                      

                       Computation: 48598 steps/s (collection: 1.926s, learning 0.097s)
             Mean action noise std: 2.16
          Mean value_function loss: 565.4711
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 48.0277
                       Mean reward: 403.92
               Mean episode length: 134.11
    Episode_Reward/reaching_object: 0.6306
     Episode_Reward/lifting_object: 72.7551
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.6250
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 2.02s
                      Time elapsed: 00:26:01
                               ETA: 00:49:27

################################################################################
                     [1m Learning iteration 690/2000 [0m                      

                       Computation: 48640 steps/s (collection: 1.935s, learning 0.086s)
             Mean action noise std: 2.16
          Mean value_function loss: 549.5984
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 48.0277
                       Mean reward: 410.79
               Mean episode length: 131.89
    Episode_Reward/reaching_object: 0.6508
     Episode_Reward/lifting_object: 74.5360
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0271
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 31.0833
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 2.02s
                      Time elapsed: 00:26:03
                               ETA: 00:49:24

################################################################################
                     [1m Learning iteration 691/2000 [0m                      

                       Computation: 47514 steps/s (collection: 1.956s, learning 0.113s)
             Mean action noise std: 2.16
          Mean value_function loss: 526.8224
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 48.0276
                       Mean reward: 378.45
               Mean episode length: 128.72
    Episode_Reward/reaching_object: 0.6731
     Episode_Reward/lifting_object: 78.0915
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 27.8750
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 2.07s
                      Time elapsed: 00:26:05
                               ETA: 00:49:21

################################################################################
                     [1m Learning iteration 692/2000 [0m                      

                       Computation: 47341 steps/s (collection: 1.971s, learning 0.105s)
             Mean action noise std: 2.16
          Mean value_function loss: 495.1048
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 48.0279
                       Mean reward: 408.02
               Mean episode length: 133.09
    Episode_Reward/reaching_object: 0.6867
     Episode_Reward/lifting_object: 79.9871
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 23.6250
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 2.08s
                      Time elapsed: 00:26:07
                               ETA: 00:49:19

################################################################################
                     [1m Learning iteration 693/2000 [0m                      

                       Computation: 45314 steps/s (collection: 2.075s, learning 0.095s)
             Mean action noise std: 2.16
          Mean value_function loss: 528.9313
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 48.0283
                       Mean reward: 449.33
               Mean episode length: 145.77
    Episode_Reward/reaching_object: 0.7312
     Episode_Reward/lifting_object: 85.0455
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.5000
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 2.17s
                      Time elapsed: 00:26:10
                               ETA: 00:49:16

################################################################################
                     [1m Learning iteration 694/2000 [0m                      

                       Computation: 47006 steps/s (collection: 2.003s, learning 0.089s)
             Mean action noise std: 2.16
          Mean value_function loss: 502.4122
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 48.0280
                       Mean reward: 439.65
               Mean episode length: 142.35
    Episode_Reward/reaching_object: 0.7398
     Episode_Reward/lifting_object: 85.9133
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 24.0833
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 2.09s
                      Time elapsed: 00:26:12
                               ETA: 00:49:14

################################################################################
                     [1m Learning iteration 695/2000 [0m                      

                       Computation: 46659 steps/s (collection: 1.992s, learning 0.115s)
             Mean action noise std: 2.16
          Mean value_function loss: 471.3721
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 48.0278
                       Mean reward: 485.98
               Mean episode length: 155.96
    Episode_Reward/reaching_object: 0.7753
     Episode_Reward/lifting_object: 90.6209
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.9167
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 2.11s
                      Time elapsed: 00:26:14
                               ETA: 00:49:11

################################################################################
                     [1m Learning iteration 696/2000 [0m                      

                       Computation: 46502 steps/s (collection: 2.023s, learning 0.091s)
             Mean action noise std: 2.16
          Mean value_function loss: 497.0902
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 48.0280
                       Mean reward: 444.03
               Mean episode length: 147.82
    Episode_Reward/reaching_object: 0.7850
     Episode_Reward/lifting_object: 89.9781
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 2.11s
                      Time elapsed: 00:26:16
                               ETA: 00:49:09

################################################################################
                     [1m Learning iteration 697/2000 [0m                      

                       Computation: 46413 steps/s (collection: 2.031s, learning 0.087s)
             Mean action noise std: 2.16
          Mean value_function loss: 513.3948
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 48.0283
                       Mean reward: 472.02
               Mean episode length: 153.80
    Episode_Reward/reaching_object: 0.7812
     Episode_Reward/lifting_object: 90.5872
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 24.7917
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 2.12s
                      Time elapsed: 00:26:18
                               ETA: 00:49:06

################################################################################
                     [1m Learning iteration 698/2000 [0m                      

                       Computation: 42960 steps/s (collection: 2.176s, learning 0.112s)
             Mean action noise std: 2.16
          Mean value_function loss: 514.7286
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 48.0284
                       Mean reward: 471.29
               Mean episode length: 153.28
    Episode_Reward/reaching_object: 0.7750
     Episode_Reward/lifting_object: 87.1754
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 22.0833
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 2.29s
                      Time elapsed: 00:26:20
                               ETA: 00:49:04

################################################################################
                     [1m Learning iteration 699/2000 [0m                      

                       Computation: 47075 steps/s (collection: 1.982s, learning 0.106s)
             Mean action noise std: 2.16
          Mean value_function loss: 464.2943
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 48.0283
                       Mean reward: 484.69
               Mean episode length: 159.96
    Episode_Reward/reaching_object: 0.8435
     Episode_Reward/lifting_object: 96.5539
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 18.3750
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 2.09s
                      Time elapsed: 00:26:22
                               ETA: 00:49:01

################################################################################
                     [1m Learning iteration 700/2000 [0m                      

                       Computation: 47123 steps/s (collection: 1.999s, learning 0.088s)
             Mean action noise std: 2.16
          Mean value_function loss: 447.6846
               Mean surrogate loss: 0.0095
                 Mean entropy loss: 48.0284
                       Mean reward: 510.46
               Mean episode length: 168.10
    Episode_Reward/reaching_object: 0.8969
     Episode_Reward/lifting_object: 103.2863
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.4167
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 2.09s
                      Time elapsed: 00:26:24
                               ETA: 00:48:59

################################################################################
                     [1m Learning iteration 701/2000 [0m                      

                       Computation: 47051 steps/s (collection: 1.999s, learning 0.090s)
             Mean action noise std: 2.16
          Mean value_function loss: 417.4477
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 48.0284
                       Mean reward: 569.13
               Mean episode length: 180.99
    Episode_Reward/reaching_object: 0.9477
     Episode_Reward/lifting_object: 110.6025
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.7500
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 2.09s
                      Time elapsed: 00:26:27
                               ETA: 00:48:56

################################################################################
                     [1m Learning iteration 702/2000 [0m                      

                       Computation: 48004 steps/s (collection: 1.951s, learning 0.097s)
             Mean action noise std: 2.16
          Mean value_function loss: 423.1093
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 48.0283
                       Mean reward: 587.35
               Mean episode length: 181.92
    Episode_Reward/reaching_object: 0.9515
     Episode_Reward/lifting_object: 111.5193
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 2.05s
                      Time elapsed: 00:26:29
                               ETA: 00:48:54

################################################################################
                     [1m Learning iteration 703/2000 [0m                      

                       Computation: 47442 steps/s (collection: 1.978s, learning 0.094s)
             Mean action noise std: 2.16
          Mean value_function loss: 408.9827
               Mean surrogate loss: 0.0142
                 Mean entropy loss: 48.0283
                       Mean reward: 574.04
               Mean episode length: 176.81
    Episode_Reward/reaching_object: 0.9296
     Episode_Reward/lifting_object: 109.0577
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.2083
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 2.07s
                      Time elapsed: 00:26:31
                               ETA: 00:48:51

################################################################################
                     [1m Learning iteration 704/2000 [0m                      

                       Computation: 48915 steps/s (collection: 1.914s, learning 0.096s)
             Mean action noise std: 2.16
          Mean value_function loss: 460.4718
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 48.0285
                       Mean reward: 536.17
               Mean episode length: 170.82
    Episode_Reward/reaching_object: 0.9799
     Episode_Reward/lifting_object: 114.7057
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.1667
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 2.01s
                      Time elapsed: 00:26:33
                               ETA: 00:48:48

################################################################################
                     [1m Learning iteration 705/2000 [0m                      

                       Computation: 47045 steps/s (collection: 1.977s, learning 0.112s)
             Mean action noise std: 2.16
          Mean value_function loss: 436.6618
               Mean surrogate loss: 0.0119
                 Mean entropy loss: 48.0290
                       Mean reward: 578.65
               Mean episode length: 174.85
    Episode_Reward/reaching_object: 0.9618
     Episode_Reward/lifting_object: 115.0143
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 2.09s
                      Time elapsed: 00:26:35
                               ETA: 00:48:46

################################################################################
                     [1m Learning iteration 706/2000 [0m                      

                       Computation: 48196 steps/s (collection: 1.930s, learning 0.110s)
             Mean action noise std: 2.16
          Mean value_function loss: 413.2117
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 48.0292
                       Mean reward: 608.40
               Mean episode length: 183.74
    Episode_Reward/reaching_object: 1.0125
     Episode_Reward/lifting_object: 120.6515
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 2.04s
                      Time elapsed: 00:26:37
                               ETA: 00:48:43

################################################################################
                     [1m Learning iteration 707/2000 [0m                      

                       Computation: 47929 steps/s (collection: 1.946s, learning 0.105s)
             Mean action noise std: 2.16
          Mean value_function loss: 407.6932
               Mean surrogate loss: 0.0105
                 Mean entropy loss: 48.0292
                       Mean reward: 594.68
               Mean episode length: 179.78
    Episode_Reward/reaching_object: 0.9772
     Episode_Reward/lifting_object: 115.3913
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 2.05s
                      Time elapsed: 00:26:39
                               ETA: 00:48:40

################################################################################
                     [1m Learning iteration 708/2000 [0m                      

                       Computation: 48583 steps/s (collection: 1.938s, learning 0.086s)
             Mean action noise std: 2.16
          Mean value_function loss: 412.7458
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 48.0291
                       Mean reward: 667.31
               Mean episode length: 201.53
    Episode_Reward/reaching_object: 1.0440
     Episode_Reward/lifting_object: 125.0812
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 2.02s
                      Time elapsed: 00:26:41
                               ETA: 00:48:38

################################################################################
                     [1m Learning iteration 709/2000 [0m                      

                       Computation: 48389 steps/s (collection: 1.946s, learning 0.086s)
             Mean action noise std: 2.16
          Mean value_function loss: 412.6892
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 48.0292
                       Mean reward: 533.39
               Mean episode length: 166.71
    Episode_Reward/reaching_object: 0.9701
     Episode_Reward/lifting_object: 113.4662
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.4583
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 2.03s
                      Time elapsed: 00:26:43
                               ETA: 00:48:35

################################################################################
                     [1m Learning iteration 710/2000 [0m                      

                       Computation: 47964 steps/s (collection: 1.963s, learning 0.086s)
             Mean action noise std: 2.16
          Mean value_function loss: 418.7097
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 48.0291
                       Mean reward: 517.74
               Mean episode length: 165.37
    Episode_Reward/reaching_object: 0.9293
     Episode_Reward/lifting_object: 107.5158
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 2.05s
                      Time elapsed: 00:26:45
                               ETA: 00:48:32

################################################################################
                     [1m Learning iteration 711/2000 [0m                      

                       Computation: 47910 steps/s (collection: 1.966s, learning 0.086s)
             Mean action noise std: 2.16
          Mean value_function loss: 452.9425
               Mean surrogate loss: 0.0106
                 Mean entropy loss: 48.0288
                       Mean reward: 474.28
               Mean episode length: 152.99
    Episode_Reward/reaching_object: 0.8584
     Episode_Reward/lifting_object: 99.2536
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 19.8333
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 2.05s
                      Time elapsed: 00:26:47
                               ETA: 00:48:30

################################################################################
                     [1m Learning iteration 712/2000 [0m                      

                       Computation: 47136 steps/s (collection: 1.998s, learning 0.087s)
             Mean action noise std: 2.16
          Mean value_function loss: 518.9919
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 48.0287
                       Mean reward: 402.35
               Mean episode length: 132.64
    Episode_Reward/reaching_object: 0.7480
     Episode_Reward/lifting_object: 84.1723
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 24.1250
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 2.09s
                      Time elapsed: 00:26:49
                               ETA: 00:48:27

################################################################################
                     [1m Learning iteration 713/2000 [0m                      

                       Computation: 48396 steps/s (collection: 1.943s, learning 0.088s)
             Mean action noise std: 2.16
          Mean value_function loss: 556.0891
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 48.0287
                       Mean reward: 411.29
               Mean episode length: 134.68
    Episode_Reward/reaching_object: 0.7257
     Episode_Reward/lifting_object: 81.1775
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 26.7500
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 2.03s
                      Time elapsed: 00:26:51
                               ETA: 00:48:25

################################################################################
                     [1m Learning iteration 714/2000 [0m                      

                       Computation: 47684 steps/s (collection: 1.969s, learning 0.093s)
             Mean action noise std: 2.16
          Mean value_function loss: 530.0564
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 48.0287
                       Mean reward: 457.00
               Mean episode length: 149.77
    Episode_Reward/reaching_object: 0.7923
     Episode_Reward/lifting_object: 88.9439
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 23.7083
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 2.06s
                      Time elapsed: 00:26:53
                               ETA: 00:48:22

################################################################################
                     [1m Learning iteration 715/2000 [0m                      

                       Computation: 47442 steps/s (collection: 1.965s, learning 0.107s)
             Mean action noise std: 2.16
          Mean value_function loss: 505.3402
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 48.0283
                       Mean reward: 481.33
               Mean episode length: 156.90
    Episode_Reward/reaching_object: 0.8356
     Episode_Reward/lifting_object: 94.5587
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 18.5417
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 2.07s
                      Time elapsed: 00:26:55
                               ETA: 00:48:19

################################################################################
                     [1m Learning iteration 716/2000 [0m                      

                       Computation: 47418 steps/s (collection: 1.985s, learning 0.089s)
             Mean action noise std: 2.16
          Mean value_function loss: 430.5612
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 48.0261
                       Mean reward: 530.77
               Mean episode length: 171.61
    Episode_Reward/reaching_object: 0.8926
     Episode_Reward/lifting_object: 101.7681
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 2.07s
                      Time elapsed: 00:26:57
                               ETA: 00:48:17

################################################################################
                     [1m Learning iteration 717/2000 [0m                      

                       Computation: 48107 steps/s (collection: 1.955s, learning 0.088s)
             Mean action noise std: 2.16
          Mean value_function loss: 363.9198
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 48.0258
                       Mean reward: 567.37
               Mean episode length: 175.67
    Episode_Reward/reaching_object: 1.0096
     Episode_Reward/lifting_object: 117.5236
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 2.04s
                      Time elapsed: 00:26:59
                               ETA: 00:48:14

################################################################################
                     [1m Learning iteration 718/2000 [0m                      

                       Computation: 48242 steps/s (collection: 1.952s, learning 0.086s)
             Mean action noise std: 2.16
          Mean value_function loss: 352.7663
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 48.0264
                       Mean reward: 605.40
               Mean episode length: 188.64
    Episode_Reward/reaching_object: 1.0568
     Episode_Reward/lifting_object: 124.0115
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 2.04s
                      Time elapsed: 00:27:01
                               ETA: 00:48:11

################################################################################
                     [1m Learning iteration 719/2000 [0m                      

                       Computation: 48089 steps/s (collection: 1.957s, learning 0.087s)
             Mean action noise std: 2.16
          Mean value_function loss: 366.6853
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 48.0272
                       Mean reward: 597.32
               Mean episode length: 188.43
    Episode_Reward/reaching_object: 0.9944
     Episode_Reward/lifting_object: 114.3349
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 2.04s
                      Time elapsed: 00:27:03
                               ETA: 00:48:09

################################################################################
                     [1m Learning iteration 720/2000 [0m                      

                       Computation: 47256 steps/s (collection: 1.971s, learning 0.110s)
             Mean action noise std: 2.16
          Mean value_function loss: 392.5186
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 48.0280
                       Mean reward: 558.77
               Mean episode length: 171.94
    Episode_Reward/reaching_object: 0.9545
     Episode_Reward/lifting_object: 110.4824
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 2.08s
                      Time elapsed: 00:27:06
                               ETA: 00:48:06

################################################################################
                     [1m Learning iteration 721/2000 [0m                      

                       Computation: 47945 steps/s (collection: 1.954s, learning 0.096s)
             Mean action noise std: 2.16
          Mean value_function loss: 422.3741
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 48.0286
                       Mean reward: 580.87
               Mean episode length: 181.38
    Episode_Reward/reaching_object: 0.9750
     Episode_Reward/lifting_object: 112.7878
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.9167
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 2.05s
                      Time elapsed: 00:27:08
                               ETA: 00:48:04

################################################################################
                     [1m Learning iteration 722/2000 [0m                      

                       Computation: 47575 steps/s (collection: 1.974s, learning 0.092s)
             Mean action noise std: 2.16
          Mean value_function loss: 477.4479
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 48.0293
                       Mean reward: 576.10
               Mean episode length: 174.69
    Episode_Reward/reaching_object: 1.0044
     Episode_Reward/lifting_object: 118.1167
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 20.4167
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 2.07s
                      Time elapsed: 00:27:10
                               ETA: 00:48:01

################################################################################
                     [1m Learning iteration 723/2000 [0m                      

                       Computation: 47430 steps/s (collection: 1.981s, learning 0.092s)
             Mean action noise std: 2.16
          Mean value_function loss: 469.4756
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 48.0307
                       Mean reward: 595.30
               Mean episode length: 183.24
    Episode_Reward/reaching_object: 1.0145
     Episode_Reward/lifting_object: 120.1253
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0350
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 20.7500
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 2.07s
                      Time elapsed: 00:27:12
                               ETA: 00:47:59

################################################################################
                     [1m Learning iteration 724/2000 [0m                      

                       Computation: 48597 steps/s (collection: 1.933s, learning 0.090s)
             Mean action noise std: 2.16
          Mean value_function loss: 489.9949
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 48.0312
                       Mean reward: 572.49
               Mean episode length: 176.32
    Episode_Reward/reaching_object: 0.9726
     Episode_Reward/lifting_object: 114.8868
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 22.3750
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 2.02s
                      Time elapsed: 00:27:14
                               ETA: 00:47:56

################################################################################
                     [1m Learning iteration 725/2000 [0m                      

                       Computation: 48233 steps/s (collection: 1.941s, learning 0.097s)
             Mean action noise std: 2.16
          Mean value_function loss: 440.6172
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 48.0304
                       Mean reward: 476.55
               Mean episode length: 154.10
    Episode_Reward/reaching_object: 0.9385
     Episode_Reward/lifting_object: 109.4621
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 18.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 2.04s
                      Time elapsed: 00:27:16
                               ETA: 00:47:53

################################################################################
                     [1m Learning iteration 726/2000 [0m                      

                       Computation: 48284 steps/s (collection: 1.948s, learning 0.088s)
             Mean action noise std: 2.16
          Mean value_function loss: 432.8702
               Mean surrogate loss: 0.0123
                 Mean entropy loss: 48.0293
                       Mean reward: 554.59
               Mean episode length: 171.46
    Episode_Reward/reaching_object: 0.8893
     Episode_Reward/lifting_object: 103.7498
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 17.1667
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 2.04s
                      Time elapsed: 00:27:18
                               ETA: 00:47:51

################################################################################
                     [1m Learning iteration 727/2000 [0m                      

                       Computation: 48064 steps/s (collection: 1.946s, learning 0.099s)
             Mean action noise std: 2.16
          Mean value_function loss: 403.9023
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 48.0293
                       Mean reward: 482.33
               Mean episode length: 156.81
    Episode_Reward/reaching_object: 0.8987
     Episode_Reward/lifting_object: 103.3709
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 4.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 2.05s
                      Time elapsed: 00:27:20
                               ETA: 00:47:48

################################################################################
                     [1m Learning iteration 728/2000 [0m                      

                       Computation: 48018 steps/s (collection: 1.955s, learning 0.092s)
             Mean action noise std: 2.16
          Mean value_function loss: 376.3249
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 48.0297
                       Mean reward: 552.32
               Mean episode length: 171.87
    Episode_Reward/reaching_object: 0.9184
     Episode_Reward/lifting_object: 106.5574
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 4.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 2.05s
                      Time elapsed: 00:27:22
                               ETA: 00:47:45

################################################################################
                     [1m Learning iteration 729/2000 [0m                      

                       Computation: 48137 steps/s (collection: 1.954s, learning 0.089s)
             Mean action noise std: 2.16
          Mean value_function loss: 383.5942
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 48.0300
                       Mean reward: 596.52
               Mean episode length: 181.42
    Episode_Reward/reaching_object: 0.9400
     Episode_Reward/lifting_object: 108.3311
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 2.04s
                      Time elapsed: 00:27:24
                               ETA: 00:47:43

################################################################################
                     [1m Learning iteration 730/2000 [0m                      

                       Computation: 48558 steps/s (collection: 1.937s, learning 0.088s)
             Mean action noise std: 2.16
          Mean value_function loss: 393.3477
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 48.0299
                       Mean reward: 566.22
               Mean episode length: 173.64
    Episode_Reward/reaching_object: 0.9628
     Episode_Reward/lifting_object: 111.5603
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0348
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.7917
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 2.02s
                      Time elapsed: 00:27:26
                               ETA: 00:47:40

################################################################################
                     [1m Learning iteration 731/2000 [0m                      

                       Computation: 47929 steps/s (collection: 1.955s, learning 0.096s)
             Mean action noise std: 2.16
          Mean value_function loss: 397.8433
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 48.0295
                       Mean reward: 647.83
               Mean episode length: 194.20
    Episode_Reward/reaching_object: 1.0340
     Episode_Reward/lifting_object: 121.9015
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 2.05s
                      Time elapsed: 00:27:28
                               ETA: 00:47:37

################################################################################
                     [1m Learning iteration 732/2000 [0m                      

                       Computation: 48222 steps/s (collection: 1.948s, learning 0.091s)
             Mean action noise std: 2.16
          Mean value_function loss: 399.4207
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 48.0301
                       Mean reward: 629.26
               Mean episode length: 190.87
    Episode_Reward/reaching_object: 1.0839
     Episode_Reward/lifting_object: 127.9100
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 2.04s
                      Time elapsed: 00:27:30
                               ETA: 00:47:35

################################################################################
                     [1m Learning iteration 733/2000 [0m                      

                       Computation: 48943 steps/s (collection: 1.922s, learning 0.086s)
             Mean action noise std: 2.16
          Mean value_function loss: 423.5362
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 48.0303
                       Mean reward: 667.66
               Mean episode length: 195.08
    Episode_Reward/reaching_object: 1.0622
     Episode_Reward/lifting_object: 126.9384
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.4583
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 2.01s
                      Time elapsed: 00:27:32
                               ETA: 00:47:32

################################################################################
                     [1m Learning iteration 734/2000 [0m                      

                       Computation: 47884 steps/s (collection: 1.953s, learning 0.100s)
             Mean action noise std: 2.16
          Mean value_function loss: 426.5992
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 48.0299
                       Mean reward: 592.64
               Mean episode length: 181.95
    Episode_Reward/reaching_object: 1.0556
     Episode_Reward/lifting_object: 125.1746
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.2500
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 2.05s
                      Time elapsed: 00:27:34
                               ETA: 00:47:30

################################################################################
                     [1m Learning iteration 735/2000 [0m                      

                       Computation: 47755 steps/s (collection: 1.951s, learning 0.107s)
             Mean action noise std: 2.16
          Mean value_function loss: 480.4588
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 48.0291
                       Mean reward: 508.59
               Mean episode length: 160.77
    Episode_Reward/reaching_object: 0.9822
     Episode_Reward/lifting_object: 115.5125
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.4583
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 2.06s
                      Time elapsed: 00:27:36
                               ETA: 00:47:27

################################################################################
                     [1m Learning iteration 736/2000 [0m                      

                       Computation: 48235 steps/s (collection: 1.936s, learning 0.102s)
             Mean action noise std: 2.16
          Mean value_function loss: 472.9924
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 48.0276
                       Mean reward: 489.87
               Mean episode length: 156.17
    Episode_Reward/reaching_object: 0.8831
     Episode_Reward/lifting_object: 99.7254
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 21.1667
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 2.04s
                      Time elapsed: 00:27:38
                               ETA: 00:47:24

################################################################################
                     [1m Learning iteration 737/2000 [0m                      

                       Computation: 48431 steps/s (collection: 1.936s, learning 0.094s)
             Mean action noise std: 2.16
          Mean value_function loss: 463.3826
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 48.0269
                       Mean reward: 449.01
               Mean episode length: 150.26
    Episode_Reward/reaching_object: 0.8585
     Episode_Reward/lifting_object: 97.3408
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 18.6250
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 2.03s
                      Time elapsed: 00:27:40
                               ETA: 00:47:22

################################################################################
                     [1m Learning iteration 738/2000 [0m                      

                       Computation: 48481 steps/s (collection: 1.940s, learning 0.088s)
             Mean action noise std: 2.16
          Mean value_function loss: 425.2178
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 48.0267
                       Mean reward: 453.45
               Mean episode length: 155.42
    Episode_Reward/reaching_object: 0.7591
     Episode_Reward/lifting_object: 81.7544
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.3750
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 2.03s
                      Time elapsed: 00:27:42
                               ETA: 00:47:19

################################################################################
                     [1m Learning iteration 739/2000 [0m                      

                       Computation: 48258 steps/s (collection: 1.942s, learning 0.096s)
             Mean action noise std: 2.16
          Mean value_function loss: 451.6268
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 48.0255
                       Mean reward: 448.15
               Mean episode length: 154.01
    Episode_Reward/reaching_object: 0.8039
     Episode_Reward/lifting_object: 87.3020
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 18.7917
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 2.04s
                      Time elapsed: 00:27:44
                               ETA: 00:47:17

################################################################################
                     [1m Learning iteration 740/2000 [0m                      

                       Computation: 48233 steps/s (collection: 1.946s, learning 0.093s)
             Mean action noise std: 2.16
          Mean value_function loss: 453.3750
               Mean surrogate loss: 0.0098
                 Mean entropy loss: 48.0224
                       Mean reward: 485.42
               Mean episode length: 158.81
    Episode_Reward/reaching_object: 0.8307
     Episode_Reward/lifting_object: 91.6790
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 18.0833
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 2.04s
                      Time elapsed: 00:27:46
                               ETA: 00:47:14

################################################################################
                     [1m Learning iteration 741/2000 [0m                      

                       Computation: 48592 steps/s (collection: 1.936s, learning 0.087s)
             Mean action noise std: 2.16
          Mean value_function loss: 421.2859
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 48.0219
                       Mean reward: 479.85
               Mean episode length: 163.16
    Episode_Reward/reaching_object: 0.8717
     Episode_Reward/lifting_object: 96.5013
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.5417
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 2.02s
                      Time elapsed: 00:27:48
                               ETA: 00:47:11

################################################################################
                     [1m Learning iteration 742/2000 [0m                      

                       Computation: 48361 steps/s (collection: 1.942s, learning 0.091s)
             Mean action noise std: 2.16
          Mean value_function loss: 404.7596
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 48.0216
                       Mean reward: 488.84
               Mean episode length: 162.35
    Episode_Reward/reaching_object: 0.9010
     Episode_Reward/lifting_object: 100.7852
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 2.03s
                      Time elapsed: 00:27:50
                               ETA: 00:47:09

################################################################################
                     [1m Learning iteration 743/2000 [0m                      

                       Computation: 48662 steps/s (collection: 1.928s, learning 0.092s)
             Mean action noise std: 2.16
          Mean value_function loss: 373.6826
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 48.0214
                       Mean reward: 549.51
               Mean episode length: 185.58
    Episode_Reward/reaching_object: 0.9902
     Episode_Reward/lifting_object: 111.6846
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 2.02s
                      Time elapsed: 00:27:52
                               ETA: 00:47:06

################################################################################
                     [1m Learning iteration 744/2000 [0m                      

                       Computation: 48233 steps/s (collection: 1.948s, learning 0.090s)
             Mean action noise std: 2.16
          Mean value_function loss: 373.2292
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 48.0213
                       Mean reward: 608.71
               Mean episode length: 192.41
    Episode_Reward/reaching_object: 1.0431
     Episode_Reward/lifting_object: 118.6237
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 2.04s
                      Time elapsed: 00:27:55
                               ETA: 00:47:03

################################################################################
                     [1m Learning iteration 745/2000 [0m                      

                       Computation: 48660 steps/s (collection: 1.931s, learning 0.090s)
             Mean action noise std: 2.16
          Mean value_function loss: 364.8064
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 48.0213
                       Mean reward: 678.87
               Mean episode length: 209.55
    Episode_Reward/reaching_object: 1.0931
     Episode_Reward/lifting_object: 126.3814
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 2.02s
                      Time elapsed: 00:27:57
                               ETA: 00:47:01

################################################################################
                     [1m Learning iteration 746/2000 [0m                      

                       Computation: 48756 steps/s (collection: 1.925s, learning 0.092s)
             Mean action noise std: 2.16
          Mean value_function loss: 389.9828
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 48.0213
                       Mean reward: 614.88
               Mean episode length: 191.94
    Episode_Reward/reaching_object: 1.0984
     Episode_Reward/lifting_object: 127.8005
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 2.02s
                      Time elapsed: 00:27:59
                               ETA: 00:46:58

################################################################################
                     [1m Learning iteration 747/2000 [0m                      

                       Computation: 45921 steps/s (collection: 2.054s, learning 0.087s)
             Mean action noise std: 2.16
          Mean value_function loss: 406.1485
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 48.0210
                       Mean reward: 614.87
               Mean episode length: 188.90
    Episode_Reward/reaching_object: 1.0437
     Episode_Reward/lifting_object: 121.9477
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.3333
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 2.14s
                      Time elapsed: 00:28:01
                               ETA: 00:46:56

################################################################################
                     [1m Learning iteration 748/2000 [0m                      

                       Computation: 48700 steps/s (collection: 1.926s, learning 0.093s)
             Mean action noise std: 2.16
          Mean value_function loss: 414.4343
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 48.0207
                       Mean reward: 590.73
               Mean episode length: 183.13
    Episode_Reward/reaching_object: 1.0209
     Episode_Reward/lifting_object: 118.5237
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 16.0833
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 2.02s
                      Time elapsed: 00:28:03
                               ETA: 00:46:53

################################################################################
                     [1m Learning iteration 749/2000 [0m                      

                       Computation: 47936 steps/s (collection: 1.945s, learning 0.106s)
             Mean action noise std: 2.16
          Mean value_function loss: 382.3260
               Mean surrogate loss: 0.0101
                 Mean entropy loss: 48.0207
                       Mean reward: 618.66
               Mean episode length: 188.80
    Episode_Reward/reaching_object: 1.0238
     Episode_Reward/lifting_object: 119.7705
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.0417
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 2.05s
                      Time elapsed: 00:28:05
                               ETA: 00:46:51

################################################################################
                     [1m Learning iteration 750/2000 [0m                      

                       Computation: 47626 steps/s (collection: 1.960s, learning 0.104s)
             Mean action noise std: 2.16
          Mean value_function loss: 358.2103
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 48.0205
                       Mean reward: 666.86
               Mean episode length: 203.46
    Episode_Reward/reaching_object: 1.0820
     Episode_Reward/lifting_object: 128.0570
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 2.06s
                      Time elapsed: 00:28:07
                               ETA: 00:46:48

################################################################################
                     [1m Learning iteration 751/2000 [0m                      

                       Computation: 46857 steps/s (collection: 2.007s, learning 0.091s)
             Mean action noise std: 2.16
          Mean value_function loss: 334.8139
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 48.0192
                       Mean reward: 671.05
               Mean episode length: 200.65
    Episode_Reward/reaching_object: 1.0920
     Episode_Reward/lifting_object: 129.1486
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 2.10s
                      Time elapsed: 00:28:09
                               ETA: 00:46:45

################################################################################
                     [1m Learning iteration 752/2000 [0m                      

                       Computation: 46890 steps/s (collection: 2.003s, learning 0.093s)
             Mean action noise std: 2.16
          Mean value_function loss: 314.3127
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 48.0182
                       Mean reward: 657.55
               Mean episode length: 198.10
    Episode_Reward/reaching_object: 1.1228
     Episode_Reward/lifting_object: 134.9600
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 2.10s
                      Time elapsed: 00:28:11
                               ETA: 00:46:43

################################################################################
                     [1m Learning iteration 753/2000 [0m                      

                       Computation: 48589 steps/s (collection: 1.932s, learning 0.092s)
             Mean action noise std: 2.16
          Mean value_function loss: 322.7946
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 48.0182
                       Mean reward: 673.71
               Mean episode length: 199.13
    Episode_Reward/reaching_object: 1.1060
     Episode_Reward/lifting_object: 132.5788
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 2.02s
                      Time elapsed: 00:28:13
                               ETA: 00:46:40

################################################################################
                     [1m Learning iteration 754/2000 [0m                      

                       Computation: 48492 steps/s (collection: 1.937s, learning 0.091s)
             Mean action noise std: 2.16
          Mean value_function loss: 320.3022
               Mean surrogate loss: 0.0095
                 Mean entropy loss: 48.0183
                       Mean reward: 721.63
               Mean episode length: 212.57
    Episode_Reward/reaching_object: 1.1324
     Episode_Reward/lifting_object: 134.6124
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 2.03s
                      Time elapsed: 00:28:15
                               ETA: 00:46:38

################################################################################
                     [1m Learning iteration 755/2000 [0m                      

                       Computation: 44020 steps/s (collection: 2.085s, learning 0.149s)
             Mean action noise std: 2.16
          Mean value_function loss: 305.3026
               Mean surrogate loss: 0.0100
                 Mean entropy loss: 48.0184
                       Mean reward: 692.34
               Mean episode length: 209.19
    Episode_Reward/reaching_object: 1.1860
     Episode_Reward/lifting_object: 141.3869
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 2.23s
                      Time elapsed: 00:28:17
                               ETA: 00:46:35

################################################################################
                     [1m Learning iteration 756/2000 [0m                      

                       Computation: 41933 steps/s (collection: 2.166s, learning 0.178s)
             Mean action noise std: 2.16
          Mean value_function loss: 314.0264
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 48.0185
                       Mean reward: 726.98
               Mean episode length: 211.30
    Episode_Reward/reaching_object: 1.1939
     Episode_Reward/lifting_object: 144.2017
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 2.34s
                      Time elapsed: 00:28:20
                               ETA: 00:46:33

################################################################################
                     [1m Learning iteration 757/2000 [0m                      

                       Computation: 46673 steps/s (collection: 2.017s, learning 0.090s)
             Mean action noise std: 2.16
          Mean value_function loss: 304.2153
               Mean surrogate loss: 0.0129
                 Mean entropy loss: 48.0186
                       Mean reward: 733.31
               Mean episode length: 211.53
    Episode_Reward/reaching_object: 1.1845
     Episode_Reward/lifting_object: 143.0229
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 2.11s
                      Time elapsed: 00:28:22
                               ETA: 00:46:31

################################################################################
                     [1m Learning iteration 758/2000 [0m                      

                       Computation: 45562 steps/s (collection: 2.002s, learning 0.156s)
             Mean action noise std: 2.16
          Mean value_function loss: 314.7905
               Mean surrogate loss: 0.0098
                 Mean entropy loss: 48.0186
                       Mean reward: 756.10
               Mean episode length: 215.17
    Episode_Reward/reaching_object: 1.2153
     Episode_Reward/lifting_object: 147.4575
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 2.16s
                      Time elapsed: 00:28:24
                               ETA: 00:46:29

################################################################################
                     [1m Learning iteration 759/2000 [0m                      

                       Computation: 47396 steps/s (collection: 1.989s, learning 0.086s)
             Mean action noise std: 2.16
          Mean value_function loss: 308.1997
               Mean surrogate loss: 0.0115
                 Mean entropy loss: 48.0186
                       Mean reward: 806.29
               Mean episode length: 228.49
    Episode_Reward/reaching_object: 1.2492
     Episode_Reward/lifting_object: 153.4606
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 2.07s
                      Time elapsed: 00:28:26
                               ETA: 00:46:26

################################################################################
                     [1m Learning iteration 760/2000 [0m                      

                       Computation: 46580 steps/s (collection: 2.006s, learning 0.105s)
             Mean action noise std: 2.16
          Mean value_function loss: 316.9236
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 48.0187
                       Mean reward: 720.03
               Mean episode length: 206.45
    Episode_Reward/reaching_object: 1.2226
     Episode_Reward/lifting_object: 149.4535
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 2.11s
                      Time elapsed: 00:28:28
                               ETA: 00:46:24

################################################################################
                     [1m Learning iteration 761/2000 [0m                      

                       Computation: 42763 steps/s (collection: 2.175s, learning 0.124s)
             Mean action noise std: 2.16
          Mean value_function loss: 312.6977
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 48.0187
                       Mean reward: 768.64
               Mean episode length: 218.11
    Episode_Reward/reaching_object: 1.2201
     Episode_Reward/lifting_object: 150.0826
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 2.30s
                      Time elapsed: 00:28:30
                               ETA: 00:46:21

################################################################################
                     [1m Learning iteration 762/2000 [0m                      

                       Computation: 48258 steps/s (collection: 1.942s, learning 0.096s)
             Mean action noise std: 2.16
          Mean value_function loss: 313.1554
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 48.0188
                       Mean reward: 703.88
               Mean episode length: 206.37
    Episode_Reward/reaching_object: 1.1546
     Episode_Reward/lifting_object: 140.2686
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 2.04s
                      Time elapsed: 00:28:32
                               ETA: 00:46:19

################################################################################
                     [1m Learning iteration 763/2000 [0m                      

                       Computation: 47072 steps/s (collection: 1.990s, learning 0.098s)
             Mean action noise std: 2.16
          Mean value_function loss: 296.3863
               Mean surrogate loss: 0.0143
                 Mean entropy loss: 48.0189
                       Mean reward: 693.68
               Mean episode length: 201.06
    Episode_Reward/reaching_object: 1.1919
     Episode_Reward/lifting_object: 145.6466
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 2.09s
                      Time elapsed: 00:28:35
                               ETA: 00:46:16

################################################################################
                     [1m Learning iteration 764/2000 [0m                      

                       Computation: 44930 steps/s (collection: 2.036s, learning 0.152s)
             Mean action noise std: 2.16
          Mean value_function loss: 309.2791
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 48.0190
                       Mean reward: 673.54
               Mean episode length: 196.72
    Episode_Reward/reaching_object: 1.1691
     Episode_Reward/lifting_object: 142.6657
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 2.19s
                      Time elapsed: 00:28:37
                               ETA: 00:46:14

################################################################################
                     [1m Learning iteration 765/2000 [0m                      

                       Computation: 45429 steps/s (collection: 2.069s, learning 0.095s)
             Mean action noise std: 2.16
          Mean value_function loss: 306.7072
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 48.0190
                       Mean reward: 734.43
               Mean episode length: 210.35
    Episode_Reward/reaching_object: 1.2021
     Episode_Reward/lifting_object: 147.7546
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 2.16s
                      Time elapsed: 00:28:39
                               ETA: 00:46:12

################################################################################
                     [1m Learning iteration 766/2000 [0m                      

                       Computation: 45265 steps/s (collection: 2.051s, learning 0.121s)
             Mean action noise std: 2.16
          Mean value_function loss: 305.0986
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 48.0190
                       Mean reward: 762.36
               Mean episode length: 217.00
    Episode_Reward/reaching_object: 1.1811
     Episode_Reward/lifting_object: 143.7728
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 2.17s
                      Time elapsed: 00:28:41
                               ETA: 00:46:09

################################################################################
                     [1m Learning iteration 767/2000 [0m                      

                       Computation: 45634 steps/s (collection: 2.051s, learning 0.103s)
             Mean action noise std: 2.16
          Mean value_function loss: 319.2762
               Mean surrogate loss: 0.0109
                 Mean entropy loss: 48.0190
                       Mean reward: 757.03
               Mean episode length: 218.24
    Episode_Reward/reaching_object: 1.2429
     Episode_Reward/lifting_object: 151.7945
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 2.15s
                      Time elapsed: 00:28:43
                               ETA: 00:46:07

################################################################################
                     [1m Learning iteration 768/2000 [0m                      

                       Computation: 44930 steps/s (collection: 2.071s, learning 0.117s)
             Mean action noise std: 2.16
          Mean value_function loss: 316.3686
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 48.0190
                       Mean reward: 778.32
               Mean episode length: 221.77
    Episode_Reward/reaching_object: 1.2090
     Episode_Reward/lifting_object: 148.5925
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 2.19s
                      Time elapsed: 00:28:45
                               ETA: 00:46:05

################################################################################
                     [1m Learning iteration 769/2000 [0m                      

                       Computation: 46351 steps/s (collection: 2.021s, learning 0.100s)
             Mean action noise std: 2.16
          Mean value_function loss: 309.4341
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 48.0190
                       Mean reward: 768.95
               Mean episode length: 217.90
    Episode_Reward/reaching_object: 1.2314
     Episode_Reward/lifting_object: 150.6503
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 2.12s
                      Time elapsed: 00:28:48
                               ETA: 00:46:02

################################################################################
                     [1m Learning iteration 770/2000 [0m                      

                       Computation: 44953 steps/s (collection: 2.095s, learning 0.092s)
             Mean action noise std: 2.16
          Mean value_function loss: 306.9188
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 48.0191
                       Mean reward: 736.43
               Mean episode length: 209.39
    Episode_Reward/reaching_object: 1.2294
     Episode_Reward/lifting_object: 151.3070
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 2.19s
                      Time elapsed: 00:28:50
                               ETA: 00:46:00

################################################################################
                     [1m Learning iteration 771/2000 [0m                      

                       Computation: 46934 steps/s (collection: 1.968s, learning 0.127s)
             Mean action noise std: 2.16
          Mean value_function loss: 302.3688
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 48.0192
                       Mean reward: 755.54
               Mean episode length: 214.32
    Episode_Reward/reaching_object: 1.2106
     Episode_Reward/lifting_object: 148.8372
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 2.09s
                      Time elapsed: 00:28:52
                               ETA: 00:45:57

################################################################################
                     [1m Learning iteration 772/2000 [0m                      

                       Computation: 42330 steps/s (collection: 2.216s, learning 0.106s)
             Mean action noise std: 2.16
          Mean value_function loss: 281.2510
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 48.0193
                       Mean reward: 794.38
               Mean episode length: 221.94
    Episode_Reward/reaching_object: 1.2307
     Episode_Reward/lifting_object: 151.3785
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 2.32s
                      Time elapsed: 00:28:54
                               ETA: 00:45:55

################################################################################
                     [1m Learning iteration 773/2000 [0m                      

                       Computation: 47189 steps/s (collection: 1.966s, learning 0.117s)
             Mean action noise std: 2.16
          Mean value_function loss: 296.7039
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 48.0193
                       Mean reward: 654.78
               Mean episode length: 191.71
    Episode_Reward/reaching_object: 1.1906
     Episode_Reward/lifting_object: 145.6537
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 2.08s
                      Time elapsed: 00:28:56
                               ETA: 00:45:53

################################################################################
                     [1m Learning iteration 774/2000 [0m                      

                       Computation: 45867 steps/s (collection: 2.039s, learning 0.104s)
             Mean action noise std: 2.16
          Mean value_function loss: 302.5955
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 48.0191
                       Mean reward: 735.13
               Mean episode length: 208.21
    Episode_Reward/reaching_object: 1.1661
     Episode_Reward/lifting_object: 142.4262
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 2.14s
                      Time elapsed: 00:28:58
                               ETA: 00:45:50

################################################################################
                     [1m Learning iteration 775/2000 [0m                      

                       Computation: 42250 steps/s (collection: 2.228s, learning 0.099s)
             Mean action noise std: 2.16
          Mean value_function loss: 304.1981
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 48.0195
                       Mean reward: 739.81
               Mean episode length: 210.82
    Episode_Reward/reaching_object: 1.1951
     Episode_Reward/lifting_object: 145.6181
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 2.33s
                      Time elapsed: 00:29:01
                               ETA: 00:45:48

################################################################################
                     [1m Learning iteration 776/2000 [0m                      

                       Computation: 45729 steps/s (collection: 2.055s, learning 0.095s)
             Mean action noise std: 2.16
          Mean value_function loss: 302.9521
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 48.0197
                       Mean reward: 748.43
               Mean episode length: 216.61
    Episode_Reward/reaching_object: 1.1833
     Episode_Reward/lifting_object: 143.3994
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 2.15s
                      Time elapsed: 00:29:03
                               ETA: 00:45:46

################################################################################
                     [1m Learning iteration 777/2000 [0m                      

                       Computation: 46681 steps/s (collection: 1.997s, learning 0.109s)
             Mean action noise std: 2.16
          Mean value_function loss: 298.2234
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 48.0197
                       Mean reward: 746.53
               Mean episode length: 214.44
    Episode_Reward/reaching_object: 1.2159
     Episode_Reward/lifting_object: 149.6040
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 2.11s
                      Time elapsed: 00:29:05
                               ETA: 00:45:43

################################################################################
                     [1m Learning iteration 778/2000 [0m                      

                       Computation: 48299 steps/s (collection: 1.948s, learning 0.088s)
             Mean action noise std: 2.16
          Mean value_function loss: 296.3595
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.0194
                       Mean reward: 743.50
               Mean episode length: 212.61
    Episode_Reward/reaching_object: 1.2277
     Episode_Reward/lifting_object: 150.0295
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 2.04s
                      Time elapsed: 00:29:07
                               ETA: 00:45:41

################################################################################
                     [1m Learning iteration 779/2000 [0m                      

                       Computation: 48041 steps/s (collection: 1.952s, learning 0.095s)
             Mean action noise std: 2.16
          Mean value_function loss: 307.2410
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 48.0166
                       Mean reward: 770.53
               Mean episode length: 218.45
    Episode_Reward/reaching_object: 1.2573
     Episode_Reward/lifting_object: 154.7938
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 2.05s
                      Time elapsed: 00:29:09
                               ETA: 00:45:38

################################################################################
                     [1m Learning iteration 780/2000 [0m                      

                       Computation: 47598 steps/s (collection: 1.976s, learning 0.089s)
             Mean action noise std: 2.16
          Mean value_function loss: 304.8836
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 48.0152
                       Mean reward: 776.74
               Mean episode length: 220.87
    Episode_Reward/reaching_object: 1.2184
     Episode_Reward/lifting_object: 149.2653
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 2.07s
                      Time elapsed: 00:29:11
                               ETA: 00:45:36

################################################################################
                     [1m Learning iteration 781/2000 [0m                      

                       Computation: 47797 steps/s (collection: 1.970s, learning 0.087s)
             Mean action noise std: 2.16
          Mean value_function loss: 304.0937
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 48.0150
                       Mean reward: 738.39
               Mean episode length: 212.90
    Episode_Reward/reaching_object: 1.2113
     Episode_Reward/lifting_object: 148.2996
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 2.06s
                      Time elapsed: 00:29:13
                               ETA: 00:45:33

################################################################################
                     [1m Learning iteration 782/2000 [0m                      

                       Computation: 46358 steps/s (collection: 2.020s, learning 0.100s)
             Mean action noise std: 2.16
          Mean value_function loss: 273.7302
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 48.0153
                       Mean reward: 797.43
               Mean episode length: 226.70
    Episode_Reward/reaching_object: 1.2096
     Episode_Reward/lifting_object: 147.7621
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 2.12s
                      Time elapsed: 00:29:15
                               ETA: 00:45:31

################################################################################
                     [1m Learning iteration 783/2000 [0m                      

                       Computation: 46280 steps/s (collection: 2.019s, learning 0.105s)
             Mean action noise std: 2.16
          Mean value_function loss: 297.1112
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 48.0155
                       Mean reward: 698.81
               Mean episode length: 205.89
    Episode_Reward/reaching_object: 1.1993
     Episode_Reward/lifting_object: 145.2971
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 2.12s
                      Time elapsed: 00:29:17
                               ETA: 00:45:28

################################################################################
                     [1m Learning iteration 784/2000 [0m                      

                       Computation: 43911 steps/s (collection: 2.143s, learning 0.096s)
             Mean action noise std: 2.16
          Mean value_function loss: 334.0984
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 48.0159
                       Mean reward: 750.36
               Mean episode length: 216.75
    Episode_Reward/reaching_object: 1.1893
     Episode_Reward/lifting_object: 144.0419
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 2.24s
                      Time elapsed: 00:29:20
                               ETA: 00:45:26

################################################################################
                     [1m Learning iteration 785/2000 [0m                      

                       Computation: 47112 steps/s (collection: 2.001s, learning 0.086s)
             Mean action noise std: 2.16
          Mean value_function loss: 334.6790
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 48.0171
                       Mean reward: 730.62
               Mean episode length: 214.28
    Episode_Reward/reaching_object: 1.2141
     Episode_Reward/lifting_object: 147.7691
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 2.09s
                      Time elapsed: 00:29:22
                               ETA: 00:45:23

################################################################################
                     [1m Learning iteration 786/2000 [0m                      

                       Computation: 45496 steps/s (collection: 2.069s, learning 0.092s)
             Mean action noise std: 2.16
          Mean value_function loss: 345.3885
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 48.0193
                       Mean reward: 771.50
               Mean episode length: 219.49
    Episode_Reward/reaching_object: 1.1937
     Episode_Reward/lifting_object: 145.3361
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 2.16s
                      Time elapsed: 00:29:24
                               ETA: 00:45:21

################################################################################
                     [1m Learning iteration 787/2000 [0m                      

                       Computation: 46200 steps/s (collection: 2.042s, learning 0.086s)
             Mean action noise std: 2.16
          Mean value_function loss: 332.3771
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 48.0207
                       Mean reward: 736.51
               Mean episode length: 207.35
    Episode_Reward/reaching_object: 1.2074
     Episode_Reward/lifting_object: 148.2360
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 2.13s
                      Time elapsed: 00:29:26
                               ETA: 00:45:19

################################################################################
                     [1m Learning iteration 788/2000 [0m                      

                       Computation: 47582 steps/s (collection: 1.976s, learning 0.090s)
             Mean action noise std: 2.16
          Mean value_function loss: 337.0861
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 48.0218
                       Mean reward: 729.86
               Mean episode length: 211.17
    Episode_Reward/reaching_object: 1.1874
     Episode_Reward/lifting_object: 145.0254
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 2.07s
                      Time elapsed: 00:29:28
                               ETA: 00:45:16

################################################################################
                     [1m Learning iteration 789/2000 [0m                      

                       Computation: 47615 steps/s (collection: 1.975s, learning 0.089s)
             Mean action noise std: 2.16
          Mean value_function loss: 327.7182
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 48.0231
                       Mean reward: 779.12
               Mean episode length: 217.68
    Episode_Reward/reaching_object: 1.2094
     Episode_Reward/lifting_object: 148.6631
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 2.06s
                      Time elapsed: 00:29:30
                               ETA: 00:45:14

################################################################################
                     [1m Learning iteration 790/2000 [0m                      

                       Computation: 48578 steps/s (collection: 1.938s, learning 0.086s)
             Mean action noise std: 2.16
          Mean value_function loss: 309.6274
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 48.0232
                       Mean reward: 719.52
               Mean episode length: 207.12
    Episode_Reward/reaching_object: 1.2087
     Episode_Reward/lifting_object: 149.3919
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 2.02s
                      Time elapsed: 00:29:32
                               ETA: 00:45:11

################################################################################
                     [1m Learning iteration 791/2000 [0m                      

                       Computation: 47158 steps/s (collection: 1.972s, learning 0.113s)
             Mean action noise std: 2.16
          Mean value_function loss: 275.1599
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 48.0224
                       Mean reward: 768.22
               Mean episode length: 220.73
    Episode_Reward/reaching_object: 1.2381
     Episode_Reward/lifting_object: 152.0473
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 2.08s
                      Time elapsed: 00:29:34
                               ETA: 00:45:09

################################################################################
                     [1m Learning iteration 792/2000 [0m                      

                       Computation: 47601 steps/s (collection: 1.959s, learning 0.106s)
             Mean action noise std: 2.16
          Mean value_function loss: 278.6423
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 48.0228
                       Mean reward: 747.80
               Mean episode length: 210.15
    Episode_Reward/reaching_object: 1.1794
     Episode_Reward/lifting_object: 144.1370
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 2.07s
                      Time elapsed: 00:29:36
                               ETA: 00:45:06

################################################################################
                     [1m Learning iteration 793/2000 [0m                      

                       Computation: 47066 steps/s (collection: 1.971s, learning 0.118s)
             Mean action noise std: 2.16
          Mean value_function loss: 275.4090
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 48.0232
                       Mean reward: 732.21
               Mean episode length: 211.12
    Episode_Reward/reaching_object: 1.1929
     Episode_Reward/lifting_object: 145.2436
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 2.09s
                      Time elapsed: 00:29:38
                               ETA: 00:45:04

################################################################################
                     [1m Learning iteration 794/2000 [0m                      

                       Computation: 47806 steps/s (collection: 1.962s, learning 0.095s)
             Mean action noise std: 2.16
          Mean value_function loss: 251.9187
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 48.0233
                       Mean reward: 778.41
               Mean episode length: 221.14
    Episode_Reward/reaching_object: 1.2634
     Episode_Reward/lifting_object: 154.7256
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 2.06s
                      Time elapsed: 00:29:40
                               ETA: 00:45:01

################################################################################
                     [1m Learning iteration 795/2000 [0m                      

                       Computation: 47122 steps/s (collection: 1.990s, learning 0.097s)
             Mean action noise std: 2.16
          Mean value_function loss: 273.3990
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 48.0234
                       Mean reward: 720.70
               Mean episode length: 207.07
    Episode_Reward/reaching_object: 1.2219
     Episode_Reward/lifting_object: 149.9142
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 2.09s
                      Time elapsed: 00:29:43
                               ETA: 00:44:59

################################################################################
                     [1m Learning iteration 796/2000 [0m                      

                       Computation: 47893 steps/s (collection: 1.965s, learning 0.088s)
             Mean action noise std: 2.16
          Mean value_function loss: 279.4557
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 48.0239
                       Mean reward: 756.53
               Mean episode length: 215.08
    Episode_Reward/reaching_object: 1.1926
     Episode_Reward/lifting_object: 144.9826
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 2.05s
                      Time elapsed: 00:29:45
                               ETA: 00:44:56

################################################################################
                     [1m Learning iteration 797/2000 [0m                      

                       Computation: 47794 steps/s (collection: 1.969s, learning 0.088s)
             Mean action noise std: 2.16
          Mean value_function loss: 291.5414
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 48.0242
                       Mean reward: 715.80
               Mean episode length: 205.92
    Episode_Reward/reaching_object: 1.2092
     Episode_Reward/lifting_object: 147.8515
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 2.06s
                      Time elapsed: 00:29:47
                               ETA: 00:44:54

################################################################################
                     [1m Learning iteration 798/2000 [0m                      

                       Computation: 48159 steps/s (collection: 1.954s, learning 0.088s)
             Mean action noise std: 2.16
          Mean value_function loss: 262.6787
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.0243
                       Mean reward: 770.96
               Mean episode length: 219.95
    Episode_Reward/reaching_object: 1.2624
     Episode_Reward/lifting_object: 154.4448
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 2.04s
                      Time elapsed: 00:29:49
                               ETA: 00:44:51

################################################################################
                     [1m Learning iteration 799/2000 [0m                      

                       Computation: 47513 steps/s (collection: 1.975s, learning 0.094s)
             Mean action noise std: 2.16
          Mean value_function loss: 261.7580
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 48.0235
                       Mean reward: 791.81
               Mean episode length: 222.31
    Episode_Reward/reaching_object: 1.2515
     Episode_Reward/lifting_object: 153.2699
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 2.07s
                      Time elapsed: 00:29:51
                               ETA: 00:44:49

################################################################################
                     [1m Learning iteration 800/2000 [0m                      

                       Computation: 48724 steps/s (collection: 1.932s, learning 0.085s)
             Mean action noise std: 2.16
          Mean value_function loss: 269.3129
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 48.0237
                       Mean reward: 767.73
               Mean episode length: 218.09
    Episode_Reward/reaching_object: 1.2353
     Episode_Reward/lifting_object: 149.7842
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 2.02s
                      Time elapsed: 00:29:53
                               ETA: 00:44:46

################################################################################
                     [1m Learning iteration 801/2000 [0m                      

                       Computation: 46106 steps/s (collection: 2.039s, learning 0.093s)
             Mean action noise std: 2.16
          Mean value_function loss: 266.4392
               Mean surrogate loss: 0.0125
                 Mean entropy loss: 48.0237
                       Mean reward: 734.20
               Mean episode length: 212.57
    Episode_Reward/reaching_object: 1.2191
     Episode_Reward/lifting_object: 149.1156
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 2.13s
                      Time elapsed: 00:29:55
                               ETA: 00:44:44

################################################################################
                     [1m Learning iteration 802/2000 [0m                      

                       Computation: 47275 steps/s (collection: 1.975s, learning 0.105s)
             Mean action noise std: 2.16
          Mean value_function loss: 300.5639
               Mean surrogate loss: 0.0133
                 Mean entropy loss: 48.0238
                       Mean reward: 740.37
               Mean episode length: 212.93
    Episode_Reward/reaching_object: 1.2114
     Episode_Reward/lifting_object: 148.3783
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 2.08s
                      Time elapsed: 00:29:57
                               ETA: 00:44:41

################################################################################
                     [1m Learning iteration 803/2000 [0m                      

                       Computation: 47544 steps/s (collection: 1.972s, learning 0.096s)
             Mean action noise std: 2.16
          Mean value_function loss: 285.4018
               Mean surrogate loss: 0.0127
                 Mean entropy loss: 48.0240
                       Mean reward: 768.93
               Mean episode length: 218.04
    Episode_Reward/reaching_object: 1.1939
     Episode_Reward/lifting_object: 145.8981
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 2.07s
                      Time elapsed: 00:29:59
                               ETA: 00:44:39

################################################################################
                     [1m Learning iteration 804/2000 [0m                      

                       Computation: 47557 steps/s (collection: 1.975s, learning 0.092s)
             Mean action noise std: 2.16
          Mean value_function loss: 290.0150
               Mean surrogate loss: 0.0136
                 Mean entropy loss: 48.0241
                       Mean reward: 802.09
               Mean episode length: 225.80
    Episode_Reward/reaching_object: 1.2222
     Episode_Reward/lifting_object: 149.8930
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 2.07s
                      Time elapsed: 00:30:01
                               ETA: 00:44:36

################################################################################
                     [1m Learning iteration 805/2000 [0m                      

                       Computation: 47003 steps/s (collection: 1.976s, learning 0.116s)
             Mean action noise std: 2.16
          Mean value_function loss: 298.0244
               Mean surrogate loss: 0.0169
                 Mean entropy loss: 48.0243
                       Mean reward: 726.38
               Mean episode length: 206.79
    Episode_Reward/reaching_object: 1.2024
     Episode_Reward/lifting_object: 147.2810
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 2.09s
                      Time elapsed: 00:30:03
                               ETA: 00:44:34

################################################################################
                     [1m Learning iteration 806/2000 [0m                      

                       Computation: 47267 steps/s (collection: 1.976s, learning 0.104s)
             Mean action noise std: 2.16
          Mean value_function loss: 326.2525
               Mean surrogate loss: 0.0125
                 Mean entropy loss: 48.0244
                       Mean reward: 687.89
               Mean episode length: 198.06
    Episode_Reward/reaching_object: 1.1879
     Episode_Reward/lifting_object: 145.4487
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 2.08s
                      Time elapsed: 00:30:05
                               ETA: 00:44:31

################################################################################
                     [1m Learning iteration 807/2000 [0m                      

                       Computation: 47778 steps/s (collection: 1.951s, learning 0.106s)
             Mean action noise std: 2.16
          Mean value_function loss: 307.5834
               Mean surrogate loss: 0.0121
                 Mean entropy loss: 48.0245
                       Mean reward: 771.81
               Mean episode length: 221.84
    Episode_Reward/reaching_object: 1.2090
     Episode_Reward/lifting_object: 147.5538
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 2.06s
                      Time elapsed: 00:30:07
                               ETA: 00:44:29

################################################################################
                     [1m Learning iteration 808/2000 [0m                      

                       Computation: 46664 steps/s (collection: 2.013s, learning 0.094s)
             Mean action noise std: 2.16
          Mean value_function loss: 298.8489
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 48.0246
                       Mean reward: 712.99
               Mean episode length: 204.57
    Episode_Reward/reaching_object: 1.1909
     Episode_Reward/lifting_object: 146.0640
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 2.11s
                      Time elapsed: 00:30:09
                               ETA: 00:44:26

################################################################################
                     [1m Learning iteration 809/2000 [0m                      

                       Computation: 47222 steps/s (collection: 1.964s, learning 0.118s)
             Mean action noise std: 2.16
          Mean value_function loss: 305.0109
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 48.0249
                       Mean reward: 703.67
               Mean episode length: 199.03
    Episode_Reward/reaching_object: 1.1495
     Episode_Reward/lifting_object: 141.3179
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 2.08s
                      Time elapsed: 00:30:12
                               ETA: 00:44:24

################################################################################
                     [1m Learning iteration 810/2000 [0m                      

                       Computation: 45699 steps/s (collection: 2.039s, learning 0.112s)
             Mean action noise std: 2.16
          Mean value_function loss: 284.2528
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 48.0253
                       Mean reward: 710.56
               Mean episode length: 201.04
    Episode_Reward/reaching_object: 1.1950
     Episode_Reward/lifting_object: 147.0374
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 2.15s
                      Time elapsed: 00:30:14
                               ETA: 00:44:21

################################################################################
                     [1m Learning iteration 811/2000 [0m                      

                       Computation: 46156 steps/s (collection: 2.015s, learning 0.115s)
             Mean action noise std: 2.16
          Mean value_function loss: 280.7472
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 48.0259
                       Mean reward: 741.68
               Mean episode length: 215.24
    Episode_Reward/reaching_object: 1.2570
     Episode_Reward/lifting_object: 154.4169
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 2.13s
                      Time elapsed: 00:30:16
                               ETA: 00:44:19

################################################################################
                     [1m Learning iteration 812/2000 [0m                      

                       Computation: 46884 steps/s (collection: 1.997s, learning 0.100s)
             Mean action noise std: 2.16
          Mean value_function loss: 253.7768
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 48.0259
                       Mean reward: 770.16
               Mean episode length: 219.59
    Episode_Reward/reaching_object: 1.2537
     Episode_Reward/lifting_object: 154.3901
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 2.10s
                      Time elapsed: 00:30:18
                               ETA: 00:44:17

################################################################################
                     [1m Learning iteration 813/2000 [0m                      

                       Computation: 45477 steps/s (collection: 2.049s, learning 0.112s)
             Mean action noise std: 2.16
          Mean value_function loss: 246.9278
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 48.0263
                       Mean reward: 813.37
               Mean episode length: 227.32
    Episode_Reward/reaching_object: 1.2804
     Episode_Reward/lifting_object: 158.1698
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 2.16s
                      Time elapsed: 00:30:20
                               ETA: 00:44:14

################################################################################
                     [1m Learning iteration 814/2000 [0m                      

                       Computation: 47984 steps/s (collection: 1.960s, learning 0.089s)
             Mean action noise std: 2.16
          Mean value_function loss: 288.2810
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.0279
                       Mean reward: 752.10
               Mean episode length: 211.14
    Episode_Reward/reaching_object: 1.2341
     Episode_Reward/lifting_object: 151.4408
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 2.05s
                      Time elapsed: 00:30:22
                               ETA: 00:44:12

################################################################################
                     [1m Learning iteration 815/2000 [0m                      

                       Computation: 47871 steps/s (collection: 1.968s, learning 0.086s)
             Mean action noise std: 2.16
          Mean value_function loss: 292.7779
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 48.0325
                       Mean reward: 790.82
               Mean episode length: 221.13
    Episode_Reward/reaching_object: 1.2374
     Episode_Reward/lifting_object: 152.4568
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 2.05s
                      Time elapsed: 00:30:24
                               ETA: 00:44:09

################################################################################
                     [1m Learning iteration 816/2000 [0m                      

                       Computation: 47523 steps/s (collection: 1.984s, learning 0.084s)
             Mean action noise std: 2.16
          Mean value_function loss: 328.5510
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 48.0354
                       Mean reward: 717.61
               Mean episode length: 205.14
    Episode_Reward/reaching_object: 1.2114
     Episode_Reward/lifting_object: 149.1999
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 2.07s
                      Time elapsed: 00:30:26
                               ETA: 00:44:07

################################################################################
                     [1m Learning iteration 817/2000 [0m                      

                       Computation: 46973 steps/s (collection: 2.003s, learning 0.090s)
             Mean action noise std: 2.17
          Mean value_function loss: 403.1103
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 48.0369
                       Mean reward: 737.98
               Mean episode length: 209.31
    Episode_Reward/reaching_object: 1.2051
     Episode_Reward/lifting_object: 148.1766
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 2.09s
                      Time elapsed: 00:30:28
                               ETA: 00:44:04

################################################################################
                     [1m Learning iteration 818/2000 [0m                      

                       Computation: 47143 steps/s (collection: 1.996s, learning 0.090s)
             Mean action noise std: 2.17
          Mean value_function loss: 421.6539
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 48.0406
                       Mean reward: 710.34
               Mean episode length: 203.61
    Episode_Reward/reaching_object: 1.1298
     Episode_Reward/lifting_object: 137.7569
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 2.09s
                      Time elapsed: 00:30:30
                               ETA: 00:44:02

################################################################################
                     [1m Learning iteration 819/2000 [0m                      

                       Computation: 47740 steps/s (collection: 1.975s, learning 0.084s)
             Mean action noise std: 2.17
          Mean value_function loss: 515.5761
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.0468
                       Mean reward: 724.73
               Mean episode length: 206.29
    Episode_Reward/reaching_object: 1.1532
     Episode_Reward/lifting_object: 141.5365
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.1250
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 2.06s
                      Time elapsed: 00:30:32
                               ETA: 00:43:59

################################################################################
                     [1m Learning iteration 820/2000 [0m                      

                       Computation: 46461 steps/s (collection: 2.023s, learning 0.093s)
             Mean action noise std: 2.17
          Mean value_function loss: 449.6091
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 48.0509
                       Mean reward: 696.84
               Mean episode length: 197.52
    Episode_Reward/reaching_object: 1.0870
     Episode_Reward/lifting_object: 132.0819
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 2.12s
                      Time elapsed: 00:30:35
                               ETA: 00:43:57

################################################################################
                     [1m Learning iteration 821/2000 [0m                      

                       Computation: 46626 steps/s (collection: 2.014s, learning 0.095s)
             Mean action noise std: 2.17
          Mean value_function loss: 410.0588
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 48.0572
                       Mean reward: 662.11
               Mean episode length: 190.65
    Episode_Reward/reaching_object: 1.0867
     Episode_Reward/lifting_object: 132.4272
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 2.11s
                      Time elapsed: 00:30:37
                               ETA: 00:43:55

################################################################################
                     [1m Learning iteration 822/2000 [0m                      

                       Computation: 47728 steps/s (collection: 1.968s, learning 0.092s)
             Mean action noise std: 2.17
          Mean value_function loss: 408.5028
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 48.0617
                       Mean reward: 596.54
               Mean episode length: 176.54
    Episode_Reward/reaching_object: 1.0682
     Episode_Reward/lifting_object: 129.3700
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 2.06s
                      Time elapsed: 00:30:39
                               ETA: 00:43:52

################################################################################
                     [1m Learning iteration 823/2000 [0m                      

                       Computation: 47506 steps/s (collection: 1.980s, learning 0.090s)
             Mean action noise std: 2.17
          Mean value_function loss: 348.2891
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 48.0630
                       Mean reward: 695.82
               Mean episode length: 199.70
    Episode_Reward/reaching_object: 1.1190
     Episode_Reward/lifting_object: 137.1295
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 2.07s
                      Time elapsed: 00:30:41
                               ETA: 00:43:50

################################################################################
                     [1m Learning iteration 824/2000 [0m                      

                       Computation: 48011 steps/s (collection: 1.957s, learning 0.090s)
             Mean action noise std: 2.17
          Mean value_function loss: 338.0108
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 48.0633
                       Mean reward: 711.34
               Mean episode length: 202.07
    Episode_Reward/reaching_object: 1.1709
     Episode_Reward/lifting_object: 143.7652
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 2.05s
                      Time elapsed: 00:30:43
                               ETA: 00:43:47

################################################################################
                     [1m Learning iteration 825/2000 [0m                      

                       Computation: 48359 steps/s (collection: 1.947s, learning 0.086s)
             Mean action noise std: 2.17
          Mean value_function loss: 318.2263
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 48.0636
                       Mean reward: 742.87
               Mean episode length: 212.85
    Episode_Reward/reaching_object: 1.1458
     Episode_Reward/lifting_object: 139.5225
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 2.03s
                      Time elapsed: 00:30:45
                               ETA: 00:43:45

################################################################################
                     [1m Learning iteration 826/2000 [0m                      

                       Computation: 47428 steps/s (collection: 1.984s, learning 0.089s)
             Mean action noise std: 2.17
          Mean value_function loss: 295.0011
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.0637
                       Mean reward: 760.36
               Mean episode length: 215.19
    Episode_Reward/reaching_object: 1.2005
     Episode_Reward/lifting_object: 145.9403
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 2.07s
                      Time elapsed: 00:30:47
                               ETA: 00:43:42

################################################################################
                     [1m Learning iteration 827/2000 [0m                      

                       Computation: 48193 steps/s (collection: 1.950s, learning 0.090s)
             Mean action noise std: 2.17
          Mean value_function loss: 278.5778
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 48.0651
                       Mean reward: 782.03
               Mean episode length: 219.00
    Episode_Reward/reaching_object: 1.2362
     Episode_Reward/lifting_object: 151.4267
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 2.04s
                      Time elapsed: 00:30:49
                               ETA: 00:43:40

################################################################################
                     [1m Learning iteration 828/2000 [0m                      

                       Computation: 48290 steps/s (collection: 1.947s, learning 0.089s)
             Mean action noise std: 2.17
          Mean value_function loss: 277.3147
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 48.0668
                       Mean reward: 757.14
               Mean episode length: 216.79
    Episode_Reward/reaching_object: 1.2611
     Episode_Reward/lifting_object: 153.6688
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 2.04s
                      Time elapsed: 00:30:51
                               ETA: 00:43:37

################################################################################
                     [1m Learning iteration 829/2000 [0m                      

                       Computation: 47779 steps/s (collection: 1.967s, learning 0.091s)
             Mean action noise std: 2.17
          Mean value_function loss: 252.5565
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 48.0677
                       Mean reward: 810.13
               Mean episode length: 228.17
    Episode_Reward/reaching_object: 1.3033
     Episode_Reward/lifting_object: 160.8018
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 2.06s
                      Time elapsed: 00:30:53
                               ETA: 00:43:35

################################################################################
                     [1m Learning iteration 830/2000 [0m                      

                       Computation: 47517 steps/s (collection: 1.977s, learning 0.092s)
             Mean action noise std: 2.17
          Mean value_function loss: 243.7142
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 48.0682
                       Mean reward: 786.28
               Mean episode length: 222.24
    Episode_Reward/reaching_object: 1.2862
     Episode_Reward/lifting_object: 158.2211
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 2.07s
                      Time elapsed: 00:30:55
                               ETA: 00:43:32

################################################################################
                     [1m Learning iteration 831/2000 [0m                      

                       Computation: 47708 steps/s (collection: 1.960s, learning 0.101s)
             Mean action noise std: 2.17
          Mean value_function loss: 230.8191
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 48.0686
                       Mean reward: 766.60
               Mean episode length: 221.46
    Episode_Reward/reaching_object: 1.2886
     Episode_Reward/lifting_object: 157.2783
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 2.06s
                      Time elapsed: 00:30:57
                               ETA: 00:43:30

################################################################################
                     [1m Learning iteration 832/2000 [0m                      

                       Computation: 47567 steps/s (collection: 1.968s, learning 0.099s)
             Mean action noise std: 2.17
          Mean value_function loss: 241.0993
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 48.0706
                       Mean reward: 759.92
               Mean episode length: 216.34
    Episode_Reward/reaching_object: 1.2634
     Episode_Reward/lifting_object: 154.6465
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 2.07s
                      Time elapsed: 00:30:59
                               ETA: 00:43:27

################################################################################
                     [1m Learning iteration 833/2000 [0m                      

                       Computation: 47834 steps/s (collection: 1.952s, learning 0.104s)
             Mean action noise std: 2.17
          Mean value_function loss: 238.5789
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 48.0757
                       Mean reward: 757.99
               Mean episode length: 215.85
    Episode_Reward/reaching_object: 1.2351
     Episode_Reward/lifting_object: 150.6014
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 2.06s
                      Time elapsed: 00:31:01
                               ETA: 00:43:25

################################################################################
                     [1m Learning iteration 834/2000 [0m                      

                       Computation: 47313 steps/s (collection: 1.965s, learning 0.113s)
             Mean action noise std: 2.17
          Mean value_function loss: 240.8360
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 48.0787
                       Mean reward: 741.73
               Mean episode length: 209.85
    Episode_Reward/reaching_object: 1.2078
     Episode_Reward/lifting_object: 147.0920
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 2.08s
                      Time elapsed: 00:31:03
                               ETA: 00:43:22

################################################################################
                     [1m Learning iteration 835/2000 [0m                      

                       Computation: 47010 steps/s (collection: 1.980s, learning 0.111s)
             Mean action noise std: 2.17
          Mean value_function loss: 245.3992
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 48.0808
                       Mean reward: 696.43
               Mean episode length: 201.36
    Episode_Reward/reaching_object: 1.2166
     Episode_Reward/lifting_object: 148.2074
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 2.09s
                      Time elapsed: 00:31:06
                               ETA: 00:43:20

################################################################################
                     [1m Learning iteration 836/2000 [0m                      

                       Computation: 46953 steps/s (collection: 1.980s, learning 0.114s)
             Mean action noise std: 2.17
          Mean value_function loss: 228.8655
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 48.0842
                       Mean reward: 816.97
               Mean episode length: 228.57
    Episode_Reward/reaching_object: 1.2571
     Episode_Reward/lifting_object: 154.6517
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 2.09s
                      Time elapsed: 00:31:08
                               ETA: 00:43:17

################################################################################
                     [1m Learning iteration 837/2000 [0m                      

                       Computation: 47408 steps/s (collection: 1.981s, learning 0.093s)
             Mean action noise std: 2.17
          Mean value_function loss: 219.8381
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 48.0880
                       Mean reward: 801.01
               Mean episode length: 226.25
    Episode_Reward/reaching_object: 1.2878
     Episode_Reward/lifting_object: 158.9656
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 2.07s
                      Time elapsed: 00:31:10
                               ETA: 00:43:15

################################################################################
                     [1m Learning iteration 838/2000 [0m                      

                       Computation: 47807 steps/s (collection: 1.961s, learning 0.096s)
             Mean action noise std: 2.17
          Mean value_function loss: 238.2139
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 48.0918
                       Mean reward: 789.01
               Mean episode length: 222.61
    Episode_Reward/reaching_object: 1.2919
     Episode_Reward/lifting_object: 159.3464
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 2.06s
                      Time elapsed: 00:31:12
                               ETA: 00:43:13

################################################################################
                     [1m Learning iteration 839/2000 [0m                      

                       Computation: 47854 steps/s (collection: 1.946s, learning 0.109s)
             Mean action noise std: 2.17
          Mean value_function loss: 230.7942
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 48.0939
                       Mean reward: 801.12
               Mean episode length: 223.90
    Episode_Reward/reaching_object: 1.2911
     Episode_Reward/lifting_object: 159.3793
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 2.05s
                      Time elapsed: 00:31:14
                               ETA: 00:43:10

################################################################################
                     [1m Learning iteration 840/2000 [0m                      

                       Computation: 47841 steps/s (collection: 1.961s, learning 0.094s)
             Mean action noise std: 2.17
          Mean value_function loss: 252.3322
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 48.0946
                       Mean reward: 776.61
               Mean episode length: 219.83
    Episode_Reward/reaching_object: 1.2972
     Episode_Reward/lifting_object: 161.1325
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 2.05s
                      Time elapsed: 00:31:16
                               ETA: 00:43:08

################################################################################
                     [1m Learning iteration 841/2000 [0m                      

                       Computation: 47513 steps/s (collection: 1.978s, learning 0.091s)
             Mean action noise std: 2.17
          Mean value_function loss: 241.1036
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.0953
                       Mean reward: 774.58
               Mean episode length: 217.70
    Episode_Reward/reaching_object: 1.2826
     Episode_Reward/lifting_object: 158.5121
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 2.07s
                      Time elapsed: 00:31:18
                               ETA: 00:43:05

################################################################################
                     [1m Learning iteration 842/2000 [0m                      

                       Computation: 47409 steps/s (collection: 1.986s, learning 0.088s)
             Mean action noise std: 2.17
          Mean value_function loss: 247.6466
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 48.0959
                       Mean reward: 743.27
               Mean episode length: 213.94
    Episode_Reward/reaching_object: 1.2670
     Episode_Reward/lifting_object: 155.6812
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 2.07s
                      Time elapsed: 00:31:20
                               ETA: 00:43:03

################################################################################
                     [1m Learning iteration 843/2000 [0m                      

                       Computation: 47997 steps/s (collection: 1.957s, learning 0.091s)
             Mean action noise std: 2.17
          Mean value_function loss: 245.6667
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 48.0968
                       Mean reward: 772.87
               Mean episode length: 218.04
    Episode_Reward/reaching_object: 1.2624
     Episode_Reward/lifting_object: 155.2246
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 2.05s
                      Time elapsed: 00:31:22
                               ETA: 00:43:00

################################################################################
                     [1m Learning iteration 844/2000 [0m                      

                       Computation: 47397 steps/s (collection: 1.972s, learning 0.102s)
             Mean action noise std: 2.17
          Mean value_function loss: 231.9648
               Mean surrogate loss: 0.0115
                 Mean entropy loss: 48.0972
                       Mean reward: 805.29
               Mean episode length: 225.55
    Episode_Reward/reaching_object: 1.2749
     Episode_Reward/lifting_object: 156.9103
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 2.07s
                      Time elapsed: 00:31:24
                               ETA: 00:42:58

################################################################################
                     [1m Learning iteration 845/2000 [0m                      

                       Computation: 47721 steps/s (collection: 1.965s, learning 0.095s)
             Mean action noise std: 2.17
          Mean value_function loss: 310.1508
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 48.0977
                       Mean reward: 770.48
               Mean episode length: 216.95
    Episode_Reward/reaching_object: 1.2701
     Episode_Reward/lifting_object: 155.7096
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 2.06s
                      Time elapsed: 00:31:26
                               ETA: 00:42:55

################################################################################
                     [1m Learning iteration 846/2000 [0m                      

                       Computation: 47944 steps/s (collection: 1.953s, learning 0.097s)
             Mean action noise std: 2.17
          Mean value_function loss: 264.0552
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 48.0980
                       Mean reward: 766.31
               Mean episode length: 216.18
    Episode_Reward/reaching_object: 1.2686
     Episode_Reward/lifting_object: 156.8290
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 2.05s
                      Time elapsed: 00:31:28
                               ETA: 00:42:53

################################################################################
                     [1m Learning iteration 847/2000 [0m                      

                       Computation: 47846 steps/s (collection: 1.969s, learning 0.085s)
             Mean action noise std: 2.17
          Mean value_function loss: 299.3004
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 48.0978
                       Mean reward: 703.34
               Mean episode length: 203.74
    Episode_Reward/reaching_object: 1.1915
     Episode_Reward/lifting_object: 145.2676
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 2.05s
                      Time elapsed: 00:31:30
                               ETA: 00:42:50

################################################################################
                     [1m Learning iteration 848/2000 [0m                      

                       Computation: 48335 steps/s (collection: 1.944s, learning 0.089s)
             Mean action noise std: 2.17
          Mean value_function loss: 363.5616
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.0991
                       Mean reward: 707.18
               Mean episode length: 202.01
    Episode_Reward/reaching_object: 1.1850
     Episode_Reward/lifting_object: 145.0545
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 2.03s
                      Time elapsed: 00:31:32
                               ETA: 00:42:48

################################################################################
                     [1m Learning iteration 849/2000 [0m                      

                       Computation: 47063 steps/s (collection: 1.989s, learning 0.100s)
             Mean action noise std: 2.17
          Mean value_function loss: 399.0160
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 48.1021
                       Mean reward: 681.75
               Mean episode length: 195.88
    Episode_Reward/reaching_object: 1.1591
     Episode_Reward/lifting_object: 141.9072
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 2.09s
                      Time elapsed: 00:31:34
                               ETA: 00:42:45

################################################################################
                     [1m Learning iteration 850/2000 [0m                      

                       Computation: 48196 steps/s (collection: 1.947s, learning 0.092s)
             Mean action noise std: 2.17
          Mean value_function loss: 406.0601
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 48.1043
                       Mean reward: 741.39
               Mean episode length: 210.74
    Episode_Reward/reaching_object: 1.1305
     Episode_Reward/lifting_object: 136.5382
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.3750
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 2.04s
                      Time elapsed: 00:31:36
                               ETA: 00:42:43

################################################################################
                     [1m Learning iteration 851/2000 [0m                      

                       Computation: 47861 steps/s (collection: 1.965s, learning 0.089s)
             Mean action noise std: 2.17
          Mean value_function loss: 394.3269
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 48.1059
                       Mean reward: 677.66
               Mean episode length: 194.65
    Episode_Reward/reaching_object: 1.1332
     Episode_Reward/lifting_object: 137.5834
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 2.05s
                      Time elapsed: 00:31:38
                               ETA: 00:42:40

################################################################################
                     [1m Learning iteration 852/2000 [0m                      

                       Computation: 47996 steps/s (collection: 1.962s, learning 0.086s)
             Mean action noise std: 2.17
          Mean value_function loss: 432.2452
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 48.1074
                       Mean reward: 683.68
               Mean episode length: 196.94
    Episode_Reward/reaching_object: 1.0978
     Episode_Reward/lifting_object: 132.6009
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 2.05s
                      Time elapsed: 00:31:41
                               ETA: 00:42:38

################################################################################
                     [1m Learning iteration 853/2000 [0m                      

                       Computation: 47276 steps/s (collection: 1.990s, learning 0.090s)
             Mean action noise std: 2.17
          Mean value_function loss: 454.9753
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 48.1086
                       Mean reward: 620.17
               Mean episode length: 182.74
    Episode_Reward/reaching_object: 1.0414
     Episode_Reward/lifting_object: 124.2861
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 2.08s
                      Time elapsed: 00:31:43
                               ETA: 00:42:36

################################################################################
                     [1m Learning iteration 854/2000 [0m                      

                       Computation: 48224 steps/s (collection: 1.948s, learning 0.090s)
             Mean action noise std: 2.17
          Mean value_function loss: 380.4555
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 48.1103
                       Mean reward: 654.71
               Mean episode length: 191.28
    Episode_Reward/reaching_object: 1.0440
     Episode_Reward/lifting_object: 125.3089
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 2.04s
                      Time elapsed: 00:31:45
                               ETA: 00:42:33

################################################################################
                     [1m Learning iteration 855/2000 [0m                      

                       Computation: 47870 steps/s (collection: 1.953s, learning 0.101s)
             Mean action noise std: 2.17
          Mean value_function loss: 399.9351
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 48.1129
                       Mean reward: 669.19
               Mean episode length: 199.43
    Episode_Reward/reaching_object: 1.0510
     Episode_Reward/lifting_object: 124.6674
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 2.05s
                      Time elapsed: 00:31:47
                               ETA: 00:42:31

################################################################################
                     [1m Learning iteration 856/2000 [0m                      

                       Computation: 47387 steps/s (collection: 1.968s, learning 0.106s)
             Mean action noise std: 2.18
          Mean value_function loss: 396.9273
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 48.1147
                       Mean reward: 642.33
               Mean episode length: 190.72
    Episode_Reward/reaching_object: 1.0679
     Episode_Reward/lifting_object: 127.5044
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 2.07s
                      Time elapsed: 00:31:49
                               ETA: 00:42:28

################################################################################
                     [1m Learning iteration 857/2000 [0m                      

                       Computation: 47710 steps/s (collection: 1.945s, learning 0.115s)
             Mean action noise std: 2.18
          Mean value_function loss: 374.9816
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 48.1156
                       Mean reward: 679.19
               Mean episode length: 198.66
    Episode_Reward/reaching_object: 1.1082
     Episode_Reward/lifting_object: 131.5124
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 2.06s
                      Time elapsed: 00:31:51
                               ETA: 00:42:26

################################################################################
                     [1m Learning iteration 858/2000 [0m                      

                       Computation: 47444 steps/s (collection: 1.980s, learning 0.092s)
             Mean action noise std: 2.18
          Mean value_function loss: 392.7573
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 48.1167
                       Mean reward: 700.20
               Mean episode length: 203.44
    Episode_Reward/reaching_object: 1.1337
     Episode_Reward/lifting_object: 136.2334
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 2.07s
                      Time elapsed: 00:31:53
                               ETA: 00:42:23

################################################################################
                     [1m Learning iteration 859/2000 [0m                      

                       Computation: 47903 steps/s (collection: 1.964s, learning 0.089s)
             Mean action noise std: 2.18
          Mean value_function loss: 469.4421
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 48.1166
                       Mean reward: 658.03
               Mean episode length: 193.23
    Episode_Reward/reaching_object: 1.1112
     Episode_Reward/lifting_object: 133.3361
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 2.05s
                      Time elapsed: 00:31:55
                               ETA: 00:42:21

################################################################################
                     [1m Learning iteration 860/2000 [0m                      

                       Computation: 47110 steps/s (collection: 1.996s, learning 0.091s)
             Mean action noise std: 2.18
          Mean value_function loss: 452.1045
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 48.1151
                       Mean reward: 649.35
               Mean episode length: 192.86
    Episode_Reward/reaching_object: 1.0911
     Episode_Reward/lifting_object: 131.1960
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.7083
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 2.09s
                      Time elapsed: 00:31:57
                               ETA: 00:42:18

################################################################################
                     [1m Learning iteration 861/2000 [0m                      

                       Computation: 47607 steps/s (collection: 1.979s, learning 0.086s)
             Mean action noise std: 2.18
          Mean value_function loss: 551.9162
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 48.1149
                       Mean reward: 687.34
               Mean episode length: 200.52
    Episode_Reward/reaching_object: 1.0911
     Episode_Reward/lifting_object: 131.4787
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 2.06s
                      Time elapsed: 00:31:59
                               ETA: 00:42:16

################################################################################
                     [1m Learning iteration 862/2000 [0m                      

                       Computation: 47662 steps/s (collection: 1.972s, learning 0.091s)
             Mean action noise std: 2.18
          Mean value_function loss: 663.0503
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 48.1155
                       Mean reward: 637.35
               Mean episode length: 188.02
    Episode_Reward/reaching_object: 1.1070
     Episode_Reward/lifting_object: 132.1661
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 2.06s
                      Time elapsed: 00:32:01
                               ETA: 00:42:14

################################################################################
                     [1m Learning iteration 863/2000 [0m                      

                       Computation: 46976 steps/s (collection: 1.997s, learning 0.096s)
             Mean action noise std: 2.18
          Mean value_function loss: 370.3539
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 48.1170
                       Mean reward: 682.19
               Mean episode length: 200.42
    Episode_Reward/reaching_object: 1.1257
     Episode_Reward/lifting_object: 135.5328
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 2.09s
                      Time elapsed: 00:32:03
                               ETA: 00:42:11

################################################################################
                     [1m Learning iteration 864/2000 [0m                      

                       Computation: 47021 steps/s (collection: 1.984s, learning 0.107s)
             Mean action noise std: 2.18
          Mean value_function loss: 333.3170
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 48.1186
                       Mean reward: 710.22
               Mean episode length: 208.44
    Episode_Reward/reaching_object: 1.1919
     Episode_Reward/lifting_object: 143.9122
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 2.09s
                      Time elapsed: 00:32:05
                               ETA: 00:42:09

################################################################################
                     [1m Learning iteration 865/2000 [0m                      

                       Computation: 47875 steps/s (collection: 1.949s, learning 0.104s)
             Mean action noise std: 2.18
          Mean value_function loss: 294.9912
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 48.1204
                       Mean reward: 708.60
               Mean episode length: 205.90
    Episode_Reward/reaching_object: 1.1775
     Episode_Reward/lifting_object: 142.2162
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 2.05s
                      Time elapsed: 00:32:07
                               ETA: 00:42:06

################################################################################
                     [1m Learning iteration 866/2000 [0m                      

                       Computation: 47923 steps/s (collection: 1.960s, learning 0.091s)
             Mean action noise std: 2.18
          Mean value_function loss: 287.0032
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 48.1208
                       Mean reward: 699.54
               Mean episode length: 206.05
    Episode_Reward/reaching_object: 1.1742
     Episode_Reward/lifting_object: 142.2367
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 2.05s
                      Time elapsed: 00:32:09
                               ETA: 00:42:04

################################################################################
                     [1m Learning iteration 867/2000 [0m                      

                       Computation: 47808 steps/s (collection: 1.949s, learning 0.108s)
             Mean action noise std: 2.18
          Mean value_function loss: 296.0676
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 48.1214
                       Mean reward: 738.36
               Mean episode length: 209.86
    Episode_Reward/reaching_object: 1.1826
     Episode_Reward/lifting_object: 143.5721
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 2.06s
                      Time elapsed: 00:32:12
                               ETA: 00:42:01

################################################################################
                     [1m Learning iteration 868/2000 [0m                      

                       Computation: 48148 steps/s (collection: 1.949s, learning 0.092s)
             Mean action noise std: 2.18
          Mean value_function loss: 352.2704
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 48.1211
                       Mean reward: 722.11
               Mean episode length: 208.61
    Episode_Reward/reaching_object: 1.1875
     Episode_Reward/lifting_object: 143.9159
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 2.04s
                      Time elapsed: 00:32:14
                               ETA: 00:41:59

################################################################################
                     [1m Learning iteration 869/2000 [0m                      

                       Computation: 48268 steps/s (collection: 1.947s, learning 0.090s)
             Mean action noise std: 2.18
          Mean value_function loss: 360.5519
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 48.1204
                       Mean reward: 721.27
               Mean episode length: 207.57
    Episode_Reward/reaching_object: 1.1804
     Episode_Reward/lifting_object: 143.9303
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 2.04s
                      Time elapsed: 00:32:16
                               ETA: 00:41:56

################################################################################
                     [1m Learning iteration 870/2000 [0m                      

                       Computation: 47272 steps/s (collection: 1.969s, learning 0.110s)
             Mean action noise std: 2.18
          Mean value_function loss: 332.3314
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 48.1200
                       Mean reward: 778.89
               Mean episode length: 220.20
    Episode_Reward/reaching_object: 1.2102
     Episode_Reward/lifting_object: 148.2443
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 2.08s
                      Time elapsed: 00:32:18
                               ETA: 00:41:54

################################################################################
                     [1m Learning iteration 871/2000 [0m                      

                       Computation: 46700 steps/s (collection: 1.995s, learning 0.110s)
             Mean action noise std: 2.18
          Mean value_function loss: 332.3609
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 48.1222
                       Mean reward: 750.81
               Mean episode length: 213.56
    Episode_Reward/reaching_object: 1.2154
     Episode_Reward/lifting_object: 148.3484
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 2.10s
                      Time elapsed: 00:32:20
                               ETA: 00:41:52

################################################################################
                     [1m Learning iteration 872/2000 [0m                      

                       Computation: 47532 steps/s (collection: 1.981s, learning 0.087s)
             Mean action noise std: 2.18
          Mean value_function loss: 298.4489
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 48.1243
                       Mean reward: 782.71
               Mean episode length: 217.59
    Episode_Reward/reaching_object: 1.2297
     Episode_Reward/lifting_object: 151.3527
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 2.07s
                      Time elapsed: 00:32:22
                               ETA: 00:41:49

################################################################################
                     [1m Learning iteration 873/2000 [0m                      

                       Computation: 47593 steps/s (collection: 1.974s, learning 0.091s)
             Mean action noise std: 2.18
          Mean value_function loss: 298.6651
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 48.1255
                       Mean reward: 751.01
               Mean episode length: 213.61
    Episode_Reward/reaching_object: 1.2062
     Episode_Reward/lifting_object: 148.4830
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 2.07s
                      Time elapsed: 00:32:24
                               ETA: 00:41:47

################################################################################
                     [1m Learning iteration 874/2000 [0m                      

                       Computation: 48290 steps/s (collection: 1.949s, learning 0.087s)
             Mean action noise std: 2.18
          Mean value_function loss: 280.7204
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 48.1287
                       Mean reward: 778.86
               Mean episode length: 220.34
    Episode_Reward/reaching_object: 1.2976
     Episode_Reward/lifting_object: 160.2200
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 2.04s
                      Time elapsed: 00:32:26
                               ETA: 00:41:44

################################################################################
                     [1m Learning iteration 875/2000 [0m                      

                       Computation: 47215 steps/s (collection: 1.988s, learning 0.094s)
             Mean action noise std: 2.18
          Mean value_function loss: 278.3604
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 48.1321
                       Mean reward: 722.57
               Mean episode length: 205.26
    Episode_Reward/reaching_object: 1.1789
     Episode_Reward/lifting_object: 143.7896
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 2.08s
                      Time elapsed: 00:32:28
                               ETA: 00:41:42

################################################################################
                     [1m Learning iteration 876/2000 [0m                      

                       Computation: 48076 steps/s (collection: 1.953s, learning 0.092s)
             Mean action noise std: 2.18
          Mean value_function loss: 290.1728
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 48.1341
                       Mean reward: 748.29
               Mean episode length: 213.00
    Episode_Reward/reaching_object: 1.2233
     Episode_Reward/lifting_object: 148.8375
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 2.04s
                      Time elapsed: 00:32:30
                               ETA: 00:41:39

################################################################################
                     [1m Learning iteration 877/2000 [0m                      

                       Computation: 48087 steps/s (collection: 1.955s, learning 0.090s)
             Mean action noise std: 2.18
          Mean value_function loss: 250.4205
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.1378
                       Mean reward: 779.42
               Mean episode length: 220.99
    Episode_Reward/reaching_object: 1.2398
     Episode_Reward/lifting_object: 153.0915
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 2.04s
                      Time elapsed: 00:32:32
                               ETA: 00:41:37

################################################################################
                     [1m Learning iteration 878/2000 [0m                      

                       Computation: 46895 steps/s (collection: 1.994s, learning 0.102s)
             Mean action noise std: 2.18
          Mean value_function loss: 278.8961
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 48.1431
                       Mean reward: 805.79
               Mean episode length: 228.45
    Episode_Reward/reaching_object: 1.2544
     Episode_Reward/lifting_object: 153.5244
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 2.10s
                      Time elapsed: 00:32:34
                               ETA: 00:41:35

################################################################################
                     [1m Learning iteration 879/2000 [0m                      

                       Computation: 47706 steps/s (collection: 1.965s, learning 0.095s)
             Mean action noise std: 2.18
          Mean value_function loss: 320.2576
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 48.1458
                       Mean reward: 786.69
               Mean episode length: 220.45
    Episode_Reward/reaching_object: 1.2666
     Episode_Reward/lifting_object: 157.0827
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 2.06s
                      Time elapsed: 00:32:36
                               ETA: 00:41:32

################################################################################
                     [1m Learning iteration 880/2000 [0m                      

                       Computation: 45898 steps/s (collection: 2.038s, learning 0.104s)
             Mean action noise std: 2.18
          Mean value_function loss: 358.5803
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 48.1470
                       Mean reward: 774.43
               Mean episode length: 215.64
    Episode_Reward/reaching_object: 1.2433
     Episode_Reward/lifting_object: 154.2874
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 2.14s
                      Time elapsed: 00:32:38
                               ETA: 00:41:30

################################################################################
                     [1m Learning iteration 881/2000 [0m                      

                       Computation: 47357 steps/s (collection: 1.982s, learning 0.094s)
             Mean action noise std: 2.18
          Mean value_function loss: 322.5530
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.1492
                       Mean reward: 748.44
               Mean episode length: 211.69
    Episode_Reward/reaching_object: 1.2377
     Episode_Reward/lifting_object: 153.1473
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 2.08s
                      Time elapsed: 00:32:41
                               ETA: 00:41:27

################################################################################
                     [1m Learning iteration 882/2000 [0m                      

                       Computation: 47931 steps/s (collection: 1.960s, learning 0.091s)
             Mean action noise std: 2.18
          Mean value_function loss: 347.8337
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 48.1534
                       Mean reward: 770.67
               Mean episode length: 215.13
    Episode_Reward/reaching_object: 1.2625
     Episode_Reward/lifting_object: 156.7652
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 2.05s
                      Time elapsed: 00:32:43
                               ETA: 00:41:25

################################################################################
                     [1m Learning iteration 883/2000 [0m                      

                       Computation: 48141 steps/s (collection: 1.955s, learning 0.087s)
             Mean action noise std: 2.18
          Mean value_function loss: 291.9099
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 48.1556
                       Mean reward: 793.10
               Mean episode length: 219.72
    Episode_Reward/reaching_object: 1.2517
     Episode_Reward/lifting_object: 156.1848
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 2.04s
                      Time elapsed: 00:32:45
                               ETA: 00:41:23

################################################################################
                     [1m Learning iteration 884/2000 [0m                      

                       Computation: 47904 steps/s (collection: 1.952s, learning 0.100s)
             Mean action noise std: 2.18
          Mean value_function loss: 288.5233
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.1578
                       Mean reward: 792.97
               Mean episode length: 219.09
    Episode_Reward/reaching_object: 1.2609
     Episode_Reward/lifting_object: 156.8705
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 2.05s
                      Time elapsed: 00:32:47
                               ETA: 00:41:20

################################################################################
                     [1m Learning iteration 885/2000 [0m                      

                       Computation: 48067 steps/s (collection: 1.952s, learning 0.093s)
             Mean action noise std: 2.18
          Mean value_function loss: 228.8020
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 48.1612
                       Mean reward: 743.11
               Mean episode length: 209.59
    Episode_Reward/reaching_object: 1.2250
     Episode_Reward/lifting_object: 151.9448
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 2.05s
                      Time elapsed: 00:32:49
                               ETA: 00:41:18

################################################################################
                     [1m Learning iteration 886/2000 [0m                      

                       Computation: 48229 steps/s (collection: 1.950s, learning 0.089s)
             Mean action noise std: 2.18
          Mean value_function loss: 199.4373
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 48.1654
                       Mean reward: 880.18
               Mean episode length: 238.52
    Episode_Reward/reaching_object: 1.2745
     Episode_Reward/lifting_object: 158.9468
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 2.04s
                      Time elapsed: 00:32:51
                               ETA: 00:41:15

################################################################################
                     [1m Learning iteration 887/2000 [0m                      

                       Computation: 47266 steps/s (collection: 1.986s, learning 0.094s)
             Mean action noise std: 2.18
          Mean value_function loss: 201.5911
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 48.1679
                       Mean reward: 827.85
               Mean episode length: 227.79
    Episode_Reward/reaching_object: 1.2629
     Episode_Reward/lifting_object: 156.5704
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 2.08s
                      Time elapsed: 00:32:53
                               ETA: 00:41:13

################################################################################
                     [1m Learning iteration 888/2000 [0m                      

                       Computation: 47452 steps/s (collection: 1.970s, learning 0.102s)
             Mean action noise std: 2.18
          Mean value_function loss: 197.3539
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 48.1689
                       Mean reward: 854.10
               Mean episode length: 232.66
    Episode_Reward/reaching_object: 1.2942
     Episode_Reward/lifting_object: 161.8093
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 2.07s
                      Time elapsed: 00:32:55
                               ETA: 00:41:10

################################################################################
                     [1m Learning iteration 889/2000 [0m                      

                       Computation: 48388 steps/s (collection: 1.940s, learning 0.092s)
             Mean action noise std: 2.18
          Mean value_function loss: 186.0185
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 48.1692
                       Mean reward: 812.59
               Mean episode length: 223.81
    Episode_Reward/reaching_object: 1.3200
     Episode_Reward/lifting_object: 164.8934
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 2.03s
                      Time elapsed: 00:32:57
                               ETA: 00:41:08

################################################################################
                     [1m Learning iteration 890/2000 [0m                      

                       Computation: 46883 steps/s (collection: 1.993s, learning 0.104s)
             Mean action noise std: 2.18
          Mean value_function loss: 220.7803
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 48.1696
                       Mean reward: 818.80
               Mean episode length: 224.98
    Episode_Reward/reaching_object: 1.2891
     Episode_Reward/lifting_object: 159.5406
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 2.10s
                      Time elapsed: 00:32:59
                               ETA: 00:41:06

################################################################################
                     [1m Learning iteration 891/2000 [0m                      

                       Computation: 47662 steps/s (collection: 1.969s, learning 0.094s)
             Mean action noise std: 2.18
          Mean value_function loss: 206.4494
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 48.1717
                       Mean reward: 851.24
               Mean episode length: 234.53
    Episode_Reward/reaching_object: 1.3074
     Episode_Reward/lifting_object: 161.9826
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 2.06s
                      Time elapsed: 00:33:01
                               ETA: 00:41:03

################################################################################
                     [1m Learning iteration 892/2000 [0m                      

                       Computation: 43546 steps/s (collection: 2.113s, learning 0.144s)
             Mean action noise std: 2.18
          Mean value_function loss: 225.4761
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 48.1776
                       Mean reward: 710.28
               Mean episode length: 201.70
    Episode_Reward/reaching_object: 1.2918
     Episode_Reward/lifting_object: 159.8610
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 18.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 2.26s
                      Time elapsed: 00:33:03
                               ETA: 00:41:01

################################################################################
                     [1m Learning iteration 893/2000 [0m                      

                       Computation: 42925 steps/s (collection: 2.180s, learning 0.111s)
             Mean action noise std: 2.19
          Mean value_function loss: 225.6818
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 48.1847
                       Mean reward: 823.72
               Mean episode length: 227.21
    Episode_Reward/reaching_object: 1.2836
     Episode_Reward/lifting_object: 159.1001
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 2.29s
                      Time elapsed: 00:33:06
                               ETA: 00:40:59

################################################################################
                     [1m Learning iteration 894/2000 [0m                      

                       Computation: 42865 steps/s (collection: 2.168s, learning 0.125s)
             Mean action noise std: 2.19
          Mean value_function loss: 233.7469
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 48.1889
                       Mean reward: 757.21
               Mean episode length: 212.43
    Episode_Reward/reaching_object: 1.2313
     Episode_Reward/lifting_object: 152.1615
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 2.29s
                      Time elapsed: 00:33:08
                               ETA: 00:40:57

################################################################################
                     [1m Learning iteration 895/2000 [0m                      

                       Computation: 43514 steps/s (collection: 2.090s, learning 0.169s)
             Mean action noise std: 2.19
          Mean value_function loss: 220.0483
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 48.1900
                       Mean reward: 768.81
               Mean episode length: 214.22
    Episode_Reward/reaching_object: 1.2610
     Episode_Reward/lifting_object: 155.7444
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 2.26s
                      Time elapsed: 00:33:10
                               ETA: 00:40:55

################################################################################
                     [1m Learning iteration 896/2000 [0m                      

                       Computation: 43970 steps/s (collection: 2.068s, learning 0.168s)
             Mean action noise std: 2.19
          Mean value_function loss: 216.9225
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.1914
                       Mean reward: 768.93
               Mean episode length: 216.10
    Episode_Reward/reaching_object: 1.1875
     Episode_Reward/lifting_object: 146.7637
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 2.24s
                      Time elapsed: 00:33:12
                               ETA: 00:40:52

################################################################################
                     [1m Learning iteration 897/2000 [0m                      

                       Computation: 45577 steps/s (collection: 2.065s, learning 0.092s)
             Mean action noise std: 2.19
          Mean value_function loss: 236.3826
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 48.2005
                       Mean reward: 762.30
               Mean episode length: 216.57
    Episode_Reward/reaching_object: 1.2531
     Episode_Reward/lifting_object: 154.6136
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 2.16s
                      Time elapsed: 00:33:15
                               ETA: 00:40:50

################################################################################
                     [1m Learning iteration 898/2000 [0m                      

                       Computation: 45624 steps/s (collection: 2.053s, learning 0.102s)
             Mean action noise std: 2.19
          Mean value_function loss: 221.1734
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 48.2142
                       Mean reward: 795.59
               Mean episode length: 220.13
    Episode_Reward/reaching_object: 1.2603
     Episode_Reward/lifting_object: 156.1974
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 2.15s
                      Time elapsed: 00:33:17
                               ETA: 00:40:48

################################################################################
                     [1m Learning iteration 899/2000 [0m                      

                       Computation: 44698 steps/s (collection: 2.082s, learning 0.117s)
             Mean action noise std: 2.19
          Mean value_function loss: 212.5276
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 48.2184
                       Mean reward: 825.84
               Mean episode length: 228.46
    Episode_Reward/reaching_object: 1.2974
     Episode_Reward/lifting_object: 161.4208
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 2.20s
                      Time elapsed: 00:33:19
                               ETA: 00:40:45

################################################################################
                     [1m Learning iteration 900/2000 [0m                      

                       Computation: 46655 steps/s (collection: 2.007s, learning 0.100s)
             Mean action noise std: 2.19
          Mean value_function loss: 202.8104
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 48.2202
                       Mean reward: 804.15
               Mean episode length: 221.54
    Episode_Reward/reaching_object: 1.3319
     Episode_Reward/lifting_object: 166.5359
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 2.11s
                      Time elapsed: 00:33:21
                               ETA: 00:40:43

################################################################################
                     [1m Learning iteration 901/2000 [0m                      

                       Computation: 45330 steps/s (collection: 2.040s, learning 0.129s)
             Mean action noise std: 2.19
          Mean value_function loss: 226.2931
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 48.2223
                       Mean reward: 835.91
               Mean episode length: 227.98
    Episode_Reward/reaching_object: 1.3237
     Episode_Reward/lifting_object: 165.7917
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 2.17s
                      Time elapsed: 00:33:23
                               ETA: 00:40:41

################################################################################
                     [1m Learning iteration 902/2000 [0m                      

                       Computation: 45081 steps/s (collection: 2.024s, learning 0.157s)
             Mean action noise std: 2.19
          Mean value_function loss: 220.5723
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 48.2263
                       Mean reward: 796.19
               Mean episode length: 222.11
    Episode_Reward/reaching_object: 1.3019
     Episode_Reward/lifting_object: 162.0073
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 2.18s
                      Time elapsed: 00:33:25
                               ETA: 00:40:39

################################################################################
                     [1m Learning iteration 903/2000 [0m                      

                       Computation: 45325 steps/s (collection: 2.052s, learning 0.117s)
             Mean action noise std: 2.19
          Mean value_function loss: 200.8585
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 48.2302
                       Mean reward: 858.56
               Mean episode length: 233.53
    Episode_Reward/reaching_object: 1.3150
     Episode_Reward/lifting_object: 164.8667
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 2.17s
                      Time elapsed: 00:33:28
                               ETA: 00:40:36

################################################################################
                     [1m Learning iteration 904/2000 [0m                      

                       Computation: 44965 steps/s (collection: 2.059s, learning 0.128s)
             Mean action noise std: 2.19
          Mean value_function loss: 224.6913
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 48.2349
                       Mean reward: 809.60
               Mean episode length: 225.03
    Episode_Reward/reaching_object: 1.2891
     Episode_Reward/lifting_object: 160.7135
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 2.19s
                      Time elapsed: 00:33:30
                               ETA: 00:40:34

################################################################################
                     [1m Learning iteration 905/2000 [0m                      

                       Computation: 45169 steps/s (collection: 2.082s, learning 0.095s)
             Mean action noise std: 2.19
          Mean value_function loss: 206.2134
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 48.2402
                       Mean reward: 798.64
               Mean episode length: 222.11
    Episode_Reward/reaching_object: 1.3015
     Episode_Reward/lifting_object: 162.5691
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 2.18s
                      Time elapsed: 00:33:32
                               ETA: 00:40:32

################################################################################
                     [1m Learning iteration 906/2000 [0m                      

                       Computation: 45910 steps/s (collection: 2.041s, learning 0.100s)
             Mean action noise std: 2.19
          Mean value_function loss: 189.3474
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 48.2457
                       Mean reward: 847.18
               Mean episode length: 231.33
    Episode_Reward/reaching_object: 1.3263
     Episode_Reward/lifting_object: 165.2728
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 2.14s
                      Time elapsed: 00:33:34
                               ETA: 00:40:29

################################################################################
                     [1m Learning iteration 907/2000 [0m                      

                       Computation: 45169 steps/s (collection: 2.065s, learning 0.111s)
             Mean action noise std: 2.19
          Mean value_function loss: 198.6865
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 48.2540
                       Mean reward: 833.88
               Mean episode length: 229.80
    Episode_Reward/reaching_object: 1.2997
     Episode_Reward/lifting_object: 161.1511
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 2.18s
                      Time elapsed: 00:33:36
                               ETA: 00:40:27

################################################################################
                     [1m Learning iteration 908/2000 [0m                      

                       Computation: 44876 steps/s (collection: 2.059s, learning 0.132s)
             Mean action noise std: 2.19
          Mean value_function loss: 193.6077
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 48.2574
                       Mean reward: 832.25
               Mean episode length: 230.41
    Episode_Reward/reaching_object: 1.3187
     Episode_Reward/lifting_object: 164.3412
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 2.19s
                      Time elapsed: 00:33:38
                               ETA: 00:40:25

################################################################################
                     [1m Learning iteration 909/2000 [0m                      

                       Computation: 45457 steps/s (collection: 2.059s, learning 0.103s)
             Mean action noise std: 2.20
          Mean value_function loss: 195.2219
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 48.2666
                       Mean reward: 794.05
               Mean episode length: 219.20
    Episode_Reward/reaching_object: 1.2671
     Episode_Reward/lifting_object: 157.4761
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 2.16s
                      Time elapsed: 00:33:41
                               ETA: 00:40:23

################################################################################
                     [1m Learning iteration 910/2000 [0m                      

                       Computation: 45357 steps/s (collection: 2.053s, learning 0.114s)
             Mean action noise std: 2.20
          Mean value_function loss: 226.4676
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 48.2800
                       Mean reward: 817.92
               Mean episode length: 226.22
    Episode_Reward/reaching_object: 1.2986
     Episode_Reward/lifting_object: 161.3936
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 2.17s
                      Time elapsed: 00:33:43
                               ETA: 00:40:20

################################################################################
                     [1m Learning iteration 911/2000 [0m                      

                       Computation: 45953 steps/s (collection: 2.045s, learning 0.094s)
             Mean action noise std: 2.20
          Mean value_function loss: 241.9218
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 48.2920
                       Mean reward: 845.26
               Mean episode length: 232.23
    Episode_Reward/reaching_object: 1.3275
     Episode_Reward/lifting_object: 165.6664
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 2.14s
                      Time elapsed: 00:33:45
                               ETA: 00:40:18

################################################################################
                     [1m Learning iteration 912/2000 [0m                      

                       Computation: 45245 steps/s (collection: 2.071s, learning 0.102s)
             Mean action noise std: 2.20
          Mean value_function loss: 200.8990
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 48.2986
                       Mean reward: 781.04
               Mean episode length: 216.51
    Episode_Reward/reaching_object: 1.2924
     Episode_Reward/lifting_object: 160.6379
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 2.17s
                      Time elapsed: 00:33:47
                               ETA: 00:40:16

################################################################################
                     [1m Learning iteration 913/2000 [0m                      

                       Computation: 42157 steps/s (collection: 2.133s, learning 0.199s)
             Mean action noise std: 2.20
          Mean value_function loss: 219.8663
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 48.3025
                       Mean reward: 805.79
               Mean episode length: 222.80
    Episode_Reward/reaching_object: 1.2869
     Episode_Reward/lifting_object: 159.5288
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 2.33s
                      Time elapsed: 00:33:49
                               ETA: 00:40:14

################################################################################
                     [1m Learning iteration 914/2000 [0m                      

                       Computation: 44520 steps/s (collection: 2.071s, learning 0.137s)
             Mean action noise std: 2.20
          Mean value_function loss: 243.9545
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 48.3075
                       Mean reward: 818.93
               Mean episode length: 224.06
    Episode_Reward/reaching_object: 1.2610
     Episode_Reward/lifting_object: 155.6832
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 2.21s
                      Time elapsed: 00:33:52
                               ETA: 00:40:11

################################################################################
                     [1m Learning iteration 915/2000 [0m                      

                       Computation: 46025 steps/s (collection: 2.044s, learning 0.092s)
             Mean action noise std: 2.20
          Mean value_function loss: 202.9910
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 48.3113
                       Mean reward: 804.27
               Mean episode length: 221.35
    Episode_Reward/reaching_object: 1.2842
     Episode_Reward/lifting_object: 160.3595
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 2.14s
                      Time elapsed: 00:33:54
                               ETA: 00:40:09

################################################################################
                     [1m Learning iteration 916/2000 [0m                      

                       Computation: 44612 steps/s (collection: 2.084s, learning 0.120s)
             Mean action noise std: 2.20
          Mean value_function loss: 172.5712
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 48.3129
                       Mean reward: 835.81
               Mean episode length: 229.71
    Episode_Reward/reaching_object: 1.3287
     Episode_Reward/lifting_object: 164.5336
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 2.20s
                      Time elapsed: 00:33:56
                               ETA: 00:40:07

################################################################################
                     [1m Learning iteration 917/2000 [0m                      

                       Computation: 46474 steps/s (collection: 2.016s, learning 0.100s)
             Mean action noise std: 2.20
          Mean value_function loss: 169.2412
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 48.3140
                       Mean reward: 857.37
               Mean episode length: 235.65
    Episode_Reward/reaching_object: 1.3022
     Episode_Reward/lifting_object: 162.2429
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 2.12s
                      Time elapsed: 00:33:58
                               ETA: 00:40:04

################################################################################
                     [1m Learning iteration 918/2000 [0m                      

                       Computation: 44901 steps/s (collection: 2.090s, learning 0.100s)
             Mean action noise std: 2.20
          Mean value_function loss: 205.4270
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 48.3158
                       Mean reward: 823.56
               Mean episode length: 226.89
    Episode_Reward/reaching_object: 1.2940
     Episode_Reward/lifting_object: 161.1058
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 2.19s
                      Time elapsed: 00:34:00
                               ETA: 00:40:02

################################################################################
                     [1m Learning iteration 919/2000 [0m                      

                       Computation: 44216 steps/s (collection: 2.059s, learning 0.165s)
             Mean action noise std: 2.20
          Mean value_function loss: 186.0691
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 48.3179
                       Mean reward: 855.63
               Mean episode length: 233.83
    Episode_Reward/reaching_object: 1.3248
     Episode_Reward/lifting_object: 166.1439
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 2.22s
                      Time elapsed: 00:34:02
                               ETA: 00:40:00

################################################################################
                     [1m Learning iteration 920/2000 [0m                      

                       Computation: 44503 steps/s (collection: 2.075s, learning 0.134s)
             Mean action noise std: 2.20
          Mean value_function loss: 175.3858
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 48.3213
                       Mean reward: 804.30
               Mean episode length: 221.91
    Episode_Reward/reaching_object: 1.3399
     Episode_Reward/lifting_object: 168.5116
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 2.21s
                      Time elapsed: 00:34:05
                               ETA: 00:39:58

################################################################################
                     [1m Learning iteration 921/2000 [0m                      

                       Computation: 45710 steps/s (collection: 2.056s, learning 0.095s)
             Mean action noise std: 2.20
          Mean value_function loss: 192.1652
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 48.3248
                       Mean reward: 799.29
               Mean episode length: 220.21
    Episode_Reward/reaching_object: 1.3321
     Episode_Reward/lifting_object: 166.5923
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 2.15s
                      Time elapsed: 00:34:07
                               ETA: 00:39:55

################################################################################
                     [1m Learning iteration 922/2000 [0m                      

                       Computation: 45101 steps/s (collection: 2.075s, learning 0.105s)
             Mean action noise std: 2.20
          Mean value_function loss: 208.3832
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 48.3309
                       Mean reward: 805.09
               Mean episode length: 221.76
    Episode_Reward/reaching_object: 1.3078
     Episode_Reward/lifting_object: 163.8802
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 2.18s
                      Time elapsed: 00:34:09
                               ETA: 00:39:53

################################################################################
                     [1m Learning iteration 923/2000 [0m                      

                       Computation: 45106 steps/s (collection: 2.062s, learning 0.118s)
             Mean action noise std: 2.20
          Mean value_function loss: 204.0171
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 48.3367
                       Mean reward: 846.31
               Mean episode length: 230.09
    Episode_Reward/reaching_object: 1.3483
     Episode_Reward/lifting_object: 169.3094
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 2.18s
                      Time elapsed: 00:34:11
                               ETA: 00:39:51

################################################################################
                     [1m Learning iteration 924/2000 [0m                      

                       Computation: 42690 steps/s (collection: 2.072s, learning 0.231s)
             Mean action noise std: 2.20
          Mean value_function loss: 195.9239
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 48.3388
                       Mean reward: 850.48
               Mean episode length: 232.31
    Episode_Reward/reaching_object: 1.3058
     Episode_Reward/lifting_object: 164.0469
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 2.30s
                      Time elapsed: 00:34:13
                               ETA: 00:39:49

################################################################################
                     [1m Learning iteration 925/2000 [0m                      

                       Computation: 41289 steps/s (collection: 2.208s, learning 0.173s)
             Mean action noise std: 2.20
          Mean value_function loss: 223.7522
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 48.3416
                       Mean reward: 750.89
               Mean episode length: 210.11
    Episode_Reward/reaching_object: 1.2696
     Episode_Reward/lifting_object: 157.9425
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 2.38s
                      Time elapsed: 00:34:16
                               ETA: 00:39:47

################################################################################
                     [1m Learning iteration 926/2000 [0m                      

                       Computation: 41437 steps/s (collection: 2.209s, learning 0.164s)
             Mean action noise std: 2.21
          Mean value_function loss: 235.3879
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.3506
                       Mean reward: 786.85
               Mean episode length: 217.49
    Episode_Reward/reaching_object: 1.2413
     Episode_Reward/lifting_object: 155.0410
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 2.37s
                      Time elapsed: 00:34:18
                               ETA: 00:39:45

################################################################################
                     [1m Learning iteration 927/2000 [0m                      

                       Computation: 40925 steps/s (collection: 2.246s, learning 0.156s)
             Mean action noise std: 2.21
          Mean value_function loss: 210.4914
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 48.3615
                       Mean reward: 795.47
               Mean episode length: 221.09
    Episode_Reward/reaching_object: 1.2794
     Episode_Reward/lifting_object: 159.2828
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 2.40s
                      Time elapsed: 00:34:21
                               ETA: 00:39:43

################################################################################
                     [1m Learning iteration 928/2000 [0m                      

                       Computation: 42465 steps/s (collection: 2.191s, learning 0.124s)
             Mean action noise std: 2.21
          Mean value_function loss: 192.4948
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 48.3689
                       Mean reward: 843.77
               Mean episode length: 231.36
    Episode_Reward/reaching_object: 1.2829
     Episode_Reward/lifting_object: 161.2938
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 2.31s
                      Time elapsed: 00:34:23
                               ETA: 00:39:41

################################################################################
                     [1m Learning iteration 929/2000 [0m                      

                       Computation: 41493 steps/s (collection: 2.259s, learning 0.111s)
             Mean action noise std: 2.21
          Mean value_function loss: 188.5283
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 48.3750
                       Mean reward: 852.07
               Mean episode length: 233.53
    Episode_Reward/reaching_object: 1.2918
     Episode_Reward/lifting_object: 161.8325
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 2.37s
                      Time elapsed: 00:34:25
                               ETA: 00:39:39

################################################################################
                     [1m Learning iteration 930/2000 [0m                      

                       Computation: 44538 steps/s (collection: 2.100s, learning 0.108s)
             Mean action noise std: 2.21
          Mean value_function loss: 189.8434
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 48.3795
                       Mean reward: 872.85
               Mean episode length: 239.30
    Episode_Reward/reaching_object: 1.3256
     Episode_Reward/lifting_object: 165.6045
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 2.21s
                      Time elapsed: 00:34:28
                               ETA: 00:39:36

################################################################################
                     [1m Learning iteration 931/2000 [0m                      

                       Computation: 44588 steps/s (collection: 2.102s, learning 0.103s)
             Mean action noise std: 2.21
          Mean value_function loss: 168.6876
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 48.3814
                       Mean reward: 856.13
               Mean episode length: 234.82
    Episode_Reward/reaching_object: 1.3210
     Episode_Reward/lifting_object: 166.0901
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 2.20s
                      Time elapsed: 00:34:30
                               ETA: 00:39:34

################################################################################
                     [1m Learning iteration 932/2000 [0m                      

                       Computation: 42975 steps/s (collection: 2.124s, learning 0.164s)
             Mean action noise std: 2.21
          Mean value_function loss: 220.7867
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 48.3893
                       Mean reward: 827.57
               Mean episode length: 226.59
    Episode_Reward/reaching_object: 1.3179
     Episode_Reward/lifting_object: 165.5230
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 2.29s
                      Time elapsed: 00:34:32
                               ETA: 00:39:32

################################################################################
                     [1m Learning iteration 933/2000 [0m                      

                       Computation: 43694 steps/s (collection: 2.160s, learning 0.090s)
             Mean action noise std: 2.21
          Mean value_function loss: 182.1176
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.3966
                       Mean reward: 819.89
               Mean episode length: 226.94
    Episode_Reward/reaching_object: 1.3387
     Episode_Reward/lifting_object: 168.4716
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 2.25s
                      Time elapsed: 00:34:34
                               ETA: 00:39:30

################################################################################
                     [1m Learning iteration 934/2000 [0m                      

                       Computation: 44732 steps/s (collection: 2.085s, learning 0.113s)
             Mean action noise std: 2.21
          Mean value_function loss: 184.9531
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.4025
                       Mean reward: 831.10
               Mean episode length: 229.94
    Episode_Reward/reaching_object: 1.3386
     Episode_Reward/lifting_object: 168.2195
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 2.20s
                      Time elapsed: 00:34:36
                               ETA: 00:39:27

################################################################################
                     [1m Learning iteration 935/2000 [0m                      

                       Computation: 44897 steps/s (collection: 2.051s, learning 0.139s)
             Mean action noise std: 2.21
          Mean value_function loss: 188.1998
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 48.4133
                       Mean reward: 849.02
               Mean episode length: 233.38
    Episode_Reward/reaching_object: 1.3259
     Episode_Reward/lifting_object: 165.5875
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 2.19s
                      Time elapsed: 00:34:39
                               ETA: 00:39:25

################################################################################
                     [1m Learning iteration 936/2000 [0m                      

                       Computation: 43958 steps/s (collection: 2.105s, learning 0.131s)
             Mean action noise std: 2.21
          Mean value_function loss: 192.3063
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 48.4180
                       Mean reward: 833.34
               Mean episode length: 227.91
    Episode_Reward/reaching_object: 1.3234
     Episode_Reward/lifting_object: 165.3105
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 2.24s
                      Time elapsed: 00:34:41
                               ETA: 00:39:23

################################################################################
                     [1m Learning iteration 937/2000 [0m                      

                       Computation: 42934 steps/s (collection: 2.162s, learning 0.128s)
             Mean action noise std: 2.21
          Mean value_function loss: 184.2947
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 48.4189
                       Mean reward: 837.44
               Mean episode length: 229.69
    Episode_Reward/reaching_object: 1.2746
     Episode_Reward/lifting_object: 159.2415
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 2.29s
                      Time elapsed: 00:34:43
                               ETA: 00:39:21

################################################################################
                     [1m Learning iteration 938/2000 [0m                      

                       Computation: 43820 steps/s (collection: 2.142s, learning 0.102s)
             Mean action noise std: 2.21
          Mean value_function loss: 169.4332
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.4237
                       Mean reward: 808.45
               Mean episode length: 220.54
    Episode_Reward/reaching_object: 1.3073
     Episode_Reward/lifting_object: 163.2039
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 2.24s
                      Time elapsed: 00:34:45
                               ETA: 00:39:19

################################################################################
                     [1m Learning iteration 939/2000 [0m                      

                       Computation: 44381 steps/s (collection: 2.073s, learning 0.142s)
             Mean action noise std: 2.21
          Mean value_function loss: 183.7984
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 48.4295
                       Mean reward: 844.45
               Mean episode length: 233.44
    Episode_Reward/reaching_object: 1.2949
     Episode_Reward/lifting_object: 161.3047
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 2.21s
                      Time elapsed: 00:34:48
                               ETA: 00:39:16

################################################################################
                     [1m Learning iteration 940/2000 [0m                      

                       Computation: 43987 steps/s (collection: 2.110s, learning 0.125s)
             Mean action noise std: 2.21
          Mean value_function loss: 177.2604
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 48.4310
                       Mean reward: 803.23
               Mean episode length: 221.26
    Episode_Reward/reaching_object: 1.3060
     Episode_Reward/lifting_object: 163.1331
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 2.23s
                      Time elapsed: 00:34:50
                               ETA: 00:39:14

################################################################################
                     [1m Learning iteration 941/2000 [0m                      

                       Computation: 44211 steps/s (collection: 2.097s, learning 0.127s)
             Mean action noise std: 2.21
          Mean value_function loss: 172.4270
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 48.4317
                       Mean reward: 849.90
               Mean episode length: 232.58
    Episode_Reward/reaching_object: 1.2930
     Episode_Reward/lifting_object: 161.2872
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 2.22s
                      Time elapsed: 00:34:52
                               ETA: 00:39:12

################################################################################
                     [1m Learning iteration 942/2000 [0m                      

                       Computation: 43959 steps/s (collection: 2.115s, learning 0.122s)
             Mean action noise std: 2.21
          Mean value_function loss: 205.8690
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 48.4330
                       Mean reward: 890.63
               Mean episode length: 241.64
    Episode_Reward/reaching_object: 1.3255
     Episode_Reward/lifting_object: 165.9829
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 2.24s
                      Time elapsed: 00:34:54
                               ETA: 00:39:10

################################################################################
                     [1m Learning iteration 943/2000 [0m                      

                       Computation: 44584 steps/s (collection: 2.097s, learning 0.108s)
             Mean action noise std: 2.21
          Mean value_function loss: 187.7208
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 48.4359
                       Mean reward: 802.87
               Mean episode length: 223.05
    Episode_Reward/reaching_object: 1.3250
     Episode_Reward/lifting_object: 165.5218
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 2.20s
                      Time elapsed: 00:34:57
                               ETA: 00:39:08

################################################################################
                     [1m Learning iteration 944/2000 [0m                      

                       Computation: 44234 steps/s (collection: 2.108s, learning 0.114s)
             Mean action noise std: 2.22
          Mean value_function loss: 196.6719
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 48.4384
                       Mean reward: 867.88
               Mean episode length: 236.54
    Episode_Reward/reaching_object: 1.3451
     Episode_Reward/lifting_object: 168.5844
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 2.22s
                      Time elapsed: 00:34:59
                               ETA: 00:39:05

################################################################################
                     [1m Learning iteration 945/2000 [0m                      

                       Computation: 43666 steps/s (collection: 2.119s, learning 0.132s)
             Mean action noise std: 2.22
          Mean value_function loss: 231.3910
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 48.4409
                       Mean reward: 821.37
               Mean episode length: 226.88
    Episode_Reward/reaching_object: 1.3135
     Episode_Reward/lifting_object: 164.3264
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 2.25s
                      Time elapsed: 00:35:01
                               ETA: 00:39:03

################################################################################
                     [1m Learning iteration 946/2000 [0m                      

                       Computation: 44524 steps/s (collection: 2.076s, learning 0.132s)
             Mean action noise std: 2.22
          Mean value_function loss: 223.4604
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.4472
                       Mean reward: 811.18
               Mean episode length: 223.28
    Episode_Reward/reaching_object: 1.3003
     Episode_Reward/lifting_object: 163.0989
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 2.21s
                      Time elapsed: 00:35:03
                               ETA: 00:39:01

################################################################################
                     [1m Learning iteration 947/2000 [0m                      

                       Computation: 44244 steps/s (collection: 2.099s, learning 0.123s)
             Mean action noise std: 2.22
          Mean value_function loss: 253.9966
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 48.4574
                       Mean reward: 792.12
               Mean episode length: 218.57
    Episode_Reward/reaching_object: 1.3067
     Episode_Reward/lifting_object: 164.0322
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 2.22s
                      Time elapsed: 00:35:05
                               ETA: 00:38:59

################################################################################
                     [1m Learning iteration 948/2000 [0m                      

                       Computation: 45396 steps/s (collection: 2.074s, learning 0.092s)
             Mean action noise std: 2.22
          Mean value_function loss: 220.6798
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 48.4611
                       Mean reward: 827.10
               Mean episode length: 226.32
    Episode_Reward/reaching_object: 1.2845
     Episode_Reward/lifting_object: 161.5705
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 2.17s
                      Time elapsed: 00:35:08
                               ETA: 00:38:56

################################################################################
                     [1m Learning iteration 949/2000 [0m                      

                       Computation: 44593 steps/s (collection: 2.084s, learning 0.120s)
             Mean action noise std: 2.22
          Mean value_function loss: 219.4473
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.4661
                       Mean reward: 824.70
               Mean episode length: 224.72
    Episode_Reward/reaching_object: 1.3013
     Episode_Reward/lifting_object: 164.2162
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 2.20s
                      Time elapsed: 00:35:10
                               ETA: 00:38:54

################################################################################
                     [1m Learning iteration 950/2000 [0m                      

                       Computation: 44083 steps/s (collection: 2.117s, learning 0.113s)
             Mean action noise std: 2.22
          Mean value_function loss: 226.4665
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 48.4754
                       Mean reward: 801.45
               Mean episode length: 219.86
    Episode_Reward/reaching_object: 1.2857
     Episode_Reward/lifting_object: 161.5739
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 2.23s
                      Time elapsed: 00:35:12
                               ETA: 00:38:52

################################################################################
                     [1m Learning iteration 951/2000 [0m                      

                       Computation: 44054 steps/s (collection: 2.108s, learning 0.124s)
             Mean action noise std: 2.22
          Mean value_function loss: 181.6464
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.4872
                       Mean reward: 794.87
               Mean episode length: 222.50
    Episode_Reward/reaching_object: 1.2896
     Episode_Reward/lifting_object: 161.6438
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 2.23s
                      Time elapsed: 00:35:14
                               ETA: 00:38:50

################################################################################
                     [1m Learning iteration 952/2000 [0m                      

                       Computation: 45139 steps/s (collection: 2.061s, learning 0.117s)
             Mean action noise std: 2.22
          Mean value_function loss: 206.0156
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.4991
                       Mean reward: 825.98
               Mean episode length: 225.42
    Episode_Reward/reaching_object: 1.3210
     Episode_Reward/lifting_object: 165.9280
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 2.18s
                      Time elapsed: 00:35:16
                               ETA: 00:38:47

################################################################################
                     [1m Learning iteration 953/2000 [0m                      

                       Computation: 43092 steps/s (collection: 2.155s, learning 0.127s)
             Mean action noise std: 2.22
          Mean value_function loss: 190.5068
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 48.5159
                       Mean reward: 833.02
               Mean episode length: 227.69
    Episode_Reward/reaching_object: 1.3063
     Episode_Reward/lifting_object: 163.8530
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 2.28s
                      Time elapsed: 00:35:19
                               ETA: 00:38:45

################################################################################
                     [1m Learning iteration 954/2000 [0m                      

                       Computation: 45503 steps/s (collection: 2.055s, learning 0.106s)
             Mean action noise std: 2.23
          Mean value_function loss: 183.0603
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 48.5217
                       Mean reward: 856.56
               Mean episode length: 234.45
    Episode_Reward/reaching_object: 1.3233
     Episode_Reward/lifting_object: 164.8509
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 2.16s
                      Time elapsed: 00:35:21
                               ETA: 00:38:43

################################################################################
                     [1m Learning iteration 955/2000 [0m                      

                       Computation: 45775 steps/s (collection: 2.042s, learning 0.106s)
             Mean action noise std: 2.23
          Mean value_function loss: 166.2562
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 48.5272
                       Mean reward: 824.74
               Mean episode length: 226.32
    Episode_Reward/reaching_object: 1.3578
     Episode_Reward/lifting_object: 169.6205
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 2.15s
                      Time elapsed: 00:35:23
                               ETA: 00:38:41

################################################################################
                     [1m Learning iteration 956/2000 [0m                      

                       Computation: 45697 steps/s (collection: 2.048s, learning 0.103s)
             Mean action noise std: 2.23
          Mean value_function loss: 143.4513
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 48.5300
                       Mean reward: 842.59
               Mean episode length: 227.14
    Episode_Reward/reaching_object: 1.3596
     Episode_Reward/lifting_object: 170.9461
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 2.15s
                      Time elapsed: 00:35:25
                               ETA: 00:38:38

################################################################################
                     [1m Learning iteration 957/2000 [0m                      

                       Computation: 45061 steps/s (collection: 2.059s, learning 0.123s)
             Mean action noise std: 2.23
          Mean value_function loss: 154.7495
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 48.5308
                       Mean reward: 864.23
               Mean episode length: 234.85
    Episode_Reward/reaching_object: 1.3500
     Episode_Reward/lifting_object: 169.1432
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 2.18s
                      Time elapsed: 00:35:27
                               ETA: 00:38:36

################################################################################
                     [1m Learning iteration 958/2000 [0m                      

                       Computation: 45415 steps/s (collection: 2.045s, learning 0.120s)
             Mean action noise std: 2.23
          Mean value_function loss: 170.6530
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 48.5321
                       Mean reward: 802.68
               Mean episode length: 221.63
    Episode_Reward/reaching_object: 1.3189
     Episode_Reward/lifting_object: 163.3792
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 2.16s
                      Time elapsed: 00:35:30
                               ETA: 00:38:34

################################################################################
                     [1m Learning iteration 959/2000 [0m                      

                       Computation: 44587 steps/s (collection: 2.068s, learning 0.137s)
             Mean action noise std: 2.23
          Mean value_function loss: 218.8764
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.5361
                       Mean reward: 784.77
               Mean episode length: 217.15
    Episode_Reward/reaching_object: 1.2507
     Episode_Reward/lifting_object: 154.8728
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 2.20s
                      Time elapsed: 00:35:32
                               ETA: 00:38:32

################################################################################
                     [1m Learning iteration 960/2000 [0m                      

                       Computation: 44906 steps/s (collection: 2.079s, learning 0.111s)
             Mean action noise std: 2.23
          Mean value_function loss: 241.8277
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.5449
                       Mean reward: 741.87
               Mean episode length: 206.69
    Episode_Reward/reaching_object: 1.2264
     Episode_Reward/lifting_object: 151.1571
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 2.19s
                      Time elapsed: 00:35:34
                               ETA: 00:38:29

################################################################################
                     [1m Learning iteration 961/2000 [0m                      

                       Computation: 44999 steps/s (collection: 2.076s, learning 0.109s)
             Mean action noise std: 2.23
          Mean value_function loss: 209.5625
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 48.5506
                       Mean reward: 747.01
               Mean episode length: 207.78
    Episode_Reward/reaching_object: 1.2562
     Episode_Reward/lifting_object: 155.1765
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 2.18s
                      Time elapsed: 00:35:36
                               ETA: 00:38:27

################################################################################
                     [1m Learning iteration 962/2000 [0m                      

                       Computation: 45146 steps/s (collection: 2.053s, learning 0.124s)
             Mean action noise std: 2.23
          Mean value_function loss: 188.4343
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 48.5579
                       Mean reward: 862.95
               Mean episode length: 236.68
    Episode_Reward/reaching_object: 1.3337
     Episode_Reward/lifting_object: 165.8893
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 2.18s
                      Time elapsed: 00:35:38
                               ETA: 00:38:25

################################################################################
                     [1m Learning iteration 963/2000 [0m                      

                       Computation: 45054 steps/s (collection: 2.059s, learning 0.123s)
             Mean action noise std: 2.23
          Mean value_function loss: 195.9337
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 48.5647
                       Mean reward: 824.77
               Mean episode length: 227.54
    Episode_Reward/reaching_object: 1.3353
     Episode_Reward/lifting_object: 167.3536
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 2.18s
                      Time elapsed: 00:35:40
                               ETA: 00:38:23

################################################################################
                     [1m Learning iteration 964/2000 [0m                      

                       Computation: 44746 steps/s (collection: 2.084s, learning 0.113s)
             Mean action noise std: 2.23
          Mean value_function loss: 174.5293
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 48.5692
                       Mean reward: 844.33
               Mean episode length: 230.19
    Episode_Reward/reaching_object: 1.3612
     Episode_Reward/lifting_object: 170.6882
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 2.20s
                      Time elapsed: 00:35:43
                               ETA: 00:38:20

################################################################################
                     [1m Learning iteration 965/2000 [0m                      

                       Computation: 44163 steps/s (collection: 2.116s, learning 0.110s)
             Mean action noise std: 2.23
          Mean value_function loss: 208.1732
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.5779
                       Mean reward: 851.32
               Mean episode length: 230.78
    Episode_Reward/reaching_object: 1.3451
     Episode_Reward/lifting_object: 169.9312
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 2.23s
                      Time elapsed: 00:35:45
                               ETA: 00:38:18

################################################################################
                     [1m Learning iteration 966/2000 [0m                      

                       Computation: 44359 steps/s (collection: 2.102s, learning 0.114s)
             Mean action noise std: 2.23
          Mean value_function loss: 216.2595
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.5858
                       Mean reward: 834.97
               Mean episode length: 227.45
    Episode_Reward/reaching_object: 1.3096
     Episode_Reward/lifting_object: 164.4723
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 2.22s
                      Time elapsed: 00:35:47
                               ETA: 00:38:16

################################################################################
                     [1m Learning iteration 967/2000 [0m                      

                       Computation: 45392 steps/s (collection: 2.061s, learning 0.105s)
             Mean action noise std: 2.23
          Mean value_function loss: 184.8163
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 48.5904
                       Mean reward: 834.44
               Mean episode length: 226.58
    Episode_Reward/reaching_object: 1.3408
     Episode_Reward/lifting_object: 169.7168
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 2.17s
                      Time elapsed: 00:35:49
                               ETA: 00:38:14

################################################################################
                     [1m Learning iteration 968/2000 [0m                      

                       Computation: 45970 steps/s (collection: 2.037s, learning 0.102s)
             Mean action noise std: 2.23
          Mean value_function loss: 172.6029
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 48.5927
                       Mean reward: 857.39
               Mean episode length: 232.69
    Episode_Reward/reaching_object: 1.3100
     Episode_Reward/lifting_object: 165.3414
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 2.14s
                      Time elapsed: 00:35:51
                               ETA: 00:38:11

################################################################################
                     [1m Learning iteration 969/2000 [0m                      

                       Computation: 44062 steps/s (collection: 2.103s, learning 0.128s)
             Mean action noise std: 2.23
          Mean value_function loss: 180.4797
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.5965
                       Mean reward: 858.71
               Mean episode length: 232.80
    Episode_Reward/reaching_object: 1.3389
     Episode_Reward/lifting_object: 169.2236
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 2.23s
                      Time elapsed: 00:35:54
                               ETA: 00:38:09

################################################################################
                     [1m Learning iteration 970/2000 [0m                      

                       Computation: 45168 steps/s (collection: 2.056s, learning 0.121s)
             Mean action noise std: 2.24
          Mean value_function loss: 147.7193
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 48.6068
                       Mean reward: 870.24
               Mean episode length: 235.62
    Episode_Reward/reaching_object: 1.3478
     Episode_Reward/lifting_object: 169.7125
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 2.18s
                      Time elapsed: 00:35:56
                               ETA: 00:38:07

################################################################################
                     [1m Learning iteration 971/2000 [0m                      

                       Computation: 44825 steps/s (collection: 2.089s, learning 0.104s)
             Mean action noise std: 2.24
          Mean value_function loss: 175.7688
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 48.6212
                       Mean reward: 888.33
               Mean episode length: 239.04
    Episode_Reward/reaching_object: 1.3164
     Episode_Reward/lifting_object: 166.1226
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 2.19s
                      Time elapsed: 00:35:58
                               ETA: 00:38:05

################################################################################
                     [1m Learning iteration 972/2000 [0m                      

                       Computation: 45030 steps/s (collection: 2.063s, learning 0.120s)
             Mean action noise std: 2.24
          Mean value_function loss: 185.7014
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.6362
                       Mean reward: 866.09
               Mean episode length: 234.64
    Episode_Reward/reaching_object: 1.3085
     Episode_Reward/lifting_object: 164.6510
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 2.18s
                      Time elapsed: 00:36:00
                               ETA: 00:38:02

################################################################################
                     [1m Learning iteration 973/2000 [0m                      

                       Computation: 44872 steps/s (collection: 2.074s, learning 0.117s)
             Mean action noise std: 2.24
          Mean value_function loss: 160.0015
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 48.6442
                       Mean reward: 842.09
               Mean episode length: 229.43
    Episode_Reward/reaching_object: 1.3228
     Episode_Reward/lifting_object: 166.2328
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 2.19s
                      Time elapsed: 00:36:02
                               ETA: 00:38:00

################################################################################
                     [1m Learning iteration 974/2000 [0m                      

                       Computation: 45552 steps/s (collection: 2.048s, learning 0.110s)
             Mean action noise std: 2.24
          Mean value_function loss: 144.6845
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.6528
                       Mean reward: 851.09
               Mean episode length: 233.89
    Episode_Reward/reaching_object: 1.3326
     Episode_Reward/lifting_object: 167.2770
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 2.16s
                      Time elapsed: 00:36:05
                               ETA: 00:37:58

################################################################################
                     [1m Learning iteration 975/2000 [0m                      

                       Computation: 44530 steps/s (collection: 2.104s, learning 0.104s)
             Mean action noise std: 2.24
          Mean value_function loss: 155.2311
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 48.6660
                       Mean reward: 881.15
               Mean episode length: 237.12
    Episode_Reward/reaching_object: 1.3537
     Episode_Reward/lifting_object: 170.5920
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 2.21s
                      Time elapsed: 00:36:07
                               ETA: 00:37:56

################################################################################
                     [1m Learning iteration 976/2000 [0m                      

                       Computation: 45797 steps/s (collection: 2.039s, learning 0.107s)
             Mean action noise std: 2.24
          Mean value_function loss: 134.5048
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 48.6721
                       Mean reward: 860.01
               Mean episode length: 234.15
    Episode_Reward/reaching_object: 1.3540
     Episode_Reward/lifting_object: 170.3172
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 2.15s
                      Time elapsed: 00:36:09
                               ETA: 00:37:53

################################################################################
                     [1m Learning iteration 977/2000 [0m                      

                       Computation: 45529 steps/s (collection: 2.055s, learning 0.104s)
             Mean action noise std: 2.24
          Mean value_function loss: 161.7067
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 48.6800
                       Mean reward: 866.50
               Mean episode length: 235.44
    Episode_Reward/reaching_object: 1.3406
     Episode_Reward/lifting_object: 168.3703
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 2.16s
                      Time elapsed: 00:36:11
                               ETA: 00:37:51

################################################################################
                     [1m Learning iteration 978/2000 [0m                      

                       Computation: 45978 steps/s (collection: 2.034s, learning 0.104s)
             Mean action noise std: 2.25
          Mean value_function loss: 154.4127
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.6928
                       Mean reward: 846.33
               Mean episode length: 229.16
    Episode_Reward/reaching_object: 1.3306
     Episode_Reward/lifting_object: 167.7631
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 2.14s
                      Time elapsed: 00:36:13
                               ETA: 00:37:49

################################################################################
                     [1m Learning iteration 979/2000 [0m                      

                       Computation: 44809 steps/s (collection: 2.072s, learning 0.122s)
             Mean action noise std: 2.25
          Mean value_function loss: 143.0067
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 48.7024
                       Mean reward: 851.20
               Mean episode length: 231.26
    Episode_Reward/reaching_object: 1.3309
     Episode_Reward/lifting_object: 167.0632
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 2.19s
                      Time elapsed: 00:36:15
                               ETA: 00:37:46

################################################################################
                     [1m Learning iteration 980/2000 [0m                      

                       Computation: 42534 steps/s (collection: 2.174s, learning 0.138s)
             Mean action noise std: 2.25
          Mean value_function loss: 148.1990
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.7069
                       Mean reward: 864.26
               Mean episode length: 233.69
    Episode_Reward/reaching_object: 1.3343
     Episode_Reward/lifting_object: 167.8174
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 2.31s
                      Time elapsed: 00:36:18
                               ETA: 00:37:44

################################################################################
                     [1m Learning iteration 981/2000 [0m                      

                       Computation: 45349 steps/s (collection: 2.073s, learning 0.095s)
             Mean action noise std: 2.25
          Mean value_function loss: 146.9930
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.7171
                       Mean reward: 826.79
               Mean episode length: 226.21
    Episode_Reward/reaching_object: 1.3378
     Episode_Reward/lifting_object: 167.9266
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 2.17s
                      Time elapsed: 00:36:20
                               ETA: 00:37:42

################################################################################
                     [1m Learning iteration 982/2000 [0m                      

                       Computation: 44498 steps/s (collection: 2.104s, learning 0.106s)
             Mean action noise std: 2.25
          Mean value_function loss: 131.8099
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 48.7328
                       Mean reward: 855.11
               Mean episode length: 230.85
    Episode_Reward/reaching_object: 1.3484
     Episode_Reward/lifting_object: 169.8969
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 2.21s
                      Time elapsed: 00:36:22
                               ETA: 00:37:40

################################################################################
                     [1m Learning iteration 983/2000 [0m                      

                       Computation: 44674 steps/s (collection: 2.106s, learning 0.095s)
             Mean action noise std: 2.25
          Mean value_function loss: 147.9305
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.7395
                       Mean reward: 857.81
               Mean episode length: 233.64
    Episode_Reward/reaching_object: 1.3306
     Episode_Reward/lifting_object: 166.5435
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 2.20s
                      Time elapsed: 00:36:24
                               ETA: 00:37:38

################################################################################
                     [1m Learning iteration 984/2000 [0m                      

                       Computation: 45358 steps/s (collection: 2.066s, learning 0.102s)
             Mean action noise std: 2.25
          Mean value_function loss: 132.2650
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.7473
                       Mean reward: 869.24
               Mean episode length: 236.72
    Episode_Reward/reaching_object: 1.3579
     Episode_Reward/lifting_object: 170.6277
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 2.17s
                      Time elapsed: 00:36:26
                               ETA: 00:37:35

################################################################################
                     [1m Learning iteration 985/2000 [0m                      

                       Computation: 44274 steps/s (collection: 2.088s, learning 0.132s)
             Mean action noise std: 2.25
          Mean value_function loss: 105.8189
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 48.7577
                       Mean reward: 870.84
               Mean episode length: 235.60
    Episode_Reward/reaching_object: 1.4004
     Episode_Reward/lifting_object: 176.4870
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 2.22s
                      Time elapsed: 00:36:29
                               ETA: 00:37:33

################################################################################
                     [1m Learning iteration 986/2000 [0m                      

                       Computation: 43990 steps/s (collection: 2.134s, learning 0.101s)
             Mean action noise std: 2.26
          Mean value_function loss: 143.9560
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 48.7706
                       Mean reward: 884.82
               Mean episode length: 239.98
    Episode_Reward/reaching_object: 1.3525
     Episode_Reward/lifting_object: 170.1209
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 2.23s
                      Time elapsed: 00:36:31
                               ETA: 00:37:31

################################################################################
                     [1m Learning iteration 987/2000 [0m                      

                       Computation: 43527 steps/s (collection: 2.145s, learning 0.113s)
             Mean action noise std: 2.26
          Mean value_function loss: 134.5903
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.7788
                       Mean reward: 829.96
               Mean episode length: 226.91
    Episode_Reward/reaching_object: 1.3517
     Episode_Reward/lifting_object: 170.0565
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 2.26s
                      Time elapsed: 00:36:33
                               ETA: 00:37:29

################################################################################
                     [1m Learning iteration 988/2000 [0m                      

                       Computation: 44958 steps/s (collection: 2.084s, learning 0.102s)
             Mean action noise std: 2.26
          Mean value_function loss: 132.9720
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.7906
                       Mean reward: 843.30
               Mean episode length: 230.42
    Episode_Reward/reaching_object: 1.3358
     Episode_Reward/lifting_object: 167.6028
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 2.19s
                      Time elapsed: 00:36:35
                               ETA: 00:37:26

################################################################################
                     [1m Learning iteration 989/2000 [0m                      

                       Computation: 44708 steps/s (collection: 2.102s, learning 0.097s)
             Mean action noise std: 2.26
          Mean value_function loss: 130.9433
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 48.8039
                       Mean reward: 798.89
               Mean episode length: 218.95
    Episode_Reward/reaching_object: 1.3458
     Episode_Reward/lifting_object: 168.5766
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 2.20s
                      Time elapsed: 00:36:38
                               ETA: 00:37:24

################################################################################
                     [1m Learning iteration 990/2000 [0m                      

                       Computation: 45287 steps/s (collection: 2.072s, learning 0.099s)
             Mean action noise std: 2.26
          Mean value_function loss: 130.6164
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 48.8120
                       Mean reward: 890.56
               Mean episode length: 239.42
    Episode_Reward/reaching_object: 1.3606
     Episode_Reward/lifting_object: 171.0478
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 2.17s
                      Time elapsed: 00:36:40
                               ETA: 00:37:22

################################################################################
                     [1m Learning iteration 991/2000 [0m                      

                       Computation: 45672 steps/s (collection: 2.024s, learning 0.128s)
             Mean action noise std: 2.26
          Mean value_function loss: 151.6019
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 48.8329
                       Mean reward: 832.26
               Mean episode length: 225.84
    Episode_Reward/reaching_object: 1.3449
     Episode_Reward/lifting_object: 168.2190
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 2.15s
                      Time elapsed: 00:36:42
                               ETA: 00:37:20

################################################################################
                     [1m Learning iteration 992/2000 [0m                      

                       Computation: 44842 steps/s (collection: 2.046s, learning 0.146s)
             Mean action noise std: 2.26
          Mean value_function loss: 148.3910
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.8544
                       Mean reward: 831.40
               Mean episode length: 227.43
    Episode_Reward/reaching_object: 1.3337
     Episode_Reward/lifting_object: 167.5312
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 2.19s
                      Time elapsed: 00:36:44
                               ETA: 00:37:17

################################################################################
                     [1m Learning iteration 993/2000 [0m                      

                       Computation: 44600 steps/s (collection: 2.081s, learning 0.123s)
             Mean action noise std: 2.27
          Mean value_function loss: 158.8743
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 48.8679
                       Mean reward: 875.98
               Mean episode length: 237.32
    Episode_Reward/reaching_object: 1.3469
     Episode_Reward/lifting_object: 169.3974
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 2.20s
                      Time elapsed: 00:36:46
                               ETA: 00:37:15

################################################################################
                     [1m Learning iteration 994/2000 [0m                      

                       Computation: 45851 steps/s (collection: 2.041s, learning 0.103s)
             Mean action noise std: 2.27
          Mean value_function loss: 157.9178
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.8757
                       Mean reward: 818.65
               Mean episode length: 222.80
    Episode_Reward/reaching_object: 1.3152
     Episode_Reward/lifting_object: 165.0671
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 2.14s
                      Time elapsed: 00:36:48
                               ETA: 00:37:13

################################################################################
                     [1m Learning iteration 995/2000 [0m                      

                       Computation: 45693 steps/s (collection: 2.051s, learning 0.100s)
             Mean action noise std: 2.27
          Mean value_function loss: 161.2235
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 48.8799
                       Mean reward: 802.40
               Mean episode length: 219.52
    Episode_Reward/reaching_object: 1.3220
     Episode_Reward/lifting_object: 165.8292
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 2.15s
                      Time elapsed: 00:36:51
                               ETA: 00:37:11

################################################################################
                     [1m Learning iteration 996/2000 [0m                      

                       Computation: 45519 steps/s (collection: 2.066s, learning 0.094s)
             Mean action noise std: 2.27
          Mean value_function loss: 133.6641
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.8851
                       Mean reward: 862.85
               Mean episode length: 233.92
    Episode_Reward/reaching_object: 1.3743
     Episode_Reward/lifting_object: 173.3830
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 2.16s
                      Time elapsed: 00:36:53
                               ETA: 00:37:08

################################################################################
                     [1m Learning iteration 997/2000 [0m                      

                       Computation: 44874 steps/s (collection: 2.074s, learning 0.117s)
             Mean action noise std: 2.27
          Mean value_function loss: 132.3649
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 48.8921
                       Mean reward: 875.74
               Mean episode length: 237.23
    Episode_Reward/reaching_object: 1.3647
     Episode_Reward/lifting_object: 171.7804
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 2.19s
                      Time elapsed: 00:36:55
                               ETA: 00:37:06

################################################################################
                     [1m Learning iteration 998/2000 [0m                      

                       Computation: 44298 steps/s (collection: 2.101s, learning 0.119s)
             Mean action noise std: 2.27
          Mean value_function loss: 138.0959
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 48.8966
                       Mean reward: 850.18
               Mean episode length: 229.88
    Episode_Reward/reaching_object: 1.3428
     Episode_Reward/lifting_object: 169.0287
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 2.22s
                      Time elapsed: 00:36:57
                               ETA: 00:37:04

################################################################################
                     [1m Learning iteration 999/2000 [0m                      

                       Computation: 45434 steps/s (collection: 2.023s, learning 0.141s)
             Mean action noise std: 2.27
          Mean value_function loss: 128.5497
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.9085
                       Mean reward: 882.71
               Mean episode length: 236.86
    Episode_Reward/reaching_object: 1.3541
     Episode_Reward/lifting_object: 170.6059
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 2.16s
                      Time elapsed: 00:36:59
                               ETA: 00:37:02

################################################################################
                     [1m Learning iteration 1000/2000 [0m                     

                       Computation: 14373 steps/s (collection: 6.717s, learning 0.123s)
             Mean action noise std: 2.27
          Mean value_function loss: 122.3157
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 48.9238
                       Mean reward: 864.20
               Mean episode length: 234.56
    Episode_Reward/reaching_object: 1.3617
     Episode_Reward/lifting_object: 170.9041
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 6.84s
                      Time elapsed: 00:37:06
                               ETA: 00:37:04

################################################################################
                     [1m Learning iteration 1001/2000 [0m                     

                       Computation: 13845 steps/s (collection: 6.977s, learning 0.124s)
             Mean action noise std: 2.27
          Mean value_function loss: 119.5107
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 48.9303
                       Mean reward: 866.77
               Mean episode length: 234.81
    Episode_Reward/reaching_object: 1.3608
     Episode_Reward/lifting_object: 170.2958
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 7.10s
                      Time elapsed: 00:37:13
                               ETA: 00:37:07

################################################################################
                     [1m Learning iteration 1002/2000 [0m                     

                       Computation: 14453 steps/s (collection: 6.675s, learning 0.126s)
             Mean action noise std: 2.27
          Mean value_function loss: 134.7952
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 48.9310
                       Mean reward: 824.44
               Mean episode length: 223.67
    Episode_Reward/reaching_object: 1.3517
     Episode_Reward/lifting_object: 170.0638
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 6.80s
                      Time elapsed: 00:37:20
                               ETA: 00:37:09

################################################################################
                     [1m Learning iteration 1003/2000 [0m                     

                       Computation: 13904 steps/s (collection: 6.940s, learning 0.130s)
             Mean action noise std: 2.27
          Mean value_function loss: 118.4804
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 48.9332
                       Mean reward: 888.03
               Mean episode length: 240.07
    Episode_Reward/reaching_object: 1.3805
     Episode_Reward/lifting_object: 173.5255
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 7.07s
                      Time elapsed: 00:37:27
                               ETA: 00:37:11

################################################################################
                     [1m Learning iteration 1004/2000 [0m                     

                       Computation: 14089 steps/s (collection: 6.829s, learning 0.148s)
             Mean action noise std: 2.27
          Mean value_function loss: 128.0645
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 48.9402
                       Mean reward: 860.82
               Mean episode length: 234.80
    Episode_Reward/reaching_object: 1.3821
     Episode_Reward/lifting_object: 173.9676
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 6.98s
                      Time elapsed: 00:37:34
                               ETA: 00:37:14

################################################################################
                     [1m Learning iteration 1005/2000 [0m                     

                       Computation: 14223 steps/s (collection: 6.786s, learning 0.126s)
             Mean action noise std: 2.28
          Mean value_function loss: 137.6322
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.9504
                       Mean reward: 919.51
               Mean episode length: 245.25
    Episode_Reward/reaching_object: 1.3796
     Episode_Reward/lifting_object: 173.1933
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 6.91s
                      Time elapsed: 00:37:41
                               ETA: 00:37:16

################################################################################
                     [1m Learning iteration 1006/2000 [0m                     

                       Computation: 14201 steps/s (collection: 6.800s, learning 0.122s)
             Mean action noise std: 2.28
          Mean value_function loss: 116.0977
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 48.9629
                       Mean reward: 904.09
               Mean episode length: 242.91
    Episode_Reward/reaching_object: 1.3935
     Episode_Reward/lifting_object: 175.2659
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 6.92s
                      Time elapsed: 00:37:48
                               ETA: 00:37:19

################################################################################
                     [1m Learning iteration 1007/2000 [0m                     

                       Computation: 13715 steps/s (collection: 7.036s, learning 0.132s)
             Mean action noise std: 2.28
          Mean value_function loss: 135.1694
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 48.9796
                       Mean reward: 852.07
               Mean episode length: 232.34
    Episode_Reward/reaching_object: 1.3736
     Episode_Reward/lifting_object: 171.2920
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 7.17s
                      Time elapsed: 00:37:55
                               ETA: 00:37:21

################################################################################
                     [1m Learning iteration 1008/2000 [0m                     

                       Computation: 15955 steps/s (collection: 6.053s, learning 0.109s)
             Mean action noise std: 2.28
          Mean value_function loss: 128.8008
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 48.9962
                       Mean reward: 853.02
               Mean episode length: 230.01
    Episode_Reward/reaching_object: 1.3601
     Episode_Reward/lifting_object: 170.1434
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 6.16s
                      Time elapsed: 00:38:01
                               ETA: 00:37:23

################################################################################
                     [1m Learning iteration 1009/2000 [0m                     

                       Computation: 48079 steps/s (collection: 1.947s, learning 0.098s)
             Mean action noise std: 2.28
          Mean value_function loss: 120.7328
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 49.0087
                       Mean reward: 861.65
               Mean episode length: 232.61
    Episode_Reward/reaching_object: 1.3740
     Episode_Reward/lifting_object: 172.3508
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 2.04s
                      Time elapsed: 00:38:03
                               ETA: 00:37:20

################################################################################
                     [1m Learning iteration 1010/2000 [0m                     

                       Computation: 49112 steps/s (collection: 1.897s, learning 0.105s)
             Mean action noise std: 2.28
          Mean value_function loss: 112.6656
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 49.0199
                       Mean reward: 879.57
               Mean episode length: 238.20
    Episode_Reward/reaching_object: 1.3987
     Episode_Reward/lifting_object: 174.8551
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 2.00s
                      Time elapsed: 00:38:05
                               ETA: 00:37:18

################################################################################
                     [1m Learning iteration 1011/2000 [0m                     

                       Computation: 49147 steps/s (collection: 1.901s, learning 0.099s)
             Mean action noise std: 2.29
          Mean value_function loss: 119.2075
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 49.0320
                       Mean reward: 886.42
               Mean episode length: 238.29
    Episode_Reward/reaching_object: 1.3801
     Episode_Reward/lifting_object: 172.9707
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 2.00s
                      Time elapsed: 00:38:07
                               ETA: 00:37:15

################################################################################
                     [1m Learning iteration 1012/2000 [0m                     

                       Computation: 46237 steps/s (collection: 2.025s, learning 0.101s)
             Mean action noise std: 2.29
          Mean value_function loss: 124.0214
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 49.0450
                       Mean reward: 871.74
               Mean episode length: 237.90
    Episode_Reward/reaching_object: 1.4083
     Episode_Reward/lifting_object: 176.5053
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 2.13s
                      Time elapsed: 00:38:09
                               ETA: 00:37:13

################################################################################
                     [1m Learning iteration 1013/2000 [0m                     

                       Computation: 47951 steps/s (collection: 1.942s, learning 0.108s)
             Mean action noise std: 2.29
          Mean value_function loss: 130.3570
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 49.0510
                       Mean reward: 860.85
               Mean episode length: 233.89
    Episode_Reward/reaching_object: 1.3642
     Episode_Reward/lifting_object: 170.2251
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 2.05s
                      Time elapsed: 00:38:11
                               ETA: 00:37:10

################################################################################
                     [1m Learning iteration 1014/2000 [0m                     

                       Computation: 47456 steps/s (collection: 1.964s, learning 0.108s)
             Mean action noise std: 2.29
          Mean value_function loss: 103.5408
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 49.0613
                       Mean reward: 860.10
               Mean episode length: 232.72
    Episode_Reward/reaching_object: 1.4148
     Episode_Reward/lifting_object: 177.0543
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 2.07s
                      Time elapsed: 00:38:14
                               ETA: 00:37:08

################################################################################
                     [1m Learning iteration 1015/2000 [0m                     

                       Computation: 47827 steps/s (collection: 1.963s, learning 0.093s)
             Mean action noise std: 2.29
          Mean value_function loss: 120.7026
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 49.0770
                       Mean reward: 871.06
               Mean episode length: 234.86
    Episode_Reward/reaching_object: 1.3952
     Episode_Reward/lifting_object: 174.6351
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 2.06s
                      Time elapsed: 00:38:16
                               ETA: 00:37:06

################################################################################
                     [1m Learning iteration 1016/2000 [0m                     

                       Computation: 46977 steps/s (collection: 1.999s, learning 0.094s)
             Mean action noise std: 2.29
          Mean value_function loss: 124.3440
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 49.0887
                       Mean reward: 865.88
               Mean episode length: 232.86
    Episode_Reward/reaching_object: 1.3953
     Episode_Reward/lifting_object: 175.0030
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 2.09s
                      Time elapsed: 00:38:18
                               ETA: 00:37:03

################################################################################
                     [1m Learning iteration 1017/2000 [0m                     

                       Computation: 48317 steps/s (collection: 1.936s, learning 0.099s)
             Mean action noise std: 2.29
          Mean value_function loss: 120.3183
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 49.1007
                       Mean reward: 876.20
               Mean episode length: 236.17
    Episode_Reward/reaching_object: 1.3990
     Episode_Reward/lifting_object: 173.8515
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 2.03s
                      Time elapsed: 00:38:20
                               ETA: 00:37:01

################################################################################
                     [1m Learning iteration 1018/2000 [0m                     

                       Computation: 46951 steps/s (collection: 1.972s, learning 0.122s)
             Mean action noise std: 2.30
          Mean value_function loss: 141.3493
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 49.1089
                       Mean reward: 886.94
               Mean episode length: 239.12
    Episode_Reward/reaching_object: 1.3817
     Episode_Reward/lifting_object: 172.8993
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 2.09s
                      Time elapsed: 00:38:22
                               ETA: 00:36:58

################################################################################
                     [1m Learning iteration 1019/2000 [0m                     

                       Computation: 47360 steps/s (collection: 1.969s, learning 0.107s)
             Mean action noise std: 2.30
          Mean value_function loss: 166.4109
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 49.1183
                       Mean reward: 822.99
               Mean episode length: 224.61
    Episode_Reward/reaching_object: 1.3580
     Episode_Reward/lifting_object: 169.4985
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 2.08s
                      Time elapsed: 00:38:24
                               ETA: 00:36:56

################################################################################
                     [1m Learning iteration 1020/2000 [0m                     

                       Computation: 46183 steps/s (collection: 1.981s, learning 0.148s)
             Mean action noise std: 2.30
          Mean value_function loss: 126.6528
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 49.1288
                       Mean reward: 912.02
               Mean episode length: 245.37
    Episode_Reward/reaching_object: 1.3770
     Episode_Reward/lifting_object: 171.8540
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 2.13s
                      Time elapsed: 00:38:26
                               ETA: 00:36:53

################################################################################
                     [1m Learning iteration 1021/2000 [0m                     

                       Computation: 46706 steps/s (collection: 1.978s, learning 0.127s)
             Mean action noise std: 2.30
          Mean value_function loss: 145.3381
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 49.1352
                       Mean reward: 875.56
               Mean episode length: 237.20
    Episode_Reward/reaching_object: 1.3580
     Episode_Reward/lifting_object: 169.8543
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 2.10s
                      Time elapsed: 00:38:28
                               ETA: 00:36:51

################################################################################
                     [1m Learning iteration 1022/2000 [0m                     

                       Computation: 47929 steps/s (collection: 1.915s, learning 0.136s)
             Mean action noise std: 2.30
          Mean value_function loss: 144.8462
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.1448
                       Mean reward: 858.93
               Mean episode length: 232.90
    Episode_Reward/reaching_object: 1.3735
     Episode_Reward/lifting_object: 171.6228
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 2.05s
                      Time elapsed: 00:38:30
                               ETA: 00:36:49

################################################################################
                     [1m Learning iteration 1023/2000 [0m                     

                       Computation: 48136 steps/s (collection: 1.939s, learning 0.104s)
             Mean action noise std: 2.30
          Mean value_function loss: 137.6156
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 49.1524
                       Mean reward: 895.63
               Mean episode length: 240.50
    Episode_Reward/reaching_object: 1.3882
     Episode_Reward/lifting_object: 174.2758
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 2.04s
                      Time elapsed: 00:38:32
                               ETA: 00:36:46

################################################################################
                     [1m Learning iteration 1024/2000 [0m                     

                       Computation: 48525 steps/s (collection: 1.920s, learning 0.106s)
             Mean action noise std: 2.30
          Mean value_function loss: 135.2634
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 49.1634
                       Mean reward: 857.17
               Mean episode length: 231.84
    Episode_Reward/reaching_object: 1.3948
     Episode_Reward/lifting_object: 174.9019
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 2.03s
                      Time elapsed: 00:38:34
                               ETA: 00:36:44

################################################################################
                     [1m Learning iteration 1025/2000 [0m                     

                       Computation: 48411 steps/s (collection: 1.930s, learning 0.101s)
             Mean action noise std: 2.30
          Mean value_function loss: 130.5589
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 49.1736
                       Mean reward: 887.43
               Mean episode length: 239.55
    Episode_Reward/reaching_object: 1.3933
     Episode_Reward/lifting_object: 174.8945
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 2.03s
                      Time elapsed: 00:38:36
                               ETA: 00:36:41

################################################################################
                     [1m Learning iteration 1026/2000 [0m                     

                       Computation: 48970 steps/s (collection: 1.904s, learning 0.103s)
             Mean action noise std: 2.30
          Mean value_function loss: 133.9674
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 49.1797
                       Mean reward: 879.52
               Mean episode length: 236.43
    Episode_Reward/reaching_object: 1.3924
     Episode_Reward/lifting_object: 174.4182
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 2.01s
                      Time elapsed: 00:38:38
                               ETA: 00:36:39

################################################################################
                     [1m Learning iteration 1027/2000 [0m                     

                       Computation: 48485 steps/s (collection: 1.909s, learning 0.119s)
             Mean action noise std: 2.30
          Mean value_function loss: 149.9764
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 49.1845
                       Mean reward: 857.15
               Mean episode length: 232.99
    Episode_Reward/reaching_object: 1.3808
     Episode_Reward/lifting_object: 172.8101
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 2.03s
                      Time elapsed: 00:38:40
                               ETA: 00:36:36

################################################################################
                     [1m Learning iteration 1028/2000 [0m                     

                       Computation: 48485 steps/s (collection: 1.935s, learning 0.093s)
             Mean action noise std: 2.31
          Mean value_function loss: 170.4031
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.1954
                       Mean reward: 892.11
               Mean episode length: 240.10
    Episode_Reward/reaching_object: 1.3965
     Episode_Reward/lifting_object: 175.4272
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 2.03s
                      Time elapsed: 00:38:42
                               ETA: 00:36:34

################################################################################
                     [1m Learning iteration 1029/2000 [0m                     

                       Computation: 48709 steps/s (collection: 1.899s, learning 0.120s)
             Mean action noise std: 2.31
          Mean value_function loss: 200.6175
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 49.2065
                       Mean reward: 865.58
               Mean episode length: 234.14
    Episode_Reward/reaching_object: 1.3830
     Episode_Reward/lifting_object: 173.4972
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 2.02s
                      Time elapsed: 00:38:44
                               ETA: 00:36:31

################################################################################
                     [1m Learning iteration 1030/2000 [0m                     

                       Computation: 48020 steps/s (collection: 1.923s, learning 0.125s)
             Mean action noise std: 2.31
          Mean value_function loss: 167.3800
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.2150
                       Mean reward: 840.07
               Mean episode length: 228.72
    Episode_Reward/reaching_object: 1.3622
     Episode_Reward/lifting_object: 170.9059
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 2.05s
                      Time elapsed: 00:38:46
                               ETA: 00:36:29

################################################################################
                     [1m Learning iteration 1031/2000 [0m                     

                       Computation: 48412 steps/s (collection: 1.915s, learning 0.116s)
             Mean action noise std: 2.31
          Mean value_function loss: 168.5612
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 49.2261
                       Mean reward: 891.25
               Mean episode length: 238.94
    Episode_Reward/reaching_object: 1.3656
     Episode_Reward/lifting_object: 171.7737
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 2.03s
                      Time elapsed: 00:38:48
                               ETA: 00:36:26

################################################################################
                     [1m Learning iteration 1032/2000 [0m                     

                       Computation: 49113 steps/s (collection: 1.904s, learning 0.098s)
             Mean action noise std: 2.31
          Mean value_function loss: 185.4677
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 49.2350
                       Mean reward: 845.67
               Mean episode length: 229.81
    Episode_Reward/reaching_object: 1.3572
     Episode_Reward/lifting_object: 170.0462
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 2.00s
                      Time elapsed: 00:38:50
                               ETA: 00:36:24

################################################################################
                     [1m Learning iteration 1033/2000 [0m                     

                       Computation: 48376 steps/s (collection: 1.938s, learning 0.094s)
             Mean action noise std: 2.31
          Mean value_function loss: 151.0068
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 49.2411
                       Mean reward: 890.05
               Mean episode length: 238.03
    Episode_Reward/reaching_object: 1.3884
     Episode_Reward/lifting_object: 175.5587
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 2.03s
                      Time elapsed: 00:38:52
                               ETA: 00:36:21

################################################################################
                     [1m Learning iteration 1034/2000 [0m                     

                       Computation: 48895 steps/s (collection: 1.917s, learning 0.094s)
             Mean action noise std: 2.31
          Mean value_function loss: 179.5220
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 49.2494
                       Mean reward: 850.98
               Mean episode length: 230.08
    Episode_Reward/reaching_object: 1.3597
     Episode_Reward/lifting_object: 170.9416
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 2.01s
                      Time elapsed: 00:38:54
                               ETA: 00:36:19

################################################################################
                     [1m Learning iteration 1035/2000 [0m                     

                       Computation: 48699 steps/s (collection: 1.912s, learning 0.107s)
             Mean action noise std: 2.31
          Mean value_function loss: 146.3523
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.2607
                       Mean reward: 891.87
               Mean episode length: 238.69
    Episode_Reward/reaching_object: 1.3756
     Episode_Reward/lifting_object: 172.9629
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 2.02s
                      Time elapsed: 00:38:57
                               ETA: 00:36:16

################################################################################
                     [1m Learning iteration 1036/2000 [0m                     

                       Computation: 48534 steps/s (collection: 1.932s, learning 0.094s)
             Mean action noise std: 2.32
          Mean value_function loss: 154.1521
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 49.2755
                       Mean reward: 901.30
               Mean episode length: 242.73
    Episode_Reward/reaching_object: 1.3855
     Episode_Reward/lifting_object: 174.0616
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 2.03s
                      Time elapsed: 00:38:59
                               ETA: 00:36:14

################################################################################
                     [1m Learning iteration 1037/2000 [0m                     

                       Computation: 48057 steps/s (collection: 1.943s, learning 0.102s)
             Mean action noise std: 2.32
          Mean value_function loss: 133.8312
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 49.2895
                       Mean reward: 894.79
               Mean episode length: 239.71
    Episode_Reward/reaching_object: 1.3874
     Episode_Reward/lifting_object: 174.5260
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 2.05s
                      Time elapsed: 00:39:01
                               ETA: 00:36:11

################################################################################
                     [1m Learning iteration 1038/2000 [0m                     

                       Computation: 48656 steps/s (collection: 1.928s, learning 0.093s)
             Mean action noise std: 2.32
          Mean value_function loss: 171.3950
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 49.3076
                       Mean reward: 845.03
               Mean episode length: 230.14
    Episode_Reward/reaching_object: 1.3569
     Episode_Reward/lifting_object: 169.3175
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 2.02s
                      Time elapsed: 00:39:03
                               ETA: 00:36:09

################################################################################
                     [1m Learning iteration 1039/2000 [0m                     

                       Computation: 48622 steps/s (collection: 1.924s, learning 0.098s)
             Mean action noise std: 2.32
          Mean value_function loss: 123.2223
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 49.3256
                       Mean reward: 912.93
               Mean episode length: 243.90
    Episode_Reward/reaching_object: 1.4170
     Episode_Reward/lifting_object: 178.1200
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 2.02s
                      Time elapsed: 00:39:05
                               ETA: 00:36:06

################################################################################
                     [1m Learning iteration 1040/2000 [0m                     

                       Computation: 47863 steps/s (collection: 1.957s, learning 0.097s)
             Mean action noise std: 2.32
          Mean value_function loss: 145.2969
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 49.3360
                       Mean reward: 871.62
               Mean episode length: 235.03
    Episode_Reward/reaching_object: 1.3798
     Episode_Reward/lifting_object: 173.2097
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 2.05s
                      Time elapsed: 00:39:07
                               ETA: 00:36:04

################################################################################
                     [1m Learning iteration 1041/2000 [0m                     

                       Computation: 48292 steps/s (collection: 1.950s, learning 0.086s)
             Mean action noise std: 2.32
          Mean value_function loss: 103.5128
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 49.3494
                       Mean reward: 902.54
               Mean episode length: 241.41
    Episode_Reward/reaching_object: 1.3979
     Episode_Reward/lifting_object: 175.0979
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 2.04s
                      Time elapsed: 00:39:09
                               ETA: 00:36:02

################################################################################
                     [1m Learning iteration 1042/2000 [0m                     

                       Computation: 48732 steps/s (collection: 1.930s, learning 0.088s)
             Mean action noise std: 2.32
          Mean value_function loss: 134.8478
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 49.3592
                       Mean reward: 850.46
               Mean episode length: 229.71
    Episode_Reward/reaching_object: 1.3664
     Episode_Reward/lifting_object: 170.4833
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 2.02s
                      Time elapsed: 00:39:11
                               ETA: 00:35:59

################################################################################
                     [1m Learning iteration 1043/2000 [0m                     

                       Computation: 46956 steps/s (collection: 1.991s, learning 0.103s)
             Mean action noise std: 2.33
          Mean value_function loss: 145.3441
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 49.3699
                       Mean reward: 853.09
               Mean episode length: 231.67
    Episode_Reward/reaching_object: 1.4095
     Episode_Reward/lifting_object: 176.7437
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 2.09s
                      Time elapsed: 00:39:13
                               ETA: 00:35:57

################################################################################
                     [1m Learning iteration 1044/2000 [0m                     

                       Computation: 48278 steps/s (collection: 1.932s, learning 0.104s)
             Mean action noise std: 2.33
          Mean value_function loss: 148.8179
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.3872
                       Mean reward: 840.26
               Mean episode length: 226.04
    Episode_Reward/reaching_object: 1.3745
     Episode_Reward/lifting_object: 172.2503
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 2.04s
                      Time elapsed: 00:39:15
                               ETA: 00:35:54

################################################################################
                     [1m Learning iteration 1045/2000 [0m                     

                       Computation: 47445 steps/s (collection: 1.954s, learning 0.118s)
             Mean action noise std: 2.33
          Mean value_function loss: 102.7626
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 49.4007
                       Mean reward: 858.61
               Mean episode length: 231.40
    Episode_Reward/reaching_object: 1.3802
     Episode_Reward/lifting_object: 172.2699
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 2.07s
                      Time elapsed: 00:39:17
                               ETA: 00:35:52

################################################################################
                     [1m Learning iteration 1046/2000 [0m                     

                       Computation: 48284 steps/s (collection: 1.932s, learning 0.104s)
             Mean action noise std: 2.33
          Mean value_function loss: 122.6932
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 49.4124
                       Mean reward: 876.75
               Mean episode length: 235.48
    Episode_Reward/reaching_object: 1.3889
     Episode_Reward/lifting_object: 173.6204
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 2.04s
                      Time elapsed: 00:39:19
                               ETA: 00:35:49

################################################################################
                     [1m Learning iteration 1047/2000 [0m                     

                       Computation: 48916 steps/s (collection: 1.920s, learning 0.090s)
             Mean action noise std: 2.33
          Mean value_function loss: 130.4095
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 49.4341
                       Mean reward: 836.54
               Mean episode length: 227.72
    Episode_Reward/reaching_object: 1.3857
     Episode_Reward/lifting_object: 173.4727
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 2.01s
                      Time elapsed: 00:39:21
                               ETA: 00:35:47

################################################################################
                     [1m Learning iteration 1048/2000 [0m                     

                       Computation: 47948 steps/s (collection: 1.929s, learning 0.122s)
             Mean action noise std: 2.33
          Mean value_function loss: 122.6298
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 49.4482
                       Mean reward: 909.66
               Mean episode length: 242.55
    Episode_Reward/reaching_object: 1.3853
     Episode_Reward/lifting_object: 173.1639
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 2.05s
                      Time elapsed: 00:39:23
                               ETA: 00:35:44

################################################################################
                     [1m Learning iteration 1049/2000 [0m                     

                       Computation: 49195 steps/s (collection: 1.912s, learning 0.087s)
             Mean action noise std: 2.34
          Mean value_function loss: 120.4350
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 49.4541
                       Mean reward: 891.53
               Mean episode length: 238.99
    Episode_Reward/reaching_object: 1.3912
     Episode_Reward/lifting_object: 173.8284
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 2.00s
                      Time elapsed: 00:39:25
                               ETA: 00:35:42

################################################################################
                     [1m Learning iteration 1050/2000 [0m                     

                       Computation: 48426 steps/s (collection: 1.935s, learning 0.095s)
             Mean action noise std: 2.34
          Mean value_function loss: 128.2408
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 49.4603
                       Mean reward: 867.17
               Mean episode length: 234.01
    Episode_Reward/reaching_object: 1.3874
     Episode_Reward/lifting_object: 173.5446
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 2.03s
                      Time elapsed: 00:39:27
                               ETA: 00:35:40

################################################################################
                     [1m Learning iteration 1051/2000 [0m                     

                       Computation: 47340 steps/s (collection: 1.979s, learning 0.097s)
             Mean action noise std: 2.34
          Mean value_function loss: 128.1010
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 49.4669
                       Mean reward: 870.63
               Mean episode length: 235.38
    Episode_Reward/reaching_object: 1.4144
     Episode_Reward/lifting_object: 177.6162
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 2.08s
                      Time elapsed: 00:39:29
                               ETA: 00:35:37

################################################################################
                     [1m Learning iteration 1052/2000 [0m                     

                       Computation: 47697 steps/s (collection: 1.939s, learning 0.122s)
             Mean action noise std: 2.34
          Mean value_function loss: 125.3815
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 49.4711
                       Mean reward: 872.63
               Mean episode length: 234.07
    Episode_Reward/reaching_object: 1.3602
     Episode_Reward/lifting_object: 170.4591
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 2.06s
                      Time elapsed: 00:39:31
                               ETA: 00:35:35

################################################################################
                     [1m Learning iteration 1053/2000 [0m                     

                       Computation: 48715 steps/s (collection: 1.930s, learning 0.088s)
             Mean action noise std: 2.34
          Mean value_function loss: 125.5882
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.4797
                       Mean reward: 883.90
               Mean episode length: 237.35
    Episode_Reward/reaching_object: 1.3741
     Episode_Reward/lifting_object: 172.2881
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 2.02s
                      Time elapsed: 00:39:33
                               ETA: 00:35:32

################################################################################
                     [1m Learning iteration 1054/2000 [0m                     

                       Computation: 47501 steps/s (collection: 1.926s, learning 0.143s)
             Mean action noise std: 2.34
          Mean value_function loss: 145.0774
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 49.4926
                       Mean reward: 866.49
               Mean episode length: 235.20
    Episode_Reward/reaching_object: 1.3736
     Episode_Reward/lifting_object: 172.1056
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 2.07s
                      Time elapsed: 00:39:35
                               ETA: 00:35:30

################################################################################
                     [1m Learning iteration 1055/2000 [0m                     

                       Computation: 48453 steps/s (collection: 1.940s, learning 0.089s)
             Mean action noise std: 2.34
          Mean value_function loss: 138.9908
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 49.5075
                       Mean reward: 863.70
               Mean episode length: 232.38
    Episode_Reward/reaching_object: 1.3756
     Episode_Reward/lifting_object: 173.0252
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 2.03s
                      Time elapsed: 00:39:37
                               ETA: 00:35:27

################################################################################
                     [1m Learning iteration 1056/2000 [0m                     

                       Computation: 48011 steps/s (collection: 1.941s, learning 0.106s)
             Mean action noise std: 2.34
          Mean value_function loss: 141.5693
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 49.5175
                       Mean reward: 886.07
               Mean episode length: 238.24
    Episode_Reward/reaching_object: 1.3855
     Episode_Reward/lifting_object: 174.1848
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 2.05s
                      Time elapsed: 00:39:39
                               ETA: 00:35:25

################################################################################
                     [1m Learning iteration 1057/2000 [0m                     

                       Computation: 48723 steps/s (collection: 1.932s, learning 0.086s)
             Mean action noise std: 2.34
          Mean value_function loss: 170.9858
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 49.5306
                       Mean reward: 881.96
               Mean episode length: 236.09
    Episode_Reward/reaching_object: 1.3691
     Episode_Reward/lifting_object: 172.3460
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 2.02s
                      Time elapsed: 00:39:41
                               ETA: 00:35:22

################################################################################
                     [1m Learning iteration 1058/2000 [0m                     

                       Computation: 48293 steps/s (collection: 1.951s, learning 0.085s)
             Mean action noise std: 2.35
          Mean value_function loss: 125.7159
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 49.5431
                       Mean reward: 898.88
               Mean episode length: 239.49
    Episode_Reward/reaching_object: 1.3846
     Episode_Reward/lifting_object: 175.4795
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 2.04s
                      Time elapsed: 00:39:43
                               ETA: 00:35:20

################################################################################
                     [1m Learning iteration 1059/2000 [0m                     

                       Computation: 48416 steps/s (collection: 1.938s, learning 0.092s)
             Mean action noise std: 2.35
          Mean value_function loss: 154.7358
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.5505
                       Mean reward: 904.75
               Mean episode length: 240.99
    Episode_Reward/reaching_object: 1.3689
     Episode_Reward/lifting_object: 172.6348
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 2.03s
                      Time elapsed: 00:39:45
                               ETA: 00:35:18

################################################################################
                     [1m Learning iteration 1060/2000 [0m                     

                       Computation: 48670 steps/s (collection: 1.908s, learning 0.112s)
             Mean action noise std: 2.35
          Mean value_function loss: 120.4997
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 49.5570
                       Mean reward: 890.04
               Mean episode length: 239.37
    Episode_Reward/reaching_object: 1.3843
     Episode_Reward/lifting_object: 173.5031
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 2.02s
                      Time elapsed: 00:39:47
                               ETA: 00:35:15

################################################################################
                     [1m Learning iteration 1061/2000 [0m                     

                       Computation: 48299 steps/s (collection: 1.931s, learning 0.105s)
             Mean action noise std: 2.35
          Mean value_function loss: 111.0892
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 49.5661
                       Mean reward: 878.59
               Mean episode length: 237.67
    Episode_Reward/reaching_object: 1.4020
     Episode_Reward/lifting_object: 176.3791
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 2.04s
                      Time elapsed: 00:39:49
                               ETA: 00:35:13

################################################################################
                     [1m Learning iteration 1062/2000 [0m                     

                       Computation: 48179 steps/s (collection: 1.952s, learning 0.089s)
             Mean action noise std: 2.35
          Mean value_function loss: 125.6685
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 49.5733
                       Mean reward: 880.85
               Mean episode length: 237.64
    Episode_Reward/reaching_object: 1.3705
     Episode_Reward/lifting_object: 172.4681
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 2.04s
                      Time elapsed: 00:39:52
                               ETA: 00:35:10

################################################################################
                     [1m Learning iteration 1063/2000 [0m                     

                       Computation: 49166 steps/s (collection: 1.906s, learning 0.094s)
             Mean action noise std: 2.35
          Mean value_function loss: 94.1432
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 49.5750
                       Mean reward: 911.05
               Mean episode length: 243.05
    Episode_Reward/reaching_object: 1.4209
     Episode_Reward/lifting_object: 178.7787
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 2.00s
                      Time elapsed: 00:39:54
                               ETA: 00:35:08

################################################################################
                     [1m Learning iteration 1064/2000 [0m                     

                       Computation: 48613 steps/s (collection: 1.918s, learning 0.104s)
             Mean action noise std: 2.35
          Mean value_function loss: 117.1026
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 49.5767
                       Mean reward: 882.54
               Mean episode length: 236.98
    Episode_Reward/reaching_object: 1.3867
     Episode_Reward/lifting_object: 174.9670
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 2.02s
                      Time elapsed: 00:39:56
                               ETA: 00:35:05

################################################################################
                     [1m Learning iteration 1065/2000 [0m                     

                       Computation: 48252 steps/s (collection: 1.944s, learning 0.094s)
             Mean action noise std: 2.35
          Mean value_function loss: 110.3863
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 49.5789
                       Mean reward: 861.58
               Mean episode length: 232.04
    Episode_Reward/reaching_object: 1.3970
     Episode_Reward/lifting_object: 175.9872
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 2.04s
                      Time elapsed: 00:39:58
                               ETA: 00:35:03

################################################################################
                     [1m Learning iteration 1066/2000 [0m                     

                       Computation: 48187 steps/s (collection: 1.943s, learning 0.097s)
             Mean action noise std: 2.35
          Mean value_function loss: 98.5461
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 49.5833
                       Mean reward: 888.89
               Mean episode length: 238.19
    Episode_Reward/reaching_object: 1.3965
     Episode_Reward/lifting_object: 175.9903
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 2.04s
                      Time elapsed: 00:40:00
                               ETA: 00:35:00

################################################################################
                     [1m Learning iteration 1067/2000 [0m                     

                       Computation: 47410 steps/s (collection: 1.974s, learning 0.099s)
             Mean action noise std: 2.35
          Mean value_function loss: 116.9689
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 49.5924
                       Mean reward: 904.94
               Mean episode length: 243.61
    Episode_Reward/reaching_object: 1.3914
     Episode_Reward/lifting_object: 175.1808
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 2.07s
                      Time elapsed: 00:40:02
                               ETA: 00:34:58

################################################################################
                     [1m Learning iteration 1068/2000 [0m                     

                       Computation: 47522 steps/s (collection: 1.978s, learning 0.091s)
             Mean action noise std: 2.35
          Mean value_function loss: 114.4495
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 49.6051
                       Mean reward: 906.30
               Mean episode length: 242.35
    Episode_Reward/reaching_object: 1.3861
     Episode_Reward/lifting_object: 174.0700
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 2.07s
                      Time elapsed: 00:40:04
                               ETA: 00:34:56

################################################################################
                     [1m Learning iteration 1069/2000 [0m                     

                       Computation: 48153 steps/s (collection: 1.953s, learning 0.088s)
             Mean action noise std: 2.36
          Mean value_function loss: 104.2668
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 49.6229
                       Mean reward: 877.01
               Mean episode length: 235.07
    Episode_Reward/reaching_object: 1.4059
     Episode_Reward/lifting_object: 176.8686
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 2.04s
                      Time elapsed: 00:40:06
                               ETA: 00:34:53

################################################################################
                     [1m Learning iteration 1070/2000 [0m                     

                       Computation: 48761 steps/s (collection: 1.928s, learning 0.088s)
             Mean action noise std: 2.36
          Mean value_function loss: 130.3883
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 49.6453
                       Mean reward: 909.03
               Mean episode length: 243.85
    Episode_Reward/reaching_object: 1.4055
     Episode_Reward/lifting_object: 177.2493
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 2.02s
                      Time elapsed: 00:40:08
                               ETA: 00:34:51

################################################################################
                     [1m Learning iteration 1071/2000 [0m                     

                       Computation: 48845 steps/s (collection: 1.926s, learning 0.087s)
             Mean action noise std: 2.36
          Mean value_function loss: 99.7287
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 49.6597
                       Mean reward: 883.72
               Mean episode length: 235.76
    Episode_Reward/reaching_object: 1.3821
     Episode_Reward/lifting_object: 173.8082
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 2.01s
                      Time elapsed: 00:40:10
                               ETA: 00:34:48

################################################################################
                     [1m Learning iteration 1072/2000 [0m                     

                       Computation: 47772 steps/s (collection: 1.972s, learning 0.086s)
             Mean action noise std: 2.36
          Mean value_function loss: 109.6063
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.6671
                       Mean reward: 895.85
               Mean episode length: 239.21
    Episode_Reward/reaching_object: 1.4148
     Episode_Reward/lifting_object: 178.4808
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 2.06s
                      Time elapsed: 00:40:12
                               ETA: 00:34:46

################################################################################
                     [1m Learning iteration 1073/2000 [0m                     

                       Computation: 48558 steps/s (collection: 1.935s, learning 0.089s)
             Mean action noise std: 2.36
          Mean value_function loss: 112.3745
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 49.6802
                       Mean reward: 824.73
               Mean episode length: 225.82
    Episode_Reward/reaching_object: 1.3815
     Episode_Reward/lifting_object: 174.0256
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 2.02s
                      Time elapsed: 00:40:14
                               ETA: 00:34:43

################################################################################
                     [1m Learning iteration 1074/2000 [0m                     

                       Computation: 46840 steps/s (collection: 1.942s, learning 0.157s)
             Mean action noise std: 2.36
          Mean value_function loss: 101.1706
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 49.6946
                       Mean reward: 893.69
               Mean episode length: 239.96
    Episode_Reward/reaching_object: 1.4127
     Episode_Reward/lifting_object: 178.5321
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 2.10s
                      Time elapsed: 00:40:16
                               ETA: 00:34:41

################################################################################
                     [1m Learning iteration 1075/2000 [0m                     

                       Computation: 48845 steps/s (collection: 1.923s, learning 0.090s)
             Mean action noise std: 2.37
          Mean value_function loss: 108.7940
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 49.7076
                       Mean reward: 900.11
               Mean episode length: 241.02
    Episode_Reward/reaching_object: 1.4121
     Episode_Reward/lifting_object: 178.2472
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 2.01s
                      Time elapsed: 00:40:18
                               ETA: 00:34:39

################################################################################
                     [1m Learning iteration 1076/2000 [0m                     

                       Computation: 48081 steps/s (collection: 1.957s, learning 0.088s)
             Mean action noise std: 2.37
          Mean value_function loss: 107.9078
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 49.7209
                       Mean reward: 864.98
               Mean episode length: 232.66
    Episode_Reward/reaching_object: 1.3908
     Episode_Reward/lifting_object: 175.6307
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 2.04s
                      Time elapsed: 00:40:20
                               ETA: 00:34:36

################################################################################
                     [1m Learning iteration 1077/2000 [0m                     

                       Computation: 48235 steps/s (collection: 1.926s, learning 0.112s)
             Mean action noise std: 2.37
          Mean value_function loss: 111.0874
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 49.7288
                       Mean reward: 887.98
               Mean episode length: 236.99
    Episode_Reward/reaching_object: 1.4063
     Episode_Reward/lifting_object: 178.2431
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 2.04s
                      Time elapsed: 00:40:22
                               ETA: 00:34:34

################################################################################
                     [1m Learning iteration 1078/2000 [0m                     

                       Computation: 47790 steps/s (collection: 1.958s, learning 0.099s)
             Mean action noise std: 2.37
          Mean value_function loss: 103.1004
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 49.7347
                       Mean reward: 891.89
               Mean episode length: 237.17
    Episode_Reward/reaching_object: 1.4211
     Episode_Reward/lifting_object: 180.1165
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 2.06s
                      Time elapsed: 00:40:24
                               ETA: 00:34:31

################################################################################
                     [1m Learning iteration 1079/2000 [0m                     

                       Computation: 47340 steps/s (collection: 1.966s, learning 0.111s)
             Mean action noise std: 2.37
          Mean value_function loss: 112.4992
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 49.7436
                       Mean reward: 926.18
               Mean episode length: 246.20
    Episode_Reward/reaching_object: 1.4008
     Episode_Reward/lifting_object: 177.2529
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 2.08s
                      Time elapsed: 00:40:26
                               ETA: 00:34:29

################################################################################
                     [1m Learning iteration 1080/2000 [0m                     

                       Computation: 48077 steps/s (collection: 1.956s, learning 0.089s)
             Mean action noise std: 2.37
          Mean value_function loss: 92.1741
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 49.7588
                       Mean reward: 907.51
               Mean episode length: 242.15
    Episode_Reward/reaching_object: 1.3910
     Episode_Reward/lifting_object: 175.9709
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 2.04s
                      Time elapsed: 00:40:28
                               ETA: 00:34:27

################################################################################
                     [1m Learning iteration 1081/2000 [0m                     

                       Computation: 48295 steps/s (collection: 1.939s, learning 0.096s)
             Mean action noise std: 2.37
          Mean value_function loss: 113.3087
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 49.7735
                       Mean reward: 855.99
               Mean episode length: 230.35
    Episode_Reward/reaching_object: 1.3899
     Episode_Reward/lifting_object: 175.6335
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 2.04s
                      Time elapsed: 00:40:30
                               ETA: 00:34:24

################################################################################
                     [1m Learning iteration 1082/2000 [0m                     

                       Computation: 47092 steps/s (collection: 1.974s, learning 0.114s)
             Mean action noise std: 2.37
          Mean value_function loss: 136.9966
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 49.7828
                       Mean reward: 868.43
               Mean episode length: 234.12
    Episode_Reward/reaching_object: 1.3880
     Episode_Reward/lifting_object: 175.6165
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 2.09s
                      Time elapsed: 00:40:32
                               ETA: 00:34:22

################################################################################
                     [1m Learning iteration 1083/2000 [0m                     

                       Computation: 48511 steps/s (collection: 1.934s, learning 0.093s)
             Mean action noise std: 2.37
          Mean value_function loss: 102.6884
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 49.7921
                       Mean reward: 887.01
               Mean episode length: 237.75
    Episode_Reward/reaching_object: 1.3902
     Episode_Reward/lifting_object: 175.5480
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 2.03s
                      Time elapsed: 00:40:34
                               ETA: 00:34:19

################################################################################
                     [1m Learning iteration 1084/2000 [0m                     

                       Computation: 48081 steps/s (collection: 1.936s, learning 0.109s)
             Mean action noise std: 2.38
          Mean value_function loss: 93.5185
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.8046
                       Mean reward: 905.56
               Mean episode length: 241.76
    Episode_Reward/reaching_object: 1.3874
     Episode_Reward/lifting_object: 175.9746
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 2.04s
                      Time elapsed: 00:40:36
                               ETA: 00:34:17

################################################################################
                     [1m Learning iteration 1085/2000 [0m                     

                       Computation: 48578 steps/s (collection: 1.915s, learning 0.109s)
             Mean action noise std: 2.38
          Mean value_function loss: 80.4296
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 49.8137
                       Mean reward: 894.73
               Mean episode length: 238.80
    Episode_Reward/reaching_object: 1.4123
     Episode_Reward/lifting_object: 178.9198
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 2.02s
                      Time elapsed: 00:40:39
                               ETA: 00:34:14

################################################################################
                     [1m Learning iteration 1086/2000 [0m                     

                       Computation: 46999 steps/s (collection: 1.953s, learning 0.138s)
             Mean action noise std: 2.38
          Mean value_function loss: 101.1756
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 49.8281
                       Mean reward: 886.68
               Mean episode length: 238.99
    Episode_Reward/reaching_object: 1.3624
     Episode_Reward/lifting_object: 172.1695
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 2.09s
                      Time elapsed: 00:40:41
                               ETA: 00:34:12

################################################################################
                     [1m Learning iteration 1087/2000 [0m                     

                       Computation: 48266 steps/s (collection: 1.933s, learning 0.104s)
             Mean action noise std: 2.38
          Mean value_function loss: 101.2979
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.8484
                       Mean reward: 846.96
               Mean episode length: 228.11
    Episode_Reward/reaching_object: 1.3884
     Episode_Reward/lifting_object: 175.0920
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 2.04s
                      Time elapsed: 00:40:43
                               ETA: 00:34:10

################################################################################
                     [1m Learning iteration 1088/2000 [0m                     

                       Computation: 47997 steps/s (collection: 1.944s, learning 0.104s)
             Mean action noise std: 2.38
          Mean value_function loss: 68.7224
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 49.8635
                       Mean reward: 907.29
               Mean episode length: 241.70
    Episode_Reward/reaching_object: 1.4245
     Episode_Reward/lifting_object: 180.7996
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 2.05s
                      Time elapsed: 00:40:45
                               ETA: 00:34:07

################################################################################
                     [1m Learning iteration 1089/2000 [0m                     

                       Computation: 47954 steps/s (collection: 1.953s, learning 0.097s)
             Mean action noise std: 2.38
          Mean value_function loss: 79.8098
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.8724
                       Mean reward: 906.30
               Mean episode length: 241.27
    Episode_Reward/reaching_object: 1.4121
     Episode_Reward/lifting_object: 178.6195
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 2.05s
                      Time elapsed: 00:40:47
                               ETA: 00:34:05

################################################################################
                     [1m Learning iteration 1090/2000 [0m                     

                       Computation: 47101 steps/s (collection: 1.996s, learning 0.092s)
             Mean action noise std: 2.38
          Mean value_function loss: 63.1183
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 49.8803
                       Mean reward: 932.76
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 1.4184
     Episode_Reward/lifting_object: 179.8062
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 2.09s
                      Time elapsed: 00:40:49
                               ETA: 00:34:02

################################################################################
                     [1m Learning iteration 1091/2000 [0m                     

                       Computation: 47349 steps/s (collection: 1.969s, learning 0.108s)
             Mean action noise std: 2.38
          Mean value_function loss: 110.3838
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 49.8849
                       Mean reward: 887.95
               Mean episode length: 237.87
    Episode_Reward/reaching_object: 1.4188
     Episode_Reward/lifting_object: 179.4378
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 2.08s
                      Time elapsed: 00:40:51
                               ETA: 00:34:00

################################################################################
                     [1m Learning iteration 1092/2000 [0m                     

                       Computation: 47360 steps/s (collection: 1.966s, learning 0.110s)
             Mean action noise std: 2.39
          Mean value_function loss: 88.3759
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 49.8917
                       Mean reward: 878.14
               Mean episode length: 236.30
    Episode_Reward/reaching_object: 1.4023
     Episode_Reward/lifting_object: 177.0927
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 2.08s
                      Time elapsed: 00:40:53
                               ETA: 00:33:58

################################################################################
                     [1m Learning iteration 1093/2000 [0m                     

                       Computation: 48123 steps/s (collection: 1.953s, learning 0.090s)
             Mean action noise std: 2.39
          Mean value_function loss: 95.4848
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.9037
                       Mean reward: 847.07
               Mean episode length: 228.70
    Episode_Reward/reaching_object: 1.4025
     Episode_Reward/lifting_object: 176.6121
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 2.04s
                      Time elapsed: 00:40:55
                               ETA: 00:33:55

################################################################################
                     [1m Learning iteration 1094/2000 [0m                     

                       Computation: 47373 steps/s (collection: 1.974s, learning 0.102s)
             Mean action noise std: 2.39
          Mean value_function loss: 93.9444
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 49.9186
                       Mean reward: 904.27
               Mean episode length: 242.96
    Episode_Reward/reaching_object: 1.4040
     Episode_Reward/lifting_object: 177.0529
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 2.08s
                      Time elapsed: 00:40:57
                               ETA: 00:33:53

################################################################################
                     [1m Learning iteration 1095/2000 [0m                     

                       Computation: 47194 steps/s (collection: 1.975s, learning 0.108s)
             Mean action noise std: 2.39
          Mean value_function loss: 89.0263
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 49.9266
                       Mean reward: 888.69
               Mean episode length: 236.86
    Episode_Reward/reaching_object: 1.3947
     Episode_Reward/lifting_object: 176.2320
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 2.08s
                      Time elapsed: 00:40:59
                               ETA: 00:33:51

################################################################################
                     [1m Learning iteration 1096/2000 [0m                     

                       Computation: 47407 steps/s (collection: 1.964s, learning 0.110s)
             Mean action noise std: 2.39
          Mean value_function loss: 101.3338
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.9343
                       Mean reward: 909.37
               Mean episode length: 243.24
    Episode_Reward/reaching_object: 1.4164
     Episode_Reward/lifting_object: 178.2101
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 2.07s
                      Time elapsed: 00:41:01
                               ETA: 00:33:48

################################################################################
                     [1m Learning iteration 1097/2000 [0m                     

                       Computation: 47749 steps/s (collection: 1.972s, learning 0.087s)
             Mean action noise std: 2.39
          Mean value_function loss: 94.4584
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 49.9482
                       Mean reward: 881.42
               Mean episode length: 236.66
    Episode_Reward/reaching_object: 1.4008
     Episode_Reward/lifting_object: 176.3160
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 2.06s
                      Time elapsed: 00:41:03
                               ETA: 00:33:46

################################################################################
                     [1m Learning iteration 1098/2000 [0m                     

                       Computation: 47706 steps/s (collection: 1.973s, learning 0.088s)
             Mean action noise std: 2.39
          Mean value_function loss: 97.4175
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.9608
                       Mean reward: 882.22
               Mean episode length: 235.55
    Episode_Reward/reaching_object: 1.4046
     Episode_Reward/lifting_object: 176.7551
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 2.06s
                      Time elapsed: 00:41:05
                               ETA: 00:33:43

################################################################################
                     [1m Learning iteration 1099/2000 [0m                     

                       Computation: 48313 steps/s (collection: 1.943s, learning 0.092s)
             Mean action noise std: 2.40
          Mean value_function loss: 88.4158
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.9753
                       Mean reward: 894.15
               Mean episode length: 238.97
    Episode_Reward/reaching_object: 1.4112
     Episode_Reward/lifting_object: 177.3237
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 2.03s
                      Time elapsed: 00:41:07
                               ETA: 00:33:41

################################################################################
                     [1m Learning iteration 1100/2000 [0m                     

                       Computation: 47225 steps/s (collection: 1.984s, learning 0.098s)
             Mean action noise std: 2.40
          Mean value_function loss: 98.0145
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 49.9885
                       Mean reward: 906.52
               Mean episode length: 241.80
    Episode_Reward/reaching_object: 1.3795
     Episode_Reward/lifting_object: 173.8523
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 2.08s
                      Time elapsed: 00:41:09
                               ETA: 00:33:39

################################################################################
                     [1m Learning iteration 1101/2000 [0m                     

                       Computation: 46486 steps/s (collection: 2.012s, learning 0.103s)
             Mean action noise std: 2.40
          Mean value_function loss: 92.3827
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 49.9967
                       Mean reward: 902.15
               Mean episode length: 241.11
    Episode_Reward/reaching_object: 1.4099
     Episode_Reward/lifting_object: 176.9258
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 2.11s
                      Time elapsed: 00:41:12
                               ETA: 00:33:36

################################################################################
                     [1m Learning iteration 1102/2000 [0m                     

                       Computation: 46711 steps/s (collection: 1.962s, learning 0.143s)
             Mean action noise std: 2.40
          Mean value_function loss: 110.4216
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.0114
                       Mean reward: 888.70
               Mean episode length: 238.56
    Episode_Reward/reaching_object: 1.4199
     Episode_Reward/lifting_object: 178.4955
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 2.10s
                      Time elapsed: 00:41:14
                               ETA: 00:33:34

################################################################################
                     [1m Learning iteration 1103/2000 [0m                     

                       Computation: 46227 steps/s (collection: 2.014s, learning 0.112s)
             Mean action noise std: 2.40
          Mean value_function loss: 84.3698
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 50.0271
                       Mean reward: 892.66
               Mean episode length: 240.14
    Episode_Reward/reaching_object: 1.4145
     Episode_Reward/lifting_object: 178.4519
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 2.13s
                      Time elapsed: 00:41:16
                               ETA: 00:33:32

################################################################################
                     [1m Learning iteration 1104/2000 [0m                     

                       Computation: 46479 steps/s (collection: 2.024s, learning 0.091s)
             Mean action noise std: 2.40
          Mean value_function loss: 70.8616
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 50.0414
                       Mean reward: 902.32
               Mean episode length: 240.93
    Episode_Reward/reaching_object: 1.4015
     Episode_Reward/lifting_object: 176.9540
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 2.11s
                      Time elapsed: 00:41:18
                               ETA: 00:33:29

################################################################################
                     [1m Learning iteration 1105/2000 [0m                     

                       Computation: 47856 steps/s (collection: 1.952s, learning 0.102s)
             Mean action noise std: 2.40
          Mean value_function loss: 105.6007
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 50.0537
                       Mean reward: 888.64
               Mean episode length: 238.16
    Episode_Reward/reaching_object: 1.3994
     Episode_Reward/lifting_object: 175.9903
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 2.05s
                      Time elapsed: 00:41:20
                               ETA: 00:33:27

################################################################################
                     [1m Learning iteration 1106/2000 [0m                     

                       Computation: 47598 steps/s (collection: 1.973s, learning 0.093s)
             Mean action noise std: 2.41
          Mean value_function loss: 84.6657
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.0628
                       Mean reward: 885.05
               Mean episode length: 237.57
    Episode_Reward/reaching_object: 1.4267
     Episode_Reward/lifting_object: 179.4662
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 2.07s
                      Time elapsed: 00:41:22
                               ETA: 00:33:24

################################################################################
                     [1m Learning iteration 1107/2000 [0m                     

                       Computation: 48807 steps/s (collection: 1.928s, learning 0.086s)
             Mean action noise std: 2.41
          Mean value_function loss: 103.2256
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 50.0784
                       Mean reward: 890.48
               Mean episode length: 238.29
    Episode_Reward/reaching_object: 1.3940
     Episode_Reward/lifting_object: 175.3594
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 2.01s
                      Time elapsed: 00:41:24
                               ETA: 00:33:22

################################################################################
                     [1m Learning iteration 1108/2000 [0m                     

                       Computation: 48496 steps/s (collection: 1.941s, learning 0.086s)
             Mean action noise std: 2.41
          Mean value_function loss: 83.6721
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.0988
                       Mean reward: 917.97
               Mean episode length: 244.06
    Episode_Reward/reaching_object: 1.4122
     Episode_Reward/lifting_object: 177.5655
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 2.03s
                      Time elapsed: 00:41:26
                               ETA: 00:33:20

################################################################################
                     [1m Learning iteration 1109/2000 [0m                     

                       Computation: 48211 steps/s (collection: 1.947s, learning 0.092s)
             Mean action noise std: 2.41
          Mean value_function loss: 82.8839
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 50.1125
                       Mean reward: 912.22
               Mean episode length: 242.32
    Episode_Reward/reaching_object: 1.4041
     Episode_Reward/lifting_object: 176.7761
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 2.04s
                      Time elapsed: 00:41:28
                               ETA: 00:33:17

################################################################################
                     [1m Learning iteration 1110/2000 [0m                     

                       Computation: 48412 steps/s (collection: 1.934s, learning 0.097s)
             Mean action noise std: 2.41
          Mean value_function loss: 87.1756
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 50.1206
                       Mean reward: 913.54
               Mean episode length: 243.75
    Episode_Reward/reaching_object: 1.4024
     Episode_Reward/lifting_object: 176.5650
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0438
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 2.03s
                      Time elapsed: 00:41:30
                               ETA: 00:33:15

################################################################################
                     [1m Learning iteration 1111/2000 [0m                     

                       Computation: 47327 steps/s (collection: 1.940s, learning 0.137s)
             Mean action noise std: 2.41
          Mean value_function loss: 97.6024
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.1305
                       Mean reward: 876.44
               Mean episode length: 234.56
    Episode_Reward/reaching_object: 1.4022
     Episode_Reward/lifting_object: 176.7442
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 2.08s
                      Time elapsed: 00:41:32
                               ETA: 00:33:12

################################################################################
                     [1m Learning iteration 1112/2000 [0m                     

                       Computation: 48555 steps/s (collection: 1.932s, learning 0.093s)
             Mean action noise std: 2.41
          Mean value_function loss: 73.9760
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 50.1445
                       Mean reward: 900.16
               Mean episode length: 239.42
    Episode_Reward/reaching_object: 1.4241
     Episode_Reward/lifting_object: 179.8098
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 2.02s
                      Time elapsed: 00:41:34
                               ETA: 00:33:10

################################################################################
                     [1m Learning iteration 1113/2000 [0m                     

                       Computation: 47602 steps/s (collection: 1.979s, learning 0.087s)
             Mean action noise std: 2.42
          Mean value_function loss: 72.6118
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 50.1572
                       Mean reward: 881.38
               Mean episode length: 236.41
    Episode_Reward/reaching_object: 1.4334
     Episode_Reward/lifting_object: 179.6730
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 2.07s
                      Time elapsed: 00:41:36
                               ETA: 00:33:08

################################################################################
                     [1m Learning iteration 1114/2000 [0m                     

                       Computation: 48436 steps/s (collection: 1.942s, learning 0.088s)
             Mean action noise std: 2.42
          Mean value_function loss: 77.9905
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.1688
                       Mean reward: 861.69
               Mean episode length: 231.59
    Episode_Reward/reaching_object: 1.4185
     Episode_Reward/lifting_object: 178.0441
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 2.03s
                      Time elapsed: 00:41:38
                               ETA: 00:33:05

################################################################################
                     [1m Learning iteration 1115/2000 [0m                     

                       Computation: 47936 steps/s (collection: 1.955s, learning 0.096s)
             Mean action noise std: 2.42
          Mean value_function loss: 66.8039
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 50.1805
                       Mean reward: 920.98
               Mean episode length: 244.25
    Episode_Reward/reaching_object: 1.4306
     Episode_Reward/lifting_object: 179.6002
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 2.05s
                      Time elapsed: 00:41:40
                               ETA: 00:33:03

################################################################################
                     [1m Learning iteration 1116/2000 [0m                     

                       Computation: 47890 steps/s (collection: 1.953s, learning 0.100s)
             Mean action noise std: 2.42
          Mean value_function loss: 80.9026
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 50.1905
                       Mean reward: 906.37
               Mean episode length: 242.49
    Episode_Reward/reaching_object: 1.4177
     Episode_Reward/lifting_object: 177.5222
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 2.05s
                      Time elapsed: 00:41:42
                               ETA: 00:33:00

################################################################################
                     [1m Learning iteration 1117/2000 [0m                     

                       Computation: 48432 steps/s (collection: 1.942s, learning 0.088s)
             Mean action noise std: 2.42
          Mean value_function loss: 91.3704
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 50.2025
                       Mean reward: 921.27
               Mean episode length: 245.44
    Episode_Reward/reaching_object: 1.4432
     Episode_Reward/lifting_object: 180.6597
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 2.03s
                      Time elapsed: 00:41:45
                               ETA: 00:32:58

################################################################################
                     [1m Learning iteration 1118/2000 [0m                     

                       Computation: 47888 steps/s (collection: 1.960s, learning 0.093s)
             Mean action noise std: 2.42
          Mean value_function loss: 73.6416
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 50.2148
                       Mean reward: 911.00
               Mean episode length: 241.27
    Episode_Reward/reaching_object: 1.4394
     Episode_Reward/lifting_object: 180.8739
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 2.05s
                      Time elapsed: 00:41:47
                               ETA: 00:32:56

################################################################################
                     [1m Learning iteration 1119/2000 [0m                     

                       Computation: 48360 steps/s (collection: 1.926s, learning 0.107s)
             Mean action noise std: 2.42
          Mean value_function loss: 57.5175
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 50.2318
                       Mean reward: 892.64
               Mean episode length: 241.07
    Episode_Reward/reaching_object: 1.4478
     Episode_Reward/lifting_object: 181.2275
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 2.03s
                      Time elapsed: 00:41:49
                               ETA: 00:32:53

################################################################################
                     [1m Learning iteration 1120/2000 [0m                     

                       Computation: 47947 steps/s (collection: 1.954s, learning 0.096s)
             Mean action noise std: 2.43
          Mean value_function loss: 96.9577
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 50.2459
                       Mean reward: 896.20
               Mean episode length: 238.60
    Episode_Reward/reaching_object: 1.4213
     Episode_Reward/lifting_object: 178.2507
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 2.05s
                      Time elapsed: 00:41:51
                               ETA: 00:32:51

################################################################################
                     [1m Learning iteration 1121/2000 [0m                     

                       Computation: 47930 steps/s (collection: 1.965s, learning 0.086s)
             Mean action noise std: 2.43
          Mean value_function loss: 92.8380
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 50.2608
                       Mean reward: 895.45
               Mean episode length: 239.72
    Episode_Reward/reaching_object: 1.4147
     Episode_Reward/lifting_object: 177.4888
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 2.05s
                      Time elapsed: 00:41:53
                               ETA: 00:32:48

################################################################################
                     [1m Learning iteration 1122/2000 [0m                     

                       Computation: 48326 steps/s (collection: 1.924s, learning 0.111s)
             Mean action noise std: 2.43
          Mean value_function loss: 95.4417
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 50.2803
                       Mean reward: 872.73
               Mean episode length: 234.31
    Episode_Reward/reaching_object: 1.4180
     Episode_Reward/lifting_object: 177.0518
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 2.03s
                      Time elapsed: 00:41:55
                               ETA: 00:32:46

################################################################################
                     [1m Learning iteration 1123/2000 [0m                     

                       Computation: 46886 steps/s (collection: 2.010s, learning 0.087s)
             Mean action noise std: 2.43
          Mean value_function loss: 124.6185
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 50.2971
                       Mean reward: 886.51
               Mean episode length: 236.59
    Episode_Reward/reaching_object: 1.4129
     Episode_Reward/lifting_object: 176.7901
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 2.10s
                      Time elapsed: 00:41:57
                               ETA: 00:32:44

################################################################################
                     [1m Learning iteration 1124/2000 [0m                     

                       Computation: 46756 steps/s (collection: 2.003s, learning 0.100s)
             Mean action noise std: 2.43
          Mean value_function loss: 87.8400
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 50.3066
                       Mean reward: 895.28
               Mean episode length: 240.66
    Episode_Reward/reaching_object: 1.4120
     Episode_Reward/lifting_object: 175.7950
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 2.10s
                      Time elapsed: 00:41:59
                               ETA: 00:32:41

################################################################################
                     [1m Learning iteration 1125/2000 [0m                     

                       Computation: 48068 steps/s (collection: 1.957s, learning 0.089s)
             Mean action noise std: 2.44
          Mean value_function loss: 69.0365
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 50.3216
                       Mean reward: 921.70
               Mean episode length: 245.20
    Episode_Reward/reaching_object: 1.4189
     Episode_Reward/lifting_object: 177.9555
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 2.05s
                      Time elapsed: 00:42:01
                               ETA: 00:32:39

################################################################################
                     [1m Learning iteration 1126/2000 [0m                     

                       Computation: 47865 steps/s (collection: 1.952s, learning 0.102s)
             Mean action noise std: 2.44
          Mean value_function loss: 115.7159
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.3380
                       Mean reward: 911.36
               Mean episode length: 242.85
    Episode_Reward/reaching_object: 1.4131
     Episode_Reward/lifting_object: 176.7122
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 2.05s
                      Time elapsed: 00:42:03
                               ETA: 00:32:37

################################################################################
                     [1m Learning iteration 1127/2000 [0m                     

                       Computation: 47963 steps/s (collection: 1.946s, learning 0.104s)
             Mean action noise std: 2.44
          Mean value_function loss: 93.8915
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 50.3463
                       Mean reward: 912.20
               Mean episode length: 242.97
    Episode_Reward/reaching_object: 1.4281
     Episode_Reward/lifting_object: 178.6751
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 2.05s
                      Time elapsed: 00:42:05
                               ETA: 00:32:34

################################################################################
                     [1m Learning iteration 1128/2000 [0m                     

                       Computation: 47666 steps/s (collection: 1.971s, learning 0.092s)
             Mean action noise std: 2.44
          Mean value_function loss: 84.0690
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 50.3574
                       Mean reward: 900.74
               Mean episode length: 239.75
    Episode_Reward/reaching_object: 1.4347
     Episode_Reward/lifting_object: 179.9056
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 2.06s
                      Time elapsed: 00:42:07
                               ETA: 00:32:32

################################################################################
                     [1m Learning iteration 1129/2000 [0m                     

                       Computation: 48412 steps/s (collection: 1.940s, learning 0.091s)
             Mean action noise std: 2.44
          Mean value_function loss: 99.3333
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 50.3746
                       Mean reward: 893.52
               Mean episode length: 238.57
    Episode_Reward/reaching_object: 1.4000
     Episode_Reward/lifting_object: 175.2212
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 2.03s
                      Time elapsed: 00:42:09
                               ETA: 00:32:29

################################################################################
                     [1m Learning iteration 1130/2000 [0m                     

                       Computation: 46573 steps/s (collection: 2.007s, learning 0.104s)
             Mean action noise std: 2.44
          Mean value_function loss: 110.6120
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.3888
                       Mean reward: 901.59
               Mean episode length: 239.99
    Episode_Reward/reaching_object: 1.3969
     Episode_Reward/lifting_object: 174.2671
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 2.11s
                      Time elapsed: 00:42:11
                               ETA: 00:32:27

################################################################################
                     [1m Learning iteration 1131/2000 [0m                     

                       Computation: 45978 steps/s (collection: 2.035s, learning 0.103s)
             Mean action noise std: 2.44
          Mean value_function loss: 85.7193
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 50.4006
                       Mean reward: 913.93
               Mean episode length: 243.00
    Episode_Reward/reaching_object: 1.4303
     Episode_Reward/lifting_object: 179.2461
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 2.14s
                      Time elapsed: 00:42:13
                               ETA: 00:32:25

################################################################################
                     [1m Learning iteration 1132/2000 [0m                     

                       Computation: 47925 steps/s (collection: 1.964s, learning 0.088s)
             Mean action noise std: 2.45
          Mean value_function loss: 87.7599
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 50.4133
                       Mean reward: 906.68
               Mean episode length: 241.89
    Episode_Reward/reaching_object: 1.4102
     Episode_Reward/lifting_object: 176.3806
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 2.05s
                      Time elapsed: 00:42:15
                               ETA: 00:32:22

################################################################################
                     [1m Learning iteration 1133/2000 [0m                     

                       Computation: 46761 steps/s (collection: 2.005s, learning 0.098s)
             Mean action noise std: 2.45
          Mean value_function loss: 83.3186
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 50.4247
                       Mean reward: 888.82
               Mean episode length: 237.85
    Episode_Reward/reaching_object: 1.4220
     Episode_Reward/lifting_object: 177.3221
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 2.10s
                      Time elapsed: 00:42:18
                               ETA: 00:32:20

################################################################################
                     [1m Learning iteration 1134/2000 [0m                     

                       Computation: 47375 steps/s (collection: 1.987s, learning 0.088s)
             Mean action noise std: 2.45
          Mean value_function loss: 82.9224
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.4399
                       Mean reward: 890.47
               Mean episode length: 237.06
    Episode_Reward/reaching_object: 1.4303
     Episode_Reward/lifting_object: 179.2229
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 2.08s
                      Time elapsed: 00:42:20
                               ETA: 00:32:18

################################################################################
                     [1m Learning iteration 1135/2000 [0m                     

                       Computation: 46469 steps/s (collection: 2.006s, learning 0.109s)
             Mean action noise std: 2.45
          Mean value_function loss: 100.8256
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 50.4503
                       Mean reward: 893.51
               Mean episode length: 238.23
    Episode_Reward/reaching_object: 1.4179
     Episode_Reward/lifting_object: 177.2198
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 2.12s
                      Time elapsed: 00:42:22
                               ETA: 00:32:15

################################################################################
                     [1m Learning iteration 1136/2000 [0m                     

                       Computation: 47315 steps/s (collection: 1.969s, learning 0.109s)
             Mean action noise std: 2.45
          Mean value_function loss: 91.3948
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 50.4557
                       Mean reward: 899.27
               Mean episode length: 240.17
    Episode_Reward/reaching_object: 1.4285
     Episode_Reward/lifting_object: 178.1314
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 2.08s
                      Time elapsed: 00:42:24
                               ETA: 00:32:13

################################################################################
                     [1m Learning iteration 1137/2000 [0m                     

                       Computation: 46367 steps/s (collection: 2.000s, learning 0.120s)
             Mean action noise std: 2.45
          Mean value_function loss: 73.4390
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 50.4642
                       Mean reward: 899.47
               Mean episode length: 240.09
    Episode_Reward/reaching_object: 1.4277
     Episode_Reward/lifting_object: 178.7001
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 2.12s
                      Time elapsed: 00:42:26
                               ETA: 00:32:11

################################################################################
                     [1m Learning iteration 1138/2000 [0m                     

                       Computation: 47285 steps/s (collection: 1.979s, learning 0.100s)
             Mean action noise std: 2.45
          Mean value_function loss: 97.1074
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.4744
                       Mean reward: 878.29
               Mean episode length: 234.85
    Episode_Reward/reaching_object: 1.4266
     Episode_Reward/lifting_object: 178.3895
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 2.08s
                      Time elapsed: 00:42:28
                               ETA: 00:32:08

################################################################################
                     [1m Learning iteration 1139/2000 [0m                     

                       Computation: 47398 steps/s (collection: 1.969s, learning 0.105s)
             Mean action noise std: 2.45
          Mean value_function loss: 101.6633
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 50.4862
                       Mean reward: 906.30
               Mean episode length: 243.03
    Episode_Reward/reaching_object: 1.4020
     Episode_Reward/lifting_object: 174.8033
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 2.07s
                      Time elapsed: 00:42:30
                               ETA: 00:32:06

################################################################################
                     [1m Learning iteration 1140/2000 [0m                     

                       Computation: 46773 steps/s (collection: 2.016s, learning 0.086s)
             Mean action noise std: 2.45
          Mean value_function loss: 75.9230
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 50.4939
                       Mean reward: 892.77
               Mean episode length: 238.63
    Episode_Reward/reaching_object: 1.4388
     Episode_Reward/lifting_object: 179.8100
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 2.10s
                      Time elapsed: 00:42:32
                               ETA: 00:32:04

################################################################################
                     [1m Learning iteration 1141/2000 [0m                     

                       Computation: 46672 steps/s (collection: 1.990s, learning 0.116s)
             Mean action noise std: 2.46
          Mean value_function loss: 119.1976
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 50.5039
                       Mean reward: 883.00
               Mean episode length: 236.20
    Episode_Reward/reaching_object: 1.4122
     Episode_Reward/lifting_object: 175.9531
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 2.11s
                      Time elapsed: 00:42:34
                               ETA: 00:32:01

################################################################################
                     [1m Learning iteration 1142/2000 [0m                     

                       Computation: 46922 steps/s (collection: 1.986s, learning 0.109s)
             Mean action noise std: 2.46
          Mean value_function loss: 100.7389
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.5167
                       Mean reward: 913.40
               Mean episode length: 243.78
    Episode_Reward/reaching_object: 1.4063
     Episode_Reward/lifting_object: 175.5448
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 2.10s
                      Time elapsed: 00:42:36
                               ETA: 00:31:59

################################################################################
                     [1m Learning iteration 1143/2000 [0m                     

                       Computation: 47705 steps/s (collection: 1.964s, learning 0.097s)
             Mean action noise std: 2.46
          Mean value_function loss: 78.2294
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 50.5243
                       Mean reward: 911.60
               Mean episode length: 243.08
    Episode_Reward/reaching_object: 1.4407
     Episode_Reward/lifting_object: 179.7006
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 2.06s
                      Time elapsed: 00:42:38
                               ETA: 00:31:56

################################################################################
                     [1m Learning iteration 1144/2000 [0m                     

                       Computation: 47506 steps/s (collection: 1.974s, learning 0.096s)
             Mean action noise std: 2.46
          Mean value_function loss: 98.0366
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 50.5340
                       Mean reward: 864.76
               Mean episode length: 231.63
    Episode_Reward/reaching_object: 1.4017
     Episode_Reward/lifting_object: 175.2251
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 2.07s
                      Time elapsed: 00:42:41
                               ETA: 00:31:54

################################################################################
                     [1m Learning iteration 1145/2000 [0m                     

                       Computation: 47938 steps/s (collection: 1.951s, learning 0.100s)
             Mean action noise std: 2.46
          Mean value_function loss: 72.8724
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.5429
                       Mean reward: 916.67
               Mean episode length: 244.06
    Episode_Reward/reaching_object: 1.4343
     Episode_Reward/lifting_object: 178.6776
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 2.05s
                      Time elapsed: 00:42:43
                               ETA: 00:31:52

################################################################################
                     [1m Learning iteration 1146/2000 [0m                     

                       Computation: 47543 steps/s (collection: 1.979s, learning 0.089s)
             Mean action noise std: 2.46
          Mean value_function loss: 80.5173
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 50.5580
                       Mean reward: 919.50
               Mean episode length: 246.06
    Episode_Reward/reaching_object: 1.4266
     Episode_Reward/lifting_object: 178.2765
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 2.07s
                      Time elapsed: 00:42:45
                               ETA: 00:31:49

################################################################################
                     [1m Learning iteration 1147/2000 [0m                     

                       Computation: 47591 steps/s (collection: 1.964s, learning 0.102s)
             Mean action noise std: 2.46
          Mean value_function loss: 83.7498
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 50.5700
                       Mean reward: 889.34
               Mean episode length: 237.11
    Episode_Reward/reaching_object: 1.4385
     Episode_Reward/lifting_object: 179.7754
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 2.07s
                      Time elapsed: 00:42:47
                               ETA: 00:31:47

################################################################################
                     [1m Learning iteration 1148/2000 [0m                     

                       Computation: 47436 steps/s (collection: 1.960s, learning 0.113s)
             Mean action noise std: 2.46
          Mean value_function loss: 73.0402
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 50.5758
                       Mean reward: 909.70
               Mean episode length: 241.19
    Episode_Reward/reaching_object: 1.4394
     Episode_Reward/lifting_object: 180.2753
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 2.07s
                      Time elapsed: 00:42:49
                               ETA: 00:31:45

################################################################################
                     [1m Learning iteration 1149/2000 [0m                     

                       Computation: 47395 steps/s (collection: 1.957s, learning 0.118s)
             Mean action noise std: 2.46
          Mean value_function loss: 86.7450
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 50.5808
                       Mean reward: 919.44
               Mean episode length: 244.34
    Episode_Reward/reaching_object: 1.4128
     Episode_Reward/lifting_object: 176.1596
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 2.07s
                      Time elapsed: 00:42:51
                               ETA: 00:31:42

################################################################################
                     [1m Learning iteration 1150/2000 [0m                     

                       Computation: 46491 steps/s (collection: 2.005s, learning 0.109s)
             Mean action noise std: 2.47
          Mean value_function loss: 88.2512
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.5908
                       Mean reward: 890.34
               Mean episode length: 238.97
    Episode_Reward/reaching_object: 1.4053
     Episode_Reward/lifting_object: 174.4187
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 2.11s
                      Time elapsed: 00:42:53
                               ETA: 00:31:40

################################################################################
                     [1m Learning iteration 1151/2000 [0m                     

                       Computation: 45872 steps/s (collection: 2.013s, learning 0.130s)
             Mean action noise std: 2.47
          Mean value_function loss: 76.9417
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.5995
                       Mean reward: 923.32
               Mean episode length: 245.04
    Episode_Reward/reaching_object: 1.4417
     Episode_Reward/lifting_object: 180.3119
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 2.14s
                      Time elapsed: 00:42:55
                               ETA: 00:31:38

################################################################################
                     [1m Learning iteration 1152/2000 [0m                     

                       Computation: 47312 steps/s (collection: 1.962s, learning 0.116s)
             Mean action noise std: 2.47
          Mean value_function loss: 91.4600
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.6111
                       Mean reward: 881.30
               Mean episode length: 234.00
    Episode_Reward/reaching_object: 1.4187
     Episode_Reward/lifting_object: 176.9331
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 2.08s
                      Time elapsed: 00:42:57
                               ETA: 00:31:35

################################################################################
                     [1m Learning iteration 1153/2000 [0m                     

                       Computation: 47163 steps/s (collection: 1.969s, learning 0.115s)
             Mean action noise std: 2.47
          Mean value_function loss: 67.6838
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.6197
                       Mean reward: 917.71
               Mean episode length: 244.11
    Episode_Reward/reaching_object: 1.4369
     Episode_Reward/lifting_object: 179.3994
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 2.08s
                      Time elapsed: 00:42:59
                               ETA: 00:31:33

################################################################################
                     [1m Learning iteration 1154/2000 [0m                     

                       Computation: 47501 steps/s (collection: 1.978s, learning 0.092s)
             Mean action noise std: 2.47
          Mean value_function loss: 79.1515
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 50.6286
                       Mean reward: 895.55
               Mean episode length: 239.24
    Episode_Reward/reaching_object: 1.4102
     Episode_Reward/lifting_object: 175.5160
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 2.07s
                      Time elapsed: 00:43:01
                               ETA: 00:31:31

################################################################################
                     [1m Learning iteration 1155/2000 [0m                     

                       Computation: 47474 steps/s (collection: 1.974s, learning 0.097s)
             Mean action noise std: 2.47
          Mean value_function loss: 69.3547
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 50.6377
                       Mean reward: 929.74
               Mean episode length: 246.36
    Episode_Reward/reaching_object: 1.4329
     Episode_Reward/lifting_object: 178.4927
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 2.07s
                      Time elapsed: 00:43:03
                               ETA: 00:31:28

################################################################################
                     [1m Learning iteration 1156/2000 [0m                     

                       Computation: 47450 steps/s (collection: 1.978s, learning 0.094s)
             Mean action noise std: 2.47
          Mean value_function loss: 85.3702
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 50.6431
                       Mean reward: 900.04
               Mean episode length: 239.01
    Episode_Reward/reaching_object: 1.4276
     Episode_Reward/lifting_object: 178.1779
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 2.07s
                      Time elapsed: 00:43:06
                               ETA: 00:31:26

################################################################################
                     [1m Learning iteration 1157/2000 [0m                     

                       Computation: 47451 steps/s (collection: 1.986s, learning 0.086s)
             Mean action noise std: 2.47
          Mean value_function loss: 95.9088
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 50.6485
                       Mean reward: 905.91
               Mean episode length: 239.92
    Episode_Reward/reaching_object: 1.4370
     Episode_Reward/lifting_object: 179.2140
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 2.07s
                      Time elapsed: 00:43:08
                               ETA: 00:31:24

################################################################################
                     [1m Learning iteration 1158/2000 [0m                     

                       Computation: 47891 steps/s (collection: 1.952s, learning 0.100s)
             Mean action noise std: 2.47
          Mean value_function loss: 82.5019
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 50.6575
                       Mean reward: 887.91
               Mean episode length: 237.18
    Episode_Reward/reaching_object: 1.4160
     Episode_Reward/lifting_object: 176.3962
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 2.05s
                      Time elapsed: 00:43:10
                               ETA: 00:31:21

################################################################################
                     [1m Learning iteration 1159/2000 [0m                     

                       Computation: 47808 steps/s (collection: 1.965s, learning 0.092s)
             Mean action noise std: 2.47
          Mean value_function loss: 85.2628
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 50.6643
                       Mean reward: 891.77
               Mean episode length: 238.85
    Episode_Reward/reaching_object: 1.4340
     Episode_Reward/lifting_object: 178.6277
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 2.06s
                      Time elapsed: 00:43:12
                               ETA: 00:31:19

################################################################################
                     [1m Learning iteration 1160/2000 [0m                     

                       Computation: 47990 steps/s (collection: 1.958s, learning 0.090s)
             Mean action noise std: 2.47
          Mean value_function loss: 90.0892
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.6687
                       Mean reward: 880.74
               Mean episode length: 234.48
    Episode_Reward/reaching_object: 1.4016
     Episode_Reward/lifting_object: 174.2723
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 2.05s
                      Time elapsed: 00:43:14
                               ETA: 00:31:16

################################################################################
                     [1m Learning iteration 1161/2000 [0m                     

                       Computation: 46643 steps/s (collection: 2.012s, learning 0.096s)
             Mean action noise std: 2.48
          Mean value_function loss: 86.8080
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 50.6744
                       Mean reward: 859.01
               Mean episode length: 230.13
    Episode_Reward/reaching_object: 1.4159
     Episode_Reward/lifting_object: 176.4901
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 2.11s
                      Time elapsed: 00:43:16
                               ETA: 00:31:14

################################################################################
                     [1m Learning iteration 1162/2000 [0m                     

                       Computation: 47747 steps/s (collection: 1.965s, learning 0.094s)
             Mean action noise std: 2.48
          Mean value_function loss: 66.9777
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.6825
                       Mean reward: 887.84
               Mean episode length: 237.52
    Episode_Reward/reaching_object: 1.4499
     Episode_Reward/lifting_object: 180.6299
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 2.06s
                      Time elapsed: 00:43:18
                               ETA: 00:31:12

################################################################################
                     [1m Learning iteration 1163/2000 [0m                     

                       Computation: 47411 steps/s (collection: 1.977s, learning 0.097s)
             Mean action noise std: 2.48
          Mean value_function loss: 62.2243
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 50.6896
                       Mean reward: 912.77
               Mean episode length: 242.26
    Episode_Reward/reaching_object: 1.4553
     Episode_Reward/lifting_object: 181.3536
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 2.07s
                      Time elapsed: 00:43:20
                               ETA: 00:31:09

################################################################################
                     [1m Learning iteration 1164/2000 [0m                     

                       Computation: 48019 steps/s (collection: 1.955s, learning 0.092s)
             Mean action noise std: 2.48
          Mean value_function loss: 61.7796
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 50.6969
                       Mean reward: 900.18
               Mean episode length: 240.21
    Episode_Reward/reaching_object: 1.4409
     Episode_Reward/lifting_object: 178.7436
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 2.05s
                      Time elapsed: 00:43:22
                               ETA: 00:31:07

################################################################################
                     [1m Learning iteration 1165/2000 [0m                     

                       Computation: 46995 steps/s (collection: 1.978s, learning 0.113s)
             Mean action noise std: 2.48
          Mean value_function loss: 69.9862
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 50.7053
                       Mean reward: 893.90
               Mean episode length: 238.53
    Episode_Reward/reaching_object: 1.4366
     Episode_Reward/lifting_object: 178.9804
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 2.09s
                      Time elapsed: 00:43:24
                               ETA: 00:31:05

################################################################################
                     [1m Learning iteration 1166/2000 [0m                     

                       Computation: 46739 steps/s (collection: 1.973s, learning 0.130s)
             Mean action noise std: 2.48
          Mean value_function loss: 74.7221
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 50.7150
                       Mean reward: 914.41
               Mean episode length: 243.94
    Episode_Reward/reaching_object: 1.4365
     Episode_Reward/lifting_object: 179.1623
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 2.10s
                      Time elapsed: 00:43:26
                               ETA: 00:31:02

################################################################################
                     [1m Learning iteration 1167/2000 [0m                     

                       Computation: 47692 steps/s (collection: 1.971s, learning 0.090s)
             Mean action noise std: 2.48
          Mean value_function loss: 80.1346
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.7255
                       Mean reward: 877.47
               Mean episode length: 234.66
    Episode_Reward/reaching_object: 1.4340
     Episode_Reward/lifting_object: 178.4169
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 2.06s
                      Time elapsed: 00:43:28
                               ETA: 00:31:00

################################################################################
                     [1m Learning iteration 1168/2000 [0m                     

                       Computation: 48521 steps/s (collection: 1.928s, learning 0.098s)
             Mean action noise std: 2.48
          Mean value_function loss: 64.5927
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.7339
                       Mean reward: 903.82
               Mean episode length: 240.20
    Episode_Reward/reaching_object: 1.4376
     Episode_Reward/lifting_object: 178.2783
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 2.03s
                      Time elapsed: 00:43:30
                               ETA: 00:30:58

################################################################################
                     [1m Learning iteration 1169/2000 [0m                     

                       Computation: 45437 steps/s (collection: 2.066s, learning 0.098s)
             Mean action noise std: 2.48
          Mean value_function loss: 95.0808
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.7411
                       Mean reward: 901.72
               Mean episode length: 240.44
    Episode_Reward/reaching_object: 1.4284
     Episode_Reward/lifting_object: 177.5418
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 2.16s
                      Time elapsed: 00:43:32
                               ETA: 00:30:55

################################################################################
                     [1m Learning iteration 1170/2000 [0m                     

                       Computation: 46338 steps/s (collection: 2.032s, learning 0.089s)
             Mean action noise std: 2.49
          Mean value_function loss: 83.8589
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 50.7532
                       Mean reward: 867.97
               Mean episode length: 233.01
    Episode_Reward/reaching_object: 1.4171
     Episode_Reward/lifting_object: 175.8434
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 2.12s
                      Time elapsed: 00:43:35
                               ETA: 00:30:53

################################################################################
                     [1m Learning iteration 1171/2000 [0m                     

                       Computation: 47051 steps/s (collection: 1.988s, learning 0.101s)
             Mean action noise std: 2.49
          Mean value_function loss: 73.8503
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.7676
                       Mean reward: 904.06
               Mean episode length: 241.01
    Episode_Reward/reaching_object: 1.4399
     Episode_Reward/lifting_object: 178.9156
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 2.09s
                      Time elapsed: 00:43:37
                               ETA: 00:30:51

################################################################################
                     [1m Learning iteration 1172/2000 [0m                     

                       Computation: 43866 steps/s (collection: 2.123s, learning 0.118s)
             Mean action noise std: 2.49
          Mean value_function loss: 67.6205
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 50.7777
                       Mean reward: 919.82
               Mean episode length: 243.33
    Episode_Reward/reaching_object: 1.4433
     Episode_Reward/lifting_object: 179.3013
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 2.24s
                      Time elapsed: 00:43:39
                               ETA: 00:30:49

################################################################################
                     [1m Learning iteration 1173/2000 [0m                     

                       Computation: 44051 steps/s (collection: 2.037s, learning 0.195s)
             Mean action noise std: 2.49
          Mean value_function loss: 87.9565
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.7962
                       Mean reward: 890.34
               Mean episode length: 237.62
    Episode_Reward/reaching_object: 1.4120
     Episode_Reward/lifting_object: 175.3433
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 2.23s
                      Time elapsed: 00:43:41
                               ETA: 00:30:46

################################################################################
                     [1m Learning iteration 1174/2000 [0m                     

                       Computation: 46197 steps/s (collection: 1.986s, learning 0.142s)
             Mean action noise std: 2.49
          Mean value_function loss: 80.5991
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 50.8096
                       Mean reward: 882.58
               Mean episode length: 235.17
    Episode_Reward/reaching_object: 1.4185
     Episode_Reward/lifting_object: 176.2657
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 2.13s
                      Time elapsed: 00:43:43
                               ETA: 00:30:44

################################################################################
                     [1m Learning iteration 1175/2000 [0m                     

                       Computation: 46244 steps/s (collection: 1.982s, learning 0.144s)
             Mean action noise std: 2.49
          Mean value_function loss: 75.2964
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 50.8186
                       Mean reward: 866.64
               Mean episode length: 232.28
    Episode_Reward/reaching_object: 1.4153
     Episode_Reward/lifting_object: 175.9375
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 2.13s
                      Time elapsed: 00:43:45
                               ETA: 00:30:42

################################################################################
                     [1m Learning iteration 1176/2000 [0m                     

                       Computation: 43661 steps/s (collection: 2.141s, learning 0.111s)
             Mean action noise std: 2.49
          Mean value_function loss: 73.5199
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.8331
                       Mean reward: 915.52
               Mean episode length: 242.71
    Episode_Reward/reaching_object: 1.4495
     Episode_Reward/lifting_object: 180.5022
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 2.25s
                      Time elapsed: 00:43:48
                               ETA: 00:30:39

################################################################################
                     [1m Learning iteration 1177/2000 [0m                     

                       Computation: 45744 steps/s (collection: 2.012s, learning 0.137s)
             Mean action noise std: 2.49
          Mean value_function loss: 55.2060
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 50.8420
                       Mean reward: 923.91
               Mean episode length: 243.98
    Episode_Reward/reaching_object: 1.4752
     Episode_Reward/lifting_object: 184.6413
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 2.15s
                      Time elapsed: 00:43:50
                               ETA: 00:30:37

################################################################################
                     [1m Learning iteration 1178/2000 [0m                     

                       Computation: 46949 steps/s (collection: 2.008s, learning 0.086s)
             Mean action noise std: 2.50
          Mean value_function loss: 86.4384
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 50.8500
                       Mean reward: 891.94
               Mean episode length: 236.41
    Episode_Reward/reaching_object: 1.4397
     Episode_Reward/lifting_object: 179.5472
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 2.09s
                      Time elapsed: 00:43:52
                               ETA: 00:30:35

################################################################################
                     [1m Learning iteration 1179/2000 [0m                     

                       Computation: 47473 steps/s (collection: 1.981s, learning 0.090s)
             Mean action noise std: 2.50
          Mean value_function loss: 78.9575
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.8598
                       Mean reward: 901.71
               Mean episode length: 240.62
    Episode_Reward/reaching_object: 1.3776
     Episode_Reward/lifting_object: 170.5346
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 2.07s
                      Time elapsed: 00:43:54
                               ETA: 00:30:32

################################################################################
                     [1m Learning iteration 1180/2000 [0m                     

                       Computation: 47748 steps/s (collection: 1.964s, learning 0.095s)
             Mean action noise std: 2.50
          Mean value_function loss: 79.6481
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 50.8672
                       Mean reward: 881.19
               Mean episode length: 235.04
    Episode_Reward/reaching_object: 1.4182
     Episode_Reward/lifting_object: 176.1727
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 2.06s
                      Time elapsed: 00:43:56
                               ETA: 00:30:30

################################################################################
                     [1m Learning iteration 1181/2000 [0m                     

                       Computation: 47157 steps/s (collection: 1.986s, learning 0.099s)
             Mean action noise std: 2.50
          Mean value_function loss: 105.5236
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 50.8760
                       Mean reward: 862.34
               Mean episode length: 231.67
    Episode_Reward/reaching_object: 1.4241
     Episode_Reward/lifting_object: 177.5434
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 2.08s
                      Time elapsed: 00:43:58
                               ETA: 00:30:28

################################################################################
                     [1m Learning iteration 1182/2000 [0m                     

                       Computation: 46118 steps/s (collection: 2.015s, learning 0.117s)
             Mean action noise std: 2.50
          Mean value_function loss: 84.9444
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 50.8836
                       Mean reward: 900.70
               Mean episode length: 239.45
    Episode_Reward/reaching_object: 1.4295
     Episode_Reward/lifting_object: 178.1131
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 2.13s
                      Time elapsed: 00:44:00
                               ETA: 00:30:25

################################################################################
                     [1m Learning iteration 1183/2000 [0m                     

                       Computation: 47362 steps/s (collection: 1.981s, learning 0.094s)
             Mean action noise std: 2.50
          Mean value_function loss: 79.6684
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 50.8929
                       Mean reward: 917.45
               Mean episode length: 243.75
    Episode_Reward/reaching_object: 1.4313
     Episode_Reward/lifting_object: 178.6025
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 2.08s
                      Time elapsed: 00:44:02
                               ETA: 00:30:23

################################################################################
                     [1m Learning iteration 1184/2000 [0m                     

                       Computation: 46935 steps/s (collection: 1.972s, learning 0.122s)
             Mean action noise std: 2.50
          Mean value_function loss: 89.1973
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 50.9090
                       Mean reward: 911.38
               Mean episode length: 241.92
    Episode_Reward/reaching_object: 1.4283
     Episode_Reward/lifting_object: 177.8099
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 2.09s
                      Time elapsed: 00:44:04
                               ETA: 00:30:21

################################################################################
                     [1m Learning iteration 1185/2000 [0m                     

                       Computation: 47284 steps/s (collection: 1.968s, learning 0.111s)
             Mean action noise std: 2.50
          Mean value_function loss: 83.3725
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 50.9210
                       Mean reward: 880.63
               Mean episode length: 234.98
    Episode_Reward/reaching_object: 1.3931
     Episode_Reward/lifting_object: 173.4322
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 2.08s
                      Time elapsed: 00:44:06
                               ETA: 00:30:18

################################################################################
                     [1m Learning iteration 1186/2000 [0m                     

                       Computation: 46420 steps/s (collection: 2.018s, learning 0.100s)
             Mean action noise std: 2.51
          Mean value_function loss: 83.0872
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 50.9295
                       Mean reward: 907.62
               Mean episode length: 240.67
    Episode_Reward/reaching_object: 1.4283
     Episode_Reward/lifting_object: 177.3562
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 2.12s
                      Time elapsed: 00:44:09
                               ETA: 00:30:16

################################################################################
                     [1m Learning iteration 1187/2000 [0m                     

                       Computation: 45514 steps/s (collection: 2.045s, learning 0.115s)
             Mean action noise std: 2.51
          Mean value_function loss: 90.4208
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 50.9422
                       Mean reward: 846.99
               Mean episode length: 227.89
    Episode_Reward/reaching_object: 1.3936
     Episode_Reward/lifting_object: 173.2358
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 2.16s
                      Time elapsed: 00:44:11
                               ETA: 00:30:14

################################################################################
                     [1m Learning iteration 1188/2000 [0m                     

                       Computation: 46595 steps/s (collection: 2.007s, learning 0.103s)
             Mean action noise std: 2.51
          Mean value_function loss: 70.0753
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 50.9494
                       Mean reward: 923.60
               Mean episode length: 243.90
    Episode_Reward/reaching_object: 1.4346
     Episode_Reward/lifting_object: 179.1995
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 2.11s
                      Time elapsed: 00:44:13
                               ETA: 00:30:12

################################################################################
                     [1m Learning iteration 1189/2000 [0m                     

                       Computation: 44916 steps/s (collection: 2.048s, learning 0.141s)
             Mean action noise std: 2.51
          Mean value_function loss: 78.8674
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 50.9550
                       Mean reward: 938.32
               Mean episode length: 248.78
    Episode_Reward/reaching_object: 1.4437
     Episode_Reward/lifting_object: 180.0207
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 2.19s
                      Time elapsed: 00:44:15
                               ETA: 00:30:09

################################################################################
                     [1m Learning iteration 1190/2000 [0m                     

                       Computation: 45123 steps/s (collection: 2.082s, learning 0.097s)
             Mean action noise std: 2.51
          Mean value_function loss: 76.0645
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.9610
                       Mean reward: 859.01
               Mean episode length: 229.97
    Episode_Reward/reaching_object: 1.4228
     Episode_Reward/lifting_object: 176.5002
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 2.18s
                      Time elapsed: 00:44:17
                               ETA: 00:30:07

################################################################################
                     [1m Learning iteration 1191/2000 [0m                     

                       Computation: 45861 steps/s (collection: 2.000s, learning 0.143s)
             Mean action noise std: 2.51
          Mean value_function loss: 81.8461
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 50.9653
                       Mean reward: 908.87
               Mean episode length: 242.10
    Episode_Reward/reaching_object: 1.4394
     Episode_Reward/lifting_object: 179.5713
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 2.14s
                      Time elapsed: 00:44:19
                               ETA: 00:30:05

################################################################################
                     [1m Learning iteration 1192/2000 [0m                     

                       Computation: 46233 steps/s (collection: 2.003s, learning 0.123s)
             Mean action noise std: 2.51
          Mean value_function loss: 64.4043
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 50.9718
                       Mean reward: 913.37
               Mean episode length: 241.85
    Episode_Reward/reaching_object: 1.4418
     Episode_Reward/lifting_object: 179.0866
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 2.13s
                      Time elapsed: 00:44:22
                               ETA: 00:30:02

################################################################################
                     [1m Learning iteration 1193/2000 [0m                     

                       Computation: 47267 steps/s (collection: 1.973s, learning 0.107s)
             Mean action noise std: 2.51
          Mean value_function loss: 80.5497
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.9822
                       Mean reward: 885.88
               Mean episode length: 236.86
    Episode_Reward/reaching_object: 1.4336
     Episode_Reward/lifting_object: 178.3047
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 2.08s
                      Time elapsed: 00:44:24
                               ETA: 00:30:00

################################################################################
                     [1m Learning iteration 1194/2000 [0m                     

                       Computation: 46951 steps/s (collection: 1.989s, learning 0.105s)
             Mean action noise std: 2.51
          Mean value_function loss: 78.8800
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 50.9966
                       Mean reward: 886.60
               Mean episode length: 236.57
    Episode_Reward/reaching_object: 1.4247
     Episode_Reward/lifting_object: 177.7309
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 2.09s
                      Time elapsed: 00:44:26
                               ETA: 00:29:58

################################################################################
                     [1m Learning iteration 1195/2000 [0m                     

                       Computation: 46071 steps/s (collection: 2.025s, learning 0.109s)
             Mean action noise std: 2.52
          Mean value_function loss: 92.1250
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.0118
                       Mean reward: 880.48
               Mean episode length: 234.03
    Episode_Reward/reaching_object: 1.4007
     Episode_Reward/lifting_object: 173.9207
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 2.13s
                      Time elapsed: 00:44:28
                               ETA: 00:29:55

################################################################################
                     [1m Learning iteration 1196/2000 [0m                     

                       Computation: 47404 steps/s (collection: 1.978s, learning 0.096s)
             Mean action noise std: 2.52
          Mean value_function loss: 70.1648
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 51.0209
                       Mean reward: 899.25
               Mean episode length: 239.35
    Episode_Reward/reaching_object: 1.4345
     Episode_Reward/lifting_object: 178.6311
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 2.07s
                      Time elapsed: 00:44:30
                               ETA: 00:29:53

################################################################################
                     [1m Learning iteration 1197/2000 [0m                     

                       Computation: 47281 steps/s (collection: 1.985s, learning 0.094s)
             Mean action noise std: 2.52
          Mean value_function loss: 91.3865
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.0301
                       Mean reward: 891.33
               Mean episode length: 237.51
    Episode_Reward/reaching_object: 1.4111
     Episode_Reward/lifting_object: 176.0501
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 2.08s
                      Time elapsed: 00:44:32
                               ETA: 00:29:51

################################################################################
                     [1m Learning iteration 1198/2000 [0m                     

                       Computation: 47565 steps/s (collection: 1.977s, learning 0.090s)
             Mean action noise std: 2.52
          Mean value_function loss: 63.3296
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.0369
                       Mean reward: 900.19
               Mean episode length: 240.45
    Episode_Reward/reaching_object: 1.4497
     Episode_Reward/lifting_object: 180.5096
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 2.07s
                      Time elapsed: 00:44:34
                               ETA: 00:29:48

################################################################################
                     [1m Learning iteration 1199/2000 [0m                     

                       Computation: 47112 steps/s (collection: 1.984s, learning 0.103s)
             Mean action noise std: 2.52
          Mean value_function loss: 79.3834
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 51.0424
                       Mean reward: 894.78
               Mean episode length: 239.26
    Episode_Reward/reaching_object: 1.4310
     Episode_Reward/lifting_object: 178.4765
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 2.09s
                      Time elapsed: 00:44:36
                               ETA: 00:29:46

################################################################################
                     [1m Learning iteration 1200/2000 [0m                     

                       Computation: 47542 steps/s (collection: 1.970s, learning 0.098s)
             Mean action noise std: 2.52
          Mean value_function loss: 90.2233
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.0502
                       Mean reward: 897.36
               Mean episode length: 239.12
    Episode_Reward/reaching_object: 1.4091
     Episode_Reward/lifting_object: 175.3547
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 2.07s
                      Time elapsed: 00:44:38
                               ETA: 00:29:44

################################################################################
                     [1m Learning iteration 1201/2000 [0m                     

                       Computation: 45836 steps/s (collection: 2.048s, learning 0.097s)
             Mean action noise std: 2.52
          Mean value_function loss: 86.9425
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.0575
                       Mean reward: 883.38
               Mean episode length: 234.26
    Episode_Reward/reaching_object: 1.4417
     Episode_Reward/lifting_object: 180.1675
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 2.14s
                      Time elapsed: 00:44:40
                               ETA: 00:29:42

################################################################################
                     [1m Learning iteration 1202/2000 [0m                     

                       Computation: 48218 steps/s (collection: 1.949s, learning 0.089s)
             Mean action noise std: 2.52
          Mean value_function loss: 74.7626
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 51.0660
                       Mean reward: 921.03
               Mean episode length: 245.32
    Episode_Reward/reaching_object: 1.4422
     Episode_Reward/lifting_object: 179.7480
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 2.04s
                      Time elapsed: 00:44:42
                               ETA: 00:29:39

################################################################################
                     [1m Learning iteration 1203/2000 [0m                     

                       Computation: 47588 steps/s (collection: 1.953s, learning 0.113s)
             Mean action noise std: 2.52
          Mean value_function loss: 79.6502
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.0793
                       Mean reward: 893.88
               Mean episode length: 237.82
    Episode_Reward/reaching_object: 1.4287
     Episode_Reward/lifting_object: 178.8050
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 2.07s
                      Time elapsed: 00:44:44
                               ETA: 00:29:37

################################################################################
                     [1m Learning iteration 1204/2000 [0m                     

                       Computation: 46770 steps/s (collection: 1.962s, learning 0.140s)
             Mean action noise std: 2.52
          Mean value_function loss: 112.2445
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 51.0898
                       Mean reward: 854.36
               Mean episode length: 228.75
    Episode_Reward/reaching_object: 1.3997
     Episode_Reward/lifting_object: 174.2525
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 2.10s
                      Time elapsed: 00:44:47
                               ETA: 00:29:35

################################################################################
                     [1m Learning iteration 1205/2000 [0m                     

                       Computation: 46735 steps/s (collection: 1.999s, learning 0.104s)
             Mean action noise std: 2.53
          Mean value_function loss: 90.9131
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.1043
                       Mean reward: 921.56
               Mean episode length: 244.50
    Episode_Reward/reaching_object: 1.4463
     Episode_Reward/lifting_object: 180.8979
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 2.10s
                      Time elapsed: 00:44:49
                               ETA: 00:29:32

################################################################################
                     [1m Learning iteration 1206/2000 [0m                     

                       Computation: 46817 steps/s (collection: 1.992s, learning 0.108s)
             Mean action noise std: 2.53
          Mean value_function loss: 81.1699
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 51.1219
                       Mean reward: 908.88
               Mean episode length: 242.38
    Episode_Reward/reaching_object: 1.4348
     Episode_Reward/lifting_object: 178.5686
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 2.10s
                      Time elapsed: 00:44:51
                               ETA: 00:29:30

################################################################################
                     [1m Learning iteration 1207/2000 [0m                     

                       Computation: 47024 steps/s (collection: 2.001s, learning 0.090s)
             Mean action noise std: 2.53
          Mean value_function loss: 76.1103
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 51.1362
                       Mean reward: 870.86
               Mean episode length: 233.22
    Episode_Reward/reaching_object: 1.4392
     Episode_Reward/lifting_object: 179.6903
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 2.09s
                      Time elapsed: 00:44:53
                               ETA: 00:29:28

################################################################################
                     [1m Learning iteration 1208/2000 [0m                     

                       Computation: 46591 steps/s (collection: 2.013s, learning 0.097s)
             Mean action noise std: 2.53
          Mean value_function loss: 88.3812
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 51.1522
                       Mean reward: 895.38
               Mean episode length: 238.70
    Episode_Reward/reaching_object: 1.4330
     Episode_Reward/lifting_object: 178.8798
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 2.11s
                      Time elapsed: 00:44:55
                               ETA: 00:29:25

################################################################################
                     [1m Learning iteration 1209/2000 [0m                     

                       Computation: 47540 steps/s (collection: 1.982s, learning 0.086s)
             Mean action noise std: 2.53
          Mean value_function loss: 79.2941
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.1651
                       Mean reward: 883.28
               Mean episode length: 235.70
    Episode_Reward/reaching_object: 1.4039
     Episode_Reward/lifting_object: 174.6511
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 2.07s
                      Time elapsed: 00:44:57
                               ETA: 00:29:23

################################################################################
                     [1m Learning iteration 1210/2000 [0m                     

                       Computation: 47340 steps/s (collection: 1.978s, learning 0.099s)
             Mean action noise std: 2.53
          Mean value_function loss: 82.4939
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.1772
                       Mean reward: 909.46
               Mean episode length: 242.10
    Episode_Reward/reaching_object: 1.4544
     Episode_Reward/lifting_object: 180.9253
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 2.08s
                      Time elapsed: 00:44:59
                               ETA: 00:29:21

################################################################################
                     [1m Learning iteration 1211/2000 [0m                     

                       Computation: 48066 steps/s (collection: 1.959s, learning 0.087s)
             Mean action noise std: 2.54
          Mean value_function loss: 71.9226
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 51.1901
                       Mean reward: 923.80
               Mean episode length: 243.73
    Episode_Reward/reaching_object: 1.4338
     Episode_Reward/lifting_object: 178.4529
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 2.05s
                      Time elapsed: 00:45:01
                               ETA: 00:29:18

################################################################################
                     [1m Learning iteration 1212/2000 [0m                     

                       Computation: 47788 steps/s (collection: 1.955s, learning 0.102s)
             Mean action noise std: 2.54
          Mean value_function loss: 86.3029
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.2042
                       Mean reward: 916.82
               Mean episode length: 242.02
    Episode_Reward/reaching_object: 1.4346
     Episode_Reward/lifting_object: 178.4428
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 2.06s
                      Time elapsed: 00:45:03
                               ETA: 00:29:16

################################################################################
                     [1m Learning iteration 1213/2000 [0m                     

                       Computation: 47550 steps/s (collection: 1.976s, learning 0.091s)
             Mean action noise std: 2.54
          Mean value_function loss: 90.5969
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.2171
                       Mean reward: 907.19
               Mean episode length: 241.39
    Episode_Reward/reaching_object: 1.4383
     Episode_Reward/lifting_object: 179.0499
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 2.07s
                      Time elapsed: 00:45:05
                               ETA: 00:29:14

################################################################################
                     [1m Learning iteration 1214/2000 [0m                     

                       Computation: 47486 steps/s (collection: 1.979s, learning 0.092s)
             Mean action noise std: 2.54
          Mean value_function loss: 99.3486
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.2247
                       Mean reward: 901.97
               Mean episode length: 240.82
    Episode_Reward/reaching_object: 1.4259
     Episode_Reward/lifting_object: 177.3295
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 2.07s
                      Time elapsed: 00:45:07
                               ETA: 00:29:11

################################################################################
                     [1m Learning iteration 1215/2000 [0m                     

                       Computation: 47123 steps/s (collection: 1.998s, learning 0.089s)
             Mean action noise std: 2.54
          Mean value_function loss: 97.0145
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 51.2360
                       Mean reward: 880.28
               Mean episode length: 235.45
    Episode_Reward/reaching_object: 1.4224
     Episode_Reward/lifting_object: 176.5746
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 2.09s
                      Time elapsed: 00:45:09
                               ETA: 00:29:09

################################################################################
                     [1m Learning iteration 1216/2000 [0m                     

                       Computation: 47893 steps/s (collection: 1.958s, learning 0.095s)
             Mean action noise std: 2.54
          Mean value_function loss: 79.4469
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.2480
                       Mean reward: 912.10
               Mean episode length: 240.89
    Episode_Reward/reaching_object: 1.4340
     Episode_Reward/lifting_object: 179.1279
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 2.05s
                      Time elapsed: 00:45:11
                               ETA: 00:29:07

################################################################################
                     [1m Learning iteration 1217/2000 [0m                     

                       Computation: 46556 steps/s (collection: 2.004s, learning 0.107s)
             Mean action noise std: 2.54
          Mean value_function loss: 76.3882
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 51.2617
                       Mean reward: 909.71
               Mean episode length: 240.39
    Episode_Reward/reaching_object: 1.4548
     Episode_Reward/lifting_object: 181.6367
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 2.11s
                      Time elapsed: 00:45:14
                               ETA: 00:29:04

################################################################################
                     [1m Learning iteration 1218/2000 [0m                     

                       Computation: 46940 steps/s (collection: 1.960s, learning 0.134s)
             Mean action noise std: 2.55
          Mean value_function loss: 100.6972
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 51.2800
                       Mean reward: 885.87
               Mean episode length: 235.39
    Episode_Reward/reaching_object: 1.4162
     Episode_Reward/lifting_object: 176.8512
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 2.09s
                      Time elapsed: 00:45:16
                               ETA: 00:29:02

################################################################################
                     [1m Learning iteration 1219/2000 [0m                     

                       Computation: 47277 steps/s (collection: 1.973s, learning 0.107s)
             Mean action noise std: 2.55
          Mean value_function loss: 79.6120
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 51.2977
                       Mean reward: 886.13
               Mean episode length: 236.67
    Episode_Reward/reaching_object: 1.4291
     Episode_Reward/lifting_object: 178.4831
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 2.08s
                      Time elapsed: 00:45:18
                               ETA: 00:29:00

################################################################################
                     [1m Learning iteration 1220/2000 [0m                     

                       Computation: 45259 steps/s (collection: 2.038s, learning 0.134s)
             Mean action noise std: 2.55
          Mean value_function loss: 69.4869
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 51.3088
                       Mean reward: 890.13
               Mean episode length: 236.94
    Episode_Reward/reaching_object: 1.4500
     Episode_Reward/lifting_object: 180.7158
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 2.17s
                      Time elapsed: 00:45:20
                               ETA: 00:28:57

################################################################################
                     [1m Learning iteration 1221/2000 [0m                     

                       Computation: 45796 steps/s (collection: 2.046s, learning 0.101s)
             Mean action noise std: 2.55
          Mean value_function loss: 71.5443
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 51.3178
                       Mean reward: 913.96
               Mean episode length: 243.54
    Episode_Reward/reaching_object: 1.4501
     Episode_Reward/lifting_object: 181.4620
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 2.15s
                      Time elapsed: 00:45:22
                               ETA: 00:28:55

################################################################################
                     [1m Learning iteration 1222/2000 [0m                     

                       Computation: 47383 steps/s (collection: 1.976s, learning 0.099s)
             Mean action noise std: 2.55
          Mean value_function loss: 66.7248
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.3269
                       Mean reward: 923.27
               Mean episode length: 243.96
    Episode_Reward/reaching_object: 1.4608
     Episode_Reward/lifting_object: 182.7382
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 2.07s
                      Time elapsed: 00:45:24
                               ETA: 00:28:53

################################################################################
                     [1m Learning iteration 1223/2000 [0m                     

                       Computation: 46785 steps/s (collection: 2.001s, learning 0.100s)
             Mean action noise std: 2.55
          Mean value_function loss: 68.9716
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.3348
                       Mean reward: 881.62
               Mean episode length: 234.73
    Episode_Reward/reaching_object: 1.4385
     Episode_Reward/lifting_object: 179.6025
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 2.10s
                      Time elapsed: 00:45:26
                               ETA: 00:28:50

################################################################################
                     [1m Learning iteration 1224/2000 [0m                     

                       Computation: 47304 steps/s (collection: 1.984s, learning 0.094s)
             Mean action noise std: 2.55
          Mean value_function loss: 68.5132
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 51.3435
                       Mean reward: 903.71
               Mean episode length: 239.56
    Episode_Reward/reaching_object: 1.4207
     Episode_Reward/lifting_object: 177.6194
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 2.08s
                      Time elapsed: 00:45:28
                               ETA: 00:28:48

################################################################################
                     [1m Learning iteration 1225/2000 [0m                     

                       Computation: 46149 steps/s (collection: 2.024s, learning 0.106s)
             Mean action noise std: 2.56
          Mean value_function loss: 68.8860
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 51.3530
                       Mean reward: 898.99
               Mean episode length: 239.84
    Episode_Reward/reaching_object: 1.4300
     Episode_Reward/lifting_object: 177.9740
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 2.13s
                      Time elapsed: 00:45:30
                               ETA: 00:28:46

################################################################################
                     [1m Learning iteration 1226/2000 [0m                     

                       Computation: 46370 steps/s (collection: 2.026s, learning 0.094s)
             Mean action noise std: 2.56
          Mean value_function loss: 72.8676
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 51.3625
                       Mean reward: 905.31
               Mean episode length: 240.21
    Episode_Reward/reaching_object: 1.4418
     Episode_Reward/lifting_object: 179.8046
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 2.12s
                      Time elapsed: 00:45:33
                               ETA: 00:28:44

################################################################################
                     [1m Learning iteration 1227/2000 [0m                     

                       Computation: 46521 steps/s (collection: 2.017s, learning 0.097s)
             Mean action noise std: 2.56
          Mean value_function loss: 156.0419
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 51.3697
                       Mean reward: 899.81
               Mean episode length: 239.05
    Episode_Reward/reaching_object: 1.4429
     Episode_Reward/lifting_object: 180.4082
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 2.11s
                      Time elapsed: 00:45:35
                               ETA: 00:28:41

################################################################################
                     [1m Learning iteration 1228/2000 [0m                     

                       Computation: 45032 steps/s (collection: 2.036s, learning 0.147s)
             Mean action noise std: 2.56
          Mean value_function loss: 78.9422
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 51.3764
                       Mean reward: 888.63
               Mean episode length: 237.47
    Episode_Reward/reaching_object: 1.4284
     Episode_Reward/lifting_object: 177.5643
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 2.18s
                      Time elapsed: 00:45:37
                               ETA: 00:28:39

################################################################################
                     [1m Learning iteration 1229/2000 [0m                     

                       Computation: 46227 steps/s (collection: 2.000s, learning 0.126s)
             Mean action noise std: 2.56
          Mean value_function loss: 75.1149
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 51.3856
                       Mean reward: 880.50
               Mean episode length: 234.54
    Episode_Reward/reaching_object: 1.4488
     Episode_Reward/lifting_object: 180.4286
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 2.13s
                      Time elapsed: 00:45:39
                               ETA: 00:28:37

################################################################################
                     [1m Learning iteration 1230/2000 [0m                     

                       Computation: 46378 steps/s (collection: 2.016s, learning 0.104s)
             Mean action noise std: 2.56
          Mean value_function loss: 78.0301
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 51.3947
                       Mean reward: 874.51
               Mean episode length: 232.65
    Episode_Reward/reaching_object: 1.4207
     Episode_Reward/lifting_object: 177.0254
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 2.12s
                      Time elapsed: 00:45:41
                               ETA: 00:28:34

################################################################################
                     [1m Learning iteration 1231/2000 [0m                     

                       Computation: 46409 steps/s (collection: 2.015s, learning 0.103s)
             Mean action noise std: 2.56
          Mean value_function loss: 82.5438
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.4031
                       Mean reward: 880.87
               Mean episode length: 236.13
    Episode_Reward/reaching_object: 1.4262
     Episode_Reward/lifting_object: 177.6855
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 2.12s
                      Time elapsed: 00:45:43
                               ETA: 00:28:32

################################################################################
                     [1m Learning iteration 1232/2000 [0m                     

                       Computation: 46001 steps/s (collection: 2.043s, learning 0.094s)
             Mean action noise std: 2.56
          Mean value_function loss: 86.9092
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 51.4166
                       Mean reward: 852.25
               Mean episode length: 229.03
    Episode_Reward/reaching_object: 1.4266
     Episode_Reward/lifting_object: 178.2268
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 2.14s
                      Time elapsed: 00:45:45
                               ETA: 00:28:30

################################################################################
                     [1m Learning iteration 1233/2000 [0m                     

                       Computation: 45567 steps/s (collection: 2.041s, learning 0.116s)
             Mean action noise std: 2.56
          Mean value_function loss: 71.8760
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 51.4310
                       Mean reward: 930.74
               Mean episode length: 246.13
    Episode_Reward/reaching_object: 1.4567
     Episode_Reward/lifting_object: 182.4456
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 2.16s
                      Time elapsed: 00:45:48
                               ETA: 00:28:28

################################################################################
                     [1m Learning iteration 1234/2000 [0m                     

                       Computation: 45301 steps/s (collection: 2.070s, learning 0.100s)
             Mean action noise std: 2.57
          Mean value_function loss: 92.0800
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.4412
                       Mean reward: 909.84
               Mean episode length: 241.39
    Episode_Reward/reaching_object: 1.4197
     Episode_Reward/lifting_object: 177.0663
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 2.17s
                      Time elapsed: 00:45:50
                               ETA: 00:28:25

################################################################################
                     [1m Learning iteration 1235/2000 [0m                     

                       Computation: 46130 steps/s (collection: 1.992s, learning 0.139s)
             Mean action noise std: 2.57
          Mean value_function loss: 63.5485
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.4513
                       Mean reward: 929.41
               Mean episode length: 245.64
    Episode_Reward/reaching_object: 1.4347
     Episode_Reward/lifting_object: 179.4321
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 2.13s
                      Time elapsed: 00:45:52
                               ETA: 00:28:23

################################################################################
                     [1m Learning iteration 1236/2000 [0m                     

                       Computation: 46014 steps/s (collection: 2.009s, learning 0.128s)
             Mean action noise std: 2.57
          Mean value_function loss: 70.2853
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 51.4607
                       Mean reward: 904.98
               Mean episode length: 240.04
    Episode_Reward/reaching_object: 1.4388
     Episode_Reward/lifting_object: 180.3185
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 2.14s
                      Time elapsed: 00:45:54
                               ETA: 00:28:21

################################################################################
                     [1m Learning iteration 1237/2000 [0m                     

                       Computation: 45783 steps/s (collection: 2.043s, learning 0.105s)
             Mean action noise std: 2.57
          Mean value_function loss: 69.6411
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 51.4649
                       Mean reward: 920.79
               Mean episode length: 243.73
    Episode_Reward/reaching_object: 1.4239
     Episode_Reward/lifting_object: 178.1546
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 2.15s
                      Time elapsed: 00:45:56
                               ETA: 00:28:18

################################################################################
                     [1m Learning iteration 1238/2000 [0m                     

                       Computation: 46410 steps/s (collection: 2.023s, learning 0.096s)
             Mean action noise std: 2.57
          Mean value_function loss: 98.4008
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 51.4669
                       Mean reward: 906.10
               Mean episode length: 240.66
    Episode_Reward/reaching_object: 1.4288
     Episode_Reward/lifting_object: 179.0588
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 2.12s
                      Time elapsed: 00:45:58
                               ETA: 00:28:16

################################################################################
                     [1m Learning iteration 1239/2000 [0m                     

                       Computation: 45496 steps/s (collection: 2.046s, learning 0.115s)
             Mean action noise std: 2.57
          Mean value_function loss: 76.0835
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.4703
                       Mean reward: 899.61
               Mean episode length: 238.72
    Episode_Reward/reaching_object: 1.4131
     Episode_Reward/lifting_object: 177.3868
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 2.16s
                      Time elapsed: 00:46:00
                               ETA: 00:28:14

################################################################################
                     [1m Learning iteration 1240/2000 [0m                     

                       Computation: 45182 steps/s (collection: 2.031s, learning 0.145s)
             Mean action noise std: 2.57
          Mean value_function loss: 79.9036
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.4749
                       Mean reward: 874.97
               Mean episode length: 232.83
    Episode_Reward/reaching_object: 1.4221
     Episode_Reward/lifting_object: 178.1694
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 2.18s
                      Time elapsed: 00:46:03
                               ETA: 00:28:12

################################################################################
                     [1m Learning iteration 1241/2000 [0m                     

                       Computation: 45986 steps/s (collection: 2.034s, learning 0.104s)
             Mean action noise std: 2.57
          Mean value_function loss: 66.9842
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 51.4830
                       Mean reward: 888.22
               Mean episode length: 236.94
    Episode_Reward/reaching_object: 1.4177
     Episode_Reward/lifting_object: 177.4597
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 2.14s
                      Time elapsed: 00:46:05
                               ETA: 00:28:09

################################################################################
                     [1m Learning iteration 1242/2000 [0m                     

                       Computation: 45099 steps/s (collection: 2.069s, learning 0.111s)
             Mean action noise std: 2.57
          Mean value_function loss: 67.5946
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 51.4944
                       Mean reward: 881.01
               Mean episode length: 233.86
    Episode_Reward/reaching_object: 1.4386
     Episode_Reward/lifting_object: 180.0560
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 2.18s
                      Time elapsed: 00:46:07
                               ETA: 00:28:07

################################################################################
                     [1m Learning iteration 1243/2000 [0m                     

                       Computation: 45118 steps/s (collection: 2.045s, learning 0.134s)
             Mean action noise std: 2.57
          Mean value_function loss: 90.7869
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 51.5070
                       Mean reward: 887.54
               Mean episode length: 236.72
    Episode_Reward/reaching_object: 1.4187
     Episode_Reward/lifting_object: 177.1065
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 2.18s
                      Time elapsed: 00:46:09
                               ETA: 00:28:05

################################################################################
                     [1m Learning iteration 1244/2000 [0m                     

                       Computation: 45449 steps/s (collection: 2.066s, learning 0.097s)
             Mean action noise std: 2.58
          Mean value_function loss: 109.1825
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.5207
                       Mean reward: 871.91
               Mean episode length: 233.14
    Episode_Reward/reaching_object: 1.4167
     Episode_Reward/lifting_object: 177.1319
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 2.16s
                      Time elapsed: 00:46:11
                               ETA: 00:28:03

################################################################################
                     [1m Learning iteration 1245/2000 [0m                     

                       Computation: 46146 steps/s (collection: 2.020s, learning 0.111s)
             Mean action noise std: 2.58
          Mean value_function loss: 81.9486
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.5330
                       Mean reward: 912.57
               Mean episode length: 243.10
    Episode_Reward/reaching_object: 1.4202
     Episode_Reward/lifting_object: 177.9374
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 2.13s
                      Time elapsed: 00:46:13
                               ETA: 00:28:00

################################################################################
                     [1m Learning iteration 1246/2000 [0m                     

                       Computation: 44961 steps/s (collection: 2.073s, learning 0.114s)
             Mean action noise std: 2.58
          Mean value_function loss: 86.5485
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 51.5426
                       Mean reward: 919.06
               Mean episode length: 242.66
    Episode_Reward/reaching_object: 1.4310
     Episode_Reward/lifting_object: 179.4216
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 2.19s
                      Time elapsed: 00:46:16
                               ETA: 00:27:58

################################################################################
                     [1m Learning iteration 1247/2000 [0m                     

                       Computation: 46400 steps/s (collection: 2.022s, learning 0.097s)
             Mean action noise std: 2.58
          Mean value_function loss: 63.8557
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 51.5500
                       Mean reward: 937.81
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 1.4572
     Episode_Reward/lifting_object: 182.1739
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 2.12s
                      Time elapsed: 00:46:18
                               ETA: 00:27:56

################################################################################
                     [1m Learning iteration 1248/2000 [0m                     

                       Computation: 46369 steps/s (collection: 2.026s, learning 0.094s)
             Mean action noise std: 2.58
          Mean value_function loss: 71.2962
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 51.5601
                       Mean reward: 912.85
               Mean episode length: 241.78
    Episode_Reward/reaching_object: 1.4451
     Episode_Reward/lifting_object: 180.9424
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 2.12s
                      Time elapsed: 00:46:20
                               ETA: 00:27:53

################################################################################
                     [1m Learning iteration 1249/2000 [0m                     

                       Computation: 45686 steps/s (collection: 2.035s, learning 0.117s)
             Mean action noise std: 2.58
          Mean value_function loss: 74.0546
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 51.5709
                       Mean reward: 894.37
               Mean episode length: 239.69
    Episode_Reward/reaching_object: 1.4301
     Episode_Reward/lifting_object: 178.4786
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 2.15s
                      Time elapsed: 00:46:22
                               ETA: 00:27:51

################################################################################
                     [1m Learning iteration 1250/2000 [0m                     

                       Computation: 43562 steps/s (collection: 2.145s, learning 0.112s)
             Mean action noise std: 2.58
          Mean value_function loss: 75.9211
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 51.5819
                       Mean reward: 926.41
               Mean episode length: 245.79
    Episode_Reward/reaching_object: 1.4416
     Episode_Reward/lifting_object: 180.1819
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 2.26s
                      Time elapsed: 00:46:24
                               ETA: 00:27:49

################################################################################
                     [1m Learning iteration 1251/2000 [0m                     

                       Computation: 44419 steps/s (collection: 2.095s, learning 0.119s)
             Mean action noise std: 2.58
          Mean value_function loss: 94.8971
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 51.5920
                       Mean reward: 895.31
               Mean episode length: 237.85
    Episode_Reward/reaching_object: 1.4243
     Episode_Reward/lifting_object: 178.2314
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 2.21s
                      Time elapsed: 00:46:26
                               ETA: 00:27:47

################################################################################
                     [1m Learning iteration 1252/2000 [0m                     

                       Computation: 46128 steps/s (collection: 2.024s, learning 0.107s)
             Mean action noise std: 2.59
          Mean value_function loss: 67.3383
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 51.6020
                       Mean reward: 893.06
               Mean episode length: 239.54
    Episode_Reward/reaching_object: 1.4263
     Episode_Reward/lifting_object: 177.9764
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 2.13s
                      Time elapsed: 00:46:29
                               ETA: 00:27:44

################################################################################
                     [1m Learning iteration 1253/2000 [0m                     

                       Computation: 46292 steps/s (collection: 2.023s, learning 0.100s)
             Mean action noise std: 2.59
          Mean value_function loss: 80.2760
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 51.6120
                       Mean reward: 897.41
               Mean episode length: 239.41
    Episode_Reward/reaching_object: 1.4235
     Episode_Reward/lifting_object: 177.6217
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 2.12s
                      Time elapsed: 00:46:31
                               ETA: 00:27:42

################################################################################
                     [1m Learning iteration 1254/2000 [0m                     

                       Computation: 45814 steps/s (collection: 2.050s, learning 0.096s)
             Mean action noise std: 2.59
          Mean value_function loss: 76.6645
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 51.6213
                       Mean reward: 895.41
               Mean episode length: 237.99
    Episode_Reward/reaching_object: 1.4491
     Episode_Reward/lifting_object: 180.9109
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 2.15s
                      Time elapsed: 00:46:33
                               ETA: 00:27:40

################################################################################
                     [1m Learning iteration 1255/2000 [0m                     

                       Computation: 45336 steps/s (collection: 2.022s, learning 0.147s)
             Mean action noise std: 2.59
          Mean value_function loss: 66.4997
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 51.6282
                       Mean reward: 932.24
               Mean episode length: 246.70
    Episode_Reward/reaching_object: 1.4493
     Episode_Reward/lifting_object: 181.3080
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 2.17s
                      Time elapsed: 00:46:35
                               ETA: 00:27:38

################################################################################
                     [1m Learning iteration 1256/2000 [0m                     

                       Computation: 45458 steps/s (collection: 2.040s, learning 0.123s)
             Mean action noise std: 2.59
          Mean value_function loss: 92.5872
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 51.6369
                       Mean reward: 900.39
               Mean episode length: 240.47
    Episode_Reward/reaching_object: 1.4282
     Episode_Reward/lifting_object: 178.2182
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 2.16s
                      Time elapsed: 00:46:37
                               ETA: 00:27:35

################################################################################
                     [1m Learning iteration 1257/2000 [0m                     

                       Computation: 45488 steps/s (collection: 2.045s, learning 0.116s)
             Mean action noise std: 2.59
          Mean value_function loss: 80.4733
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 51.6532
                       Mean reward: 905.46
               Mean episode length: 240.24
    Episode_Reward/reaching_object: 1.4246
     Episode_Reward/lifting_object: 177.7735
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 2.16s
                      Time elapsed: 00:46:39
                               ETA: 00:27:33

################################################################################
                     [1m Learning iteration 1258/2000 [0m                     

                       Computation: 45967 steps/s (collection: 2.045s, learning 0.094s)
             Mean action noise std: 2.59
          Mean value_function loss: 62.7003
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 51.6677
                       Mean reward: 925.50
               Mean episode length: 245.96
    Episode_Reward/reaching_object: 1.4554
     Episode_Reward/lifting_object: 182.2632
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 2.14s
                      Time elapsed: 00:46:41
                               ETA: 00:27:31

################################################################################
                     [1m Learning iteration 1259/2000 [0m                     

                       Computation: 46117 steps/s (collection: 2.026s, learning 0.106s)
             Mean action noise std: 2.59
          Mean value_function loss: 101.4213
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 51.6807
                       Mean reward: 886.96
               Mean episode length: 236.27
    Episode_Reward/reaching_object: 1.4227
     Episode_Reward/lifting_object: 177.6428
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 2.13s
                      Time elapsed: 00:46:44
                               ETA: 00:27:29

################################################################################
                     [1m Learning iteration 1260/2000 [0m                     

                       Computation: 45849 steps/s (collection: 2.041s, learning 0.103s)
             Mean action noise std: 2.60
          Mean value_function loss: 85.2078
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 51.6888
                       Mean reward: 903.70
               Mean episode length: 241.06
    Episode_Reward/reaching_object: 1.4198
     Episode_Reward/lifting_object: 177.4500
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 2.14s
                      Time elapsed: 00:46:46
                               ETA: 00:27:26

################################################################################
                     [1m Learning iteration 1261/2000 [0m                     

                       Computation: 45487 steps/s (collection: 2.066s, learning 0.095s)
             Mean action noise std: 2.60
          Mean value_function loss: 88.8698
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 51.6984
                       Mean reward: 872.33
               Mean episode length: 232.32
    Episode_Reward/reaching_object: 1.3942
     Episode_Reward/lifting_object: 174.8020
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 2.16s
                      Time elapsed: 00:46:48
                               ETA: 00:27:24

################################################################################
                     [1m Learning iteration 1262/2000 [0m                     

                       Computation: 45147 steps/s (collection: 2.039s, learning 0.139s)
             Mean action noise std: 2.60
          Mean value_function loss: 76.8219
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.7063
                       Mean reward: 920.36
               Mean episode length: 243.62
    Episode_Reward/reaching_object: 1.4288
     Episode_Reward/lifting_object: 180.0493
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 2.18s
                      Time elapsed: 00:46:50
                               ETA: 00:27:22

################################################################################
                     [1m Learning iteration 1263/2000 [0m                     

                       Computation: 43169 steps/s (collection: 2.166s, learning 0.112s)
             Mean action noise std: 2.60
          Mean value_function loss: 92.6355
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 51.7147
                       Mean reward: 928.24
               Mean episode length: 245.79
    Episode_Reward/reaching_object: 1.4196
     Episode_Reward/lifting_object: 178.3406
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 2.28s
                      Time elapsed: 00:46:52
                               ETA: 00:27:20

################################################################################
                     [1m Learning iteration 1264/2000 [0m                     

                       Computation: 44022 steps/s (collection: 2.121s, learning 0.112s)
             Mean action noise std: 2.60
          Mean value_function loss: 75.2697
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 51.7245
                       Mean reward: 931.19
               Mean episode length: 246.04
    Episode_Reward/reaching_object: 1.4439
     Episode_Reward/lifting_object: 181.0735
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 2.23s
                      Time elapsed: 00:46:55
                               ETA: 00:27:17

################################################################################
                     [1m Learning iteration 1265/2000 [0m                     

                       Computation: 33607 steps/s (collection: 2.558s, learning 0.367s)
             Mean action noise std: 2.60
          Mean value_function loss: 74.2251
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 51.7284
                       Mean reward: 919.48
               Mean episode length: 242.57
    Episode_Reward/reaching_object: 1.4288
     Episode_Reward/lifting_object: 179.7338
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 2.93s
                      Time elapsed: 00:46:57
                               ETA: 00:27:16

################################################################################
                     [1m Learning iteration 1266/2000 [0m                     

                       Computation: 30418 steps/s (collection: 2.965s, learning 0.267s)
             Mean action noise std: 2.60
          Mean value_function loss: 83.1214
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 51.7325
                       Mean reward: 878.24
               Mean episode length: 235.47
    Episode_Reward/reaching_object: 1.4202
     Episode_Reward/lifting_object: 179.4380
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 3.23s
                      Time elapsed: 00:47:01
                               ETA: 00:27:14

################################################################################
                     [1m Learning iteration 1267/2000 [0m                     

                       Computation: 36288 steps/s (collection: 2.602s, learning 0.107s)
             Mean action noise std: 2.60
          Mean value_function loss: 81.9297
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 51.7365
                       Mean reward: 887.38
               Mean episode length: 236.86
    Episode_Reward/reaching_object: 1.4223
     Episode_Reward/lifting_object: 178.1752
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 2.71s
                      Time elapsed: 00:47:03
                               ETA: 00:27:12

################################################################################
                     [1m Learning iteration 1268/2000 [0m                     

                       Computation: 41448 steps/s (collection: 2.269s, learning 0.103s)
             Mean action noise std: 2.60
          Mean value_function loss: 89.0012
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 51.7413
                       Mean reward: 893.02
               Mean episode length: 237.40
    Episode_Reward/reaching_object: 1.4210
     Episode_Reward/lifting_object: 178.3018
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 2.37s
                      Time elapsed: 00:47:06
                               ETA: 00:27:10

################################################################################
                     [1m Learning iteration 1269/2000 [0m                     

                       Computation: 42593 steps/s (collection: 2.174s, learning 0.134s)
             Mean action noise std: 2.60
          Mean value_function loss: 84.3907
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 51.7497
                       Mean reward: 882.64
               Mean episode length: 235.10
    Episode_Reward/reaching_object: 1.4161
     Episode_Reward/lifting_object: 178.3705
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 2.31s
                      Time elapsed: 00:47:08
                               ETA: 00:27:08

################################################################################
                     [1m Learning iteration 1270/2000 [0m                     

                       Computation: 42419 steps/s (collection: 2.169s, learning 0.148s)
             Mean action noise std: 2.60
          Mean value_function loss: 68.2215
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 51.7616
                       Mean reward: 928.19
               Mean episode length: 245.59
    Episode_Reward/reaching_object: 1.4244
     Episode_Reward/lifting_object: 178.3538
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 2.32s
                      Time elapsed: 00:47:10
                               ETA: 00:27:05

################################################################################
                     [1m Learning iteration 1271/2000 [0m                     

                       Computation: 43736 steps/s (collection: 2.131s, learning 0.117s)
             Mean action noise std: 2.60
          Mean value_function loss: 79.8977
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 51.7696
                       Mean reward: 918.25
               Mean episode length: 244.29
    Episode_Reward/reaching_object: 1.4420
     Episode_Reward/lifting_object: 181.3851
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 2.25s
                      Time elapsed: 00:47:13
                               ETA: 00:27:03

################################################################################
                     [1m Learning iteration 1272/2000 [0m                     

                       Computation: 42321 steps/s (collection: 2.184s, learning 0.139s)
             Mean action noise std: 2.61
          Mean value_function loss: 71.7132
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.7760
                       Mean reward: 888.14
               Mean episode length: 236.69
    Episode_Reward/reaching_object: 1.4242
     Episode_Reward/lifting_object: 178.5252
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 2.32s
                      Time elapsed: 00:47:15
                               ETA: 00:27:01

################################################################################
                     [1m Learning iteration 1273/2000 [0m                     

                       Computation: 43372 steps/s (collection: 2.165s, learning 0.101s)
             Mean action noise std: 2.61
          Mean value_function loss: 83.5887
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.7882
                       Mean reward: 916.01
               Mean episode length: 243.83
    Episode_Reward/reaching_object: 1.4145
     Episode_Reward/lifting_object: 177.4093
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 2.27s
                      Time elapsed: 00:47:17
                               ETA: 00:26:59

################################################################################
                     [1m Learning iteration 1274/2000 [0m                     

                       Computation: 43202 steps/s (collection: 2.169s, learning 0.107s)
             Mean action noise std: 2.61
          Mean value_function loss: 52.6799
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 51.7983
                       Mean reward: 926.05
               Mean episode length: 244.95
    Episode_Reward/reaching_object: 1.4415
     Episode_Reward/lifting_object: 181.0722
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 2.28s
                      Time elapsed: 00:47:20
                               ETA: 00:26:57

################################################################################
                     [1m Learning iteration 1275/2000 [0m                     

                       Computation: 43922 steps/s (collection: 2.142s, learning 0.096s)
             Mean action noise std: 2.61
          Mean value_function loss: 76.5197
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.8010
                       Mean reward: 872.00
               Mean episode length: 233.54
    Episode_Reward/reaching_object: 1.4349
     Episode_Reward/lifting_object: 179.9648
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 2.24s
                      Time elapsed: 00:47:22
                               ETA: 00:26:54

################################################################################
                     [1m Learning iteration 1276/2000 [0m                     

                       Computation: 40792 steps/s (collection: 2.244s, learning 0.166s)
             Mean action noise std: 2.61
          Mean value_function loss: 115.3401
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 51.8061
                       Mean reward: 872.49
               Mean episode length: 232.71
    Episode_Reward/reaching_object: 1.3823
     Episode_Reward/lifting_object: 173.0766
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 2.41s
                      Time elapsed: 00:47:24
                               ETA: 00:26:52

################################################################################
                     [1m Learning iteration 1277/2000 [0m                     

                       Computation: 38079 steps/s (collection: 2.431s, learning 0.151s)
             Mean action noise std: 2.61
          Mean value_function loss: 95.8265
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 51.8160
                       Mean reward: 881.27
               Mean episode length: 235.65
    Episode_Reward/reaching_object: 1.4107
     Episode_Reward/lifting_object: 176.3644
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 2.58s
                      Time elapsed: 00:47:27
                               ETA: 00:26:50

################################################################################
                     [1m Learning iteration 1278/2000 [0m                     

                       Computation: 37275 steps/s (collection: 2.519s, learning 0.118s)
             Mean action noise std: 2.61
          Mean value_function loss: 80.7385
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.8291
                       Mean reward: 899.79
               Mean episode length: 240.50
    Episode_Reward/reaching_object: 1.4177
     Episode_Reward/lifting_object: 177.8910
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 2.64s
                      Time elapsed: 00:47:29
                               ETA: 00:26:48

################################################################################
                     [1m Learning iteration 1279/2000 [0m                     

                       Computation: 38048 steps/s (collection: 2.409s, learning 0.175s)
             Mean action noise std: 2.61
          Mean value_function loss: 77.8115
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 51.8376
                       Mean reward: 885.45
               Mean episode length: 235.41
    Episode_Reward/reaching_object: 1.4209
     Episode_Reward/lifting_object: 177.9002
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 2.58s
                      Time elapsed: 00:47:32
                               ETA: 00:26:46

################################################################################
                     [1m Learning iteration 1280/2000 [0m                     

                       Computation: 44495 steps/s (collection: 2.095s, learning 0.114s)
             Mean action noise std: 2.61
          Mean value_function loss: 74.4156
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 51.8401
                       Mean reward: 889.17
               Mean episode length: 236.39
    Episode_Reward/reaching_object: 1.4391
     Episode_Reward/lifting_object: 180.8704
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 2.21s
                      Time elapsed: 00:47:34
                               ETA: 00:26:44

################################################################################
                     [1m Learning iteration 1281/2000 [0m                     

                       Computation: 45580 steps/s (collection: 2.059s, learning 0.098s)
             Mean action noise std: 2.61
          Mean value_function loss: 103.4837
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.8446
                       Mean reward: 869.01
               Mean episode length: 231.69
    Episode_Reward/reaching_object: 1.4261
     Episode_Reward/lifting_object: 178.7099
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 2.16s
                      Time elapsed: 00:47:36
                               ETA: 00:26:42

################################################################################
                     [1m Learning iteration 1282/2000 [0m                     

                       Computation: 45966 steps/s (collection: 2.033s, learning 0.106s)
             Mean action noise std: 2.62
          Mean value_function loss: 58.6402
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 51.8530
                       Mean reward: 922.85
               Mean episode length: 246.10
    Episode_Reward/reaching_object: 1.4424
     Episode_Reward/lifting_object: 181.1328
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 2.14s
                      Time elapsed: 00:47:39
                               ETA: 00:26:39

################################################################################
                     [1m Learning iteration 1283/2000 [0m                     

                       Computation: 46355 steps/s (collection: 2.025s, learning 0.096s)
             Mean action noise std: 2.62
          Mean value_function loss: 85.9298
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 51.8620
                       Mean reward: 861.89
               Mean episode length: 227.96
    Episode_Reward/reaching_object: 1.4293
     Episode_Reward/lifting_object: 180.0514
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 2.12s
                      Time elapsed: 00:47:41
                               ETA: 00:26:37

################################################################################
                     [1m Learning iteration 1284/2000 [0m                     

                       Computation: 46457 steps/s (collection: 2.018s, learning 0.098s)
             Mean action noise std: 2.62
          Mean value_function loss: 73.4743
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.8709
                       Mean reward: 923.26
               Mean episode length: 245.34
    Episode_Reward/reaching_object: 1.4182
     Episode_Reward/lifting_object: 177.1241
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 2.12s
                      Time elapsed: 00:47:43
                               ETA: 00:26:35

################################################################################
                     [1m Learning iteration 1285/2000 [0m                     

                       Computation: 46365 steps/s (collection: 2.025s, learning 0.096s)
             Mean action noise std: 2.62
          Mean value_function loss: 91.6249
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 51.8792
                       Mean reward: 876.14
               Mean episode length: 233.61
    Episode_Reward/reaching_object: 1.4013
     Episode_Reward/lifting_object: 175.4911
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 2.12s
                      Time elapsed: 00:47:45
                               ETA: 00:26:33

################################################################################
                     [1m Learning iteration 1286/2000 [0m                     

                       Computation: 46131 steps/s (collection: 2.034s, learning 0.097s)
             Mean action noise std: 2.62
          Mean value_function loss: 70.7454
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.8895
                       Mean reward: 924.03
               Mean episode length: 244.34
    Episode_Reward/reaching_object: 1.4206
     Episode_Reward/lifting_object: 178.3064
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 2.13s
                      Time elapsed: 00:47:47
                               ETA: 00:26:30

################################################################################
                     [1m Learning iteration 1287/2000 [0m                     

                       Computation: 46480 steps/s (collection: 2.019s, learning 0.096s)
             Mean action noise std: 2.62
          Mean value_function loss: 77.4617
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 51.9003
                       Mean reward: 869.47
               Mean episode length: 232.61
    Episode_Reward/reaching_object: 1.4092
     Episode_Reward/lifting_object: 176.5696
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 2.11s
                      Time elapsed: 00:47:49
                               ETA: 00:26:28

################################################################################
                     [1m Learning iteration 1288/2000 [0m                     

                       Computation: 46707 steps/s (collection: 2.009s, learning 0.096s)
             Mean action noise std: 2.62
          Mean value_function loss: 108.4236
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 51.9139
                       Mean reward: 914.60
               Mean episode length: 242.15
    Episode_Reward/reaching_object: 1.4391
     Episode_Reward/lifting_object: 180.5663
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 2.10s
                      Time elapsed: 00:47:51
                               ETA: 00:26:26

################################################################################
                     [1m Learning iteration 1289/2000 [0m                     

                       Computation: 46100 steps/s (collection: 2.021s, learning 0.112s)
             Mean action noise std: 2.62
          Mean value_function loss: 114.4180
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.9251
                       Mean reward: 866.66
               Mean episode length: 232.37
    Episode_Reward/reaching_object: 1.4137
     Episode_Reward/lifting_object: 176.5061
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 2.13s
                      Time elapsed: 00:47:53
                               ETA: 00:26:23

################################################################################
                     [1m Learning iteration 1290/2000 [0m                     

                       Computation: 45474 steps/s (collection: 2.043s, learning 0.119s)
             Mean action noise std: 2.63
          Mean value_function loss: 103.8205
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 51.9330
                       Mean reward: 909.46
               Mean episode length: 241.67
    Episode_Reward/reaching_object: 1.4254
     Episode_Reward/lifting_object: 178.0980
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 2.16s
                      Time elapsed: 00:47:56
                               ETA: 00:26:21

################################################################################
                     [1m Learning iteration 1291/2000 [0m                     

                       Computation: 45933 steps/s (collection: 2.037s, learning 0.104s)
             Mean action noise std: 2.63
          Mean value_function loss: 72.2962
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.9444
                       Mean reward: 910.15
               Mean episode length: 241.43
    Episode_Reward/reaching_object: 1.4307
     Episode_Reward/lifting_object: 179.2925
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 2.14s
                      Time elapsed: 00:47:58
                               ETA: 00:26:19

################################################################################
                     [1m Learning iteration 1292/2000 [0m                     

                       Computation: 45350 steps/s (collection: 2.058s, learning 0.110s)
             Mean action noise std: 2.63
          Mean value_function loss: 74.3106
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.9560
                       Mean reward: 920.49
               Mean episode length: 243.74
    Episode_Reward/reaching_object: 1.4222
     Episode_Reward/lifting_object: 178.1139
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 2.17s
                      Time elapsed: 00:48:00
                               ETA: 00:26:17

################################################################################
                     [1m Learning iteration 1293/2000 [0m                     

                       Computation: 46246 steps/s (collection: 2.029s, learning 0.097s)
             Mean action noise std: 2.63
          Mean value_function loss: 82.4475
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.9678
                       Mean reward: 866.01
               Mean episode length: 230.74
    Episode_Reward/reaching_object: 1.4156
     Episode_Reward/lifting_object: 176.5264
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 2.13s
                      Time elapsed: 00:48:02
                               ETA: 00:26:14

################################################################################
                     [1m Learning iteration 1294/2000 [0m                     

                       Computation: 46205 steps/s (collection: 2.018s, learning 0.110s)
             Mean action noise std: 2.63
          Mean value_function loss: 71.9462
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.9753
                       Mean reward: 884.15
               Mean episode length: 234.31
    Episode_Reward/reaching_object: 1.4396
     Episode_Reward/lifting_object: 180.7955
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 2.13s
                      Time elapsed: 00:48:04
                               ETA: 00:26:12

################################################################################
                     [1m Learning iteration 1295/2000 [0m                     

                       Computation: 46534 steps/s (collection: 2.018s, learning 0.095s)
             Mean action noise std: 2.63
          Mean value_function loss: 64.9890
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 51.9836
                       Mean reward: 908.81
               Mean episode length: 241.19
    Episode_Reward/reaching_object: 1.4345
     Episode_Reward/lifting_object: 179.5118
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 2.11s
                      Time elapsed: 00:48:06
                               ETA: 00:26:10

################################################################################
                     [1m Learning iteration 1296/2000 [0m                     

                       Computation: 45781 steps/s (collection: 2.044s, learning 0.104s)
             Mean action noise std: 2.63
          Mean value_function loss: 101.4487
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 51.9913
                       Mean reward: 924.54
               Mean episode length: 244.66
    Episode_Reward/reaching_object: 1.4218
     Episode_Reward/lifting_object: 178.1978
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 2.15s
                      Time elapsed: 00:48:08
                               ETA: 00:26:08

################################################################################
                     [1m Learning iteration 1297/2000 [0m                     

                       Computation: 46080 steps/s (collection: 2.030s, learning 0.103s)
             Mean action noise std: 2.63
          Mean value_function loss: 70.8666
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.9974
                       Mean reward: 895.27
               Mean episode length: 238.35
    Episode_Reward/reaching_object: 1.4341
     Episode_Reward/lifting_object: 179.3125
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 2.13s
                      Time elapsed: 00:48:10
                               ETA: 00:26:05

################################################################################
                     [1m Learning iteration 1298/2000 [0m                     

                       Computation: 45248 steps/s (collection: 2.061s, learning 0.112s)
             Mean action noise std: 2.64
          Mean value_function loss: 99.3632
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.0080
                       Mean reward: 868.98
               Mean episode length: 232.54
    Episode_Reward/reaching_object: 1.4114
     Episode_Reward/lifting_object: 176.2037
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 2.17s
                      Time elapsed: 00:48:13
                               ETA: 00:26:03

################################################################################
                     [1m Learning iteration 1299/2000 [0m                     

                       Computation: 44668 steps/s (collection: 2.098s, learning 0.103s)
             Mean action noise std: 2.64
          Mean value_function loss: 69.7872
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 52.0250
                       Mean reward: 891.64
               Mean episode length: 237.62
    Episode_Reward/reaching_object: 1.4304
     Episode_Reward/lifting_object: 178.7681
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 2.20s
                      Time elapsed: 00:48:15
                               ETA: 00:26:01

################################################################################
                     [1m Learning iteration 1300/2000 [0m                     

                       Computation: 45605 steps/s (collection: 2.063s, learning 0.092s)
             Mean action noise std: 2.64
          Mean value_function loss: 79.0959
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 52.0429
                       Mean reward: 914.79
               Mean episode length: 243.18
    Episode_Reward/reaching_object: 1.4147
     Episode_Reward/lifting_object: 176.9057
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 2.16s
                      Time elapsed: 00:48:17
                               ETA: 00:25:58

################################################################################
                     [1m Learning iteration 1301/2000 [0m                     

                       Computation: 45865 steps/s (collection: 2.045s, learning 0.098s)
             Mean action noise std: 2.64
          Mean value_function loss: 67.7749
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.0488
                       Mean reward: 898.33
               Mean episode length: 239.52
    Episode_Reward/reaching_object: 1.4398
     Episode_Reward/lifting_object: 180.3472
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 2.14s
                      Time elapsed: 00:48:19
                               ETA: 00:25:56

################################################################################
                     [1m Learning iteration 1302/2000 [0m                     

                       Computation: 45730 steps/s (collection: 2.044s, learning 0.106s)
             Mean action noise std: 2.64
          Mean value_function loss: 63.5694
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 52.0586
                       Mean reward: 889.25
               Mean episode length: 235.34
    Episode_Reward/reaching_object: 1.4451
     Episode_Reward/lifting_object: 180.5636
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 2.15s
                      Time elapsed: 00:48:21
                               ETA: 00:25:54

################################################################################
                     [1m Learning iteration 1303/2000 [0m                     

                       Computation: 45134 steps/s (collection: 2.061s, learning 0.117s)
             Mean action noise std: 2.64
          Mean value_function loss: 86.8385
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.0712
                       Mean reward: 876.20
               Mean episode length: 231.92
    Episode_Reward/reaching_object: 1.4275
     Episode_Reward/lifting_object: 178.0403
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 2.18s
                      Time elapsed: 00:48:23
                               ETA: 00:25:52

################################################################################
                     [1m Learning iteration 1304/2000 [0m                     

                       Computation: 44807 steps/s (collection: 2.083s, learning 0.111s)
             Mean action noise std: 2.64
          Mean value_function loss: 71.9528
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.0793
                       Mean reward: 890.87
               Mean episode length: 236.46
    Episode_Reward/reaching_object: 1.4129
     Episode_Reward/lifting_object: 177.0236
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 2.19s
                      Time elapsed: 00:48:26
                               ETA: 00:25:49

################################################################################
                     [1m Learning iteration 1305/2000 [0m                     

                       Computation: 44986 steps/s (collection: 2.070s, learning 0.116s)
             Mean action noise std: 2.64
          Mean value_function loss: 87.4083
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.0914
                       Mean reward: 870.83
               Mean episode length: 231.90
    Episode_Reward/reaching_object: 1.4068
     Episode_Reward/lifting_object: 176.1380
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 2.19s
                      Time elapsed: 00:48:28
                               ETA: 00:25:47

################################################################################
                     [1m Learning iteration 1306/2000 [0m                     

                       Computation: 45642 steps/s (collection: 2.031s, learning 0.123s)
             Mean action noise std: 2.65
          Mean value_function loss: 61.6317
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 52.1012
                       Mean reward: 896.38
               Mean episode length: 239.58
    Episode_Reward/reaching_object: 1.4343
     Episode_Reward/lifting_object: 179.6636
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 2.15s
                      Time elapsed: 00:48:30
                               ETA: 00:25:45

################################################################################
                     [1m Learning iteration 1307/2000 [0m                     

                       Computation: 45335 steps/s (collection: 2.045s, learning 0.124s)
             Mean action noise std: 2.65
          Mean value_function loss: 56.9657
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 52.1115
                       Mean reward: 911.19
               Mean episode length: 241.74
    Episode_Reward/reaching_object: 1.4429
     Episode_Reward/lifting_object: 180.8803
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 2.17s
                      Time elapsed: 00:48:32
                               ETA: 00:25:43

################################################################################
                     [1m Learning iteration 1308/2000 [0m                     

                       Computation: 45078 steps/s (collection: 2.063s, learning 0.118s)
             Mean action noise std: 2.65
          Mean value_function loss: 65.6649
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.1161
                       Mean reward: 933.49
               Mean episode length: 246.98
    Episode_Reward/reaching_object: 1.4339
     Episode_Reward/lifting_object: 179.2589
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 2.18s
                      Time elapsed: 00:48:34
                               ETA: 00:25:40

################################################################################
                     [1m Learning iteration 1309/2000 [0m                     

                       Computation: 45986 steps/s (collection: 2.023s, learning 0.115s)
             Mean action noise std: 2.65
          Mean value_function loss: 88.4118
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 52.1277
                       Mean reward: 919.83
               Mean episode length: 242.82
    Episode_Reward/reaching_object: 1.4257
     Episode_Reward/lifting_object: 178.8031
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 2.14s
                      Time elapsed: 00:48:36
                               ETA: 00:25:38

################################################################################
                     [1m Learning iteration 1310/2000 [0m                     

                       Computation: 45816 steps/s (collection: 2.024s, learning 0.122s)
             Mean action noise std: 2.65
          Mean value_function loss: 56.0708
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.1393
                       Mean reward: 906.35
               Mean episode length: 240.96
    Episode_Reward/reaching_object: 1.4636
     Episode_Reward/lifting_object: 183.7421
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 2.15s
                      Time elapsed: 00:48:39
                               ETA: 00:25:36

################################################################################
                     [1m Learning iteration 1311/2000 [0m                     

                       Computation: 45436 steps/s (collection: 2.049s, learning 0.115s)
             Mean action noise std: 2.65
          Mean value_function loss: 84.1335
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.1475
                       Mean reward: 920.62
               Mean episode length: 244.48
    Episode_Reward/reaching_object: 1.4267
     Episode_Reward/lifting_object: 178.3784
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 2.16s
                      Time elapsed: 00:48:41
                               ETA: 00:25:34

################################################################################
                     [1m Learning iteration 1312/2000 [0m                     

                       Computation: 46256 steps/s (collection: 2.033s, learning 0.092s)
             Mean action noise std: 2.65
          Mean value_function loss: 80.8430
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.1555
                       Mean reward: 911.83
               Mean episode length: 242.05
    Episode_Reward/reaching_object: 1.4238
     Episode_Reward/lifting_object: 178.2253
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 2.13s
                      Time elapsed: 00:48:43
                               ETA: 00:25:31

################################################################################
                     [1m Learning iteration 1313/2000 [0m                     

                       Computation: 46661 steps/s (collection: 2.013s, learning 0.094s)
             Mean action noise std: 2.65
          Mean value_function loss: 78.1467
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.1649
                       Mean reward: 913.21
               Mean episode length: 242.42
    Episode_Reward/reaching_object: 1.4294
     Episode_Reward/lifting_object: 178.6845
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 2.11s
                      Time elapsed: 00:48:45
                               ETA: 00:25:29

################################################################################
                     [1m Learning iteration 1314/2000 [0m                     

                       Computation: 46600 steps/s (collection: 2.011s, learning 0.099s)
             Mean action noise std: 2.66
          Mean value_function loss: 84.7380
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 52.1788
                       Mean reward: 923.20
               Mean episode length: 244.15
    Episode_Reward/reaching_object: 1.4058
     Episode_Reward/lifting_object: 176.3326
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 2.11s
                      Time elapsed: 00:48:47
                               ETA: 00:25:27

################################################################################
                     [1m Learning iteration 1315/2000 [0m                     

                       Computation: 46564 steps/s (collection: 2.015s, learning 0.097s)
             Mean action noise std: 2.66
          Mean value_function loss: 63.8702
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 52.1910
                       Mean reward: 925.28
               Mean episode length: 245.05
    Episode_Reward/reaching_object: 1.4503
     Episode_Reward/lifting_object: 181.6011
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 2.11s
                      Time elapsed: 00:48:49
                               ETA: 00:25:24

################################################################################
                     [1m Learning iteration 1316/2000 [0m                     

                       Computation: 46119 steps/s (collection: 2.030s, learning 0.101s)
             Mean action noise std: 2.66
          Mean value_function loss: 62.9880
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 52.1986
                       Mean reward: 920.10
               Mean episode length: 243.14
    Episode_Reward/reaching_object: 1.4355
     Episode_Reward/lifting_object: 179.9818
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 2.13s
                      Time elapsed: 00:48:51
                               ETA: 00:25:22

################################################################################
                     [1m Learning iteration 1317/2000 [0m                     

                       Computation: 45090 steps/s (collection: 2.058s, learning 0.123s)
             Mean action noise std: 2.66
          Mean value_function loss: 85.6385
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 52.2080
                       Mean reward: 879.72
               Mean episode length: 234.01
    Episode_Reward/reaching_object: 1.4316
     Episode_Reward/lifting_object: 179.5929
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 2.18s
                      Time elapsed: 00:48:54
                               ETA: 00:25:20

################################################################################
                     [1m Learning iteration 1318/2000 [0m                     

                       Computation: 46204 steps/s (collection: 2.013s, learning 0.115s)
             Mean action noise std: 2.66
          Mean value_function loss: 108.0115
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 52.2204
                       Mean reward: 868.85
               Mean episode length: 230.97
    Episode_Reward/reaching_object: 1.4128
     Episode_Reward/lifting_object: 176.6225
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 2.13s
                      Time elapsed: 00:48:56
                               ETA: 00:25:18

################################################################################
                     [1m Learning iteration 1319/2000 [0m                     

                       Computation: 46236 steps/s (collection: 2.015s, learning 0.111s)
             Mean action noise std: 2.66
          Mean value_function loss: 119.6125
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.2363
                       Mean reward: 875.28
               Mean episode length: 234.03
    Episode_Reward/reaching_object: 1.4136
     Episode_Reward/lifting_object: 177.0604
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 2.13s
                      Time elapsed: 00:48:58
                               ETA: 00:25:15

################################################################################
                     [1m Learning iteration 1320/2000 [0m                     

                       Computation: 46410 steps/s (collection: 2.013s, learning 0.105s)
             Mean action noise std: 2.66
          Mean value_function loss: 85.3922
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 52.2487
                       Mean reward: 885.49
               Mean episode length: 236.56
    Episode_Reward/reaching_object: 1.4141
     Episode_Reward/lifting_object: 177.4283
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 2.12s
                      Time elapsed: 00:49:00
                               ETA: 00:25:13

################################################################################
                     [1m Learning iteration 1321/2000 [0m                     

                       Computation: 46770 steps/s (collection: 2.008s, learning 0.094s)
             Mean action noise std: 2.67
          Mean value_function loss: 70.0532
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 52.2640
                       Mean reward: 882.34
               Mean episode length: 234.96
    Episode_Reward/reaching_object: 1.4220
     Episode_Reward/lifting_object: 178.2656
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 2.10s
                      Time elapsed: 00:49:02
                               ETA: 00:25:11

################################################################################
                     [1m Learning iteration 1322/2000 [0m                     

                       Computation: 46682 steps/s (collection: 1.996s, learning 0.110s)
             Mean action noise std: 2.67
          Mean value_function loss: 74.4253
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.2735
                       Mean reward: 870.50
               Mean episode length: 231.36
    Episode_Reward/reaching_object: 1.4302
     Episode_Reward/lifting_object: 178.6941
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 2.11s
                      Time elapsed: 00:49:04
                               ETA: 00:25:09

################################################################################
                     [1m Learning iteration 1323/2000 [0m                     

                       Computation: 45765 steps/s (collection: 2.038s, learning 0.110s)
             Mean action noise std: 2.67
          Mean value_function loss: 58.8305
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.2864
                       Mean reward: 928.49
               Mean episode length: 245.89
    Episode_Reward/reaching_object: 1.4179
     Episode_Reward/lifting_object: 177.9352
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 2.15s
                      Time elapsed: 00:49:06
                               ETA: 00:25:06

################################################################################
                     [1m Learning iteration 1324/2000 [0m                     

                       Computation: 45931 steps/s (collection: 2.033s, learning 0.107s)
             Mean action noise std: 2.67
          Mean value_function loss: 63.0225
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.2957
                       Mean reward: 885.90
               Mean episode length: 234.52
    Episode_Reward/reaching_object: 1.4318
     Episode_Reward/lifting_object: 179.7071
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 2.14s
                      Time elapsed: 00:49:08
                               ETA: 00:25:04

################################################################################
                     [1m Learning iteration 1325/2000 [0m                     

                       Computation: 46589 steps/s (collection: 2.011s, learning 0.099s)
             Mean action noise std: 2.67
          Mean value_function loss: 67.0850
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.3066
                       Mean reward: 897.12
               Mean episode length: 236.79
    Episode_Reward/reaching_object: 1.4021
     Episode_Reward/lifting_object: 176.0194
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 2.11s
                      Time elapsed: 00:49:11
                               ETA: 00:25:02

################################################################################
                     [1m Learning iteration 1326/2000 [0m                     

                       Computation: 45945 steps/s (collection: 2.040s, learning 0.100s)
             Mean action noise std: 2.67
          Mean value_function loss: 74.7144
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 52.3211
                       Mean reward: 916.69
               Mean episode length: 242.40
    Episode_Reward/reaching_object: 1.4282
     Episode_Reward/lifting_object: 179.0923
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 2.14s
                      Time elapsed: 00:49:13
                               ETA: 00:24:59

################################################################################
                     [1m Learning iteration 1327/2000 [0m                     

                       Computation: 46154 steps/s (collection: 2.031s, learning 0.099s)
             Mean action noise std: 2.67
          Mean value_function loss: 91.5236
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 52.3273
                       Mean reward: 870.85
               Mean episode length: 231.35
    Episode_Reward/reaching_object: 1.4202
     Episode_Reward/lifting_object: 178.4120
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 2.13s
                      Time elapsed: 00:49:15
                               ETA: 00:24:57

################################################################################
                     [1m Learning iteration 1328/2000 [0m                     

                       Computation: 46518 steps/s (collection: 2.020s, learning 0.093s)
             Mean action noise std: 2.67
          Mean value_function loss: 108.1108
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.3348
                       Mean reward: 892.79
               Mean episode length: 236.16
    Episode_Reward/reaching_object: 1.4063
     Episode_Reward/lifting_object: 176.6069
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 2.11s
                      Time elapsed: 00:49:17
                               ETA: 00:24:55

################################################################################
                     [1m Learning iteration 1329/2000 [0m                     

                       Computation: 46401 steps/s (collection: 2.027s, learning 0.091s)
             Mean action noise std: 2.68
          Mean value_function loss: 73.9152
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.3424
                       Mean reward: 935.74
               Mean episode length: 246.86
    Episode_Reward/reaching_object: 1.4383
     Episode_Reward/lifting_object: 180.2641
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 2.12s
                      Time elapsed: 00:49:19
                               ETA: 00:24:53

################################################################################
                     [1m Learning iteration 1330/2000 [0m                     

                       Computation: 46336 steps/s (collection: 2.027s, learning 0.094s)
             Mean action noise std: 2.68
          Mean value_function loss: 75.1409
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.3511
                       Mean reward: 883.57
               Mean episode length: 234.85
    Episode_Reward/reaching_object: 1.4411
     Episode_Reward/lifting_object: 181.5070
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 2.12s
                      Time elapsed: 00:49:21
                               ETA: 00:24:50

################################################################################
                     [1m Learning iteration 1331/2000 [0m                     

                       Computation: 46677 steps/s (collection: 2.006s, learning 0.100s)
             Mean action noise std: 2.68
          Mean value_function loss: 61.9603
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.3609
                       Mean reward: 933.52
               Mean episode length: 246.48
    Episode_Reward/reaching_object: 1.4223
     Episode_Reward/lifting_object: 177.7277
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 2.11s
                      Time elapsed: 00:49:23
                               ETA: 00:24:48

################################################################################
                     [1m Learning iteration 1332/2000 [0m                     

                       Computation: 45942 steps/s (collection: 2.030s, learning 0.110s)
             Mean action noise std: 2.68
          Mean value_function loss: 78.1386
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.3664
                       Mean reward: 887.03
               Mean episode length: 236.64
    Episode_Reward/reaching_object: 1.4286
     Episode_Reward/lifting_object: 179.4164
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 2.14s
                      Time elapsed: 00:49:25
                               ETA: 00:24:46

################################################################################
                     [1m Learning iteration 1333/2000 [0m                     

                       Computation: 19352 steps/s (collection: 4.952s, learning 0.127s)
             Mean action noise std: 2.68
          Mean value_function loss: 98.0399
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 52.3735
                       Mean reward: 850.97
               Mean episode length: 226.90
    Episode_Reward/reaching_object: 1.4010
     Episode_Reward/lifting_object: 175.7934
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 5.08s
                      Time elapsed: 00:49:30
                               ETA: 00:24:45

################################################################################
                     [1m Learning iteration 1334/2000 [0m                     

                       Computation: 14516 steps/s (collection: 6.653s, learning 0.119s)
             Mean action noise std: 2.68
          Mean value_function loss: 80.8107
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.3889
                       Mean reward: 929.93
               Mean episode length: 245.00
    Episode_Reward/reaching_object: 1.4341
     Episode_Reward/lifting_object: 179.9720
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 6.77s
                      Time elapsed: 00:49:37
                               ETA: 00:24:45

################################################################################
                     [1m Learning iteration 1335/2000 [0m                     

                       Computation: 14282 steps/s (collection: 6.763s, learning 0.120s)
             Mean action noise std: 2.68
          Mean value_function loss: 67.6837
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.4012
                       Mean reward: 924.08
               Mean episode length: 244.27
    Episode_Reward/reaching_object: 1.4318
     Episode_Reward/lifting_object: 179.9798
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 6.88s
                      Time elapsed: 00:49:44
                               ETA: 00:24:45

################################################################################
                     [1m Learning iteration 1336/2000 [0m                     

                       Computation: 14320 steps/s (collection: 6.758s, learning 0.107s)
             Mean action noise std: 2.68
          Mean value_function loss: 80.6324
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.4133
                       Mean reward: 917.59
               Mean episode length: 242.49
    Episode_Reward/reaching_object: 1.4375
     Episode_Reward/lifting_object: 179.9933
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 6.86s
                      Time elapsed: 00:49:51
                               ETA: 00:24:45

################################################################################
                     [1m Learning iteration 1337/2000 [0m                     

                       Computation: 14276 steps/s (collection: 6.775s, learning 0.111s)
             Mean action noise std: 2.69
          Mean value_function loss: 81.1209
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.4238
                       Mean reward: 894.67
               Mean episode length: 237.58
    Episode_Reward/reaching_object: 1.4275
     Episode_Reward/lifting_object: 178.9671
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 6.89s
                      Time elapsed: 00:49:58
                               ETA: 00:24:45

################################################################################
                     [1m Learning iteration 1338/2000 [0m                     

                       Computation: 14635 steps/s (collection: 6.586s, learning 0.131s)
             Mean action noise std: 2.69
          Mean value_function loss: 128.7234
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.4313
                       Mean reward: 911.24
               Mean episode length: 242.16
    Episode_Reward/reaching_object: 1.4356
     Episode_Reward/lifting_object: 179.4196
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 6.72s
                      Time elapsed: 00:50:05
                               ETA: 00:24:45

################################################################################
                     [1m Learning iteration 1339/2000 [0m                     

                       Computation: 14324 steps/s (collection: 6.735s, learning 0.128s)
             Mean action noise std: 2.69
          Mean value_function loss: 163.0494
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.4411
                       Mean reward: 894.67
               Mean episode length: 239.60
    Episode_Reward/reaching_object: 1.4484
     Episode_Reward/lifting_object: 180.9653
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 6.86s
                      Time elapsed: 00:50:11
                               ETA: 00:24:45

################################################################################
                     [1m Learning iteration 1340/2000 [0m                     

                       Computation: 15063 steps/s (collection: 6.403s, learning 0.123s)
             Mean action noise std: 2.69
          Mean value_function loss: 63.5059
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.4529
                       Mean reward: 924.51
               Mean episode length: 244.71
    Episode_Reward/reaching_object: 1.4484
     Episode_Reward/lifting_object: 180.8769
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 6.53s
                      Time elapsed: 00:50:18
                               ETA: 00:24:45

################################################################################
                     [1m Learning iteration 1341/2000 [0m                     

                       Computation: 13433 steps/s (collection: 7.213s, learning 0.105s)
             Mean action noise std: 2.69
          Mean value_function loss: 64.0168
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.4614
                       Mean reward: 924.54
               Mean episode length: 244.31
    Episode_Reward/reaching_object: 1.4535
     Episode_Reward/lifting_object: 182.1630
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 7.32s
                      Time elapsed: 00:50:25
                               ETA: 00:24:45

################################################################################
                     [1m Learning iteration 1342/2000 [0m                     

                       Computation: 48316 steps/s (collection: 1.945s, learning 0.090s)
             Mean action noise std: 2.69
          Mean value_function loss: 86.8455
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.4686
                       Mean reward: 867.14
               Mean episode length: 230.88
    Episode_Reward/reaching_object: 1.4361
     Episode_Reward/lifting_object: 179.2653
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 2.03s
                      Time elapsed: 00:50:27
                               ETA: 00:24:43

################################################################################
                     [1m Learning iteration 1343/2000 [0m                     

                       Computation: 48926 steps/s (collection: 1.915s, learning 0.094s)
             Mean action noise std: 2.69
          Mean value_function loss: 64.2266
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.4788
                       Mean reward: 903.12
               Mean episode length: 241.09
    Episode_Reward/reaching_object: 1.4376
     Episode_Reward/lifting_object: 179.9082
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 2.01s
                      Time elapsed: 00:50:29
                               ETA: 00:24:41

################################################################################
                     [1m Learning iteration 1344/2000 [0m                     

                       Computation: 48257 steps/s (collection: 1.939s, learning 0.098s)
             Mean action noise std: 2.69
          Mean value_function loss: 81.6180
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 52.4951
                       Mean reward: 905.33
               Mean episode length: 240.81
    Episode_Reward/reaching_object: 1.4359
     Episode_Reward/lifting_object: 179.6739
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 2.04s
                      Time elapsed: 00:50:31
                               ETA: 00:24:38

################################################################################
                     [1m Learning iteration 1345/2000 [0m                     

                       Computation: 47007 steps/s (collection: 2.000s, learning 0.091s)
             Mean action noise std: 2.70
          Mean value_function loss: 72.7678
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.5077
                       Mean reward: 908.37
               Mean episode length: 242.09
    Episode_Reward/reaching_object: 1.4354
     Episode_Reward/lifting_object: 178.3566
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 2.09s
                      Time elapsed: 00:50:33
                               ETA: 00:24:36

################################################################################
                     [1m Learning iteration 1346/2000 [0m                     

                       Computation: 48348 steps/s (collection: 1.942s, learning 0.091s)
             Mean action noise std: 2.70
          Mean value_function loss: 69.2720
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 52.5163
                       Mean reward: 892.80
               Mean episode length: 237.61
    Episode_Reward/reaching_object: 1.4359
     Episode_Reward/lifting_object: 180.0649
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 2.03s
                      Time elapsed: 00:50:36
                               ETA: 00:24:34

################################################################################
                     [1m Learning iteration 1347/2000 [0m                     

                       Computation: 49481 steps/s (collection: 1.901s, learning 0.086s)
             Mean action noise std: 2.70
          Mean value_function loss: 45.8492
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.5233
                       Mean reward: 927.21
               Mean episode length: 245.59
    Episode_Reward/reaching_object: 1.4588
     Episode_Reward/lifting_object: 182.3220
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 1.99s
                      Time elapsed: 00:50:38
                               ETA: 00:24:31

################################################################################
                     [1m Learning iteration 1348/2000 [0m                     

                       Computation: 47071 steps/s (collection: 1.974s, learning 0.114s)
             Mean action noise std: 2.70
          Mean value_function loss: 74.7047
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 52.5317
                       Mean reward: 921.51
               Mean episode length: 245.05
    Episode_Reward/reaching_object: 1.4372
     Episode_Reward/lifting_object: 180.0007
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 2.09s
                      Time elapsed: 00:50:40
                               ETA: 00:24:29

################################################################################
                     [1m Learning iteration 1349/2000 [0m                     

                       Computation: 46191 steps/s (collection: 2.015s, learning 0.113s)
             Mean action noise std: 2.70
          Mean value_function loss: 84.4560
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 52.5351
                       Mean reward: 919.12
               Mean episode length: 243.65
    Episode_Reward/reaching_object: 1.4463
     Episode_Reward/lifting_object: 181.1237
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 2.13s
                      Time elapsed: 00:50:42
                               ETA: 00:24:27

################################################################################
                     [1m Learning iteration 1350/2000 [0m                     

                       Computation: 46592 steps/s (collection: 2.016s, learning 0.094s)
             Mean action noise std: 2.70
          Mean value_function loss: 71.9718
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.5436
                       Mean reward: 940.34
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 1.4416
     Episode_Reward/lifting_object: 180.5119
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 2.11s
                      Time elapsed: 00:50:44
                               ETA: 00:24:24

################################################################################
                     [1m Learning iteration 1351/2000 [0m                     

                       Computation: 41801 steps/s (collection: 2.191s, learning 0.161s)
             Mean action noise std: 2.70
          Mean value_function loss: 63.8051
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.5559
                       Mean reward: 919.62
               Mean episode length: 242.93
    Episode_Reward/reaching_object: 1.4518
     Episode_Reward/lifting_object: 181.8195
      Episode_Reward/object_height: 0.0273
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 2.35s
                      Time elapsed: 00:50:46
                               ETA: 00:24:22

################################################################################
                     [1m Learning iteration 1352/2000 [0m                     

                       Computation: 44274 steps/s (collection: 2.122s, learning 0.098s)
             Mean action noise std: 2.70
          Mean value_function loss: 87.3980
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.5660
                       Mean reward: 868.01
               Mean episode length: 230.90
    Episode_Reward/reaching_object: 1.4079
     Episode_Reward/lifting_object: 175.8076
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 2.22s
                      Time elapsed: 00:50:48
                               ETA: 00:24:20

################################################################################
                     [1m Learning iteration 1353/2000 [0m                     

                       Computation: 39846 steps/s (collection: 2.333s, learning 0.134s)
             Mean action noise std: 2.70
          Mean value_function loss: 58.1208
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 52.5715
                       Mean reward: 908.92
               Mean episode length: 241.07
    Episode_Reward/reaching_object: 1.4383
     Episode_Reward/lifting_object: 180.3191
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 2.47s
                      Time elapsed: 00:50:51
                               ETA: 00:24:18

################################################################################
                     [1m Learning iteration 1354/2000 [0m                     

                       Computation: 40754 steps/s (collection: 2.169s, learning 0.243s)
             Mean action noise std: 2.70
          Mean value_function loss: 64.9556
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 52.5791
                       Mean reward: 879.13
               Mean episode length: 232.73
    Episode_Reward/reaching_object: 1.4267
     Episode_Reward/lifting_object: 178.2137
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 2.41s
                      Time elapsed: 00:50:53
                               ETA: 00:24:15

################################################################################
                     [1m Learning iteration 1355/2000 [0m                     

                       Computation: 43091 steps/s (collection: 2.134s, learning 0.148s)
             Mean action noise std: 2.70
          Mean value_function loss: 71.4114
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 52.5813
                       Mean reward: 895.75
               Mean episode length: 237.18
    Episode_Reward/reaching_object: 1.4391
     Episode_Reward/lifting_object: 180.0296
      Episode_Reward/object_height: 0.0268
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 2.28s
                      Time elapsed: 00:50:56
                               ETA: 00:24:13

################################################################################
                     [1m Learning iteration 1356/2000 [0m                     

                       Computation: 47298 steps/s (collection: 1.977s, learning 0.101s)
             Mean action noise std: 2.71
          Mean value_function loss: 64.8676
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.5848
                       Mean reward: 932.72
               Mean episode length: 247.38
    Episode_Reward/reaching_object: 1.4269
     Episode_Reward/lifting_object: 178.1160
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 2.08s
                      Time elapsed: 00:50:58
                               ETA: 00:24:11

################################################################################
                     [1m Learning iteration 1357/2000 [0m                     

                       Computation: 46968 steps/s (collection: 1.991s, learning 0.102s)
             Mean action noise std: 2.71
          Mean value_function loss: 64.7418
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.5919
                       Mean reward: 908.88
               Mean episode length: 240.82
    Episode_Reward/reaching_object: 1.4330
     Episode_Reward/lifting_object: 179.3365
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 2.09s
                      Time elapsed: 00:51:00
                               ETA: 00:24:08

################################################################################
                     [1m Learning iteration 1358/2000 [0m                     

                       Computation: 47013 steps/s (collection: 2.000s, learning 0.091s)
             Mean action noise std: 2.71
          Mean value_function loss: 66.7157
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.6058
                       Mean reward: 909.15
               Mean episode length: 240.30
    Episode_Reward/reaching_object: 1.4367
     Episode_Reward/lifting_object: 180.5434
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 2.09s
                      Time elapsed: 00:51:02
                               ETA: 00:24:06

################################################################################
                     [1m Learning iteration 1359/2000 [0m                     

                       Computation: 48115 steps/s (collection: 1.942s, learning 0.102s)
             Mean action noise std: 2.71
          Mean value_function loss: 62.1140
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.6206
                       Mean reward: 914.18
               Mean episode length: 242.43
    Episode_Reward/reaching_object: 1.4440
     Episode_Reward/lifting_object: 180.9027
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 2.04s
                      Time elapsed: 00:51:04
                               ETA: 00:24:04

################################################################################
                     [1m Learning iteration 1360/2000 [0m                     

                       Computation: 48033 steps/s (collection: 1.944s, learning 0.102s)
             Mean action noise std: 2.71
          Mean value_function loss: 63.2624
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.6339
                       Mean reward: 921.89
               Mean episode length: 243.94
    Episode_Reward/reaching_object: 1.4468
     Episode_Reward/lifting_object: 181.7403
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 2.05s
                      Time elapsed: 00:51:06
                               ETA: 00:24:01

################################################################################
                     [1m Learning iteration 1361/2000 [0m                     

                       Computation: 47224 steps/s (collection: 1.987s, learning 0.095s)
             Mean action noise std: 2.71
          Mean value_function loss: 51.6327
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 52.6455
                       Mean reward: 932.95
               Mean episode length: 245.63
    Episode_Reward/reaching_object: 1.4437
     Episode_Reward/lifting_object: 181.6081
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 2.08s
                      Time elapsed: 00:51:08
                               ETA: 00:23:59

################################################################################
                     [1m Learning iteration 1362/2000 [0m                     

                       Computation: 44675 steps/s (collection: 2.080s, learning 0.121s)
             Mean action noise std: 2.71
          Mean value_function loss: 150.3017
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 52.6590
                       Mean reward: 888.11
               Mean episode length: 235.72
    Episode_Reward/reaching_object: 1.3982
     Episode_Reward/lifting_object: 175.1131
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 2.20s
                      Time elapsed: 00:51:10
                               ETA: 00:23:57

################################################################################
                     [1m Learning iteration 1363/2000 [0m                     

                       Computation: 46454 steps/s (collection: 2.021s, learning 0.095s)
             Mean action noise std: 2.72
          Mean value_function loss: 78.3880
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.6678
                       Mean reward: 921.53
               Mean episode length: 244.18
    Episode_Reward/reaching_object: 1.4490
     Episode_Reward/lifting_object: 180.8437
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 2.12s
                      Time elapsed: 00:51:12
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 1364/2000 [0m                     

                       Computation: 45345 steps/s (collection: 2.082s, learning 0.086s)
             Mean action noise std: 2.72
          Mean value_function loss: 74.0377
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 52.6799
                       Mean reward: 915.19
               Mean episode length: 241.68
    Episode_Reward/reaching_object: 1.4373
     Episode_Reward/lifting_object: 180.1145
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 2.17s
                      Time elapsed: 00:51:14
                               ETA: 00:23:52

################################################################################
                     [1m Learning iteration 1365/2000 [0m                     

                       Computation: 45835 steps/s (collection: 2.055s, learning 0.090s)
             Mean action noise std: 2.72
          Mean value_function loss: 81.2446
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 52.6932
                       Mean reward: 885.01
               Mean episode length: 235.51
    Episode_Reward/reaching_object: 1.4362
     Episode_Reward/lifting_object: 179.7663
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 2.14s
                      Time elapsed: 00:51:17
                               ETA: 00:23:50

################################################################################
                     [1m Learning iteration 1366/2000 [0m                     

                       Computation: 47224 steps/s (collection: 1.995s, learning 0.087s)
             Mean action noise std: 2.72
          Mean value_function loss: 59.9806
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.7087
                       Mean reward: 923.93
               Mean episode length: 243.76
    Episode_Reward/reaching_object: 1.4456
     Episode_Reward/lifting_object: 181.3127
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 2.08s
                      Time elapsed: 00:51:19
                               ETA: 00:23:48

################################################################################
                     [1m Learning iteration 1367/2000 [0m                     

                       Computation: 44538 steps/s (collection: 2.115s, learning 0.092s)
             Mean action noise std: 2.72
          Mean value_function loss: 62.8742
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.7153
                       Mean reward: 907.99
               Mean episode length: 240.24
    Episode_Reward/reaching_object: 1.4552
     Episode_Reward/lifting_object: 182.3040
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 2.21s
                      Time elapsed: 00:51:21
                               ETA: 00:23:45

################################################################################
                     [1m Learning iteration 1368/2000 [0m                     

                       Computation: 45304 steps/s (collection: 2.074s, learning 0.096s)
             Mean action noise std: 2.72
          Mean value_function loss: 66.9548
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.7240
                       Mean reward: 915.50
               Mean episode length: 242.75
    Episode_Reward/reaching_object: 1.4531
     Episode_Reward/lifting_object: 181.4283
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 2.17s
                      Time elapsed: 00:51:23
                               ETA: 00:23:43

################################################################################
                     [1m Learning iteration 1369/2000 [0m                     

                       Computation: 47407 steps/s (collection: 1.977s, learning 0.097s)
             Mean action noise std: 2.72
          Mean value_function loss: 76.0800
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.7329
                       Mean reward: 910.86
               Mean episode length: 241.09
    Episode_Reward/reaching_object: 1.4268
     Episode_Reward/lifting_object: 178.3760
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 2.07s
                      Time elapsed: 00:51:25
                               ETA: 00:23:41

################################################################################
                     [1m Learning iteration 1370/2000 [0m                     

                       Computation: 43679 steps/s (collection: 2.121s, learning 0.129s)
             Mean action noise std: 2.73
          Mean value_function loss: 86.9289
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.7403
                       Mean reward: 895.92
               Mean episode length: 236.72
    Episode_Reward/reaching_object: 1.4315
     Episode_Reward/lifting_object: 178.4502
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 2.25s
                      Time elapsed: 00:51:27
                               ETA: 00:23:38

################################################################################
                     [1m Learning iteration 1371/2000 [0m                     

                       Computation: 46669 steps/s (collection: 2.001s, learning 0.106s)
             Mean action noise std: 2.73
          Mean value_function loss: 79.1750
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 52.7470
                       Mean reward: 910.30
               Mean episode length: 241.46
    Episode_Reward/reaching_object: 1.4357
     Episode_Reward/lifting_object: 178.5199
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 2.11s
                      Time elapsed: 00:51:30
                               ETA: 00:23:36

################################################################################
                     [1m Learning iteration 1372/2000 [0m                     

                       Computation: 46735 steps/s (collection: 1.995s, learning 0.108s)
             Mean action noise std: 2.73
          Mean value_function loss: 95.5356
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.7503
                       Mean reward: 865.68
               Mean episode length: 231.60
    Episode_Reward/reaching_object: 1.4136
     Episode_Reward/lifting_object: 175.4653
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 2.10s
                      Time elapsed: 00:51:32
                               ETA: 00:23:34

################################################################################
                     [1m Learning iteration 1373/2000 [0m                     

                       Computation: 44802 steps/s (collection: 2.093s, learning 0.102s)
             Mean action noise std: 2.73
          Mean value_function loss: 83.1680
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.7581
                       Mean reward: 891.70
               Mean episode length: 235.54
    Episode_Reward/reaching_object: 1.4307
     Episode_Reward/lifting_object: 178.2255
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 2.19s
                      Time elapsed: 00:51:34
                               ETA: 00:23:32

################################################################################
                     [1m Learning iteration 1374/2000 [0m                     

                       Computation: 45252 steps/s (collection: 2.040s, learning 0.133s)
             Mean action noise std: 2.73
          Mean value_function loss: 88.0152
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 52.7656
                       Mean reward: 907.59
               Mean episode length: 241.04
    Episode_Reward/reaching_object: 1.4329
     Episode_Reward/lifting_object: 178.3142
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 2.17s
                      Time elapsed: 00:51:36
                               ETA: 00:23:29

################################################################################
                     [1m Learning iteration 1375/2000 [0m                     

                       Computation: 38977 steps/s (collection: 2.283s, learning 0.239s)
             Mean action noise std: 2.73
          Mean value_function loss: 72.0649
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.7812
                       Mean reward: 893.82
               Mean episode length: 235.91
    Episode_Reward/reaching_object: 1.4337
     Episode_Reward/lifting_object: 178.5068
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 2.52s
                      Time elapsed: 00:51:39
                               ETA: 00:23:27

################################################################################
                     [1m Learning iteration 1376/2000 [0m                     

                       Computation: 38805 steps/s (collection: 2.407s, learning 0.127s)
             Mean action noise std: 2.73
          Mean value_function loss: 63.3502
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.7932
                       Mean reward: 916.43
               Mean episode length: 241.85
    Episode_Reward/reaching_object: 1.4322
     Episode_Reward/lifting_object: 178.6352
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 2.53s
                      Time elapsed: 00:51:41
                               ETA: 00:23:25

################################################################################
                     [1m Learning iteration 1377/2000 [0m                     

                       Computation: 45775 steps/s (collection: 2.040s, learning 0.108s)
             Mean action noise std: 2.73
          Mean value_function loss: 94.5934
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.8033
                       Mean reward: 890.50
               Mean episode length: 236.31
    Episode_Reward/reaching_object: 1.4178
     Episode_Reward/lifting_object: 176.7390
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 2.15s
                      Time elapsed: 00:51:43
                               ETA: 00:23:23

################################################################################
                     [1m Learning iteration 1378/2000 [0m                     

                       Computation: 47392 steps/s (collection: 1.970s, learning 0.104s)
             Mean action noise std: 2.73
          Mean value_function loss: 92.0820
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.8125
                       Mean reward: 867.33
               Mean episode length: 229.96
    Episode_Reward/reaching_object: 1.4284
     Episode_Reward/lifting_object: 178.2749
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 2.07s
                      Time elapsed: 00:51:45
                               ETA: 00:23:20

################################################################################
                     [1m Learning iteration 1379/2000 [0m                     

                       Computation: 48869 steps/s (collection: 1.920s, learning 0.092s)
             Mean action noise std: 2.74
          Mean value_function loss: 106.1656
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.8239
                       Mean reward: 920.33
               Mean episode length: 242.49
    Episode_Reward/reaching_object: 1.4310
     Episode_Reward/lifting_object: 178.6802
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 2.01s
                      Time elapsed: 00:51:47
                               ETA: 00:23:18

################################################################################
                     [1m Learning iteration 1380/2000 [0m                     

                       Computation: 48916 steps/s (collection: 1.920s, learning 0.090s)
             Mean action noise std: 2.74
          Mean value_function loss: 91.8112
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.8358
                       Mean reward: 886.38
               Mean episode length: 235.98
    Episode_Reward/reaching_object: 1.4066
     Episode_Reward/lifting_object: 175.2401
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 2.01s
                      Time elapsed: 00:51:49
                               ETA: 00:23:16

################################################################################
                     [1m Learning iteration 1381/2000 [0m                     

                       Computation: 48581 steps/s (collection: 1.930s, learning 0.094s)
             Mean action noise std: 2.74
          Mean value_function loss: 81.2218
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.8446
                       Mean reward: 883.63
               Mean episode length: 234.61
    Episode_Reward/reaching_object: 1.4445
     Episode_Reward/lifting_object: 181.1994
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 2.02s
                      Time elapsed: 00:51:51
                               ETA: 00:23:13

################################################################################
                     [1m Learning iteration 1382/2000 [0m                     

                       Computation: 47855 steps/s (collection: 1.950s, learning 0.105s)
             Mean action noise std: 2.74
          Mean value_function loss: 83.9799
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 52.8499
                       Mean reward: 915.28
               Mean episode length: 241.79
    Episode_Reward/reaching_object: 1.4359
     Episode_Reward/lifting_object: 179.2097
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 2.05s
                      Time elapsed: 00:51:53
                               ETA: 00:23:11

################################################################################
                     [1m Learning iteration 1383/2000 [0m                     

                       Computation: 45678 steps/s (collection: 2.054s, learning 0.099s)
             Mean action noise std: 2.74
          Mean value_function loss: 82.9467
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 52.8548
                       Mean reward: 886.51
               Mean episode length: 234.92
    Episode_Reward/reaching_object: 1.4243
     Episode_Reward/lifting_object: 177.9022
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 2.15s
                      Time elapsed: 00:51:56
                               ETA: 00:23:09

################################################################################
                     [1m Learning iteration 1384/2000 [0m                     

                       Computation: 48648 steps/s (collection: 1.930s, learning 0.091s)
             Mean action noise std: 2.74
          Mean value_function loss: 92.4547
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.8640
                       Mean reward: 879.18
               Mean episode length: 234.24
    Episode_Reward/reaching_object: 1.4392
     Episode_Reward/lifting_object: 179.8655
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 2.02s
                      Time elapsed: 00:51:58
                               ETA: 00:23:06

################################################################################
                     [1m Learning iteration 1385/2000 [0m                     

                       Computation: 49123 steps/s (collection: 1.912s, learning 0.089s)
             Mean action noise std: 2.74
          Mean value_function loss: 92.0125
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 52.8709
                       Mean reward: 869.41
               Mean episode length: 230.86
    Episode_Reward/reaching_object: 1.4244
     Episode_Reward/lifting_object: 177.9903
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 2.00s
                      Time elapsed: 00:52:00
                               ETA: 00:23:04

################################################################################
                     [1m Learning iteration 1386/2000 [0m                     

                       Computation: 48830 steps/s (collection: 1.924s, learning 0.090s)
             Mean action noise std: 2.74
          Mean value_function loss: 96.4851
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.8756
                       Mean reward: 876.83
               Mean episode length: 232.09
    Episode_Reward/reaching_object: 1.4246
     Episode_Reward/lifting_object: 178.2052
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 2.01s
                      Time elapsed: 00:52:02
                               ETA: 00:23:02

################################################################################
                     [1m Learning iteration 1387/2000 [0m                     

                       Computation: 47829 steps/s (collection: 1.964s, learning 0.092s)
             Mean action noise std: 2.74
          Mean value_function loss: 90.6788
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.8851
                       Mean reward: 932.36
               Mean episode length: 245.92
    Episode_Reward/reaching_object: 1.4415
     Episode_Reward/lifting_object: 179.9933
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 2.06s
                      Time elapsed: 00:52:04
                               ETA: 00:22:59

################################################################################
                     [1m Learning iteration 1388/2000 [0m                     

                       Computation: 49150 steps/s (collection: 1.911s, learning 0.089s)
             Mean action noise std: 2.74
          Mean value_function loss: 78.5498
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 52.8962
                       Mean reward: 906.22
               Mean episode length: 239.42
    Episode_Reward/reaching_object: 1.4263
     Episode_Reward/lifting_object: 177.4509
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 2.00s
                      Time elapsed: 00:52:06
                               ETA: 00:22:57

################################################################################
                     [1m Learning iteration 1389/2000 [0m                     

                       Computation: 48168 steps/s (collection: 1.944s, learning 0.097s)
             Mean action noise std: 2.75
          Mean value_function loss: 81.7137
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.9055
                       Mean reward: 901.84
               Mean episode length: 239.62
    Episode_Reward/reaching_object: 1.4480
     Episode_Reward/lifting_object: 180.4330
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 2.04s
                      Time elapsed: 00:52:08
                               ETA: 00:22:55

################################################################################
                     [1m Learning iteration 1390/2000 [0m                     

                       Computation: 48112 steps/s (collection: 1.941s, learning 0.102s)
             Mean action noise std: 2.75
          Mean value_function loss: 89.1641
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 52.9090
                       Mean reward: 871.44
               Mean episode length: 233.05
    Episode_Reward/reaching_object: 1.4245
     Episode_Reward/lifting_object: 177.1389
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 2.04s
                      Time elapsed: 00:52:10
                               ETA: 00:22:52

################################################################################
                     [1m Learning iteration 1391/2000 [0m                     

                       Computation: 48744 steps/s (collection: 1.919s, learning 0.098s)
             Mean action noise std: 2.75
          Mean value_function loss: 97.9294
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 52.9100
                       Mean reward: 891.67
               Mean episode length: 236.10
    Episode_Reward/reaching_object: 1.4226
     Episode_Reward/lifting_object: 177.1275
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 2.02s
                      Time elapsed: 00:52:12
                               ETA: 00:22:50

################################################################################
                     [1m Learning iteration 1392/2000 [0m                     

                       Computation: 48634 steps/s (collection: 1.917s, learning 0.104s)
             Mean action noise std: 2.75
          Mean value_function loss: 88.9122
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 52.9112
                       Mean reward: 890.82
               Mean episode length: 236.68
    Episode_Reward/reaching_object: 1.4033
     Episode_Reward/lifting_object: 175.1609
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 2.02s
                      Time elapsed: 00:52:14
                               ETA: 00:22:47

################################################################################
                     [1m Learning iteration 1393/2000 [0m                     

                       Computation: 48647 steps/s (collection: 1.928s, learning 0.093s)
             Mean action noise std: 2.75
          Mean value_function loss: 105.8223
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 52.9124
                       Mean reward: 900.22
               Mean episode length: 238.13
    Episode_Reward/reaching_object: 1.4128
     Episode_Reward/lifting_object: 176.9003
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 2.02s
                      Time elapsed: 00:52:16
                               ETA: 00:22:45

################################################################################
                     [1m Learning iteration 1394/2000 [0m                     

                       Computation: 48077 steps/s (collection: 1.956s, learning 0.089s)
             Mean action noise std: 2.75
          Mean value_function loss: 71.8011
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 52.9136
                       Mean reward: 912.64
               Mean episode length: 242.04
    Episode_Reward/reaching_object: 1.4298
     Episode_Reward/lifting_object: 178.2839
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 2.04s
                      Time elapsed: 00:52:18
                               ETA: 00:22:43

################################################################################
                     [1m Learning iteration 1395/2000 [0m                     

                       Computation: 47977 steps/s (collection: 1.950s, learning 0.099s)
             Mean action noise std: 2.75
          Mean value_function loss: 69.3323
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 52.9146
                       Mean reward: 917.49
               Mean episode length: 242.33
    Episode_Reward/reaching_object: 1.4367
     Episode_Reward/lifting_object: 178.9635
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 2.05s
                      Time elapsed: 00:52:20
                               ETA: 00:22:40

################################################################################
                     [1m Learning iteration 1396/2000 [0m                     

                       Computation: 48421 steps/s (collection: 1.933s, learning 0.097s)
             Mean action noise std: 2.75
          Mean value_function loss: 63.9331
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.9173
                       Mean reward: 915.42
               Mean episode length: 242.24
    Episode_Reward/reaching_object: 1.4279
     Episode_Reward/lifting_object: 177.7694
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 2.03s
                      Time elapsed: 00:52:22
                               ETA: 00:22:38

################################################################################
                     [1m Learning iteration 1397/2000 [0m                     

                       Computation: 47846 steps/s (collection: 1.964s, learning 0.090s)
             Mean action noise std: 2.75
          Mean value_function loss: 82.0096
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 52.9265
                       Mean reward: 876.31
               Mean episode length: 234.29
    Episode_Reward/reaching_object: 1.4425
     Episode_Reward/lifting_object: 179.9812
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 2.05s
                      Time elapsed: 00:52:24
                               ETA: 00:22:36

################################################################################
                     [1m Learning iteration 1398/2000 [0m                     

                       Computation: 48530 steps/s (collection: 1.939s, learning 0.087s)
             Mean action noise std: 2.75
          Mean value_function loss: 90.2996
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 52.9408
                       Mean reward: 879.69
               Mean episode length: 233.94
    Episode_Reward/reaching_object: 1.4255
     Episode_Reward/lifting_object: 177.3876
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 2.03s
                      Time elapsed: 00:52:26
                               ETA: 00:22:33

################################################################################
                     [1m Learning iteration 1399/2000 [0m                     

                       Computation: 47637 steps/s (collection: 1.974s, learning 0.090s)
             Mean action noise std: 2.75
          Mean value_function loss: 70.5344
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.9532
                       Mean reward: 936.83
               Mean episode length: 246.83
    Episode_Reward/reaching_object: 1.4484
     Episode_Reward/lifting_object: 180.3525
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 2.06s
                      Time elapsed: 00:52:28
                               ETA: 00:22:31

################################################################################
                     [1m Learning iteration 1400/2000 [0m                     

                       Computation: 47262 steps/s (collection: 1.994s, learning 0.086s)
             Mean action noise std: 2.75
          Mean value_function loss: 70.5974
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 52.9615
                       Mean reward: 913.06
               Mean episode length: 240.61
    Episode_Reward/reaching_object: 1.4362
     Episode_Reward/lifting_object: 178.8025
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 2.08s
                      Time elapsed: 00:52:30
                               ETA: 00:22:29

################################################################################
                     [1m Learning iteration 1401/2000 [0m                     

                       Computation: 47757 steps/s (collection: 1.975s, learning 0.084s)
             Mean action noise std: 2.75
          Mean value_function loss: 67.2856
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.9735
                       Mean reward: 909.21
               Mean episode length: 240.41
    Episode_Reward/reaching_object: 1.4711
     Episode_Reward/lifting_object: 183.6902
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 2.06s
                      Time elapsed: 00:52:32
                               ETA: 00:22:26

################################################################################
                     [1m Learning iteration 1402/2000 [0m                     

                       Computation: 46373 steps/s (collection: 2.026s, learning 0.094s)
             Mean action noise std: 2.76
          Mean value_function loss: 75.5075
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.9883
                       Mean reward: 916.86
               Mean episode length: 242.09
    Episode_Reward/reaching_object: 1.4491
     Episode_Reward/lifting_object: 181.1559
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 2.12s
                      Time elapsed: 00:52:34
                               ETA: 00:22:24

################################################################################
                     [1m Learning iteration 1403/2000 [0m                     

                       Computation: 48836 steps/s (collection: 1.924s, learning 0.089s)
             Mean action noise std: 2.76
          Mean value_function loss: 95.0159
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.0016
                       Mean reward: 862.20
               Mean episode length: 231.15
    Episode_Reward/reaching_object: 1.4332
     Episode_Reward/lifting_object: 178.5265
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 2.01s
                      Time elapsed: 00:52:36
                               ETA: 00:22:22

################################################################################
                     [1m Learning iteration 1404/2000 [0m                     

                       Computation: 47405 steps/s (collection: 1.961s, learning 0.113s)
             Mean action noise std: 2.76
          Mean value_function loss: 84.2982
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.0173
                       Mean reward: 916.51
               Mean episode length: 242.09
    Episode_Reward/reaching_object: 1.4392
     Episode_Reward/lifting_object: 179.4135
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 2.07s
                      Time elapsed: 00:52:38
                               ETA: 00:22:19

################################################################################
                     [1m Learning iteration 1405/2000 [0m                     

                       Computation: 45719 steps/s (collection: 2.037s, learning 0.113s)
             Mean action noise std: 2.76
          Mean value_function loss: 76.8977
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 53.0307
                       Mean reward: 872.24
               Mean episode length: 231.92
    Episode_Reward/reaching_object: 1.4343
     Episode_Reward/lifting_object: 179.5347
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 2.15s
                      Time elapsed: 00:52:41
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 1406/2000 [0m                     

                       Computation: 48010 steps/s (collection: 1.934s, learning 0.114s)
             Mean action noise std: 2.76
          Mean value_function loss: 89.6470
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.0387
                       Mean reward: 889.31
               Mean episode length: 235.64
    Episode_Reward/reaching_object: 1.4025
     Episode_Reward/lifting_object: 174.7263
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 2.05s
                      Time elapsed: 00:52:43
                               ETA: 00:22:15

################################################################################
                     [1m Learning iteration 1407/2000 [0m                     

                       Computation: 47350 steps/s (collection: 1.981s, learning 0.095s)
             Mean action noise std: 2.76
          Mean value_function loss: 86.8526
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 53.0480
                       Mean reward: 898.99
               Mean episode length: 238.36
    Episode_Reward/reaching_object: 1.4416
     Episode_Reward/lifting_object: 179.7728
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 2.08s
                      Time elapsed: 00:52:45
                               ETA: 00:22:13

################################################################################
                     [1m Learning iteration 1408/2000 [0m                     

                       Computation: 47665 steps/s (collection: 1.967s, learning 0.095s)
             Mean action noise std: 2.77
          Mean value_function loss: 90.6575
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.0592
                       Mean reward: 886.30
               Mean episode length: 234.72
    Episode_Reward/reaching_object: 1.4367
     Episode_Reward/lifting_object: 180.2992
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 2.06s
                      Time elapsed: 00:52:47
                               ETA: 00:22:10

################################################################################
                     [1m Learning iteration 1409/2000 [0m                     

                       Computation: 48164 steps/s (collection: 1.950s, learning 0.091s)
             Mean action noise std: 2.77
          Mean value_function loss: 125.3132
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.0689
                       Mean reward: 879.55
               Mean episode length: 232.77
    Episode_Reward/reaching_object: 1.3996
     Episode_Reward/lifting_object: 174.6806
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 2.04s
                      Time elapsed: 00:52:49
                               ETA: 00:22:08

################################################################################
                     [1m Learning iteration 1410/2000 [0m                     

                       Computation: 47590 steps/s (collection: 1.955s, learning 0.111s)
             Mean action noise std: 2.77
          Mean value_function loss: 95.2217
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.0773
                       Mean reward: 886.27
               Mean episode length: 234.11
    Episode_Reward/reaching_object: 1.4123
     Episode_Reward/lifting_object: 176.1495
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 2.07s
                      Time elapsed: 00:52:51
                               ETA: 00:22:06

################################################################################
                     [1m Learning iteration 1411/2000 [0m                     

                       Computation: 48140 steps/s (collection: 1.952s, learning 0.090s)
             Mean action noise std: 2.77
          Mean value_function loss: 119.4217
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.0880
                       Mean reward: 895.43
               Mean episode length: 238.26
    Episode_Reward/reaching_object: 1.3975
     Episode_Reward/lifting_object: 174.6712
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 2.04s
                      Time elapsed: 00:52:53
                               ETA: 00:22:03

################################################################################
                     [1m Learning iteration 1412/2000 [0m                     

                       Computation: 48491 steps/s (collection: 1.942s, learning 0.085s)
             Mean action noise std: 2.77
          Mean value_function loss: 102.4661
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.0972
                       Mean reward: 900.89
               Mean episode length: 237.77
    Episode_Reward/reaching_object: 1.3973
     Episode_Reward/lifting_object: 174.6578
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 2.03s
                      Time elapsed: 00:52:55
                               ETA: 00:22:01

################################################################################
                     [1m Learning iteration 1413/2000 [0m                     

                       Computation: 48192 steps/s (collection: 1.940s, learning 0.100s)
             Mean action noise std: 2.77
          Mean value_function loss: 91.2281
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.1035
                       Mean reward: 918.29
               Mean episode length: 242.08
    Episode_Reward/reaching_object: 1.4236
     Episode_Reward/lifting_object: 177.9980
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 2.04s
                      Time elapsed: 00:52:57
                               ETA: 00:21:59

################################################################################
                     [1m Learning iteration 1414/2000 [0m                     

                       Computation: 48374 steps/s (collection: 1.942s, learning 0.090s)
             Mean action noise std: 2.77
          Mean value_function loss: 115.8118
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 53.1106
                       Mean reward: 865.98
               Mean episode length: 229.71
    Episode_Reward/reaching_object: 1.4048
     Episode_Reward/lifting_object: 175.2038
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 2.03s
                      Time elapsed: 00:52:59
                               ETA: 00:21:56

################################################################################
                     [1m Learning iteration 1415/2000 [0m                     

                       Computation: 48683 steps/s (collection: 1.924s, learning 0.096s)
             Mean action noise std: 2.77
          Mean value_function loss: 83.6077
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.1169
                       Mean reward: 908.99
               Mean episode length: 240.04
    Episode_Reward/reaching_object: 1.4374
     Episode_Reward/lifting_object: 179.7223
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 2.02s
                      Time elapsed: 00:53:01
                               ETA: 00:21:54

################################################################################
                     [1m Learning iteration 1416/2000 [0m                     

                       Computation: 47571 steps/s (collection: 1.968s, learning 0.099s)
             Mean action noise std: 2.77
          Mean value_function loss: 78.1599
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.1258
                       Mean reward: 923.26
               Mean episode length: 244.55
    Episode_Reward/reaching_object: 1.4433
     Episode_Reward/lifting_object: 179.7992
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 2.07s
                      Time elapsed: 00:53:03
                               ETA: 00:21:52

################################################################################
                     [1m Learning iteration 1417/2000 [0m                     

                       Computation: 47434 steps/s (collection: 1.986s, learning 0.087s)
             Mean action noise std: 2.78
          Mean value_function loss: 93.0653
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.1352
                       Mean reward: 874.52
               Mean episode length: 232.98
    Episode_Reward/reaching_object: 1.4060
     Episode_Reward/lifting_object: 174.4521
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 2.07s
                      Time elapsed: 00:53:05
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 1418/2000 [0m                     

                       Computation: 48386 steps/s (collection: 1.942s, learning 0.090s)
             Mean action noise std: 2.78
          Mean value_function loss: 108.0068
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 53.1439
                       Mean reward: 857.48
               Mean episode length: 228.33
    Episode_Reward/reaching_object: 1.3893
     Episode_Reward/lifting_object: 172.4942
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 2.03s
                      Time elapsed: 00:53:07
                               ETA: 00:21:47

################################################################################
                     [1m Learning iteration 1419/2000 [0m                     

                       Computation: 48671 steps/s (collection: 1.927s, learning 0.093s)
             Mean action noise std: 2.78
          Mean value_function loss: 95.7623
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.1591
                       Mean reward: 895.24
               Mean episode length: 236.57
    Episode_Reward/reaching_object: 1.4169
     Episode_Reward/lifting_object: 176.2406
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 2.02s
                      Time elapsed: 00:53:09
                               ETA: 00:21:45

################################################################################
                     [1m Learning iteration 1420/2000 [0m                     

                       Computation: 47839 steps/s (collection: 1.955s, learning 0.100s)
             Mean action noise std: 2.78
          Mean value_function loss: 76.3489
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.1725
                       Mean reward: 895.17
               Mean episode length: 237.09
    Episode_Reward/reaching_object: 1.4257
     Episode_Reward/lifting_object: 177.4758
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 2.05s
                      Time elapsed: 00:53:11
                               ETA: 00:21:42

################################################################################
                     [1m Learning iteration 1421/2000 [0m                     

                       Computation: 48459 steps/s (collection: 1.923s, learning 0.106s)
             Mean action noise std: 2.78
          Mean value_function loss: 78.6836
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 53.1820
                       Mean reward: 907.79
               Mean episode length: 240.85
    Episode_Reward/reaching_object: 1.4659
     Episode_Reward/lifting_object: 182.5345
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 2.03s
                      Time elapsed: 00:53:13
                               ETA: 00:21:40

################################################################################
                     [1m Learning iteration 1422/2000 [0m                     

                       Computation: 47635 steps/s (collection: 1.976s, learning 0.088s)
             Mean action noise std: 2.78
          Mean value_function loss: 108.7456
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 53.1972
                       Mean reward: 904.12
               Mean episode length: 239.06
    Episode_Reward/reaching_object: 1.4000
     Episode_Reward/lifting_object: 174.0403
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 2.06s
                      Time elapsed: 00:53:15
                               ETA: 00:21:38

################################################################################
                     [1m Learning iteration 1423/2000 [0m                     

                       Computation: 47813 steps/s (collection: 1.964s, learning 0.092s)
             Mean action noise std: 2.78
          Mean value_function loss: 93.1793
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 53.2086
                       Mean reward: 893.75
               Mean episode length: 236.65
    Episode_Reward/reaching_object: 1.4296
     Episode_Reward/lifting_object: 178.5582
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 2.06s
                      Time elapsed: 00:53:17
                               ETA: 00:21:35

################################################################################
                     [1m Learning iteration 1424/2000 [0m                     

                       Computation: 47626 steps/s (collection: 1.967s, learning 0.097s)
             Mean action noise std: 2.79
          Mean value_function loss: 108.5429
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.2164
                       Mean reward: 882.14
               Mean episode length: 233.55
    Episode_Reward/reaching_object: 1.4118
     Episode_Reward/lifting_object: 175.9253
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 2.06s
                      Time elapsed: 00:53:19
                               ETA: 00:21:33

################################################################################
                     [1m Learning iteration 1425/2000 [0m                     

                       Computation: 48453 steps/s (collection: 1.935s, learning 0.094s)
             Mean action noise std: 2.79
          Mean value_function loss: 90.2082
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 53.2252
                       Mean reward: 900.89
               Mean episode length: 237.67
    Episode_Reward/reaching_object: 1.4170
     Episode_Reward/lifting_object: 176.0016
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 2.03s
                      Time elapsed: 00:53:21
                               ETA: 00:21:31

################################################################################
                     [1m Learning iteration 1426/2000 [0m                     

                       Computation: 48679 steps/s (collection: 1.934s, learning 0.086s)
             Mean action noise std: 2.79
          Mean value_function loss: 95.0901
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 53.2342
                       Mean reward: 877.76
               Mean episode length: 233.58
    Episode_Reward/reaching_object: 1.4032
     Episode_Reward/lifting_object: 174.3919
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 2.02s
                      Time elapsed: 00:53:23
                               ETA: 00:21:28

################################################################################
                     [1m Learning iteration 1427/2000 [0m                     

                       Computation: 49193 steps/s (collection: 1.911s, learning 0.087s)
             Mean action noise std: 2.79
          Mean value_function loss: 81.9262
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.2457
                       Mean reward: 882.98
               Mean episode length: 234.16
    Episode_Reward/reaching_object: 1.4473
     Episode_Reward/lifting_object: 179.9564
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 2.00s
                      Time elapsed: 00:53:25
                               ETA: 00:21:26

################################################################################
                     [1m Learning iteration 1428/2000 [0m                     

                       Computation: 47535 steps/s (collection: 1.974s, learning 0.094s)
             Mean action noise std: 2.79
          Mean value_function loss: 72.4935
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.2549
                       Mean reward: 909.53
               Mean episode length: 239.69
    Episode_Reward/reaching_object: 1.4558
     Episode_Reward/lifting_object: 181.2516
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 2.07s
                      Time elapsed: 00:53:28
                               ETA: 00:21:24

################################################################################
                     [1m Learning iteration 1429/2000 [0m                     

                       Computation: 47565 steps/s (collection: 1.969s, learning 0.098s)
             Mean action noise std: 2.79
          Mean value_function loss: 79.2040
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.2646
                       Mean reward: 872.66
               Mean episode length: 230.53
    Episode_Reward/reaching_object: 1.4332
     Episode_Reward/lifting_object: 178.2853
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 2.07s
                      Time elapsed: 00:53:30
                               ETA: 00:21:21

################################################################################
                     [1m Learning iteration 1430/2000 [0m                     

                       Computation: 48164 steps/s (collection: 1.945s, learning 0.096s)
             Mean action noise std: 2.79
          Mean value_function loss: 83.3598
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.2723
                       Mean reward: 846.73
               Mean episode length: 225.10
    Episode_Reward/reaching_object: 1.4356
     Episode_Reward/lifting_object: 178.6999
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 2.04s
                      Time elapsed: 00:53:32
                               ETA: 00:21:19

################################################################################
                     [1m Learning iteration 1431/2000 [0m                     

                       Computation: 48351 steps/s (collection: 1.937s, learning 0.097s)
             Mean action noise std: 2.79
          Mean value_function loss: 60.3128
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 53.2777
                       Mean reward: 935.88
               Mean episode length: 246.60
    Episode_Reward/reaching_object: 1.4581
     Episode_Reward/lifting_object: 181.4436
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 2.03s
                      Time elapsed: 00:53:34
                               ETA: 00:21:17

################################################################################
                     [1m Learning iteration 1432/2000 [0m                     

                       Computation: 48104 steps/s (collection: 1.950s, learning 0.094s)
             Mean action noise std: 2.79
          Mean value_function loss: 66.4437
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.2852
                       Mean reward: 909.53
               Mean episode length: 240.28
    Episode_Reward/reaching_object: 1.4540
     Episode_Reward/lifting_object: 181.4237
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 2.04s
                      Time elapsed: 00:53:36
                               ETA: 00:21:14

################################################################################
                     [1m Learning iteration 1433/2000 [0m                     

                       Computation: 47625 steps/s (collection: 1.956s, learning 0.108s)
             Mean action noise std: 2.80
          Mean value_function loss: 74.4004
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.2917
                       Mean reward: 896.56
               Mean episode length: 237.52
    Episode_Reward/reaching_object: 1.4427
     Episode_Reward/lifting_object: 179.3980
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 2.06s
                      Time elapsed: 00:53:38
                               ETA: 00:21:12

################################################################################
                     [1m Learning iteration 1434/2000 [0m                     

                       Computation: 47045 steps/s (collection: 1.972s, learning 0.118s)
             Mean action noise std: 2.80
          Mean value_function loss: 89.3792
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.2991
                       Mean reward: 914.78
               Mean episode length: 242.25
    Episode_Reward/reaching_object: 1.4296
     Episode_Reward/lifting_object: 178.2085
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 2.09s
                      Time elapsed: 00:53:40
                               ETA: 00:21:10

################################################################################
                     [1m Learning iteration 1435/2000 [0m                     

                       Computation: 47741 steps/s (collection: 1.943s, learning 0.116s)
             Mean action noise std: 2.80
          Mean value_function loss: 79.1953
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.3091
                       Mean reward: 927.79
               Mean episode length: 245.01
    Episode_Reward/reaching_object: 1.4543
     Episode_Reward/lifting_object: 181.0658
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 2.06s
                      Time elapsed: 00:53:42
                               ETA: 00:21:07

################################################################################
                     [1m Learning iteration 1436/2000 [0m                     

                       Computation: 48095 steps/s (collection: 1.948s, learning 0.096s)
             Mean action noise std: 2.80
          Mean value_function loss: 74.4447
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.3212
                       Mean reward: 904.93
               Mean episode length: 238.50
    Episode_Reward/reaching_object: 1.4312
     Episode_Reward/lifting_object: 178.5450
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 2.04s
                      Time elapsed: 00:53:44
                               ETA: 00:21:05

################################################################################
                     [1m Learning iteration 1437/2000 [0m                     

                       Computation: 48185 steps/s (collection: 1.951s, learning 0.089s)
             Mean action noise std: 2.80
          Mean value_function loss: 82.8619
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.3315
                       Mean reward: 894.20
               Mean episode length: 237.83
    Episode_Reward/reaching_object: 1.4316
     Episode_Reward/lifting_object: 178.6356
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 2.04s
                      Time elapsed: 00:53:46
                               ETA: 00:21:03

################################################################################
                     [1m Learning iteration 1438/2000 [0m                     

                       Computation: 47921 steps/s (collection: 1.961s, learning 0.090s)
             Mean action noise std: 2.80
          Mean value_function loss: 88.5145
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 53.3427
                       Mean reward: 875.74
               Mean episode length: 232.61
    Episode_Reward/reaching_object: 1.4359
     Episode_Reward/lifting_object: 179.1117
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 2.05s
                      Time elapsed: 00:53:48
                               ETA: 00:21:00

################################################################################
                     [1m Learning iteration 1439/2000 [0m                     

                       Computation: 47096 steps/s (collection: 1.986s, learning 0.102s)
             Mean action noise std: 2.80
          Mean value_function loss: 94.0208
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.3495
                       Mean reward: 876.94
               Mean episode length: 233.83
    Episode_Reward/reaching_object: 1.4247
     Episode_Reward/lifting_object: 177.3648
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 2.09s
                      Time elapsed: 00:53:50
                               ETA: 00:20:58

################################################################################
                     [1m Learning iteration 1440/2000 [0m                     

                       Computation: 47511 steps/s (collection: 1.973s, learning 0.097s)
             Mean action noise std: 2.80
          Mean value_function loss: 93.6448
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.3575
                       Mean reward: 907.32
               Mean episode length: 239.90
    Episode_Reward/reaching_object: 1.4198
     Episode_Reward/lifting_object: 177.2846
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 2.07s
                      Time elapsed: 00:53:52
                               ETA: 00:20:56

################################################################################
                     [1m Learning iteration 1441/2000 [0m                     

                       Computation: 46479 steps/s (collection: 2.006s, learning 0.109s)
             Mean action noise std: 2.81
          Mean value_function loss: 89.4483
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.3681
                       Mean reward: 879.82
               Mean episode length: 234.20
    Episode_Reward/reaching_object: 1.4274
     Episode_Reward/lifting_object: 178.2692
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 2.12s
                      Time elapsed: 00:53:54
                               ETA: 00:20:54

################################################################################
                     [1m Learning iteration 1442/2000 [0m                     

                       Computation: 48416 steps/s (collection: 1.942s, learning 0.089s)
             Mean action noise std: 2.81
          Mean value_function loss: 90.8509
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.3750
                       Mean reward: 937.31
               Mean episode length: 246.23
    Episode_Reward/reaching_object: 1.4472
     Episode_Reward/lifting_object: 180.9270
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 2.03s
                      Time elapsed: 00:53:56
                               ETA: 00:20:51

################################################################################
                     [1m Learning iteration 1443/2000 [0m                     

                       Computation: 48293 steps/s (collection: 1.949s, learning 0.087s)
             Mean action noise std: 2.81
          Mean value_function loss: 67.2980
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.3828
                       Mean reward: 906.57
               Mean episode length: 239.27
    Episode_Reward/reaching_object: 1.4515
     Episode_Reward/lifting_object: 181.7419
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 2.04s
                      Time elapsed: 00:53:58
                               ETA: 00:20:49

################################################################################
                     [1m Learning iteration 1444/2000 [0m                     

                       Computation: 47800 steps/s (collection: 1.968s, learning 0.089s)
             Mean action noise std: 2.81
          Mean value_function loss: 84.5003
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.3903
                       Mean reward: 870.74
               Mean episode length: 232.12
    Episode_Reward/reaching_object: 1.4231
     Episode_Reward/lifting_object: 178.0780
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 2.06s
                      Time elapsed: 00:54:00
                               ETA: 00:20:47

################################################################################
                     [1m Learning iteration 1445/2000 [0m                     

                       Computation: 47523 steps/s (collection: 1.975s, learning 0.093s)
             Mean action noise std: 2.81
          Mean value_function loss: 54.7213
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.3932
                       Mean reward: 941.32
               Mean episode length: 246.97
    Episode_Reward/reaching_object: 1.4586
     Episode_Reward/lifting_object: 182.3017
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 2.07s
                      Time elapsed: 00:54:03
                               ETA: 00:20:44

################################################################################
                     [1m Learning iteration 1446/2000 [0m                     

                       Computation: 48404 steps/s (collection: 1.946s, learning 0.085s)
             Mean action noise std: 2.81
          Mean value_function loss: 85.6387
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 53.3969
                       Mean reward: 904.58
               Mean episode length: 239.26
    Episode_Reward/reaching_object: 1.4388
     Episode_Reward/lifting_object: 179.6525
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 2.03s
                      Time elapsed: 00:54:05
                               ETA: 00:20:42

################################################################################
                     [1m Learning iteration 1447/2000 [0m                     

                       Computation: 48274 steps/s (collection: 1.949s, learning 0.087s)
             Mean action noise std: 2.81
          Mean value_function loss: 77.1153
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 53.4003
                       Mean reward: 875.48
               Mean episode length: 232.81
    Episode_Reward/reaching_object: 1.4333
     Episode_Reward/lifting_object: 179.0492
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 2.04s
                      Time elapsed: 00:54:07
                               ETA: 00:20:40

################################################################################
                     [1m Learning iteration 1448/2000 [0m                     

                       Computation: 47870 steps/s (collection: 1.944s, learning 0.110s)
             Mean action noise std: 2.81
          Mean value_function loss: 86.5308
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.4044
                       Mean reward: 922.40
               Mean episode length: 243.70
    Episode_Reward/reaching_object: 1.4289
     Episode_Reward/lifting_object: 178.1888
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 2.05s
                      Time elapsed: 00:54:09
                               ETA: 00:20:37

################################################################################
                     [1m Learning iteration 1449/2000 [0m                     

                       Computation: 47697 steps/s (collection: 1.965s, learning 0.096s)
             Mean action noise std: 2.81
          Mean value_function loss: 97.5331
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.4144
                       Mean reward: 913.00
               Mean episode length: 241.25
    Episode_Reward/reaching_object: 1.4397
     Episode_Reward/lifting_object: 179.4122
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 2.06s
                      Time elapsed: 00:54:11
                               ETA: 00:20:35

################################################################################
                     [1m Learning iteration 1450/2000 [0m                     

                       Computation: 47588 steps/s (collection: 1.971s, learning 0.095s)
             Mean action noise std: 2.81
          Mean value_function loss: 85.3627
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.4270
                       Mean reward: 928.47
               Mean episode length: 245.70
    Episode_Reward/reaching_object: 1.4219
     Episode_Reward/lifting_object: 176.4667
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 2.07s
                      Time elapsed: 00:54:13
                               ETA: 00:20:33

################################################################################
                     [1m Learning iteration 1451/2000 [0m                     

                       Computation: 47018 steps/s (collection: 1.999s, learning 0.092s)
             Mean action noise std: 2.81
          Mean value_function loss: 88.8919
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.4346
                       Mean reward: 892.63
               Mean episode length: 236.37
    Episode_Reward/reaching_object: 1.4430
     Episode_Reward/lifting_object: 179.5743
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 2.09s
                      Time elapsed: 00:54:15
                               ETA: 00:20:30

################################################################################
                     [1m Learning iteration 1452/2000 [0m                     

                       Computation: 47784 steps/s (collection: 1.961s, learning 0.096s)
             Mean action noise std: 2.82
          Mean value_function loss: 88.1402
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.4451
                       Mean reward: 894.87
               Mean episode length: 235.76
    Episode_Reward/reaching_object: 1.4206
     Episode_Reward/lifting_object: 176.3142
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 2.06s
                      Time elapsed: 00:54:17
                               ETA: 00:20:28

################################################################################
                     [1m Learning iteration 1453/2000 [0m                     

                       Computation: 48276 steps/s (collection: 1.935s, learning 0.102s)
             Mean action noise std: 2.82
          Mean value_function loss: 80.8658
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 53.4581
                       Mean reward: 876.59
               Mean episode length: 233.29
    Episode_Reward/reaching_object: 1.4102
     Episode_Reward/lifting_object: 175.1204
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 2.04s
                      Time elapsed: 00:54:19
                               ETA: 00:20:26

################################################################################
                     [1m Learning iteration 1454/2000 [0m                     

                       Computation: 46803 steps/s (collection: 1.979s, learning 0.122s)
             Mean action noise std: 2.82
          Mean value_function loss: 91.4310
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.4663
                       Mean reward: 910.02
               Mean episode length: 240.48
    Episode_Reward/reaching_object: 1.4377
     Episode_Reward/lifting_object: 179.4530
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 2.10s
                      Time elapsed: 00:54:21
                               ETA: 00:20:23

################################################################################
                     [1m Learning iteration 1455/2000 [0m                     

                       Computation: 46213 steps/s (collection: 2.019s, learning 0.108s)
             Mean action noise std: 2.82
          Mean value_function loss: 85.8003
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 53.4741
                       Mean reward: 914.27
               Mean episode length: 241.11
    Episode_Reward/reaching_object: 1.4470
     Episode_Reward/lifting_object: 179.4113
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 2.13s
                      Time elapsed: 00:54:23
                               ETA: 00:20:21

################################################################################
                     [1m Learning iteration 1456/2000 [0m                     

                       Computation: 47375 steps/s (collection: 1.978s, learning 0.097s)
             Mean action noise std: 2.82
          Mean value_function loss: 79.8720
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 53.4873
                       Mean reward: 916.50
               Mean episode length: 242.47
    Episode_Reward/reaching_object: 1.4633
     Episode_Reward/lifting_object: 181.2937
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 2.08s
                      Time elapsed: 00:54:25
                               ETA: 00:20:19

################################################################################
                     [1m Learning iteration 1457/2000 [0m                     

                       Computation: 46554 steps/s (collection: 1.995s, learning 0.117s)
             Mean action noise std: 2.82
          Mean value_function loss: 90.7833
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.4992
                       Mean reward: 877.87
               Mean episode length: 233.98
    Episode_Reward/reaching_object: 1.4223
     Episode_Reward/lifting_object: 175.5196
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 2.11s
                      Time elapsed: 00:54:27
                               ETA: 00:20:17

################################################################################
                     [1m Learning iteration 1458/2000 [0m                     

                       Computation: 46955 steps/s (collection: 1.991s, learning 0.103s)
             Mean action noise std: 2.82
          Mean value_function loss: 94.0269
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 53.5124
                       Mean reward: 901.39
               Mean episode length: 238.76
    Episode_Reward/reaching_object: 1.4201
     Episode_Reward/lifting_object: 176.1670
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 2.09s
                      Time elapsed: 00:54:29
                               ETA: 00:20:14

################################################################################
                     [1m Learning iteration 1459/2000 [0m                     

                       Computation: 47783 steps/s (collection: 1.957s, learning 0.100s)
             Mean action noise std: 2.82
          Mean value_function loss: 88.0861
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 53.5190
                       Mean reward: 916.45
               Mean episode length: 242.50
    Episode_Reward/reaching_object: 1.4444
     Episode_Reward/lifting_object: 178.6047
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 2.06s
                      Time elapsed: 00:54:32
                               ETA: 00:20:12

################################################################################
                     [1m Learning iteration 1460/2000 [0m                     

                       Computation: 46750 steps/s (collection: 1.994s, learning 0.109s)
             Mean action noise std: 2.83
          Mean value_function loss: 88.6219
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.5215
                       Mean reward: 889.40
               Mean episode length: 235.96
    Episode_Reward/reaching_object: 1.4393
     Episode_Reward/lifting_object: 177.9128
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 2.10s
                      Time elapsed: 00:54:34
                               ETA: 00:20:10

################################################################################
                     [1m Learning iteration 1461/2000 [0m                     

                       Computation: 47752 steps/s (collection: 1.951s, learning 0.108s)
             Mean action noise std: 2.83
          Mean value_function loss: 94.0264
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.5253
                       Mean reward: 910.18
               Mean episode length: 240.72
    Episode_Reward/reaching_object: 1.4637
     Episode_Reward/lifting_object: 181.6200
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 2.06s
                      Time elapsed: 00:54:36
                               ETA: 00:20:07

################################################################################
                     [1m Learning iteration 1462/2000 [0m                     

                       Computation: 46506 steps/s (collection: 2.017s, learning 0.097s)
             Mean action noise std: 2.83
          Mean value_function loss: 77.3444
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.5288
                       Mean reward: 887.35
               Mean episode length: 234.94
    Episode_Reward/reaching_object: 1.4434
     Episode_Reward/lifting_object: 178.2436
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 2.11s
                      Time elapsed: 00:54:38
                               ETA: 00:20:05

################################################################################
                     [1m Learning iteration 1463/2000 [0m                     

                       Computation: 47834 steps/s (collection: 1.961s, learning 0.095s)
             Mean action noise std: 2.83
          Mean value_function loss: 80.5078
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.5346
                       Mean reward: 901.27
               Mean episode length: 239.14
    Episode_Reward/reaching_object: 1.4486
     Episode_Reward/lifting_object: 178.7840
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 2.06s
                      Time elapsed: 00:54:40
                               ETA: 00:20:03

################################################################################
                     [1m Learning iteration 1464/2000 [0m                     

                       Computation: 48109 steps/s (collection: 1.952s, learning 0.091s)
             Mean action noise std: 2.83
          Mean value_function loss: 75.3678
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.5400
                       Mean reward: 905.62
               Mean episode length: 239.15
    Episode_Reward/reaching_object: 1.4540
     Episode_Reward/lifting_object: 180.1898
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 2.04s
                      Time elapsed: 00:54:42
                               ETA: 00:20:00

################################################################################
                     [1m Learning iteration 1465/2000 [0m                     

                       Computation: 47924 steps/s (collection: 1.954s, learning 0.097s)
             Mean action noise std: 2.83
          Mean value_function loss: 75.1494
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 53.5490
                       Mean reward: 881.72
               Mean episode length: 234.16
    Episode_Reward/reaching_object: 1.4452
     Episode_Reward/lifting_object: 179.2132
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 2.05s
                      Time elapsed: 00:54:44
                               ETA: 00:19:58

################################################################################
                     [1m Learning iteration 1466/2000 [0m                     

                       Computation: 47996 steps/s (collection: 1.950s, learning 0.098s)
             Mean action noise std: 2.83
          Mean value_function loss: 110.9360
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.5641
                       Mean reward: 885.20
               Mean episode length: 236.07
    Episode_Reward/reaching_object: 1.4358
     Episode_Reward/lifting_object: 177.4861
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 2.05s
                      Time elapsed: 00:54:46
                               ETA: 00:19:56

################################################################################
                     [1m Learning iteration 1467/2000 [0m                     

                       Computation: 47304 steps/s (collection: 1.980s, learning 0.099s)
             Mean action noise std: 2.83
          Mean value_function loss: 97.5290
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 53.5759
                       Mean reward: 900.63
               Mean episode length: 238.85
    Episode_Reward/reaching_object: 1.4332
     Episode_Reward/lifting_object: 177.7975
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 2.08s
                      Time elapsed: 00:54:48
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 1468/2000 [0m                     

                       Computation: 48033 steps/s (collection: 1.945s, learning 0.101s)
             Mean action noise std: 2.83
          Mean value_function loss: 68.4060
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 53.5808
                       Mean reward: 912.06
               Mean episode length: 241.04
    Episode_Reward/reaching_object: 1.4407
     Episode_Reward/lifting_object: 178.7023
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 2.05s
                      Time elapsed: 00:54:50
                               ETA: 00:19:51

################################################################################
                     [1m Learning iteration 1469/2000 [0m                     

                       Computation: 48010 steps/s (collection: 1.944s, learning 0.104s)
             Mean action noise std: 2.83
          Mean value_function loss: 74.5820
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.5897
                       Mean reward: 862.85
               Mean episode length: 229.48
    Episode_Reward/reaching_object: 1.4388
     Episode_Reward/lifting_object: 177.8091
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 2.05s
                      Time elapsed: 00:54:52
                               ETA: 00:19:49

################################################################################
                     [1m Learning iteration 1470/2000 [0m                     

                       Computation: 48207 steps/s (collection: 1.949s, learning 0.090s)
             Mean action noise std: 2.83
          Mean value_function loss: 83.0540
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.5958
                       Mean reward: 886.95
               Mean episode length: 236.39
    Episode_Reward/reaching_object: 1.4462
     Episode_Reward/lifting_object: 179.0635
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 2.04s
                      Time elapsed: 00:54:54
                               ETA: 00:19:47

################################################################################
                     [1m Learning iteration 1471/2000 [0m                     

                       Computation: 47709 steps/s (collection: 1.965s, learning 0.095s)
             Mean action noise std: 2.84
          Mean value_function loss: 80.1090
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.6032
                       Mean reward: 899.18
               Mean episode length: 238.58
    Episode_Reward/reaching_object: 1.4620
     Episode_Reward/lifting_object: 181.5575
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 2.06s
                      Time elapsed: 00:54:56
                               ETA: 00:19:44

################################################################################
                     [1m Learning iteration 1472/2000 [0m                     

                       Computation: 47457 steps/s (collection: 1.974s, learning 0.098s)
             Mean action noise std: 2.84
          Mean value_function loss: 110.0605
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.6125
                       Mean reward: 855.19
               Mean episode length: 228.57
    Episode_Reward/reaching_object: 1.4251
     Episode_Reward/lifting_object: 176.1110
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 2.07s
                      Time elapsed: 00:54:58
                               ETA: 00:19:42

################################################################################
                     [1m Learning iteration 1473/2000 [0m                     

                       Computation: 47986 steps/s (collection: 1.958s, learning 0.091s)
             Mean action noise std: 2.84
          Mean value_function loss: 82.0095
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.6222
                       Mean reward: 895.77
               Mean episode length: 236.98
    Episode_Reward/reaching_object: 1.4324
     Episode_Reward/lifting_object: 177.4911
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 2.05s
                      Time elapsed: 00:55:00
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 1474/2000 [0m                     

                       Computation: 47470 steps/s (collection: 1.973s, learning 0.098s)
             Mean action noise std: 2.84
          Mean value_function loss: 64.7149
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 53.6296
                       Mean reward: 878.35
               Mean episode length: 233.21
    Episode_Reward/reaching_object: 1.4493
     Episode_Reward/lifting_object: 180.3302
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 2.07s
                      Time elapsed: 00:55:02
                               ETA: 00:19:37

################################################################################
                     [1m Learning iteration 1475/2000 [0m                     

                       Computation: 47503 steps/s (collection: 1.967s, learning 0.102s)
             Mean action noise std: 2.84
          Mean value_function loss: 67.9153
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.6369
                       Mean reward: 921.08
               Mean episode length: 243.50
    Episode_Reward/reaching_object: 1.4535
     Episode_Reward/lifting_object: 180.3783
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 2.07s
                      Time elapsed: 00:55:05
                               ETA: 00:19:35

################################################################################
                     [1m Learning iteration 1476/2000 [0m                     

                       Computation: 46729 steps/s (collection: 2.016s, learning 0.088s)
             Mean action noise std: 2.84
          Mean value_function loss: 70.2255
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.6443
                       Mean reward: 921.59
               Mean episode length: 243.24
    Episode_Reward/reaching_object: 1.4390
     Episode_Reward/lifting_object: 179.3033
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 2.10s
                      Time elapsed: 00:55:07
                               ETA: 00:19:33

################################################################################
                     [1m Learning iteration 1477/2000 [0m                     

                       Computation: 47388 steps/s (collection: 1.969s, learning 0.106s)
             Mean action noise std: 2.84
          Mean value_function loss: 72.7435
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.6500
                       Mean reward: 899.50
               Mean episode length: 238.49
    Episode_Reward/reaching_object: 1.4662
     Episode_Reward/lifting_object: 182.2873
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 2.07s
                      Time elapsed: 00:55:09
                               ETA: 00:19:30

################################################################################
                     [1m Learning iteration 1478/2000 [0m                     

                       Computation: 47820 steps/s (collection: 1.950s, learning 0.106s)
             Mean action noise std: 2.84
          Mean value_function loss: 57.7287
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.6589
                       Mean reward: 915.69
               Mean episode length: 242.17
    Episode_Reward/reaching_object: 1.4489
     Episode_Reward/lifting_object: 179.8890
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 2.06s
                      Time elapsed: 00:55:11
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 1479/2000 [0m                     

                       Computation: 47618 steps/s (collection: 1.962s, learning 0.102s)
             Mean action noise std: 2.85
          Mean value_function loss: 103.1246
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.6686
                       Mean reward: 904.98
               Mean episode length: 239.97
    Episode_Reward/reaching_object: 1.4347
     Episode_Reward/lifting_object: 178.2759
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 2.06s
                      Time elapsed: 00:55:13
                               ETA: 00:19:26

################################################################################
                     [1m Learning iteration 1480/2000 [0m                     

                       Computation: 47816 steps/s (collection: 1.968s, learning 0.088s)
             Mean action noise std: 2.85
          Mean value_function loss: 66.4877
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.6797
                       Mean reward: 899.29
               Mean episode length: 239.87
    Episode_Reward/reaching_object: 1.4475
     Episode_Reward/lifting_object: 180.1919
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 2.06s
                      Time elapsed: 00:55:15
                               ETA: 00:19:24

################################################################################
                     [1m Learning iteration 1481/2000 [0m                     

                       Computation: 47780 steps/s (collection: 1.969s, learning 0.089s)
             Mean action noise std: 2.85
          Mean value_function loss: 76.5017
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.6897
                       Mean reward: 906.46
               Mean episode length: 239.98
    Episode_Reward/reaching_object: 1.4590
     Episode_Reward/lifting_object: 181.0464
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 2.06s
                      Time elapsed: 00:55:17
                               ETA: 00:19:21

################################################################################
                     [1m Learning iteration 1482/2000 [0m                     

                       Computation: 47523 steps/s (collection: 1.977s, learning 0.092s)
             Mean action noise std: 2.85
          Mean value_function loss: 86.0048
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.6964
                       Mean reward: 928.01
               Mean episode length: 245.40
    Episode_Reward/reaching_object: 1.4570
     Episode_Reward/lifting_object: 180.6910
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 2.07s
                      Time elapsed: 00:55:19
                               ETA: 00:19:19

################################################################################
                     [1m Learning iteration 1483/2000 [0m                     

                       Computation: 48160 steps/s (collection: 1.952s, learning 0.089s)
             Mean action noise std: 2.85
          Mean value_function loss: 74.7047
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.7054
                       Mean reward: 912.09
               Mean episode length: 241.64
    Episode_Reward/reaching_object: 1.4566
     Episode_Reward/lifting_object: 180.6156
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 2.04s
                      Time elapsed: 00:55:21
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 1484/2000 [0m                     

                       Computation: 47223 steps/s (collection: 1.990s, learning 0.092s)
             Mean action noise std: 2.85
          Mean value_function loss: 72.6211
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.7135
                       Mean reward: 905.92
               Mean episode length: 239.74
    Episode_Reward/reaching_object: 1.4543
     Episode_Reward/lifting_object: 180.5725
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 2.08s
                      Time elapsed: 00:55:23
                               ETA: 00:19:14

################################################################################
                     [1m Learning iteration 1485/2000 [0m                     

                       Computation: 48092 steps/s (collection: 1.951s, learning 0.093s)
             Mean action noise std: 2.85
          Mean value_function loss: 73.2800
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 53.7220
                       Mean reward: 905.80
               Mean episode length: 240.83
    Episode_Reward/reaching_object: 1.4577
     Episode_Reward/lifting_object: 180.5464
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 2.04s
                      Time elapsed: 00:55:25
                               ETA: 00:19:12

################################################################################
                     [1m Learning iteration 1486/2000 [0m                     

                       Computation: 47847 steps/s (collection: 1.959s, learning 0.095s)
             Mean action noise std: 2.85
          Mean value_function loss: 67.3649
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.7313
                       Mean reward: 926.49
               Mean episode length: 244.31
    Episode_Reward/reaching_object: 1.4766
     Episode_Reward/lifting_object: 183.5081
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 2.05s
                      Time elapsed: 00:55:27
                               ETA: 00:19:10

################################################################################
                     [1m Learning iteration 1487/2000 [0m                     

                       Computation: 47812 steps/s (collection: 1.960s, learning 0.096s)
             Mean action noise std: 2.86
          Mean value_function loss: 88.7214
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 53.7457
                       Mean reward: 873.73
               Mean episode length: 232.55
    Episode_Reward/reaching_object: 1.4291
     Episode_Reward/lifting_object: 176.5326
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 2.06s
                      Time elapsed: 00:55:29
                               ETA: 00:19:07

################################################################################
                     [1m Learning iteration 1488/2000 [0m                     

                       Computation: 45949 steps/s (collection: 2.001s, learning 0.138s)
             Mean action noise std: 2.86
          Mean value_function loss: 87.3234
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.7570
                       Mean reward: 900.11
               Mean episode length: 238.44
    Episode_Reward/reaching_object: 1.4303
     Episode_Reward/lifting_object: 177.9040
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 2.14s
                      Time elapsed: 00:55:31
                               ETA: 00:19:05

################################################################################
                     [1m Learning iteration 1489/2000 [0m                     

                       Computation: 47124 steps/s (collection: 1.999s, learning 0.087s)
             Mean action noise std: 2.86
          Mean value_function loss: 87.1176
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.7640
                       Mean reward: 932.63
               Mean episode length: 245.95
    Episode_Reward/reaching_object: 1.4313
     Episode_Reward/lifting_object: 178.6267
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 2.09s
                      Time elapsed: 00:55:34
                               ETA: 00:19:03

################################################################################
                     [1m Learning iteration 1490/2000 [0m                     

                       Computation: 48025 steps/s (collection: 1.958s, learning 0.089s)
             Mean action noise std: 2.86
          Mean value_function loss: 68.4332
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 53.7694
                       Mean reward: 938.74
               Mean episode length: 247.45
    Episode_Reward/reaching_object: 1.4425
     Episode_Reward/lifting_object: 180.0680
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 2.05s
                      Time elapsed: 00:55:36
                               ETA: 00:19:01

################################################################################
                     [1m Learning iteration 1491/2000 [0m                     

                       Computation: 48424 steps/s (collection: 1.943s, learning 0.087s)
             Mean action noise std: 2.86
          Mean value_function loss: 63.7770
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.7736
                       Mean reward: 907.35
               Mean episode length: 240.95
    Episode_Reward/reaching_object: 1.4511
     Episode_Reward/lifting_object: 180.6595
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 2.03s
                      Time elapsed: 00:55:38
                               ETA: 00:18:58

################################################################################
                     [1m Learning iteration 1492/2000 [0m                     

                       Computation: 46990 steps/s (collection: 1.982s, learning 0.110s)
             Mean action noise std: 2.86
          Mean value_function loss: 75.5863
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.7805
                       Mean reward: 928.97
               Mean episode length: 244.61
    Episode_Reward/reaching_object: 1.4279
     Episode_Reward/lifting_object: 177.8832
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 2.09s
                      Time elapsed: 00:55:40
                               ETA: 00:18:56

################################################################################
                     [1m Learning iteration 1493/2000 [0m                     

                       Computation: 44267 steps/s (collection: 2.121s, learning 0.100s)
             Mean action noise std: 2.86
          Mean value_function loss: 70.3705
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.7877
                       Mean reward: 912.84
               Mean episode length: 239.90
    Episode_Reward/reaching_object: 1.4411
     Episode_Reward/lifting_object: 179.7990
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 2.22s
                      Time elapsed: 00:55:42
                               ETA: 00:18:54

################################################################################
                     [1m Learning iteration 1494/2000 [0m                     

                       Computation: 45404 steps/s (collection: 2.055s, learning 0.110s)
             Mean action noise std: 2.86
          Mean value_function loss: 66.4471
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 53.7923
                       Mean reward: 913.21
               Mean episode length: 241.22
    Episode_Reward/reaching_object: 1.4435
     Episode_Reward/lifting_object: 180.6230
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 2.17s
                      Time elapsed: 00:55:44
                               ETA: 00:18:52

################################################################################
                     [1m Learning iteration 1495/2000 [0m                     

                       Computation: 45549 steps/s (collection: 2.048s, learning 0.110s)
             Mean action noise std: 2.86
          Mean value_function loss: 78.8781
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 53.8005
                       Mean reward: 929.81
               Mean episode length: 246.01
    Episode_Reward/reaching_object: 1.4458
     Episode_Reward/lifting_object: 179.8461
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 2.16s
                      Time elapsed: 00:55:46
                               ETA: 00:18:49

################################################################################
                     [1m Learning iteration 1496/2000 [0m                     

                       Computation: 45616 steps/s (collection: 2.023s, learning 0.132s)
             Mean action noise std: 2.86
          Mean value_function loss: 61.4313
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 53.8075
                       Mean reward: 930.47
               Mean episode length: 245.29
    Episode_Reward/reaching_object: 1.4530
     Episode_Reward/lifting_object: 181.4153
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 2.16s
                      Time elapsed: 00:55:48
                               ETA: 00:18:47

################################################################################
                     [1m Learning iteration 1497/2000 [0m                     

                       Computation: 45730 steps/s (collection: 2.028s, learning 0.122s)
             Mean action noise std: 2.86
          Mean value_function loss: 72.0330
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.8147
                       Mean reward: 923.50
               Mean episode length: 243.66
    Episode_Reward/reaching_object: 1.4354
     Episode_Reward/lifting_object: 179.9225
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 2.15s
                      Time elapsed: 00:55:51
                               ETA: 00:18:45

################################################################################
                     [1m Learning iteration 1498/2000 [0m                     

                       Computation: 45491 steps/s (collection: 2.075s, learning 0.086s)
             Mean action noise std: 2.87
          Mean value_function loss: 81.3949
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.8216
                       Mean reward: 938.71
               Mean episode length: 246.68
    Episode_Reward/reaching_object: 1.4256
     Episode_Reward/lifting_object: 178.4156
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 2.16s
                      Time elapsed: 00:55:53
                               ETA: 00:18:42

################################################################################
                     [1m Learning iteration 1499/2000 [0m                     

                       Computation: 46689 steps/s (collection: 2.017s, learning 0.088s)
             Mean action noise std: 2.87
          Mean value_function loss: 73.5199
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.8295
                       Mean reward: 942.53
               Mean episode length: 247.84
    Episode_Reward/reaching_object: 1.4509
     Episode_Reward/lifting_object: 181.4226
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 2.11s
                      Time elapsed: 00:55:55
                               ETA: 00:18:40

################################################################################
                     [1m Learning iteration 1500/2000 [0m                     

                       Computation: 46632 steps/s (collection: 2.020s, learning 0.088s)
             Mean action noise std: 2.87
          Mean value_function loss: 77.0292
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.8367
                       Mean reward: 912.84
               Mean episode length: 241.22
    Episode_Reward/reaching_object: 1.4245
     Episode_Reward/lifting_object: 178.7457
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 2.11s
                      Time elapsed: 00:55:57
                               ETA: 00:18:38

################################################################################
                     [1m Learning iteration 1501/2000 [0m                     

                       Computation: 45063 steps/s (collection: 2.074s, learning 0.108s)
             Mean action noise std: 2.87
          Mean value_function loss: 67.9358
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 53.8459
                       Mean reward: 928.40
               Mean episode length: 244.40
    Episode_Reward/reaching_object: 1.4432
     Episode_Reward/lifting_object: 180.9640
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 2.18s
                      Time elapsed: 00:55:59
                               ETA: 00:18:36

################################################################################
                     [1m Learning iteration 1502/2000 [0m                     

                       Computation: 46941 steps/s (collection: 2.005s, learning 0.090s)
             Mean action noise std: 2.87
          Mean value_function loss: 98.2422
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.8554
                       Mean reward: 880.05
               Mean episode length: 233.65
    Episode_Reward/reaching_object: 1.4238
     Episode_Reward/lifting_object: 178.9428
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 2.09s
                      Time elapsed: 00:56:01
                               ETA: 00:18:33

################################################################################
                     [1m Learning iteration 1503/2000 [0m                     

                       Computation: 44224 steps/s (collection: 2.113s, learning 0.110s)
             Mean action noise std: 2.87
          Mean value_function loss: 96.4264
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.8643
                       Mean reward: 895.48
               Mean episode length: 237.74
    Episode_Reward/reaching_object: 1.4108
     Episode_Reward/lifting_object: 177.3077
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 2.22s
                      Time elapsed: 00:56:03
                               ETA: 00:18:31

################################################################################
                     [1m Learning iteration 1504/2000 [0m                     

                       Computation: 45480 steps/s (collection: 2.043s, learning 0.118s)
             Mean action noise std: 2.87
          Mean value_function loss: 77.5611
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 53.8730
                       Mean reward: 903.03
               Mean episode length: 239.04
    Episode_Reward/reaching_object: 1.4165
     Episode_Reward/lifting_object: 177.8065
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 2.16s
                      Time elapsed: 00:56:06
                               ETA: 00:18:29

################################################################################
                     [1m Learning iteration 1505/2000 [0m                     

                       Computation: 45977 steps/s (collection: 2.023s, learning 0.116s)
             Mean action noise std: 2.88
          Mean value_function loss: 60.4655
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.8890
                       Mean reward: 922.45
               Mean episode length: 244.23
    Episode_Reward/reaching_object: 1.4318
     Episode_Reward/lifting_object: 179.2014
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 2.14s
                      Time elapsed: 00:56:08
                               ETA: 00:18:27

################################################################################
                     [1m Learning iteration 1506/2000 [0m                     

                       Computation: 44744 steps/s (collection: 2.064s, learning 0.133s)
             Mean action noise std: 2.88
          Mean value_function loss: 64.4465
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.9011
                       Mean reward: 883.73
               Mean episode length: 233.89
    Episode_Reward/reaching_object: 1.4344
     Episode_Reward/lifting_object: 179.5398
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 2.20s
                      Time elapsed: 00:56:10
                               ETA: 00:18:24

################################################################################
                     [1m Learning iteration 1507/2000 [0m                     

                       Computation: 43529 steps/s (collection: 2.114s, learning 0.144s)
             Mean action noise std: 2.88
          Mean value_function loss: 81.8349
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 53.9084
                       Mean reward: 911.47
               Mean episode length: 241.94
    Episode_Reward/reaching_object: 1.4461
     Episode_Reward/lifting_object: 181.6043
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 2.26s
                      Time elapsed: 00:56:12
                               ETA: 00:18:22

################################################################################
                     [1m Learning iteration 1508/2000 [0m                     

                       Computation: 44285 steps/s (collection: 2.101s, learning 0.119s)
             Mean action noise std: 2.88
          Mean value_function loss: 72.6635
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.9122
                       Mean reward: 912.35
               Mean episode length: 241.09
    Episode_Reward/reaching_object: 1.4163
     Episode_Reward/lifting_object: 178.1357
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 2.22s
                      Time elapsed: 00:56:14
                               ETA: 00:18:20

################################################################################
                     [1m Learning iteration 1509/2000 [0m                     

                       Computation: 46087 steps/s (collection: 2.016s, learning 0.117s)
             Mean action noise std: 2.88
          Mean value_function loss: 78.7275
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.9189
                       Mean reward: 897.87
               Mean episode length: 237.93
    Episode_Reward/reaching_object: 1.4192
     Episode_Reward/lifting_object: 178.2617
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 2.13s
                      Time elapsed: 00:56:17
                               ETA: 00:18:18

################################################################################
                     [1m Learning iteration 1510/2000 [0m                     

                       Computation: 45884 steps/s (collection: 2.028s, learning 0.114s)
             Mean action noise std: 2.88
          Mean value_function loss: 56.4255
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.9263
                       Mean reward: 921.44
               Mean episode length: 242.39
    Episode_Reward/reaching_object: 1.4513
     Episode_Reward/lifting_object: 182.9333
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 2.14s
                      Time elapsed: 00:56:19
                               ETA: 00:18:15

################################################################################
                     [1m Learning iteration 1511/2000 [0m                     

                       Computation: 46106 steps/s (collection: 2.047s, learning 0.086s)
             Mean action noise std: 2.88
          Mean value_function loss: 78.6461
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 53.9297
                       Mean reward: 907.04
               Mean episode length: 239.72
    Episode_Reward/reaching_object: 1.4117
     Episode_Reward/lifting_object: 177.2837
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 2.13s
                      Time elapsed: 00:56:21
                               ETA: 00:18:13

################################################################################
                     [1m Learning iteration 1512/2000 [0m                     

                       Computation: 46564 steps/s (collection: 1.999s, learning 0.113s)
             Mean action noise std: 2.88
          Mean value_function loss: 79.6020
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.9351
                       Mean reward: 899.65
               Mean episode length: 238.84
    Episode_Reward/reaching_object: 1.4425
     Episode_Reward/lifting_object: 180.5450
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 2.11s
                      Time elapsed: 00:56:23
                               ETA: 00:18:11

################################################################################
                     [1m Learning iteration 1513/2000 [0m                     

                       Computation: 47246 steps/s (collection: 1.987s, learning 0.094s)
             Mean action noise std: 2.88
          Mean value_function loss: 71.1555
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 53.9427
                       Mean reward: 904.34
               Mean episode length: 239.31
    Episode_Reward/reaching_object: 1.4389
     Episode_Reward/lifting_object: 179.7574
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 2.08s
                      Time elapsed: 00:56:25
                               ETA: 00:18:08

################################################################################
                     [1m Learning iteration 1514/2000 [0m                     

                       Computation: 46481 steps/s (collection: 2.013s, learning 0.102s)
             Mean action noise std: 2.88
          Mean value_function loss: 81.6210
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.9515
                       Mean reward: 915.81
               Mean episode length: 242.10
    Episode_Reward/reaching_object: 1.4239
     Episode_Reward/lifting_object: 178.4114
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 2.11s
                      Time elapsed: 00:56:27
                               ETA: 00:18:06

################################################################################
                     [1m Learning iteration 1515/2000 [0m                     

                       Computation: 46331 steps/s (collection: 2.027s, learning 0.095s)
             Mean action noise std: 2.88
          Mean value_function loss: 85.4002
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 53.9607
                       Mean reward: 906.41
               Mean episode length: 238.86
    Episode_Reward/reaching_object: 1.4378
     Episode_Reward/lifting_object: 180.1650
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 2.12s
                      Time elapsed: 00:56:29
                               ETA: 00:18:04

################################################################################
                     [1m Learning iteration 1516/2000 [0m                     

                       Computation: 46963 steps/s (collection: 1.982s, learning 0.112s)
             Mean action noise std: 2.89
          Mean value_function loss: 105.9346
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.9681
                       Mean reward: 862.12
               Mean episode length: 229.69
    Episode_Reward/reaching_object: 1.4008
     Episode_Reward/lifting_object: 175.2876
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 2.09s
                      Time elapsed: 00:56:31
                               ETA: 00:18:02

################################################################################
                     [1m Learning iteration 1517/2000 [0m                     

                       Computation: 45422 steps/s (collection: 2.063s, learning 0.101s)
             Mean action noise std: 2.89
          Mean value_function loss: 79.7564
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.9818
                       Mean reward: 897.38
               Mean episode length: 238.55
    Episode_Reward/reaching_object: 1.4207
     Episode_Reward/lifting_object: 178.3889
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 2.16s
                      Time elapsed: 00:56:33
                               ETA: 00:17:59

################################################################################
                     [1m Learning iteration 1518/2000 [0m                     

                       Computation: 46909 steps/s (collection: 1.957s, learning 0.139s)
             Mean action noise std: 2.89
          Mean value_function loss: 78.7237
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 53.9913
                       Mean reward: 890.43
               Mean episode length: 238.14
    Episode_Reward/reaching_object: 1.4369
     Episode_Reward/lifting_object: 179.7932
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 2.10s
                      Time elapsed: 00:56:36
                               ETA: 00:17:57

################################################################################
                     [1m Learning iteration 1519/2000 [0m                     

                       Computation: 44590 steps/s (collection: 2.081s, learning 0.124s)
             Mean action noise std: 2.89
          Mean value_function loss: 83.8540
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.9962
                       Mean reward: 921.31
               Mean episode length: 242.62
    Episode_Reward/reaching_object: 1.4471
     Episode_Reward/lifting_object: 180.9540
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 2.20s
                      Time elapsed: 00:56:38
                               ETA: 00:17:55

################################################################################
                     [1m Learning iteration 1520/2000 [0m                     

                       Computation: 45499 steps/s (collection: 2.036s, learning 0.125s)
             Mean action noise std: 2.89
          Mean value_function loss: 53.4352
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.0033
                       Mean reward: 937.78
               Mean episode length: 247.39
    Episode_Reward/reaching_object: 1.4372
     Episode_Reward/lifting_object: 179.1839
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 2.16s
                      Time elapsed: 00:56:40
                               ETA: 00:17:53

################################################################################
                     [1m Learning iteration 1521/2000 [0m                     

                       Computation: 47393 steps/s (collection: 1.985s, learning 0.090s)
             Mean action noise std: 2.89
          Mean value_function loss: 65.2375
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.0092
                       Mean reward: 883.03
               Mean episode length: 235.16
    Episode_Reward/reaching_object: 1.4382
     Episode_Reward/lifting_object: 180.1719
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 2.07s
                      Time elapsed: 00:56:42
                               ETA: 00:17:50

################################################################################
                     [1m Learning iteration 1522/2000 [0m                     

                       Computation: 46710 steps/s (collection: 2.011s, learning 0.093s)
             Mean action noise std: 2.89
          Mean value_function loss: 69.7190
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.0160
                       Mean reward: 905.48
               Mean episode length: 240.19
    Episode_Reward/reaching_object: 1.4492
     Episode_Reward/lifting_object: 181.7392
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 2.10s
                      Time elapsed: 00:56:44
                               ETA: 00:17:48

################################################################################
                     [1m Learning iteration 1523/2000 [0m                     

                       Computation: 47531 steps/s (collection: 1.978s, learning 0.090s)
             Mean action noise std: 2.89
          Mean value_function loss: 82.3536
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.0259
                       Mean reward: 915.41
               Mean episode length: 241.72
    Episode_Reward/reaching_object: 1.4277
     Episode_Reward/lifting_object: 178.5992
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 2.07s
                      Time elapsed: 00:56:46
                               ETA: 00:17:46

################################################################################
                     [1m Learning iteration 1524/2000 [0m                     

                       Computation: 46841 steps/s (collection: 1.987s, learning 0.112s)
             Mean action noise std: 2.89
          Mean value_function loss: 93.4543
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.0370
                       Mean reward: 881.98
               Mean episode length: 234.07
    Episode_Reward/reaching_object: 1.4167
     Episode_Reward/lifting_object: 177.8713
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 2.10s
                      Time elapsed: 00:56:48
                               ETA: 00:17:43

################################################################################
                     [1m Learning iteration 1525/2000 [0m                     

                       Computation: 48098 steps/s (collection: 1.948s, learning 0.096s)
             Mean action noise std: 2.90
          Mean value_function loss: 84.1592
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.0452
                       Mean reward: 914.42
               Mean episode length: 242.47
    Episode_Reward/reaching_object: 1.4199
     Episode_Reward/lifting_object: 178.1691
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 2.04s
                      Time elapsed: 00:56:50
                               ETA: 00:17:41

################################################################################
                     [1m Learning iteration 1526/2000 [0m                     

                       Computation: 44122 steps/s (collection: 2.088s, learning 0.140s)
             Mean action noise std: 2.90
          Mean value_function loss: 70.0998
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.0559
                       Mean reward: 909.04
               Mean episode length: 240.40
    Episode_Reward/reaching_object: 1.4446
     Episode_Reward/lifting_object: 181.7610
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 2.23s
                      Time elapsed: 00:56:53
                               ETA: 00:17:39

################################################################################
                     [1m Learning iteration 1527/2000 [0m                     

                       Computation: 46074 steps/s (collection: 1.991s, learning 0.143s)
             Mean action noise std: 2.90
          Mean value_function loss: 69.8582
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.0667
                       Mean reward: 922.98
               Mean episode length: 243.88
    Episode_Reward/reaching_object: 1.4336
     Episode_Reward/lifting_object: 180.4467
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 2.13s
                      Time elapsed: 00:56:55
                               ETA: 00:17:37

################################################################################
                     [1m Learning iteration 1528/2000 [0m                     

                       Computation: 46831 steps/s (collection: 1.995s, learning 0.105s)
             Mean action noise std: 2.90
          Mean value_function loss: 64.5234
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 54.0730
                       Mean reward: 900.89
               Mean episode length: 238.55
    Episode_Reward/reaching_object: 1.4649
     Episode_Reward/lifting_object: 183.6631
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 2.10s
                      Time elapsed: 00:56:57
                               ETA: 00:17:34

################################################################################
                     [1m Learning iteration 1529/2000 [0m                     

                       Computation: 45933 steps/s (collection: 2.031s, learning 0.110s)
             Mean action noise std: 2.90
          Mean value_function loss: 60.0849
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.0802
                       Mean reward: 926.70
               Mean episode length: 244.41
    Episode_Reward/reaching_object: 1.4555
     Episode_Reward/lifting_object: 181.8707
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 2.14s
                      Time elapsed: 00:56:59
                               ETA: 00:17:32

################################################################################
                     [1m Learning iteration 1530/2000 [0m                     

                       Computation: 47483 steps/s (collection: 1.957s, learning 0.114s)
             Mean action noise std: 2.90
          Mean value_function loss: 84.1599
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.0867
                       Mean reward: 894.85
               Mean episode length: 237.57
    Episode_Reward/reaching_object: 1.4109
     Episode_Reward/lifting_object: 176.6918
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 2.07s
                      Time elapsed: 00:57:01
                               ETA: 00:17:30

################################################################################
                     [1m Learning iteration 1531/2000 [0m                     

                       Computation: 47451 steps/s (collection: 1.982s, learning 0.090s)
             Mean action noise std: 2.90
          Mean value_function loss: 79.6349
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.0943
                       Mean reward: 894.45
               Mean episode length: 236.82
    Episode_Reward/reaching_object: 1.4348
     Episode_Reward/lifting_object: 179.9373
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 2.07s
                      Time elapsed: 00:57:03
                               ETA: 00:17:28

################################################################################
                     [1m Learning iteration 1532/2000 [0m                     

                       Computation: 47805 steps/s (collection: 1.958s, learning 0.099s)
             Mean action noise std: 2.90
          Mean value_function loss: 99.9079
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 54.1013
                       Mean reward: 888.81
               Mean episode length: 234.35
    Episode_Reward/reaching_object: 1.4083
     Episode_Reward/lifting_object: 175.6184
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 2.06s
                      Time elapsed: 00:57:05
                               ETA: 00:17:25

################################################################################
                     [1m Learning iteration 1533/2000 [0m                     

                       Computation: 47518 steps/s (collection: 1.977s, learning 0.092s)
             Mean action noise std: 2.91
          Mean value_function loss: 86.5140
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.1109
                       Mean reward: 888.53
               Mean episode length: 235.66
    Episode_Reward/reaching_object: 1.4202
     Episode_Reward/lifting_object: 177.8277
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 2.07s
                      Time elapsed: 00:57:07
                               ETA: 00:17:23

################################################################################
                     [1m Learning iteration 1534/2000 [0m                     

                       Computation: 46315 steps/s (collection: 2.000s, learning 0.123s)
             Mean action noise std: 2.91
          Mean value_function loss: 69.9626
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.1211
                       Mean reward: 932.12
               Mean episode length: 245.36
    Episode_Reward/reaching_object: 1.4589
     Episode_Reward/lifting_object: 183.0476
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 2.12s
                      Time elapsed: 00:57:09
                               ETA: 00:17:21

################################################################################
                     [1m Learning iteration 1535/2000 [0m                     

                       Computation: 46419 steps/s (collection: 2.030s, learning 0.088s)
             Mean action noise std: 2.91
          Mean value_function loss: 73.3913
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.1317
                       Mean reward: 892.51
               Mean episode length: 236.74
    Episode_Reward/reaching_object: 1.4331
     Episode_Reward/lifting_object: 178.8495
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 2.12s
                      Time elapsed: 00:57:11
                               ETA: 00:17:18

################################################################################
                     [1m Learning iteration 1536/2000 [0m                     

                       Computation: 45518 steps/s (collection: 2.042s, learning 0.118s)
             Mean action noise std: 2.91
          Mean value_function loss: 70.6693
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.1386
                       Mean reward: 899.74
               Mean episode length: 238.10
    Episode_Reward/reaching_object: 1.4474
     Episode_Reward/lifting_object: 180.3864
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 2.16s
                      Time elapsed: 00:57:14
                               ETA: 00:17:16

################################################################################
                     [1m Learning iteration 1537/2000 [0m                     

                       Computation: 46179 steps/s (collection: 1.986s, learning 0.143s)
             Mean action noise std: 2.91
          Mean value_function loss: 81.7955
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.1477
                       Mean reward: 922.82
               Mean episode length: 244.71
    Episode_Reward/reaching_object: 1.4396
     Episode_Reward/lifting_object: 179.3218
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 2.13s
                      Time elapsed: 00:57:16
                               ETA: 00:17:14

################################################################################
                     [1m Learning iteration 1538/2000 [0m                     

                       Computation: 47387 steps/s (collection: 1.979s, learning 0.096s)
             Mean action noise std: 2.91
          Mean value_function loss: 82.9977
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.1570
                       Mean reward: 882.89
               Mean episode length: 234.49
    Episode_Reward/reaching_object: 1.4074
     Episode_Reward/lifting_object: 174.7072
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 2.07s
                      Time elapsed: 00:57:18
                               ETA: 00:17:12

################################################################################
                     [1m Learning iteration 1539/2000 [0m                     

                       Computation: 45836 steps/s (collection: 2.035s, learning 0.110s)
             Mean action noise std: 2.91
          Mean value_function loss: 74.1687
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 54.1621
                       Mean reward: 928.69
               Mean episode length: 245.49
    Episode_Reward/reaching_object: 1.4579
     Episode_Reward/lifting_object: 182.0172
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 2.14s
                      Time elapsed: 00:57:20
                               ETA: 00:17:09

################################################################################
                     [1m Learning iteration 1540/2000 [0m                     

                       Computation: 47658 steps/s (collection: 1.977s, learning 0.086s)
             Mean action noise std: 2.91
          Mean value_function loss: 71.4010
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.1711
                       Mean reward: 917.95
               Mean episode length: 244.01
    Episode_Reward/reaching_object: 1.4474
     Episode_Reward/lifting_object: 179.5366
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 2.06s
                      Time elapsed: 00:57:22
                               ETA: 00:17:07

################################################################################
                     [1m Learning iteration 1541/2000 [0m                     

                       Computation: 47579 steps/s (collection: 1.981s, learning 0.085s)
             Mean action noise std: 2.92
          Mean value_function loss: 78.9439
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 54.1824
                       Mean reward: 868.27
               Mean episode length: 232.72
    Episode_Reward/reaching_object: 1.4259
     Episode_Reward/lifting_object: 176.9846
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 2.07s
                      Time elapsed: 00:57:24
                               ETA: 00:17:05

################################################################################
                     [1m Learning iteration 1542/2000 [0m                     

                       Computation: 47581 steps/s (collection: 1.976s, learning 0.090s)
             Mean action noise std: 2.92
          Mean value_function loss: 73.0915
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.1948
                       Mean reward: 918.04
               Mean episode length: 241.85
    Episode_Reward/reaching_object: 1.4350
     Episode_Reward/lifting_object: 179.4840
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 2.07s
                      Time elapsed: 00:57:26
                               ETA: 00:17:03

################################################################################
                     [1m Learning iteration 1543/2000 [0m                     

                       Computation: 45996 steps/s (collection: 2.024s, learning 0.114s)
             Mean action noise std: 2.92
          Mean value_function loss: 74.1538
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.2069
                       Mean reward: 912.79
               Mean episode length: 241.34
    Episode_Reward/reaching_object: 1.4429
     Episode_Reward/lifting_object: 179.8672
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 2.14s
                      Time elapsed: 00:57:28
                               ETA: 00:17:00

################################################################################
                     [1m Learning iteration 1544/2000 [0m                     

                       Computation: 47272 steps/s (collection: 1.980s, learning 0.100s)
             Mean action noise std: 2.92
          Mean value_function loss: 96.0074
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.2209
                       Mean reward: 903.44
               Mean episode length: 239.74
    Episode_Reward/reaching_object: 1.4379
     Episode_Reward/lifting_object: 180.2413
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 2.08s
                      Time elapsed: 00:57:30
                               ETA: 00:16:58

################################################################################
                     [1m Learning iteration 1545/2000 [0m                     

                       Computation: 47137 steps/s (collection: 1.975s, learning 0.111s)
             Mean action noise std: 2.92
          Mean value_function loss: 119.5268
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.2300
                       Mean reward: 881.79
               Mean episode length: 234.29
    Episode_Reward/reaching_object: 1.4185
     Episode_Reward/lifting_object: 176.9890
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 2.09s
                      Time elapsed: 00:57:32
                               ETA: 00:16:56

################################################################################
                     [1m Learning iteration 1546/2000 [0m                     

                       Computation: 45976 steps/s (collection: 2.036s, learning 0.103s)
             Mean action noise std: 2.92
          Mean value_function loss: 95.6913
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.2363
                       Mean reward: 907.16
               Mean episode length: 239.22
    Episode_Reward/reaching_object: 1.3999
     Episode_Reward/lifting_object: 175.1187
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 2.14s
                      Time elapsed: 00:57:35
                               ETA: 00:16:53

################################################################################
                     [1m Learning iteration 1547/2000 [0m                     

                       Computation: 46890 steps/s (collection: 1.993s, learning 0.104s)
             Mean action noise std: 2.92
          Mean value_function loss: 77.7477
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.2435
                       Mean reward: 916.94
               Mean episode length: 242.42
    Episode_Reward/reaching_object: 1.4292
     Episode_Reward/lifting_object: 179.0518
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 2.10s
                      Time elapsed: 00:57:37
                               ETA: 00:16:51

################################################################################
                     [1m Learning iteration 1548/2000 [0m                     

                       Computation: 46775 steps/s (collection: 1.984s, learning 0.118s)
             Mean action noise std: 2.92
          Mean value_function loss: 60.6160
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.2506
                       Mean reward: 930.73
               Mean episode length: 246.15
    Episode_Reward/reaching_object: 1.4374
     Episode_Reward/lifting_object: 180.1556
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 2.10s
                      Time elapsed: 00:57:39
                               ETA: 00:16:49

################################################################################
                     [1m Learning iteration 1549/2000 [0m                     

                       Computation: 46619 steps/s (collection: 1.999s, learning 0.110s)
             Mean action noise std: 2.93
          Mean value_function loss: 67.5282
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.2581
                       Mean reward: 889.26
               Mean episode length: 235.78
    Episode_Reward/reaching_object: 1.4270
     Episode_Reward/lifting_object: 179.0069
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 2.11s
                      Time elapsed: 00:57:41
                               ETA: 00:16:47

################################################################################
                     [1m Learning iteration 1550/2000 [0m                     

                       Computation: 46846 steps/s (collection: 1.988s, learning 0.111s)
             Mean action noise std: 2.93
          Mean value_function loss: 83.0336
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 54.2664
                       Mean reward: 914.56
               Mean episode length: 241.96
    Episode_Reward/reaching_object: 1.4216
     Episode_Reward/lifting_object: 178.3285
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 2.10s
                      Time elapsed: 00:57:43
                               ETA: 00:16:44

################################################################################
                     [1m Learning iteration 1551/2000 [0m                     

                       Computation: 45465 steps/s (collection: 2.071s, learning 0.092s)
             Mean action noise std: 2.93
          Mean value_function loss: 65.6504
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.2707
                       Mean reward: 897.40
               Mean episode length: 239.77
    Episode_Reward/reaching_object: 1.4361
     Episode_Reward/lifting_object: 180.0900
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 2.16s
                      Time elapsed: 00:57:45
                               ETA: 00:16:42

################################################################################
                     [1m Learning iteration 1552/2000 [0m                     

                       Computation: 45883 steps/s (collection: 2.047s, learning 0.095s)
             Mean action noise std: 2.93
          Mean value_function loss: 115.5967
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.2746
                       Mean reward: 899.72
               Mean episode length: 237.53
    Episode_Reward/reaching_object: 1.4221
     Episode_Reward/lifting_object: 177.5323
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 2.14s
                      Time elapsed: 00:57:47
                               ETA: 00:16:40

################################################################################
                     [1m Learning iteration 1553/2000 [0m                     

                       Computation: 46619 steps/s (collection: 2.011s, learning 0.098s)
             Mean action noise std: 2.93
          Mean value_function loss: 99.8390
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.2819
                       Mean reward: 920.75
               Mean episode length: 243.42
    Episode_Reward/reaching_object: 1.4373
     Episode_Reward/lifting_object: 179.1741
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 2.11s
                      Time elapsed: 00:57:49
                               ETA: 00:16:38

################################################################################
                     [1m Learning iteration 1554/2000 [0m                     

                       Computation: 46268 steps/s (collection: 2.020s, learning 0.105s)
             Mean action noise std: 2.93
          Mean value_function loss: 78.8272
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.2937
                       Mean reward: 922.97
               Mean episode length: 243.90
    Episode_Reward/reaching_object: 1.4618
     Episode_Reward/lifting_object: 182.8294
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 2.12s
                      Time elapsed: 00:57:52
                               ETA: 00:16:35

################################################################################
                     [1m Learning iteration 1555/2000 [0m                     

                       Computation: 46287 steps/s (collection: 2.022s, learning 0.102s)
             Mean action noise std: 2.93
          Mean value_function loss: 61.6583
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 54.3065
                       Mean reward: 898.06
               Mean episode length: 237.99
    Episode_Reward/reaching_object: 1.4529
     Episode_Reward/lifting_object: 180.9868
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 2.12s
                      Time elapsed: 00:57:54
                               ETA: 00:16:33

################################################################################
                     [1m Learning iteration 1556/2000 [0m                     

                       Computation: 44931 steps/s (collection: 2.085s, learning 0.103s)
             Mean action noise std: 2.93
          Mean value_function loss: 68.0882
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 54.3159
                       Mean reward: 922.26
               Mean episode length: 245.29
    Episode_Reward/reaching_object: 1.4362
     Episode_Reward/lifting_object: 178.6963
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 2.19s
                      Time elapsed: 00:57:56
                               ETA: 00:16:31

################################################################################
                     [1m Learning iteration 1557/2000 [0m                     

                       Computation: 45527 steps/s (collection: 2.044s, learning 0.115s)
             Mean action noise std: 2.93
          Mean value_function loss: 80.8464
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.3217
                       Mean reward: 885.29
               Mean episode length: 234.36
    Episode_Reward/reaching_object: 1.4426
     Episode_Reward/lifting_object: 180.5324
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 2.16s
                      Time elapsed: 00:57:58
                               ETA: 00:16:29

################################################################################
                     [1m Learning iteration 1558/2000 [0m                     

                       Computation: 46408 steps/s (collection: 2.013s, learning 0.106s)
             Mean action noise std: 2.94
          Mean value_function loss: 95.1464
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 54.3322
                       Mean reward: 852.43
               Mean episode length: 227.29
    Episode_Reward/reaching_object: 1.4160
     Episode_Reward/lifting_object: 176.6765
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 2.12s
                      Time elapsed: 00:58:00
                               ETA: 00:16:26

################################################################################
                     [1m Learning iteration 1559/2000 [0m                     

                       Computation: 45688 steps/s (collection: 2.046s, learning 0.106s)
             Mean action noise std: 2.94
          Mean value_function loss: 97.1355
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.3465
                       Mean reward: 881.68
               Mean episode length: 234.50
    Episode_Reward/reaching_object: 1.4013
     Episode_Reward/lifting_object: 174.5987
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 2.15s
                      Time elapsed: 00:58:02
                               ETA: 00:16:24

################################################################################
                     [1m Learning iteration 1560/2000 [0m                     

                       Computation: 46043 steps/s (collection: 2.032s, learning 0.103s)
             Mean action noise std: 2.94
          Mean value_function loss: 64.5118
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.3544
                       Mean reward: 920.54
               Mean episode length: 242.17
    Episode_Reward/reaching_object: 1.4397
     Episode_Reward/lifting_object: 180.3765
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 2.14s
                      Time elapsed: 00:58:04
                               ETA: 00:16:22

################################################################################
                     [1m Learning iteration 1561/2000 [0m                     

                       Computation: 46954 steps/s (collection: 2.001s, learning 0.093s)
             Mean action noise std: 2.94
          Mean value_function loss: 84.8144
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 54.3651
                       Mean reward: 889.50
               Mean episode length: 235.36
    Episode_Reward/reaching_object: 1.4381
     Episode_Reward/lifting_object: 179.8891
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 2.09s
                      Time elapsed: 00:58:06
                               ETA: 00:16:20

################################################################################
                     [1m Learning iteration 1562/2000 [0m                     

                       Computation: 46544 steps/s (collection: 2.006s, learning 0.106s)
             Mean action noise std: 2.94
          Mean value_function loss: 90.4465
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.3748
                       Mean reward: 889.98
               Mean episode length: 235.19
    Episode_Reward/reaching_object: 1.4250
     Episode_Reward/lifting_object: 178.3713
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 2.11s
                      Time elapsed: 00:58:09
                               ETA: 00:16:17

################################################################################
                     [1m Learning iteration 1563/2000 [0m                     

                       Computation: 46359 steps/s (collection: 2.013s, learning 0.107s)
             Mean action noise std: 2.94
          Mean value_function loss: 197.3450
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.3819
                       Mean reward: 915.07
               Mean episode length: 241.98
    Episode_Reward/reaching_object: 1.4501
     Episode_Reward/lifting_object: 181.5317
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 2.12s
                      Time elapsed: 00:58:11
                               ETA: 00:16:15

################################################################################
                     [1m Learning iteration 1564/2000 [0m                     

                       Computation: 45863 steps/s (collection: 2.027s, learning 0.116s)
             Mean action noise std: 2.94
          Mean value_function loss: 76.2294
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.3911
                       Mean reward: 895.93
               Mean episode length: 236.10
    Episode_Reward/reaching_object: 1.4406
     Episode_Reward/lifting_object: 180.6631
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 2.14s
                      Time elapsed: 00:58:13
                               ETA: 00:16:13

################################################################################
                     [1m Learning iteration 1565/2000 [0m                     

                       Computation: 46273 steps/s (collection: 2.015s, learning 0.110s)
             Mean action noise std: 2.94
          Mean value_function loss: 99.5880
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 54.3977
                       Mean reward: 862.75
               Mean episode length: 229.00
    Episode_Reward/reaching_object: 1.4189
     Episode_Reward/lifting_object: 177.9191
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 2.12s
                      Time elapsed: 00:58:15
                               ETA: 00:16:10

################################################################################
                     [1m Learning iteration 1566/2000 [0m                     

                       Computation: 45359 steps/s (collection: 2.067s, learning 0.100s)
             Mean action noise std: 2.95
          Mean value_function loss: 60.1943
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.4048
                       Mean reward: 928.19
               Mean episode length: 245.31
    Episode_Reward/reaching_object: 1.4444
     Episode_Reward/lifting_object: 180.7883
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 2.17s
                      Time elapsed: 00:58:17
                               ETA: 00:16:08

################################################################################
                     [1m Learning iteration 1567/2000 [0m                     

                       Computation: 46033 steps/s (collection: 2.043s, learning 0.092s)
             Mean action noise std: 2.95
          Mean value_function loss: 78.7968
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 54.4172
                       Mean reward: 877.06
               Mean episode length: 234.20
    Episode_Reward/reaching_object: 1.4293
     Episode_Reward/lifting_object: 178.7276
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 2.14s
                      Time elapsed: 00:58:19
                               ETA: 00:16:06

################################################################################
                     [1m Learning iteration 1568/2000 [0m                     

                       Computation: 45752 steps/s (collection: 2.049s, learning 0.100s)
             Mean action noise std: 2.95
          Mean value_function loss: 83.5374
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 54.4274
                       Mean reward: 912.41
               Mean episode length: 241.83
    Episode_Reward/reaching_object: 1.4380
     Episode_Reward/lifting_object: 180.1343
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 2.15s
                      Time elapsed: 00:58:21
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 1569/2000 [0m                     

                       Computation: 44728 steps/s (collection: 2.084s, learning 0.114s)
             Mean action noise std: 2.95
          Mean value_function loss: 79.2799
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 54.4374
                       Mean reward: 896.43
               Mean episode length: 237.78
    Episode_Reward/reaching_object: 1.4216
     Episode_Reward/lifting_object: 178.5910
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 2.20s
                      Time elapsed: 00:58:24
                               ETA: 00:16:01

################################################################################
                     [1m Learning iteration 1570/2000 [0m                     

                       Computation: 45752 steps/s (collection: 2.030s, learning 0.119s)
             Mean action noise std: 2.95
          Mean value_function loss: 86.7647
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 54.4502
                       Mean reward: 861.37
               Mean episode length: 229.49
    Episode_Reward/reaching_object: 1.4036
     Episode_Reward/lifting_object: 176.1426
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 2.15s
                      Time elapsed: 00:58:26
                               ETA: 00:15:59

################################################################################
                     [1m Learning iteration 1571/2000 [0m                     

                       Computation: 45636 steps/s (collection: 2.049s, learning 0.105s)
             Mean action noise std: 2.95
          Mean value_function loss: 95.2335
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.4600
                       Mean reward: 904.42
               Mean episode length: 239.72
    Episode_Reward/reaching_object: 1.4227
     Episode_Reward/lifting_object: 178.5324
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 2.15s
                      Time elapsed: 00:58:28
                               ETA: 00:15:57

################################################################################
                     [1m Learning iteration 1572/2000 [0m                     

                       Computation: 45649 steps/s (collection: 2.049s, learning 0.104s)
             Mean action noise std: 2.95
          Mean value_function loss: 70.0086
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.4672
                       Mean reward: 909.26
               Mean episode length: 240.60
    Episode_Reward/reaching_object: 1.4593
     Episode_Reward/lifting_object: 183.1269
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 2.15s
                      Time elapsed: 00:58:30
                               ETA: 00:15:55

################################################################################
                     [1m Learning iteration 1573/2000 [0m                     

                       Computation: 45272 steps/s (collection: 2.071s, learning 0.100s)
             Mean action noise std: 2.96
          Mean value_function loss: 77.3880
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.4767
                       Mean reward: 911.99
               Mean episode length: 241.48
    Episode_Reward/reaching_object: 1.4331
     Episode_Reward/lifting_object: 179.5249
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 2.17s
                      Time elapsed: 00:58:32
                               ETA: 00:15:52

################################################################################
                     [1m Learning iteration 1574/2000 [0m                     

                       Computation: 45797 steps/s (collection: 2.044s, learning 0.103s)
             Mean action noise std: 2.96
          Mean value_function loss: 108.5625
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.4831
                       Mean reward: 898.23
               Mean episode length: 244.04
    Episode_Reward/reaching_object: 1.4120
     Episode_Reward/lifting_object: 176.5833
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 2.15s
                      Time elapsed: 00:58:34
                               ETA: 00:15:50

################################################################################
                     [1m Learning iteration 1575/2000 [0m                     

                       Computation: 43949 steps/s (collection: 2.136s, learning 0.101s)
             Mean action noise std: 2.96
          Mean value_function loss: 104.9714
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 54.4899
                       Mean reward: 893.45
               Mean episode length: 236.22
    Episode_Reward/reaching_object: 1.4227
     Episode_Reward/lifting_object: 177.9001
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 2.24s
                      Time elapsed: 00:58:37
                               ETA: 00:15:48

################################################################################
                     [1m Learning iteration 1576/2000 [0m                     

                       Computation: 42980 steps/s (collection: 2.163s, learning 0.124s)
             Mean action noise std: 2.96
          Mean value_function loss: 85.9371
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.4985
                       Mean reward: 868.38
               Mean episode length: 231.82
    Episode_Reward/reaching_object: 1.4264
     Episode_Reward/lifting_object: 178.4831
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 2.29s
                      Time elapsed: 00:58:39
                               ETA: 00:15:46

################################################################################
                     [1m Learning iteration 1577/2000 [0m                     

                       Computation: 44828 steps/s (collection: 2.053s, learning 0.140s)
             Mean action noise std: 2.96
          Mean value_function loss: 83.9327
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.5073
                       Mean reward: 870.86
               Mean episode length: 231.16
    Episode_Reward/reaching_object: 1.4167
     Episode_Reward/lifting_object: 177.6394
      Episode_Reward/object_height: 0.0268
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 2.19s
                      Time elapsed: 00:58:41
                               ETA: 00:15:44

################################################################################
                     [1m Learning iteration 1578/2000 [0m                     

                       Computation: 44611 steps/s (collection: 2.106s, learning 0.098s)
             Mean action noise std: 2.96
          Mean value_function loss: 63.6332
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 54.5153
                       Mean reward: 924.25
               Mean episode length: 243.26
    Episode_Reward/reaching_object: 1.4386
     Episode_Reward/lifting_object: 180.6620
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 2.20s
                      Time elapsed: 00:58:43
                               ETA: 00:15:41

################################################################################
                     [1m Learning iteration 1579/2000 [0m                     

                       Computation: 45809 steps/s (collection: 2.048s, learning 0.098s)
             Mean action noise std: 2.96
          Mean value_function loss: 85.2276
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.5280
                       Mean reward: 935.45
               Mean episode length: 245.96
    Episode_Reward/reaching_object: 1.4384
     Episode_Reward/lifting_object: 180.5968
      Episode_Reward/object_height: 0.0275
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 2.15s
                      Time elapsed: 00:58:45
                               ETA: 00:15:39

################################################################################
                     [1m Learning iteration 1580/2000 [0m                     

                       Computation: 44684 steps/s (collection: 2.103s, learning 0.097s)
             Mean action noise std: 2.96
          Mean value_function loss: 65.3770
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.5445
                       Mean reward: 898.92
               Mean episode length: 237.89
    Episode_Reward/reaching_object: 1.4359
     Episode_Reward/lifting_object: 179.6370
      Episode_Reward/object_height: 0.0274
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 2.20s
                      Time elapsed: 00:58:48
                               ETA: 00:15:37

################################################################################
                     [1m Learning iteration 1581/2000 [0m                     

                       Computation: 46039 steps/s (collection: 2.029s, learning 0.106s)
             Mean action noise std: 2.97
          Mean value_function loss: 81.3645
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.5604
                       Mean reward: 923.81
               Mean episode length: 243.54
    Episode_Reward/reaching_object: 1.4234
     Episode_Reward/lifting_object: 178.2662
      Episode_Reward/object_height: 0.0271
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 2.14s
                      Time elapsed: 00:58:50
                               ETA: 00:15:35

################################################################################
                     [1m Learning iteration 1582/2000 [0m                     

                       Computation: 44531 steps/s (collection: 2.092s, learning 0.116s)
             Mean action noise std: 2.97
          Mean value_function loss: 125.7002
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.5737
                       Mean reward: 889.48
               Mean episode length: 235.15
    Episode_Reward/reaching_object: 1.4345
     Episode_Reward/lifting_object: 180.2625
      Episode_Reward/object_height: 0.0273
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 2.21s
                      Time elapsed: 00:58:52
                               ETA: 00:15:32

################################################################################
                     [1m Learning iteration 1583/2000 [0m                     

                       Computation: 45893 steps/s (collection: 2.028s, learning 0.114s)
             Mean action noise std: 2.97
          Mean value_function loss: 90.1866
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 54.5868
                       Mean reward: 923.13
               Mean episode length: 243.32
    Episode_Reward/reaching_object: 1.4402
     Episode_Reward/lifting_object: 180.5627
      Episode_Reward/object_height: 0.0275
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 2.14s
                      Time elapsed: 00:58:54
                               ETA: 00:15:30

################################################################################
                     [1m Learning iteration 1584/2000 [0m                     

                       Computation: 46239 steps/s (collection: 2.029s, learning 0.097s)
             Mean action noise std: 2.97
          Mean value_function loss: 92.3004
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.5989
                       Mean reward: 913.13
               Mean episode length: 241.33
    Episode_Reward/reaching_object: 1.4204
     Episode_Reward/lifting_object: 178.3714
      Episode_Reward/object_height: 0.0273
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 2.13s
                      Time elapsed: 00:58:56
                               ETA: 00:15:28

################################################################################
                     [1m Learning iteration 1585/2000 [0m                     

                       Computation: 45576 steps/s (collection: 2.039s, learning 0.118s)
             Mean action noise std: 2.97
          Mean value_function loss: 107.2954
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 54.6084
                       Mean reward: 876.92
               Mean episode length: 232.73
    Episode_Reward/reaching_object: 1.4124
     Episode_Reward/lifting_object: 176.8410
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 2.16s
                      Time elapsed: 00:58:58
                               ETA: 00:15:26

################################################################################
                     [1m Learning iteration 1586/2000 [0m                     

                       Computation: 46053 steps/s (collection: 2.030s, learning 0.104s)
             Mean action noise std: 2.97
          Mean value_function loss: 115.2709
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.6140
                       Mean reward: 887.58
               Mean episode length: 236.10
    Episode_Reward/reaching_object: 1.4029
     Episode_Reward/lifting_object: 174.9305
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 18.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 2.13s
                      Time elapsed: 00:59:01
                               ETA: 00:15:23

################################################################################
                     [1m Learning iteration 1587/2000 [0m                     

                       Computation: 46348 steps/s (collection: 2.019s, learning 0.102s)
             Mean action noise std: 2.97
          Mean value_function loss: 88.4208
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 54.6194
                       Mean reward: 883.34
               Mean episode length: 234.11
    Episode_Reward/reaching_object: 1.4226
     Episode_Reward/lifting_object: 178.3913
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 2.12s
                      Time elapsed: 00:59:03
                               ETA: 00:15:21

################################################################################
                     [1m Learning iteration 1588/2000 [0m                     

                       Computation: 45946 steps/s (collection: 2.036s, learning 0.103s)
             Mean action noise std: 2.98
          Mean value_function loss: 97.1585
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.6306
                       Mean reward: 875.58
               Mean episode length: 233.32
    Episode_Reward/reaching_object: 1.4025
     Episode_Reward/lifting_object: 175.2061
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 2.14s
                      Time elapsed: 00:59:05
                               ETA: 00:15:19

################################################################################
                     [1m Learning iteration 1589/2000 [0m                     

                       Computation: 46182 steps/s (collection: 2.029s, learning 0.100s)
             Mean action noise std: 2.98
          Mean value_function loss: 85.3792
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 54.6394
                       Mean reward: 911.25
               Mean episode length: 240.89
    Episode_Reward/reaching_object: 1.4301
     Episode_Reward/lifting_object: 179.4700
      Episode_Reward/object_height: 0.0273
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 2.13s
                      Time elapsed: 00:59:07
                               ETA: 00:15:16

################################################################################
                     [1m Learning iteration 1590/2000 [0m                     

                       Computation: 45829 steps/s (collection: 2.028s, learning 0.117s)
             Mean action noise std: 2.98
          Mean value_function loss: 106.8696
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.6494
                       Mean reward: 862.33
               Mean episode length: 230.39
    Episode_Reward/reaching_object: 1.3976
     Episode_Reward/lifting_object: 174.3246
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 2.14s
                      Time elapsed: 00:59:09
                               ETA: 00:15:14

################################################################################
                     [1m Learning iteration 1591/2000 [0m                     

                       Computation: 45891 steps/s (collection: 2.039s, learning 0.104s)
             Mean action noise std: 2.98
          Mean value_function loss: 92.1364
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.6567
                       Mean reward: 858.22
               Mean episode length: 230.14
    Episode_Reward/reaching_object: 1.3921
     Episode_Reward/lifting_object: 173.0427
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 2.14s
                      Time elapsed: 00:59:11
                               ETA: 00:15:12

################################################################################
                     [1m Learning iteration 1592/2000 [0m                     

                       Computation: 43786 steps/s (collection: 2.129s, learning 0.116s)
             Mean action noise std: 2.98
          Mean value_function loss: 126.9185
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.6655
                       Mean reward: 868.88
               Mean episode length: 229.43
    Episode_Reward/reaching_object: 1.3783
     Episode_Reward/lifting_object: 173.0556
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 2.25s
                      Time elapsed: 00:59:13
                               ETA: 00:15:10

################################################################################
                     [1m Learning iteration 1593/2000 [0m                     

                       Computation: 44599 steps/s (collection: 2.109s, learning 0.096s)
             Mean action noise std: 2.98
          Mean value_function loss: 106.8779
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.6734
                       Mean reward: 891.60
               Mean episode length: 236.67
    Episode_Reward/reaching_object: 1.3729
     Episode_Reward/lifting_object: 172.7459
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 2.20s
                      Time elapsed: 00:59:16
                               ETA: 00:15:08

################################################################################
                     [1m Learning iteration 1594/2000 [0m                     

                       Computation: 43407 steps/s (collection: 2.120s, learning 0.145s)
             Mean action noise std: 2.98
          Mean value_function loss: 98.2520
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.6838
                       Mean reward: 877.24
               Mean episode length: 232.62
    Episode_Reward/reaching_object: 1.3764
     Episode_Reward/lifting_object: 172.9183
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 2.26s
                      Time elapsed: 00:59:18
                               ETA: 00:15:05

################################################################################
                     [1m Learning iteration 1595/2000 [0m                     

                       Computation: 44399 steps/s (collection: 2.097s, learning 0.117s)
             Mean action noise std: 2.98
          Mean value_function loss: 78.0253
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.6957
                       Mean reward: 924.27
               Mean episode length: 242.13
    Episode_Reward/reaching_object: 1.4389
     Episode_Reward/lifting_object: 181.3958
      Episode_Reward/object_height: 0.0271
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 2.21s
                      Time elapsed: 00:59:20
                               ETA: 00:15:03

################################################################################
                     [1m Learning iteration 1596/2000 [0m                     

                       Computation: 44570 steps/s (collection: 2.100s, learning 0.105s)
             Mean action noise std: 2.99
          Mean value_function loss: 103.8617
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.7022
                       Mean reward: 926.32
               Mean episode length: 244.08
    Episode_Reward/reaching_object: 1.4403
     Episode_Reward/lifting_object: 181.1084
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 18.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 2.21s
                      Time elapsed: 00:59:22
                               ETA: 00:15:01

################################################################################
                     [1m Learning iteration 1597/2000 [0m                     

                       Computation: 45870 steps/s (collection: 2.039s, learning 0.104s)
             Mean action noise std: 2.99
          Mean value_function loss: 87.9620
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 54.7053
                       Mean reward: 899.39
               Mean episode length: 238.58
    Episode_Reward/reaching_object: 1.4089
     Episode_Reward/lifting_object: 176.8970
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 2.14s
                      Time elapsed: 00:59:25
                               ETA: 00:14:59

################################################################################
                     [1m Learning iteration 1598/2000 [0m                     

                       Computation: 44595 steps/s (collection: 2.102s, learning 0.102s)
             Mean action noise std: 2.99
          Mean value_function loss: 74.4755
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 54.7085
                       Mean reward: 904.39
               Mean episode length: 238.72
    Episode_Reward/reaching_object: 1.4239
     Episode_Reward/lifting_object: 179.5037
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 2.20s
                      Time elapsed: 00:59:27
                               ETA: 00:14:56

################################################################################
                     [1m Learning iteration 1599/2000 [0m                     

                       Computation: 45724 steps/s (collection: 2.055s, learning 0.095s)
             Mean action noise std: 2.99
          Mean value_function loss: 94.9265
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.7153
                       Mean reward: 896.21
               Mean episode length: 238.37
    Episode_Reward/reaching_object: 1.4175
     Episode_Reward/lifting_object: 178.2956
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 2.15s
                      Time elapsed: 00:59:29
                               ETA: 00:14:54

################################################################################
                     [1m Learning iteration 1600/2000 [0m                     

                       Computation: 46029 steps/s (collection: 2.035s, learning 0.101s)
             Mean action noise std: 2.99
          Mean value_function loss: 79.6433
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.7228
                       Mean reward: 891.79
               Mean episode length: 236.76
    Episode_Reward/reaching_object: 1.4203
     Episode_Reward/lifting_object: 178.7288
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 2.14s
                      Time elapsed: 00:59:31
                               ETA: 00:14:52

################################################################################
                     [1m Learning iteration 1601/2000 [0m                     

                       Computation: 45268 steps/s (collection: 2.068s, learning 0.103s)
             Mean action noise std: 2.99
          Mean value_function loss: 102.5940
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.7343
                       Mean reward: 882.60
               Mean episode length: 236.85
    Episode_Reward/reaching_object: 1.3849
     Episode_Reward/lifting_object: 172.8864
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 2.17s
                      Time elapsed: 00:59:33
                               ETA: 00:14:50

################################################################################
                     [1m Learning iteration 1602/2000 [0m                     

                       Computation: 45833 steps/s (collection: 2.038s, learning 0.107s)
             Mean action noise std: 2.99
          Mean value_function loss: 85.2708
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 54.7437
                       Mean reward: 908.52
               Mean episode length: 240.00
    Episode_Reward/reaching_object: 1.4151
     Episode_Reward/lifting_object: 178.0647
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 2.14s
                      Time elapsed: 00:59:35
                               ETA: 00:14:47

################################################################################
                     [1m Learning iteration 1603/2000 [0m                     

                       Computation: 45082 steps/s (collection: 2.080s, learning 0.100s)
             Mean action noise std: 2.99
          Mean value_function loss: 74.9475
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.7499
                       Mean reward: 890.74
               Mean episode length: 236.07
    Episode_Reward/reaching_object: 1.4289
     Episode_Reward/lifting_object: 180.0060
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 2.18s
                      Time elapsed: 00:59:38
                               ETA: 00:14:45

################################################################################
                     [1m Learning iteration 1604/2000 [0m                     

                       Computation: 45344 steps/s (collection: 2.046s, learning 0.122s)
             Mean action noise std: 2.99
          Mean value_function loss: 89.0453
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.7591
                       Mean reward: 915.72
               Mean episode length: 241.03
    Episode_Reward/reaching_object: 1.4190
     Episode_Reward/lifting_object: 178.7252
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 2.17s
                      Time elapsed: 00:59:40
                               ETA: 00:14:43

################################################################################
                     [1m Learning iteration 1605/2000 [0m                     

                       Computation: 45352 steps/s (collection: 2.057s, learning 0.111s)
             Mean action noise std: 3.00
          Mean value_function loss: 78.6858
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.7700
                       Mean reward: 890.03
               Mean episode length: 235.14
    Episode_Reward/reaching_object: 1.4031
     Episode_Reward/lifting_object: 176.6882
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 2.17s
                      Time elapsed: 00:59:42
                               ETA: 00:14:41

################################################################################
                     [1m Learning iteration 1606/2000 [0m                     

                       Computation: 45499 steps/s (collection: 2.057s, learning 0.104s)
             Mean action noise std: 3.00
          Mean value_function loss: 90.1566
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.7831
                       Mean reward: 932.13
               Mean episode length: 245.65
    Episode_Reward/reaching_object: 1.4430
     Episode_Reward/lifting_object: 181.8819
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 2.16s
                      Time elapsed: 00:59:44
                               ETA: 00:14:38

################################################################################
                     [1m Learning iteration 1607/2000 [0m                     

                       Computation: 44823 steps/s (collection: 2.075s, learning 0.119s)
             Mean action noise std: 3.00
          Mean value_function loss: 83.0855
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.7951
                       Mean reward: 890.68
               Mean episode length: 234.89
    Episode_Reward/reaching_object: 1.4340
     Episode_Reward/lifting_object: 180.0415
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 2.19s
                      Time elapsed: 00:59:46
                               ETA: 00:14:36

################################################################################
                     [1m Learning iteration 1608/2000 [0m                     

                       Computation: 45666 steps/s (collection: 2.045s, learning 0.108s)
             Mean action noise std: 3.00
          Mean value_function loss: 103.0255
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.8063
                       Mean reward: 915.71
               Mean episode length: 241.85
    Episode_Reward/reaching_object: 1.3837
     Episode_Reward/lifting_object: 174.7837
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 2.15s
                      Time elapsed: 00:59:48
                               ETA: 00:14:34

################################################################################
                     [1m Learning iteration 1609/2000 [0m                     

                       Computation: 45987 steps/s (collection: 2.033s, learning 0.105s)
             Mean action noise std: 3.00
          Mean value_function loss: 97.5701
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.8186
                       Mean reward: 847.26
               Mean episode length: 225.29
    Episode_Reward/reaching_object: 1.4049
     Episode_Reward/lifting_object: 176.6198
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 2.14s
                      Time elapsed: 00:59:50
                               ETA: 00:14:32

################################################################################
                     [1m Learning iteration 1610/2000 [0m                     

                       Computation: 45007 steps/s (collection: 2.075s, learning 0.109s)
             Mean action noise std: 3.00
          Mean value_function loss: 107.8978
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.8287
                       Mean reward: 877.09
               Mean episode length: 232.35
    Episode_Reward/reaching_object: 1.3988
     Episode_Reward/lifting_object: 176.2378
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 2.18s
                      Time elapsed: 00:59:53
                               ETA: 00:14:29

################################################################################
                     [1m Learning iteration 1611/2000 [0m                     

                       Computation: 45501 steps/s (collection: 2.049s, learning 0.112s)
             Mean action noise std: 3.00
          Mean value_function loss: 92.8889
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.8379
                       Mean reward: 887.11
               Mean episode length: 234.47
    Episode_Reward/reaching_object: 1.4003
     Episode_Reward/lifting_object: 177.2197
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 2.16s
                      Time elapsed: 00:59:55
                               ETA: 00:14:27

################################################################################
                     [1m Learning iteration 1612/2000 [0m                     

                       Computation: 45327 steps/s (collection: 2.058s, learning 0.111s)
             Mean action noise std: 3.01
          Mean value_function loss: 104.1881
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.8444
                       Mean reward: 884.76
               Mean episode length: 233.60
    Episode_Reward/reaching_object: 1.4155
     Episode_Reward/lifting_object: 178.6440
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 2.17s
                      Time elapsed: 00:59:57
                               ETA: 00:14:25

################################################################################
                     [1m Learning iteration 1613/2000 [0m                     

                       Computation: 45928 steps/s (collection: 2.038s, learning 0.103s)
             Mean action noise std: 3.01
          Mean value_function loss: 122.5942
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.8505
                       Mean reward: 873.73
               Mean episode length: 232.16
    Episode_Reward/reaching_object: 1.3851
     Episode_Reward/lifting_object: 173.6921
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 2.14s
                      Time elapsed: 00:59:59
                               ETA: 00:14:23

################################################################################
                     [1m Learning iteration 1614/2000 [0m                     

                       Computation: 44541 steps/s (collection: 2.110s, learning 0.097s)
             Mean action noise std: 3.01
          Mean value_function loss: 110.2861
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 54.8593
                       Mean reward: 891.98
               Mean episode length: 235.68
    Episode_Reward/reaching_object: 1.3789
     Episode_Reward/lifting_object: 173.7731
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 2.21s
                      Time elapsed: 01:00:01
                               ETA: 00:14:20

################################################################################
                     [1m Learning iteration 1615/2000 [0m                     

                       Computation: 45534 steps/s (collection: 2.066s, learning 0.093s)
             Mean action noise std: 3.01
          Mean value_function loss: 99.3022
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.8767
                       Mean reward: 914.45
               Mean episode length: 241.71
    Episode_Reward/reaching_object: 1.4147
     Episode_Reward/lifting_object: 177.7745
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 2.16s
                      Time elapsed: 01:00:04
                               ETA: 00:14:18

################################################################################
                     [1m Learning iteration 1616/2000 [0m                     

                       Computation: 45511 steps/s (collection: 2.057s, learning 0.103s)
             Mean action noise std: 3.01
          Mean value_function loss: 110.7936
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 54.8922
                       Mean reward: 880.41
               Mean episode length: 232.14
    Episode_Reward/reaching_object: 1.4018
     Episode_Reward/lifting_object: 175.8647
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 2.16s
                      Time elapsed: 01:00:06
                               ETA: 00:14:16

################################################################################
                     [1m Learning iteration 1617/2000 [0m                     

                       Computation: 43783 steps/s (collection: 2.147s, learning 0.099s)
             Mean action noise std: 3.01
          Mean value_function loss: 112.5472
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.8972
                       Mean reward: 885.01
               Mean episode length: 235.47
    Episode_Reward/reaching_object: 1.4038
     Episode_Reward/lifting_object: 176.0420
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 2.25s
                      Time elapsed: 01:00:08
                               ETA: 00:14:14

################################################################################
                     [1m Learning iteration 1618/2000 [0m                     

                       Computation: 45729 steps/s (collection: 2.049s, learning 0.101s)
             Mean action noise std: 3.01
          Mean value_function loss: 132.7896
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 54.9045
                       Mean reward: 873.05
               Mean episode length: 231.31
    Episode_Reward/reaching_object: 1.3895
     Episode_Reward/lifting_object: 174.8326
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 2.15s
                      Time elapsed: 01:00:10
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 1619/2000 [0m                     

                       Computation: 46031 steps/s (collection: 2.037s, learning 0.099s)
             Mean action noise std: 3.02
          Mean value_function loss: 135.1390
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.9159
                       Mean reward: 868.06
               Mean episode length: 230.28
    Episode_Reward/reaching_object: 1.3923
     Episode_Reward/lifting_object: 175.2132
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 2.14s
                      Time elapsed: 01:00:12
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 1620/2000 [0m                     

                       Computation: 45070 steps/s (collection: 2.081s, learning 0.100s)
             Mean action noise std: 3.02
          Mean value_function loss: 122.1865
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.9248
                       Mean reward: 909.71
               Mean episode length: 240.74
    Episode_Reward/reaching_object: 1.3978
     Episode_Reward/lifting_object: 175.3306
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 2.18s
                      Time elapsed: 01:00:14
                               ETA: 00:14:07

################################################################################
                     [1m Learning iteration 1621/2000 [0m                     

                       Computation: 44474 steps/s (collection: 2.115s, learning 0.096s)
             Mean action noise std: 3.02
          Mean value_function loss: 105.0842
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 54.9304
                       Mean reward: 880.02
               Mean episode length: 233.33
    Episode_Reward/reaching_object: 1.3994
     Episode_Reward/lifting_object: 175.3855
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 2.21s
                      Time elapsed: 01:00:17
                               ETA: 00:14:05

################################################################################
                     [1m Learning iteration 1622/2000 [0m                     

                       Computation: 45280 steps/s (collection: 2.078s, learning 0.093s)
             Mean action noise std: 3.02
          Mean value_function loss: 68.4512
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.9416
                       Mean reward: 894.10
               Mean episode length: 237.27
    Episode_Reward/reaching_object: 1.4297
     Episode_Reward/lifting_object: 179.5291
      Episode_Reward/object_height: 0.0274
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 2.17s
                      Time elapsed: 01:00:19
                               ETA: 00:14:02

################################################################################
                     [1m Learning iteration 1623/2000 [0m                     

                       Computation: 44915 steps/s (collection: 2.071s, learning 0.118s)
             Mean action noise std: 3.02
          Mean value_function loss: 98.7392
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 54.9502
                       Mean reward: 903.99
               Mean episode length: 240.52
    Episode_Reward/reaching_object: 1.4274
     Episode_Reward/lifting_object: 179.5789
      Episode_Reward/object_height: 0.0271
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 2.19s
                      Time elapsed: 01:00:21
                               ETA: 00:14:00

################################################################################
                     [1m Learning iteration 1624/2000 [0m                     

                       Computation: 44686 steps/s (collection: 2.094s, learning 0.106s)
             Mean action noise std: 3.02
          Mean value_function loss: 108.1604
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.9618
                       Mean reward: 874.48
               Mean episode length: 231.87
    Episode_Reward/reaching_object: 1.4031
     Episode_Reward/lifting_object: 176.7920
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 2.20s
                      Time elapsed: 01:00:23
                               ETA: 00:13:58

################################################################################
                     [1m Learning iteration 1625/2000 [0m                     

                       Computation: 45359 steps/s (collection: 2.050s, learning 0.118s)
             Mean action noise std: 3.02
          Mean value_function loss: 114.1123
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 54.9718
                       Mean reward: 864.98
               Mean episode length: 230.62
    Episode_Reward/reaching_object: 1.3724
     Episode_Reward/lifting_object: 172.4541
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 2.17s
                      Time elapsed: 01:00:25
                               ETA: 00:13:56

################################################################################
                     [1m Learning iteration 1626/2000 [0m                     

                       Computation: 45529 steps/s (collection: 2.066s, learning 0.093s)
             Mean action noise std: 3.03
          Mean value_function loss: 85.2549
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.9821
                       Mean reward: 912.15
               Mean episode length: 241.07
    Episode_Reward/reaching_object: 1.4284
     Episode_Reward/lifting_object: 179.3766
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 2.16s
                      Time elapsed: 01:00:27
                               ETA: 00:13:53

################################################################################
                     [1m Learning iteration 1627/2000 [0m                     

                       Computation: 44297 steps/s (collection: 2.115s, learning 0.105s)
             Mean action noise std: 3.03
          Mean value_function loss: 73.4162
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.9873
                       Mean reward: 915.91
               Mean episode length: 241.85
    Episode_Reward/reaching_object: 1.4197
     Episode_Reward/lifting_object: 178.4115
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 2.22s
                      Time elapsed: 01:00:30
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 1628/2000 [0m                     

                       Computation: 44208 steps/s (collection: 2.120s, learning 0.104s)
             Mean action noise std: 3.03
          Mean value_function loss: 86.8017
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.9955
                       Mean reward: 912.27
               Mean episode length: 240.02
    Episode_Reward/reaching_object: 1.4167
     Episode_Reward/lifting_object: 178.5199
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 2.22s
                      Time elapsed: 01:00:32
                               ETA: 00:13:49

################################################################################
                     [1m Learning iteration 1629/2000 [0m                     

                       Computation: 45551 steps/s (collection: 2.061s, learning 0.097s)
             Mean action noise std: 3.03
          Mean value_function loss: 104.1760
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.0062
                       Mean reward: 891.18
               Mean episode length: 235.66
    Episode_Reward/reaching_object: 1.4042
     Episode_Reward/lifting_object: 176.4441
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 2.16s
                      Time elapsed: 01:00:34
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 1630/2000 [0m                     

                       Computation: 45508 steps/s (collection: 2.066s, learning 0.094s)
             Mean action noise std: 3.03
          Mean value_function loss: 111.6955
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 55.0122
                       Mean reward: 894.49
               Mean episode length: 236.93
    Episode_Reward/reaching_object: 1.3828
     Episode_Reward/lifting_object: 173.9922
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 2.16s
                      Time elapsed: 01:00:36
                               ETA: 00:13:45

################################################################################
                     [1m Learning iteration 1631/2000 [0m                     

                       Computation: 45857 steps/s (collection: 2.039s, learning 0.105s)
             Mean action noise std: 3.03
          Mean value_function loss: 89.8074
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.0171
                       Mean reward: 886.81
               Mean episode length: 235.02
    Episode_Reward/reaching_object: 1.4203
     Episode_Reward/lifting_object: 178.1171
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 2.14s
                      Time elapsed: 01:00:38
                               ETA: 00:13:42

################################################################################
                     [1m Learning iteration 1632/2000 [0m                     

                       Computation: 45481 steps/s (collection: 2.059s, learning 0.103s)
             Mean action noise std: 3.03
          Mean value_function loss: 88.6054
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.0244
                       Mean reward: 896.02
               Mean episode length: 237.33
    Episode_Reward/reaching_object: 1.3982
     Episode_Reward/lifting_object: 175.9656
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 2.16s
                      Time elapsed: 01:00:41
                               ETA: 00:13:40

################################################################################
                     [1m Learning iteration 1633/2000 [0m                     

                       Computation: 44887 steps/s (collection: 2.078s, learning 0.112s)
             Mean action noise std: 3.03
          Mean value_function loss: 100.0487
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 55.0325
                       Mean reward: 892.03
               Mean episode length: 236.23
    Episode_Reward/reaching_object: 1.4181
     Episode_Reward/lifting_object: 178.3335
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 2.19s
                      Time elapsed: 01:00:43
                               ETA: 00:13:38

################################################################################
                     [1m Learning iteration 1634/2000 [0m                     

                       Computation: 45784 steps/s (collection: 2.052s, learning 0.096s)
             Mean action noise std: 3.03
          Mean value_function loss: 102.4246
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 55.0354
                       Mean reward: 895.11
               Mean episode length: 236.76
    Episode_Reward/reaching_object: 1.4049
     Episode_Reward/lifting_object: 176.8598
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 2.15s
                      Time elapsed: 01:00:45
                               ETA: 00:13:36

################################################################################
                     [1m Learning iteration 1635/2000 [0m                     

                       Computation: 45750 steps/s (collection: 2.046s, learning 0.103s)
             Mean action noise std: 3.03
          Mean value_function loss: 92.9449
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 55.0370
                       Mean reward: 897.06
               Mean episode length: 237.87
    Episode_Reward/reaching_object: 1.4173
     Episode_Reward/lifting_object: 177.7162
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 2.15s
                      Time elapsed: 01:00:47
                               ETA: 00:13:33

################################################################################
                     [1m Learning iteration 1636/2000 [0m                     

                       Computation: 46034 steps/s (collection: 2.032s, learning 0.103s)
             Mean action noise std: 3.03
          Mean value_function loss: 79.1965
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 55.0379
                       Mean reward: 921.51
               Mean episode length: 242.98
    Episode_Reward/reaching_object: 1.4296
     Episode_Reward/lifting_object: 179.4447
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 2.14s
                      Time elapsed: 01:00:49
                               ETA: 00:13:31

################################################################################
                     [1m Learning iteration 1637/2000 [0m                     

                       Computation: 45754 steps/s (collection: 2.045s, learning 0.104s)
             Mean action noise std: 3.03
          Mean value_function loss: 84.2503
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.0408
                       Mean reward: 888.20
               Mean episode length: 236.04
    Episode_Reward/reaching_object: 1.4359
     Episode_Reward/lifting_object: 180.3408
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 2.15s
                      Time elapsed: 01:00:51
                               ETA: 00:13:29

################################################################################
                     [1m Learning iteration 1638/2000 [0m                     

                       Computation: 45893 steps/s (collection: 2.039s, learning 0.103s)
             Mean action noise std: 3.03
          Mean value_function loss: 79.1848
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 55.0486
                       Mean reward: 907.71
               Mean episode length: 240.47
    Episode_Reward/reaching_object: 1.4121
     Episode_Reward/lifting_object: 176.4107
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 2.14s
                      Time elapsed: 01:00:53
                               ETA: 00:13:27

################################################################################
                     [1m Learning iteration 1639/2000 [0m                     

                       Computation: 45796 steps/s (collection: 2.038s, learning 0.108s)
             Mean action noise std: 3.04
          Mean value_function loss: 74.2128
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.0584
                       Mean reward: 890.21
               Mean episode length: 235.14
    Episode_Reward/reaching_object: 1.4234
     Episode_Reward/lifting_object: 178.6736
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 2.15s
                      Time elapsed: 01:00:56
                               ETA: 00:13:24

################################################################################
                     [1m Learning iteration 1640/2000 [0m                     

                       Computation: 45965 steps/s (collection: 2.043s, learning 0.096s)
             Mean action noise std: 3.04
          Mean value_function loss: 107.3016
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.0679
                       Mean reward: 871.08
               Mean episode length: 230.54
    Episode_Reward/reaching_object: 1.4274
     Episode_Reward/lifting_object: 179.3241
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 2.14s
                      Time elapsed: 01:00:58
                               ETA: 00:13:22

################################################################################
                     [1m Learning iteration 1641/2000 [0m                     

                       Computation: 46566 steps/s (collection: 2.013s, learning 0.098s)
             Mean action noise std: 3.04
          Mean value_function loss: 86.5627
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 55.0746
                       Mean reward: 910.08
               Mean episode length: 240.88
    Episode_Reward/reaching_object: 1.4393
     Episode_Reward/lifting_object: 179.7063
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 2.11s
                      Time elapsed: 01:01:00
                               ETA: 00:13:20

################################################################################
                     [1m Learning iteration 1642/2000 [0m                     

                       Computation: 45753 steps/s (collection: 2.048s, learning 0.101s)
             Mean action noise std: 3.04
          Mean value_function loss: 133.9724
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.0829
                       Mean reward: 896.31
               Mean episode length: 236.35
    Episode_Reward/reaching_object: 1.4142
     Episode_Reward/lifting_object: 177.2270
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 2.15s
                      Time elapsed: 01:01:02
                               ETA: 00:13:18

################################################################################
                     [1m Learning iteration 1643/2000 [0m                     

                       Computation: 45074 steps/s (collection: 2.073s, learning 0.108s)
             Mean action noise std: 3.04
          Mean value_function loss: 110.2050
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.0896
                       Mean reward: 899.47
               Mean episode length: 238.95
    Episode_Reward/reaching_object: 1.4261
     Episode_Reward/lifting_object: 178.1537
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 2.18s
                      Time elapsed: 01:01:04
                               ETA: 00:13:15

################################################################################
                     [1m Learning iteration 1644/2000 [0m                     

                       Computation: 46223 steps/s (collection: 2.028s, learning 0.099s)
             Mean action noise std: 3.04
          Mean value_function loss: 87.0046
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.1033
                       Mean reward: 922.80
               Mean episode length: 242.74
    Episode_Reward/reaching_object: 1.4502
     Episode_Reward/lifting_object: 180.8594
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 2.13s
                      Time elapsed: 01:01:06
                               ETA: 00:13:13

################################################################################
                     [1m Learning iteration 1645/2000 [0m                     

                       Computation: 45454 steps/s (collection: 2.054s, learning 0.109s)
             Mean action noise std: 3.05
          Mean value_function loss: 95.9126
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 55.1222
                       Mean reward: 883.96
               Mean episode length: 234.60
    Episode_Reward/reaching_object: 1.4364
     Episode_Reward/lifting_object: 179.5657
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 2.16s
                      Time elapsed: 01:01:08
                               ETA: 00:13:11

################################################################################
                     [1m Learning iteration 1646/2000 [0m                     

                       Computation: 45149 steps/s (collection: 2.072s, learning 0.106s)
             Mean action noise std: 3.05
          Mean value_function loss: 104.2265
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.1323
                       Mean reward: 861.47
               Mean episode length: 228.63
    Episode_Reward/reaching_object: 1.4155
     Episode_Reward/lifting_object: 175.8818
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 2.18s
                      Time elapsed: 01:01:11
                               ETA: 00:13:09

################################################################################
                     [1m Learning iteration 1647/2000 [0m                     

                       Computation: 44939 steps/s (collection: 2.073s, learning 0.114s)
             Mean action noise std: 3.05
          Mean value_function loss: 112.1988
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 55.1390
                       Mean reward: 872.47
               Mean episode length: 231.97
    Episode_Reward/reaching_object: 1.4183
     Episode_Reward/lifting_object: 176.0648
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 2.19s
                      Time elapsed: 01:01:13
                               ETA: 00:13:06

################################################################################
                     [1m Learning iteration 1648/2000 [0m                     

                       Computation: 46084 steps/s (collection: 2.026s, learning 0.107s)
             Mean action noise std: 3.05
          Mean value_function loss: 79.9991
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.1463
                       Mean reward: 886.38
               Mean episode length: 236.65
    Episode_Reward/reaching_object: 1.4118
     Episode_Reward/lifting_object: 175.6897
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 2.13s
                      Time elapsed: 01:01:15
                               ETA: 00:13:04

################################################################################
                     [1m Learning iteration 1649/2000 [0m                     

                       Computation: 45196 steps/s (collection: 2.064s, learning 0.111s)
             Mean action noise std: 3.05
          Mean value_function loss: 106.0353
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.1533
                       Mean reward: 891.63
               Mean episode length: 235.63
    Episode_Reward/reaching_object: 1.4001
     Episode_Reward/lifting_object: 174.6525
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 2.18s
                      Time elapsed: 01:01:17
                               ETA: 00:13:02

################################################################################
                     [1m Learning iteration 1650/2000 [0m                     

                       Computation: 45628 steps/s (collection: 2.052s, learning 0.103s)
             Mean action noise std: 3.05
          Mean value_function loss: 81.1998
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 55.1616
                       Mean reward: 920.77
               Mean episode length: 243.11
    Episode_Reward/reaching_object: 1.4449
     Episode_Reward/lifting_object: 180.2694
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 2.15s
                      Time elapsed: 01:01:19
                               ETA: 00:13:00

################################################################################
                     [1m Learning iteration 1651/2000 [0m                     

                       Computation: 45322 steps/s (collection: 2.059s, learning 0.110s)
             Mean action noise std: 3.05
          Mean value_function loss: 84.1534
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.1666
                       Mean reward: 901.85
               Mean episode length: 239.70
    Episode_Reward/reaching_object: 1.4510
     Episode_Reward/lifting_object: 180.7980
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 2.17s
                      Time elapsed: 01:01:21
                               ETA: 00:12:57

################################################################################
                     [1m Learning iteration 1652/2000 [0m                     

                       Computation: 45785 steps/s (collection: 2.040s, learning 0.107s)
             Mean action noise std: 3.05
          Mean value_function loss: 468.6952
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 55.1710
                       Mean reward: 915.76
               Mean episode length: 240.48
    Episode_Reward/reaching_object: 1.4322
     Episode_Reward/lifting_object: 179.2133
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 2.15s
                      Time elapsed: 01:01:24
                               ETA: 00:12:55

################################################################################
                     [1m Learning iteration 1653/2000 [0m                     

                       Computation: 46000 steps/s (collection: 2.029s, learning 0.108s)
             Mean action noise std: 3.05
          Mean value_function loss: 1141.8627
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 55.1777
                       Mean reward: 888.67
               Mean episode length: 235.34
    Episode_Reward/reaching_object: 1.4146
     Episode_Reward/lifting_object: 176.4348
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 2.14s
                      Time elapsed: 01:01:26
                               ETA: 00:12:53

################################################################################
                     [1m Learning iteration 1654/2000 [0m                     

                       Computation: 45409 steps/s (collection: 2.056s, learning 0.108s)
             Mean action noise std: 3.06
          Mean value_function loss: 155.1528
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.1916
                       Mean reward: 878.74
               Mean episode length: 234.21
    Episode_Reward/reaching_object: 1.4122
     Episode_Reward/lifting_object: 175.8772
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 2.16s
                      Time elapsed: 01:01:28
                               ETA: 00:12:51

################################################################################
                     [1m Learning iteration 1655/2000 [0m                     

                       Computation: 45794 steps/s (collection: 2.042s, learning 0.105s)
             Mean action noise std: 3.06
          Mean value_function loss: 115.6630
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.2030
                       Mean reward: 854.62
               Mean episode length: 228.41
    Episode_Reward/reaching_object: 1.3990
     Episode_Reward/lifting_object: 174.1179
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 2.15s
                      Time elapsed: 01:01:30
                               ETA: 00:12:48

################################################################################
                     [1m Learning iteration 1656/2000 [0m                     

                       Computation: 43273 steps/s (collection: 2.042s, learning 0.230s)
             Mean action noise std: 3.06
          Mean value_function loss: 76.3300
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 55.2110
                       Mean reward: 923.03
               Mean episode length: 242.64
    Episode_Reward/reaching_object: 1.4218
     Episode_Reward/lifting_object: 177.9387
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 2.27s
                      Time elapsed: 01:01:32
                               ETA: 00:12:46

################################################################################
                     [1m Learning iteration 1657/2000 [0m                     

                       Computation: 39027 steps/s (collection: 2.406s, learning 0.113s)
             Mean action noise std: 3.06
          Mean value_function loss: 81.3936
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.2215
                       Mean reward: 915.47
               Mean episode length: 241.54
    Episode_Reward/reaching_object: 1.4448
     Episode_Reward/lifting_object: 180.5568
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 2.52s
                      Time elapsed: 01:01:35
                               ETA: 00:12:44

################################################################################
                     [1m Learning iteration 1658/2000 [0m                     

                       Computation: 44127 steps/s (collection: 2.135s, learning 0.093s)
             Mean action noise std: 3.06
          Mean value_function loss: 88.1043
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.2290
                       Mean reward: 884.84
               Mean episode length: 235.50
    Episode_Reward/reaching_object: 1.4460
     Episode_Reward/lifting_object: 180.3472
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 2.23s
                      Time elapsed: 01:01:37
                               ETA: 00:12:42

################################################################################
                     [1m Learning iteration 1659/2000 [0m                     

                       Computation: 39903 steps/s (collection: 2.359s, learning 0.105s)
             Mean action noise std: 3.06
          Mean value_function loss: 88.7234
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 55.2366
                       Mean reward: 923.81
               Mean episode length: 243.83
    Episode_Reward/reaching_object: 1.4326
     Episode_Reward/lifting_object: 178.8562
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 2.46s
                      Time elapsed: 01:01:40
                               ETA: 00:12:40

################################################################################
                     [1m Learning iteration 1660/2000 [0m                     

                       Computation: 40839 steps/s (collection: 2.295s, learning 0.113s)
             Mean action noise std: 3.06
          Mean value_function loss: 105.0599
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.2472
                       Mean reward: 867.20
               Mean episode length: 230.66
    Episode_Reward/reaching_object: 1.4134
     Episode_Reward/lifting_object: 176.4391
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 2.41s
                      Time elapsed: 01:01:42
                               ETA: 00:12:37

################################################################################
                     [1m Learning iteration 1661/2000 [0m                     

                       Computation: 43575 steps/s (collection: 2.150s, learning 0.106s)
             Mean action noise std: 3.07
          Mean value_function loss: 96.7980
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.2564
                       Mean reward: 855.94
               Mean episode length: 229.42
    Episode_Reward/reaching_object: 1.4038
     Episode_Reward/lifting_object: 175.4798
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 2.26s
                      Time elapsed: 01:01:44
                               ETA: 00:12:35

################################################################################
                     [1m Learning iteration 1662/2000 [0m                     

                       Computation: 45548 steps/s (collection: 2.058s, learning 0.100s)
             Mean action noise std: 3.07
          Mean value_function loss: 76.2907
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.2676
                       Mean reward: 910.48
               Mean episode length: 242.06
    Episode_Reward/reaching_object: 1.4447
     Episode_Reward/lifting_object: 180.3179
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 2.16s
                      Time elapsed: 01:01:46
                               ETA: 00:12:33

################################################################################
                     [1m Learning iteration 1663/2000 [0m                     

                       Computation: 42398 steps/s (collection: 2.148s, learning 0.170s)
             Mean action noise std: 3.07
          Mean value_function loss: 84.5247
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.2809
                       Mean reward: 904.80
               Mean episode length: 239.40
    Episode_Reward/reaching_object: 1.4338
     Episode_Reward/lifting_object: 178.2792
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 2.32s
                      Time elapsed: 01:01:49
                               ETA: 00:12:31

################################################################################
                     [1m Learning iteration 1664/2000 [0m                     

                       Computation: 43655 steps/s (collection: 2.144s, learning 0.108s)
             Mean action noise std: 3.07
          Mean value_function loss: 103.7691
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.2928
                       Mean reward: 882.68
               Mean episode length: 235.33
    Episode_Reward/reaching_object: 1.4407
     Episode_Reward/lifting_object: 179.8702
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 2.25s
                      Time elapsed: 01:01:51
                               ETA: 00:12:28

################################################################################
                     [1m Learning iteration 1665/2000 [0m                     

                       Computation: 39739 steps/s (collection: 2.273s, learning 0.201s)
             Mean action noise std: 3.07
          Mean value_function loss: 98.5888
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 55.3020
                       Mean reward: 872.85
               Mean episode length: 232.21
    Episode_Reward/reaching_object: 1.4288
     Episode_Reward/lifting_object: 178.5845
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 2.47s
                      Time elapsed: 01:01:53
                               ETA: 00:12:26

################################################################################
                     [1m Learning iteration 1666/2000 [0m                     

                       Computation: 25206 steps/s (collection: 3.794s, learning 0.106s)
             Mean action noise std: 3.07
          Mean value_function loss: 74.9183
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 55.3111
                       Mean reward: 911.77
               Mean episode length: 241.53
    Episode_Reward/reaching_object: 1.4196
     Episode_Reward/lifting_object: 177.6849
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 3.90s
                      Time elapsed: 01:01:57
                               ETA: 00:12:24

################################################################################
                     [1m Learning iteration 1667/2000 [0m                     

                       Computation: 13850 steps/s (collection: 6.988s, learning 0.109s)
             Mean action noise std: 3.07
          Mean value_function loss: 90.1179
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 55.3180
                       Mean reward: 906.78
               Mean episode length: 240.08
    Episode_Reward/reaching_object: 1.4376
     Episode_Reward/lifting_object: 179.3056
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 7.10s
                      Time elapsed: 01:02:04
                               ETA: 00:12:23

################################################################################
                     [1m Learning iteration 1668/2000 [0m                     

                       Computation: 14275 steps/s (collection: 6.728s, learning 0.158s)
             Mean action noise std: 3.07
          Mean value_function loss: 113.7839
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 55.3241
                       Mean reward: 862.93
               Mean episode length: 227.31
    Episode_Reward/reaching_object: 1.3627
     Episode_Reward/lifting_object: 170.6740
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 6.89s
                      Time elapsed: 01:02:11
                               ETA: 00:12:22

################################################################################
                     [1m Learning iteration 1669/2000 [0m                     

                       Computation: 14418 steps/s (collection: 6.706s, learning 0.112s)
             Mean action noise std: 3.07
          Mean value_function loss: 90.9664
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.3267
                       Mean reward: 921.56
               Mean episode length: 242.67
    Episode_Reward/reaching_object: 1.4074
     Episode_Reward/lifting_object: 176.7116
      Episode_Reward/object_height: 0.0279
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 6.82s
                      Time elapsed: 01:02:18
                               ETA: 00:12:21

################################################################################
                     [1m Learning iteration 1670/2000 [0m                     

                       Computation: 14063 steps/s (collection: 6.881s, learning 0.109s)
             Mean action noise std: 3.08
          Mean value_function loss: 89.3099
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 55.3337
                       Mean reward: 888.61
               Mean episode length: 235.39
    Episode_Reward/reaching_object: 1.4495
     Episode_Reward/lifting_object: 182.0813
      Episode_Reward/object_height: 0.0292
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 6.99s
                      Time elapsed: 01:02:25
                               ETA: 00:12:19

################################################################################
                     [1m Learning iteration 1671/2000 [0m                     

                       Computation: 13992 steps/s (collection: 6.915s, learning 0.111s)
             Mean action noise std: 3.08
          Mean value_function loss: 107.7547
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 55.3446
                       Mean reward: 875.34
               Mean episode length: 231.27
    Episode_Reward/reaching_object: 1.4170
     Episode_Reward/lifting_object: 178.0076
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 7.03s
                      Time elapsed: 01:02:32
                               ETA: 00:12:18

################################################################################
                     [1m Learning iteration 1672/2000 [0m                     

                       Computation: 14114 steps/s (collection: 6.851s, learning 0.114s)
             Mean action noise std: 3.08
          Mean value_function loss: 94.6840
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.3530
                       Mean reward: 915.40
               Mean episode length: 240.64
    Episode_Reward/reaching_object: 1.4268
     Episode_Reward/lifting_object: 179.5665
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 6.96s
                      Time elapsed: 01:02:39
                               ETA: 00:12:17

################################################################################
                     [1m Learning iteration 1673/2000 [0m                     

                       Computation: 14042 steps/s (collection: 6.879s, learning 0.122s)
             Mean action noise std: 3.08
          Mean value_function loss: 80.5866
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.3647
                       Mean reward: 884.47
               Mean episode length: 234.74
    Episode_Reward/reaching_object: 1.4035
     Episode_Reward/lifting_object: 176.0508
      Episode_Reward/object_height: 0.0292
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 7.00s
                      Time elapsed: 01:02:46
                               ETA: 00:12:15

################################################################################
                     [1m Learning iteration 1674/2000 [0m                     

                       Computation: 14098 steps/s (collection: 6.856s, learning 0.116s)
             Mean action noise std: 3.08
          Mean value_function loss: 98.7051
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.3776
                       Mean reward: 849.08
               Mean episode length: 227.34
    Episode_Reward/reaching_object: 1.3841
     Episode_Reward/lifting_object: 173.1884
      Episode_Reward/object_height: 0.0288
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 6.97s
                      Time elapsed: 01:02:53
                               ETA: 00:12:14

################################################################################
                     [1m Learning iteration 1675/2000 [0m                     

                       Computation: 22020 steps/s (collection: 4.354s, learning 0.111s)
             Mean action noise std: 3.08
          Mean value_function loss: 95.3786
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 55.3843
                       Mean reward: 883.44
               Mean episode length: 232.88
    Episode_Reward/reaching_object: 1.4296
     Episode_Reward/lifting_object: 179.4691
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 4.46s
                      Time elapsed: 01:02:58
                               ETA: 00:12:12

################################################################################
                     [1m Learning iteration 1676/2000 [0m                     

                       Computation: 47388 steps/s (collection: 1.984s, learning 0.090s)
             Mean action noise std: 3.08
          Mean value_function loss: 285.4942
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 55.3906
                       Mean reward: 901.53
               Mean episode length: 237.34
    Episode_Reward/reaching_object: 1.4187
     Episode_Reward/lifting_object: 178.3382
      Episode_Reward/object_height: 0.0293
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 2.07s
                      Time elapsed: 01:03:00
                               ETA: 00:12:10

################################################################################
                     [1m Learning iteration 1677/2000 [0m                     

                       Computation: 48769 steps/s (collection: 1.926s, learning 0.089s)
             Mean action noise std: 3.09
          Mean value_function loss: 310.2687
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 55.4025
                       Mean reward: 897.99
               Mean episode length: 237.69
    Episode_Reward/reaching_object: 1.4136
     Episode_Reward/lifting_object: 177.5610
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 2.02s
                      Time elapsed: 01:03:02
                               ETA: 00:12:08

################################################################################
                     [1m Learning iteration 1678/2000 [0m                     

                       Computation: 47905 steps/s (collection: 1.957s, learning 0.095s)
             Mean action noise std: 3.09
          Mean value_function loss: 80.2188
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 55.4110
                       Mean reward: 863.51
               Mean episode length: 229.39
    Episode_Reward/reaching_object: 1.3951
     Episode_Reward/lifting_object: 174.9203
      Episode_Reward/object_height: 0.0283
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 2.05s
                      Time elapsed: 01:03:04
                               ETA: 00:12:05

################################################################################
                     [1m Learning iteration 1679/2000 [0m                     

                       Computation: 47059 steps/s (collection: 1.990s, learning 0.099s)
             Mean action noise std: 3.09
          Mean value_function loss: 84.7783
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 55.4142
                       Mean reward: 927.68
               Mean episode length: 244.84
    Episode_Reward/reaching_object: 1.4206
     Episode_Reward/lifting_object: 178.9683
      Episode_Reward/object_height: 0.0288
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 2.09s
                      Time elapsed: 01:03:06
                               ETA: 00:12:03

################################################################################
                     [1m Learning iteration 1680/2000 [0m                     

                       Computation: 47310 steps/s (collection: 1.985s, learning 0.093s)
             Mean action noise std: 3.09
          Mean value_function loss: 81.5042
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.4191
                       Mean reward: 890.13
               Mean episode length: 236.00
    Episode_Reward/reaching_object: 1.4326
     Episode_Reward/lifting_object: 179.5451
      Episode_Reward/object_height: 0.0290
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 2.08s
                      Time elapsed: 01:03:08
                               ETA: 00:12:01

################################################################################
                     [1m Learning iteration 1681/2000 [0m                     

                       Computation: 48211 steps/s (collection: 1.941s, learning 0.098s)
             Mean action noise std: 3.09
          Mean value_function loss: 170.1073
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.4293
                       Mean reward: 922.09
               Mean episode length: 243.71
    Episode_Reward/reaching_object: 1.4338
     Episode_Reward/lifting_object: 179.8650
      Episode_Reward/object_height: 0.0286
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 2.04s
                      Time elapsed: 01:03:10
                               ETA: 00:11:58

################################################################################
                     [1m Learning iteration 1682/2000 [0m                     

                       Computation: 46381 steps/s (collection: 2.017s, learning 0.102s)
             Mean action noise std: 3.09
          Mean value_function loss: 162.8531
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 55.4400
                       Mean reward: 908.44
               Mean episode length: 239.43
    Episode_Reward/reaching_object: 1.4332
     Episode_Reward/lifting_object: 179.1096
      Episode_Reward/object_height: 0.0279
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 2.12s
                      Time elapsed: 01:03:12
                               ETA: 00:11:56

################################################################################
                     [1m Learning iteration 1683/2000 [0m                     

                       Computation: 48199 steps/s (collection: 1.939s, learning 0.100s)
             Mean action noise std: 3.09
          Mean value_function loss: 81.3842
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.4491
                       Mean reward: 860.52
               Mean episode length: 228.93
    Episode_Reward/reaching_object: 1.4363
     Episode_Reward/lifting_object: 179.2412
      Episode_Reward/object_height: 0.0272
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 2.04s
                      Time elapsed: 01:03:14
                               ETA: 00:11:54

################################################################################
                     [1m Learning iteration 1684/2000 [0m                     

                       Computation: 45312 steps/s (collection: 2.052s, learning 0.117s)
             Mean action noise std: 3.09
          Mean value_function loss: 83.9046
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 55.4549
                       Mean reward: 911.17
               Mean episode length: 240.88
    Episode_Reward/reaching_object: 1.4176
     Episode_Reward/lifting_object: 176.8652
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 2.17s
                      Time elapsed: 01:03:16
                               ETA: 00:11:52

################################################################################
                     [1m Learning iteration 1685/2000 [0m                     

                       Computation: 46413 steps/s (collection: 2.017s, learning 0.101s)
             Mean action noise std: 3.09
          Mean value_function loss: 81.7655
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.4609
                       Mean reward: 883.46
               Mean episode length: 233.62
    Episode_Reward/reaching_object: 1.4214
     Episode_Reward/lifting_object: 178.3346
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 2.12s
                      Time elapsed: 01:03:18
                               ETA: 00:11:49

################################################################################
                     [1m Learning iteration 1686/2000 [0m                     

                       Computation: 48896 steps/s (collection: 1.917s, learning 0.094s)
             Mean action noise std: 3.10
          Mean value_function loss: 96.5510
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.4732
                       Mean reward: 852.46
               Mean episode length: 226.93
    Episode_Reward/reaching_object: 1.4003
     Episode_Reward/lifting_object: 174.7912
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 2.01s
                      Time elapsed: 01:03:20
                               ETA: 00:11:47

################################################################################
                     [1m Learning iteration 1687/2000 [0m                     

                       Computation: 47521 steps/s (collection: 1.974s, learning 0.095s)
             Mean action noise std: 3.10
          Mean value_function loss: 83.8020
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.4864
                       Mean reward: 909.93
               Mean episode length: 240.46
    Episode_Reward/reaching_object: 1.4442
     Episode_Reward/lifting_object: 180.3354
      Episode_Reward/object_height: 0.0273
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 2.07s
                      Time elapsed: 01:03:22
                               ETA: 00:11:45

################################################################################
                     [1m Learning iteration 1688/2000 [0m                     

                       Computation: 48748 steps/s (collection: 1.919s, learning 0.098s)
             Mean action noise std: 3.10
          Mean value_function loss: 73.8152
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.4965
                       Mean reward: 902.40
               Mean episode length: 238.90
    Episode_Reward/reaching_object: 1.4317
     Episode_Reward/lifting_object: 179.2417
      Episode_Reward/object_height: 0.0275
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 2.02s
                      Time elapsed: 01:03:24
                               ETA: 00:11:42

################################################################################
                     [1m Learning iteration 1689/2000 [0m                     

                       Computation: 45875 steps/s (collection: 1.996s, learning 0.147s)
             Mean action noise std: 3.10
          Mean value_function loss: 71.8564
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.5077
                       Mean reward: 923.67
               Mean episode length: 243.57
    Episode_Reward/reaching_object: 1.4443
     Episode_Reward/lifting_object: 180.2278
      Episode_Reward/object_height: 0.0274
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 2.14s
                      Time elapsed: 01:03:27
                               ETA: 00:11:40

################################################################################
                     [1m Learning iteration 1690/2000 [0m                     

                       Computation: 43037 steps/s (collection: 2.185s, learning 0.099s)
             Mean action noise std: 3.10
          Mean value_function loss: 90.2061
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.5192
                       Mean reward: 879.75
               Mean episode length: 234.36
    Episode_Reward/reaching_object: 1.4277
     Episode_Reward/lifting_object: 178.3142
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 2.28s
                      Time elapsed: 01:03:29
                               ETA: 00:11:38

################################################################################
                     [1m Learning iteration 1691/2000 [0m                     

                       Computation: 45361 steps/s (collection: 2.063s, learning 0.105s)
             Mean action noise std: 3.10
          Mean value_function loss: 86.0969
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.5291
                       Mean reward: 906.84
               Mean episode length: 241.19
    Episode_Reward/reaching_object: 1.4042
     Episode_Reward/lifting_object: 174.8383
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 2.17s
                      Time elapsed: 01:03:31
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 1692/2000 [0m                     

                       Computation: 41886 steps/s (collection: 2.242s, learning 0.105s)
             Mean action noise std: 3.11
          Mean value_function loss: 71.6297
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.5448
                       Mean reward: 900.49
               Mean episode length: 238.26
    Episode_Reward/reaching_object: 1.4499
     Episode_Reward/lifting_object: 181.0573
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 2.35s
                      Time elapsed: 01:03:33
                               ETA: 00:11:33

################################################################################
                     [1m Learning iteration 1693/2000 [0m                     

                       Computation: 40462 steps/s (collection: 2.247s, learning 0.183s)
             Mean action noise std: 3.11
          Mean value_function loss: 77.6381
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.5535
                       Mean reward: 915.19
               Mean episode length: 241.11
    Episode_Reward/reaching_object: 1.4366
     Episode_Reward/lifting_object: 179.3282
      Episode_Reward/object_height: 0.0272
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 2.43s
                      Time elapsed: 01:03:36
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 1694/2000 [0m                     

                       Computation: 44024 steps/s (collection: 2.139s, learning 0.094s)
             Mean action noise std: 3.11
          Mean value_function loss: 75.5700
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 55.5606
                       Mean reward: 915.65
               Mean episode length: 242.23
    Episode_Reward/reaching_object: 1.4378
     Episode_Reward/lifting_object: 179.5613
      Episode_Reward/object_height: 0.0276
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 2.23s
                      Time elapsed: 01:03:38
                               ETA: 00:11:29

################################################################################
                     [1m Learning iteration 1695/2000 [0m                     

                       Computation: 46522 steps/s (collection: 2.011s, learning 0.102s)
             Mean action noise std: 3.11
          Mean value_function loss: 97.5564
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.5662
                       Mean reward: 888.89
               Mean episode length: 235.02
    Episode_Reward/reaching_object: 1.4161
     Episode_Reward/lifting_object: 176.7646
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 2.11s
                      Time elapsed: 01:03:40
                               ETA: 00:11:27

################################################################################
                     [1m Learning iteration 1696/2000 [0m                     

                       Computation: 48568 steps/s (collection: 1.930s, learning 0.094s)
             Mean action noise std: 3.11
          Mean value_function loss: 92.3274
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.5738
                       Mean reward: 862.60
               Mean episode length: 229.24
    Episode_Reward/reaching_object: 1.3922
     Episode_Reward/lifting_object: 174.3177
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 2.02s
                      Time elapsed: 01:03:42
                               ETA: 00:11:24

################################################################################
                     [1m Learning iteration 1697/2000 [0m                     

                       Computation: 48474 steps/s (collection: 1.936s, learning 0.092s)
             Mean action noise std: 3.11
          Mean value_function loss: 79.5703
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 55.5849
                       Mean reward: 898.73
               Mean episode length: 238.08
    Episode_Reward/reaching_object: 1.4291
     Episode_Reward/lifting_object: 178.5590
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 2.03s
                      Time elapsed: 01:03:44
                               ETA: 00:11:22

################################################################################
                     [1m Learning iteration 1698/2000 [0m                     

                       Computation: 46799 steps/s (collection: 1.973s, learning 0.128s)
             Mean action noise std: 3.11
          Mean value_function loss: 89.7855
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.5964
                       Mean reward: 905.17
               Mean episode length: 239.74
    Episode_Reward/reaching_object: 1.4309
     Episode_Reward/lifting_object: 178.2852
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 2.10s
                      Time elapsed: 01:03:46
                               ETA: 00:11:20

################################################################################
                     [1m Learning iteration 1699/2000 [0m                     

                       Computation: 47437 steps/s (collection: 1.965s, learning 0.108s)
             Mean action noise std: 3.12
          Mean value_function loss: 94.3590
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 55.6076
                       Mean reward: 882.04
               Mean episode length: 232.87
    Episode_Reward/reaching_object: 1.4015
     Episode_Reward/lifting_object: 174.6305
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 2.07s
                      Time elapsed: 01:03:48
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 1700/2000 [0m                     

                       Computation: 47594 steps/s (collection: 1.949s, learning 0.117s)
             Mean action noise std: 3.12
          Mean value_function loss: 74.2577
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 55.6174
                       Mean reward: 940.25
               Mean episode length: 246.57
    Episode_Reward/reaching_object: 1.4448
     Episode_Reward/lifting_object: 180.4242
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 2.07s
                      Time elapsed: 01:03:50
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 1701/2000 [0m                     

                       Computation: 46743 steps/s (collection: 2.012s, learning 0.091s)
             Mean action noise std: 3.12
          Mean value_function loss: 81.1612
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.6246
                       Mean reward: 920.13
               Mean episode length: 242.83
    Episode_Reward/reaching_object: 1.4272
     Episode_Reward/lifting_object: 178.9258
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 2.10s
                      Time elapsed: 01:03:53
                               ETA: 00:11:13

################################################################################
                     [1m Learning iteration 1702/2000 [0m                     

                       Computation: 48578 steps/s (collection: 1.913s, learning 0.111s)
             Mean action noise std: 3.12
          Mean value_function loss: 95.8992
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 55.6323
                       Mean reward: 900.45
               Mean episode length: 238.69
    Episode_Reward/reaching_object: 1.4347
     Episode_Reward/lifting_object: 179.6535
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 2.02s
                      Time elapsed: 01:03:55
                               ETA: 00:11:11

################################################################################
                     [1m Learning iteration 1703/2000 [0m                     

                       Computation: 46668 steps/s (collection: 1.997s, learning 0.109s)
             Mean action noise std: 3.12
          Mean value_function loss: 100.0051
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.6378
                       Mean reward: 894.95
               Mean episode length: 237.11
    Episode_Reward/reaching_object: 1.4258
     Episode_Reward/lifting_object: 178.5529
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 2.11s
                      Time elapsed: 01:03:57
                               ETA: 00:11:08

################################################################################
                     [1m Learning iteration 1704/2000 [0m                     

                       Computation: 48082 steps/s (collection: 1.936s, learning 0.108s)
             Mean action noise std: 3.12
          Mean value_function loss: 67.7527
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.6455
                       Mean reward: 896.45
               Mean episode length: 237.50
    Episode_Reward/reaching_object: 1.4294
     Episode_Reward/lifting_object: 179.1525
      Episode_Reward/object_height: 0.0271
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 2.04s
                      Time elapsed: 01:03:59
                               ETA: 00:11:06

################################################################################
                     [1m Learning iteration 1705/2000 [0m                     

                       Computation: 48761 steps/s (collection: 1.920s, learning 0.096s)
             Mean action noise std: 3.12
          Mean value_function loss: 76.2517
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.6531
                       Mean reward: 899.53
               Mean episode length: 237.77
    Episode_Reward/reaching_object: 1.4341
     Episode_Reward/lifting_object: 180.3351
      Episode_Reward/object_height: 0.0274
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 2.02s
                      Time elapsed: 01:04:01
                               ETA: 00:11:04

################################################################################
                     [1m Learning iteration 1706/2000 [0m                     

                       Computation: 48186 steps/s (collection: 1.949s, learning 0.091s)
             Mean action noise std: 3.12
          Mean value_function loss: 96.7726
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.6649
                       Mean reward: 928.43
               Mean episode length: 244.18
    Episode_Reward/reaching_object: 1.4306
     Episode_Reward/lifting_object: 179.3854
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 2.04s
                      Time elapsed: 01:04:03
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 1707/2000 [0m                     

                       Computation: 47742 steps/s (collection: 1.963s, learning 0.096s)
             Mean action noise std: 3.13
          Mean value_function loss: 142.1376
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.6749
                       Mean reward: 866.12
               Mean episode length: 228.84
    Episode_Reward/reaching_object: 1.3973
     Episode_Reward/lifting_object: 174.7366
      Episode_Reward/object_height: 0.0275
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 2.06s
                      Time elapsed: 01:04:05
                               ETA: 00:10:59

################################################################################
                     [1m Learning iteration 1708/2000 [0m                     

                       Computation: 47767 steps/s (collection: 1.965s, learning 0.093s)
             Mean action noise std: 3.13
          Mean value_function loss: 97.4744
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.6834
                       Mean reward: 897.37
               Mean episode length: 239.65
    Episode_Reward/reaching_object: 1.4325
     Episode_Reward/lifting_object: 179.2791
      Episode_Reward/object_height: 0.0278
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 2.06s
                      Time elapsed: 01:04:07
                               ETA: 00:10:57

################################################################################
                     [1m Learning iteration 1709/2000 [0m                     

                       Computation: 47685 steps/s (collection: 1.964s, learning 0.097s)
             Mean action noise std: 3.13
          Mean value_function loss: 100.4861
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.6934
                       Mean reward: 887.89
               Mean episode length: 234.09
    Episode_Reward/reaching_object: 1.4277
     Episode_Reward/lifting_object: 179.1772
      Episode_Reward/object_height: 0.0278
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 2.06s
                      Time elapsed: 01:04:09
                               ETA: 00:10:55

################################################################################
                     [1m Learning iteration 1710/2000 [0m                     

                       Computation: 48397 steps/s (collection: 1.934s, learning 0.097s)
             Mean action noise std: 3.13
          Mean value_function loss: 82.3697
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 55.7041
                       Mean reward: 904.64
               Mean episode length: 239.19
    Episode_Reward/reaching_object: 1.4513
     Episode_Reward/lifting_object: 182.2084
      Episode_Reward/object_height: 0.0275
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 2.03s
                      Time elapsed: 01:04:11
                               ETA: 00:10:52

################################################################################
                     [1m Learning iteration 1711/2000 [0m                     

                       Computation: 46915 steps/s (collection: 2.005s, learning 0.091s)
             Mean action noise std: 3.13
          Mean value_function loss: 78.1766
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 55.7207
                       Mean reward: 873.78
               Mean episode length: 230.67
    Episode_Reward/reaching_object: 1.4061
     Episode_Reward/lifting_object: 176.1787
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 2.10s
                      Time elapsed: 01:04:13
                               ETA: 00:10:50

################################################################################
                     [1m Learning iteration 1712/2000 [0m                     

                       Computation: 47031 steps/s (collection: 1.997s, learning 0.093s)
             Mean action noise std: 3.13
          Mean value_function loss: 97.6180
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.7351
                       Mean reward: 886.51
               Mean episode length: 234.79
    Episode_Reward/reaching_object: 1.3894
     Episode_Reward/lifting_object: 174.1095
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 2.09s
                      Time elapsed: 01:04:15
                               ETA: 00:10:48

################################################################################
                     [1m Learning iteration 1713/2000 [0m                     

                       Computation: 47849 steps/s (collection: 1.954s, learning 0.101s)
             Mean action noise std: 3.14
          Mean value_function loss: 99.9695
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.7451
                       Mean reward: 886.74
               Mean episode length: 233.18
    Episode_Reward/reaching_object: 1.4199
     Episode_Reward/lifting_object: 178.3235
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 2.05s
                      Time elapsed: 01:04:17
                               ETA: 00:10:45

################################################################################
                     [1m Learning iteration 1714/2000 [0m                     

                       Computation: 47634 steps/s (collection: 1.968s, learning 0.096s)
             Mean action noise std: 3.14
          Mean value_function loss: 96.3549
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 55.7519
                       Mean reward: 900.13
               Mean episode length: 238.14
    Episode_Reward/reaching_object: 1.4199
     Episode_Reward/lifting_object: 178.0100
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0523
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 2.06s
                      Time elapsed: 01:04:19
                               ETA: 00:10:43

################################################################################
                     [1m Learning iteration 1715/2000 [0m                     

                       Computation: 47931 steps/s (collection: 1.948s, learning 0.103s)
             Mean action noise std: 3.14
          Mean value_function loss: 82.6199
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 55.7646
                       Mean reward: 903.75
               Mean episode length: 237.99
    Episode_Reward/reaching_object: 1.3940
     Episode_Reward/lifting_object: 175.2511
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 2.05s
                      Time elapsed: 01:04:21
                               ETA: 00:10:41

################################################################################
                     [1m Learning iteration 1716/2000 [0m                     

                       Computation: 46773 steps/s (collection: 2.008s, learning 0.094s)
             Mean action noise std: 3.14
          Mean value_function loss: 113.2072
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 55.7822
                       Mean reward: 859.69
               Mean episode length: 228.17
    Episode_Reward/reaching_object: 1.3757
     Episode_Reward/lifting_object: 172.8371
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 2.10s
                      Time elapsed: 01:04:23
                               ETA: 00:10:39

################################################################################
                     [1m Learning iteration 1717/2000 [0m                     

                       Computation: 47328 steps/s (collection: 1.963s, learning 0.114s)
             Mean action noise std: 3.14
          Mean value_function loss: 81.9677
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.7921
                       Mean reward: 908.70
               Mean episode length: 239.14
    Episode_Reward/reaching_object: 1.4195
     Episode_Reward/lifting_object: 178.7493
      Episode_Reward/object_height: 0.0274
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 2.08s
                      Time elapsed: 01:04:26
                               ETA: 00:10:36

################################################################################
                     [1m Learning iteration 1718/2000 [0m                     

                       Computation: 47304 steps/s (collection: 1.963s, learning 0.116s)
             Mean action noise std: 3.14
          Mean value_function loss: 104.9040
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.7990
                       Mean reward: 893.54
               Mean episode length: 235.20
    Episode_Reward/reaching_object: 1.3976
     Episode_Reward/lifting_object: 176.4313
      Episode_Reward/object_height: 0.0274
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 2.08s
                      Time elapsed: 01:04:28
                               ETA: 00:10:34

################################################################################
                     [1m Learning iteration 1719/2000 [0m                     

                       Computation: 48494 steps/s (collection: 1.924s, learning 0.103s)
             Mean action noise std: 3.14
          Mean value_function loss: 91.5334
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.8048
                       Mean reward: 888.49
               Mean episode length: 234.91
    Episode_Reward/reaching_object: 1.3931
     Episode_Reward/lifting_object: 176.0010
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 2.03s
                      Time elapsed: 01:04:30
                               ETA: 00:10:32

################################################################################
                     [1m Learning iteration 1720/2000 [0m                     

                       Computation: 48259 steps/s (collection: 1.948s, learning 0.089s)
             Mean action noise std: 3.14
          Mean value_function loss: 86.1518
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.8103
                       Mean reward: 872.90
               Mean episode length: 231.57
    Episode_Reward/reaching_object: 1.4025
     Episode_Reward/lifting_object: 177.1235
      Episode_Reward/object_height: 0.0283
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 2.04s
                      Time elapsed: 01:04:32
                               ETA: 00:10:29

################################################################################
                     [1m Learning iteration 1721/2000 [0m                     

                       Computation: 47742 steps/s (collection: 1.965s, learning 0.094s)
             Mean action noise std: 3.15
          Mean value_function loss: 107.0153
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.8176
                       Mean reward: 891.38
               Mean episode length: 235.80
    Episode_Reward/reaching_object: 1.3951
     Episode_Reward/lifting_object: 176.1868
      Episode_Reward/object_height: 0.0284
        Episode_Reward/action_rate: -0.0523
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 2.06s
                      Time elapsed: 01:04:34
                               ETA: 00:10:27

################################################################################
                     [1m Learning iteration 1722/2000 [0m                     

                       Computation: 47914 steps/s (collection: 1.954s, learning 0.098s)
             Mean action noise std: 3.15
          Mean value_function loss: 97.4650
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 55.8246
                       Mean reward: 860.54
               Mean episode length: 230.05
    Episode_Reward/reaching_object: 1.4015
     Episode_Reward/lifting_object: 176.7496
      Episode_Reward/object_height: 0.0286
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 2.05s
                      Time elapsed: 01:04:36
                               ETA: 00:10:25

################################################################################
                     [1m Learning iteration 1723/2000 [0m                     

                       Computation: 45745 steps/s (collection: 2.053s, learning 0.096s)
             Mean action noise std: 3.15
          Mean value_function loss: 139.9673
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.8346
                       Mean reward: 876.78
               Mean episode length: 231.77
    Episode_Reward/reaching_object: 1.3857
     Episode_Reward/lifting_object: 174.3496
      Episode_Reward/object_height: 0.0283
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 2.15s
                      Time elapsed: 01:04:38
                               ETA: 00:10:23

################################################################################
                     [1m Learning iteration 1724/2000 [0m                     

                       Computation: 48013 steps/s (collection: 1.947s, learning 0.101s)
             Mean action noise std: 3.15
          Mean value_function loss: 130.6188
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.8461
                       Mean reward: 922.50
               Mean episode length: 242.41
    Episode_Reward/reaching_object: 1.3937
     Episode_Reward/lifting_object: 176.1143
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 2.05s
                      Time elapsed: 01:04:40
                               ETA: 00:10:20

################################################################################
                     [1m Learning iteration 1725/2000 [0m                     

                       Computation: 48340 steps/s (collection: 1.936s, learning 0.098s)
             Mean action noise std: 3.15
          Mean value_function loss: 98.5285
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.8569
                       Mean reward: 886.06
               Mean episode length: 233.87
    Episode_Reward/reaching_object: 1.4085
     Episode_Reward/lifting_object: 177.3921
      Episode_Reward/object_height: 0.0293
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 2.03s
                      Time elapsed: 01:04:42
                               ETA: 00:10:18

################################################################################
                     [1m Learning iteration 1726/2000 [0m                     

                       Computation: 48137 steps/s (collection: 1.952s, learning 0.091s)
             Mean action noise std: 3.15
          Mean value_function loss: 90.4458
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.8665
                       Mean reward: 913.09
               Mean episode length: 239.78
    Episode_Reward/reaching_object: 1.3949
     Episode_Reward/lifting_object: 175.2529
      Episode_Reward/object_height: 0.0291
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 2.04s
                      Time elapsed: 01:04:44
                               ETA: 00:10:16

################################################################################
                     [1m Learning iteration 1727/2000 [0m                     

                       Computation: 47522 steps/s (collection: 1.975s, learning 0.094s)
             Mean action noise std: 3.15
          Mean value_function loss: 121.8987
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.8758
                       Mean reward: 868.80
               Mean episode length: 230.41
    Episode_Reward/reaching_object: 1.4193
     Episode_Reward/lifting_object: 178.6852
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 2.07s
                      Time elapsed: 01:04:46
                               ETA: 00:10:14

################################################################################
                     [1m Learning iteration 1728/2000 [0m                     

                       Computation: 48457 steps/s (collection: 1.936s, learning 0.093s)
             Mean action noise std: 3.16
          Mean value_function loss: 105.7389
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 55.8840
                       Mean reward: 888.20
               Mean episode length: 235.91
    Episode_Reward/reaching_object: 1.4150
     Episode_Reward/lifting_object: 178.0147
      Episode_Reward/object_height: 0.0300
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 2.03s
                      Time elapsed: 01:04:48
                               ETA: 00:10:11

################################################################################
                     [1m Learning iteration 1729/2000 [0m                     

                       Computation: 47649 steps/s (collection: 1.967s, learning 0.097s)
             Mean action noise std: 3.16
          Mean value_function loss: 84.5373
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.9001
                       Mean reward: 886.94
               Mean episode length: 234.55
    Episode_Reward/reaching_object: 1.4135
     Episode_Reward/lifting_object: 177.4206
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 2.06s
                      Time elapsed: 01:04:50
                               ETA: 00:10:09

################################################################################
                     [1m Learning iteration 1730/2000 [0m                     

                       Computation: 47727 steps/s (collection: 1.968s, learning 0.092s)
             Mean action noise std: 3.16
          Mean value_function loss: 78.5385
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 55.9199
                       Mean reward: 909.87
               Mean episode length: 241.81
    Episode_Reward/reaching_object: 1.4325
     Episode_Reward/lifting_object: 180.0400
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 2.06s
                      Time elapsed: 01:04:52
                               ETA: 00:10:07

################################################################################
                     [1m Learning iteration 1731/2000 [0m                     

                       Computation: 46553 steps/s (collection: 1.994s, learning 0.118s)
             Mean action noise std: 3.16
          Mean value_function loss: 87.6537
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.9274
                       Mean reward: 870.04
               Mean episode length: 231.72
    Episode_Reward/reaching_object: 1.4011
     Episode_Reward/lifting_object: 176.3216
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 2.11s
                      Time elapsed: 01:04:54
                               ETA: 00:10:04

################################################################################
                     [1m Learning iteration 1732/2000 [0m                     

                       Computation: 47421 steps/s (collection: 1.956s, learning 0.117s)
             Mean action noise std: 3.16
          Mean value_function loss: 86.3443
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.9336
                       Mean reward: 891.20
               Mean episode length: 235.61
    Episode_Reward/reaching_object: 1.4103
     Episode_Reward/lifting_object: 176.9770
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 2.07s
                      Time elapsed: 01:04:56
                               ETA: 00:10:02

################################################################################
                     [1m Learning iteration 1733/2000 [0m                     

                       Computation: 47778 steps/s (collection: 1.944s, learning 0.114s)
             Mean action noise std: 3.16
          Mean value_function loss: 72.9598
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.9409
                       Mean reward: 930.42
               Mean episode length: 245.18
    Episode_Reward/reaching_object: 1.4492
     Episode_Reward/lifting_object: 182.2005
      Episode_Reward/object_height: 0.0323
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 2.06s
                      Time elapsed: 01:04:58
                               ETA: 00:10:00

################################################################################
                     [1m Learning iteration 1734/2000 [0m                     

                       Computation: 48559 steps/s (collection: 1.935s, learning 0.090s)
             Mean action noise std: 3.17
          Mean value_function loss: 71.4289
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.9508
                       Mean reward: 943.78
               Mean episode length: 247.08
    Episode_Reward/reaching_object: 1.4559
     Episode_Reward/lifting_object: 183.1094
      Episode_Reward/object_height: 0.0323
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 2.02s
                      Time elapsed: 01:05:01
                               ETA: 00:09:58

################################################################################
                     [1m Learning iteration 1735/2000 [0m                     

                       Computation: 47993 steps/s (collection: 1.952s, learning 0.097s)
             Mean action noise std: 3.17
          Mean value_function loss: 90.6530
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.9590
                       Mean reward: 923.65
               Mean episode length: 243.90
    Episode_Reward/reaching_object: 1.4171
     Episode_Reward/lifting_object: 177.6437
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 2.05s
                      Time elapsed: 01:05:03
                               ETA: 00:09:55

################################################################################
                     [1m Learning iteration 1736/2000 [0m                     

                       Computation: 46988 steps/s (collection: 2.001s, learning 0.091s)
             Mean action noise std: 3.17
          Mean value_function loss: 84.2027
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 55.9661
                       Mean reward: 923.25
               Mean episode length: 242.47
    Episode_Reward/reaching_object: 1.4231
     Episode_Reward/lifting_object: 178.5681
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 2.09s
                      Time elapsed: 01:05:05
                               ETA: 00:09:53

################################################################################
                     [1m Learning iteration 1737/2000 [0m                     

                       Computation: 47998 steps/s (collection: 1.956s, learning 0.092s)
             Mean action noise std: 3.17
          Mean value_function loss: 88.6433
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.9717
                       Mean reward: 879.45
               Mean episode length: 232.93
    Episode_Reward/reaching_object: 1.4201
     Episode_Reward/lifting_object: 177.5916
      Episode_Reward/object_height: 0.0301
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 2.05s
                      Time elapsed: 01:05:07
                               ETA: 00:09:51

################################################################################
                     [1m Learning iteration 1738/2000 [0m                     

                       Computation: 47797 steps/s (collection: 1.969s, learning 0.088s)
             Mean action noise std: 3.17
          Mean value_function loss: 92.3511
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.9790
                       Mean reward: 883.03
               Mean episode length: 233.29
    Episode_Reward/reaching_object: 1.4005
     Episode_Reward/lifting_object: 175.3159
      Episode_Reward/object_height: 0.0294
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 2.06s
                      Time elapsed: 01:05:09
                               ETA: 00:09:48

################################################################################
                     [1m Learning iteration 1739/2000 [0m                     

                       Computation: 47166 steps/s (collection: 1.999s, learning 0.085s)
             Mean action noise std: 3.17
          Mean value_function loss: 97.1643
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.9883
                       Mean reward: 914.30
               Mean episode length: 241.79
    Episode_Reward/reaching_object: 1.4019
     Episode_Reward/lifting_object: 175.3206
      Episode_Reward/object_height: 0.0290
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 2.08s
                      Time elapsed: 01:05:11
                               ETA: 00:09:46

################################################################################
                     [1m Learning iteration 1740/2000 [0m                     

                       Computation: 47618 steps/s (collection: 1.976s, learning 0.088s)
             Mean action noise std: 3.17
          Mean value_function loss: 73.4747
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 55.9972
                       Mean reward: 894.69
               Mean episode length: 237.89
    Episode_Reward/reaching_object: 1.4326
     Episode_Reward/lifting_object: 179.3707
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 2.06s
                      Time elapsed: 01:05:13
                               ETA: 00:09:44

################################################################################
                     [1m Learning iteration 1741/2000 [0m                     

                       Computation: 48063 steps/s (collection: 1.952s, learning 0.093s)
             Mean action noise std: 3.17
          Mean value_function loss: 77.4061
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.0050
                       Mean reward: 934.34
               Mean episode length: 245.44
    Episode_Reward/reaching_object: 1.4357
     Episode_Reward/lifting_object: 179.6617
      Episode_Reward/object_height: 0.0298
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 2.05s
                      Time elapsed: 01:05:15
                               ETA: 00:09:42

################################################################################
                     [1m Learning iteration 1742/2000 [0m                     

                       Computation: 48566 steps/s (collection: 1.937s, learning 0.088s)
             Mean action noise std: 3.18
          Mean value_function loss: 96.5207
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 56.0137
                       Mean reward: 873.90
               Mean episode length: 231.48
    Episode_Reward/reaching_object: 1.4105
     Episode_Reward/lifting_object: 175.9156
      Episode_Reward/object_height: 0.0293
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 2.02s
                      Time elapsed: 01:05:17
                               ETA: 00:09:39

################################################################################
                     [1m Learning iteration 1743/2000 [0m                     

                       Computation: 46898 steps/s (collection: 2.002s, learning 0.094s)
             Mean action noise std: 3.18
          Mean value_function loss: 94.3627
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 56.0268
                       Mean reward: 882.86
               Mean episode length: 234.26
    Episode_Reward/reaching_object: 1.3908
     Episode_Reward/lifting_object: 173.9503
      Episode_Reward/object_height: 0.0293
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 2.10s
                      Time elapsed: 01:05:19
                               ETA: 00:09:37

################################################################################
                     [1m Learning iteration 1744/2000 [0m                     

                       Computation: 47516 steps/s (collection: 1.971s, learning 0.098s)
             Mean action noise std: 3.18
          Mean value_function loss: 82.0180
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 56.0372
                       Mean reward: 920.28
               Mean episode length: 243.12
    Episode_Reward/reaching_object: 1.4302
     Episode_Reward/lifting_object: 179.3876
      Episode_Reward/object_height: 0.0302
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 2.07s
                      Time elapsed: 01:05:21
                               ETA: 00:09:35

################################################################################
                     [1m Learning iteration 1745/2000 [0m                     

                       Computation: 47660 steps/s (collection: 1.973s, learning 0.090s)
             Mean action noise std: 3.18
          Mean value_function loss: 139.7733
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 56.0453
                       Mean reward: 916.00
               Mean episode length: 241.28
    Episode_Reward/reaching_object: 1.4052
     Episode_Reward/lifting_object: 176.0257
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 2.06s
                      Time elapsed: 01:05:23
                               ETA: 00:09:33

################################################################################
                     [1m Learning iteration 1746/2000 [0m                     

                       Computation: 47544 steps/s (collection: 1.969s, learning 0.099s)
             Mean action noise std: 3.18
          Mean value_function loss: 142.9825
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.0508
                       Mean reward: 882.32
               Mean episode length: 234.30
    Episode_Reward/reaching_object: 1.4038
     Episode_Reward/lifting_object: 175.9858
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 2.07s
                      Time elapsed: 01:05:25
                               ETA: 00:09:30

################################################################################
                     [1m Learning iteration 1747/2000 [0m                     

                       Computation: 47521 steps/s (collection: 1.950s, learning 0.119s)
             Mean action noise std: 3.18
          Mean value_function loss: 98.4632
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.0595
                       Mean reward: 878.56
               Mean episode length: 232.60
    Episode_Reward/reaching_object: 1.4167
     Episode_Reward/lifting_object: 178.4150
      Episode_Reward/object_height: 0.0301
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 2.07s
                      Time elapsed: 01:05:27
                               ETA: 00:09:28

################################################################################
                     [1m Learning iteration 1748/2000 [0m                     

                       Computation: 47244 steps/s (collection: 1.980s, learning 0.101s)
             Mean action noise std: 3.18
          Mean value_function loss: 81.8928
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.0660
                       Mean reward: 933.22
               Mean episode length: 244.52
    Episode_Reward/reaching_object: 1.4138
     Episode_Reward/lifting_object: 177.3477
      Episode_Reward/object_height: 0.0290
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 2.08s
                      Time elapsed: 01:05:29
                               ETA: 00:09:26

################################################################################
                     [1m Learning iteration 1749/2000 [0m                     

                       Computation: 48065 steps/s (collection: 1.947s, learning 0.098s)
             Mean action noise std: 3.18
          Mean value_function loss: 81.6974
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 56.0714
                       Mean reward: 880.10
               Mean episode length: 234.37
    Episode_Reward/reaching_object: 1.4287
     Episode_Reward/lifting_object: 179.3443
      Episode_Reward/object_height: 0.0297
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 2.05s
                      Time elapsed: 01:05:31
                               ETA: 00:09:23

################################################################################
                     [1m Learning iteration 1750/2000 [0m                     

                       Computation: 47230 steps/s (collection: 1.978s, learning 0.103s)
             Mean action noise std: 3.18
          Mean value_function loss: 88.4892
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.0780
                       Mean reward: 884.27
               Mean episode length: 233.30
    Episode_Reward/reaching_object: 1.4129
     Episode_Reward/lifting_object: 178.0084
      Episode_Reward/object_height: 0.0292
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 2.08s
                      Time elapsed: 01:05:34
                               ETA: 00:09:21

################################################################################
                     [1m Learning iteration 1751/2000 [0m                     

                       Computation: 46842 steps/s (collection: 1.989s, learning 0.110s)
             Mean action noise std: 3.19
          Mean value_function loss: 85.0241
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.0840
                       Mean reward: 927.87
               Mean episode length: 244.23
    Episode_Reward/reaching_object: 1.3925
     Episode_Reward/lifting_object: 175.3518
      Episode_Reward/object_height: 0.0286
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 2.10s
                      Time elapsed: 01:05:36
                               ETA: 00:09:19

################################################################################
                     [1m Learning iteration 1752/2000 [0m                     

                       Computation: 47454 steps/s (collection: 1.962s, learning 0.110s)
             Mean action noise std: 3.19
          Mean value_function loss: 105.0958
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.0942
                       Mean reward: 880.48
               Mean episode length: 232.17
    Episode_Reward/reaching_object: 1.3866
     Episode_Reward/lifting_object: 174.4404
      Episode_Reward/object_height: 0.0279
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 2.07s
                      Time elapsed: 01:05:38
                               ETA: 00:09:17

################################################################################
                     [1m Learning iteration 1753/2000 [0m                     

                       Computation: 47494 steps/s (collection: 1.960s, learning 0.110s)
             Mean action noise std: 3.19
          Mean value_function loss: 107.2314
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.1013
                       Mean reward: 886.68
               Mean episode length: 234.00
    Episode_Reward/reaching_object: 1.3784
     Episode_Reward/lifting_object: 173.4912
      Episode_Reward/object_height: 0.0271
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 2.07s
                      Time elapsed: 01:05:40
                               ETA: 00:09:14

################################################################################
                     [1m Learning iteration 1754/2000 [0m                     

                       Computation: 47997 steps/s (collection: 1.955s, learning 0.093s)
             Mean action noise std: 3.19
          Mean value_function loss: 108.6417
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 56.1046
                       Mean reward: 863.64
               Mean episode length: 229.69
    Episode_Reward/reaching_object: 1.3909
     Episode_Reward/lifting_object: 175.3522
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 2.05s
                      Time elapsed: 01:05:42
                               ETA: 00:09:12

################################################################################
                     [1m Learning iteration 1755/2000 [0m                     

                       Computation: 46914 steps/s (collection: 2.005s, learning 0.090s)
             Mean action noise std: 3.19
          Mean value_function loss: 90.7787
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.1127
                       Mean reward: 903.50
               Mean episode length: 238.08
    Episode_Reward/reaching_object: 1.4051
     Episode_Reward/lifting_object: 176.9690
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 2.10s
                      Time elapsed: 01:05:44
                               ETA: 00:09:10

################################################################################
                     [1m Learning iteration 1756/2000 [0m                     

                       Computation: 48021 steps/s (collection: 1.955s, learning 0.092s)
             Mean action noise std: 3.19
          Mean value_function loss: 99.5423
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.1231
                       Mean reward: 868.54
               Mean episode length: 229.56
    Episode_Reward/reaching_object: 1.3926
     Episode_Reward/lifting_object: 175.4092
      Episode_Reward/object_height: 0.0268
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 2.05s
                      Time elapsed: 01:05:46
                               ETA: 00:09:08

################################################################################
                     [1m Learning iteration 1757/2000 [0m                     

                       Computation: 47861 steps/s (collection: 1.961s, learning 0.093s)
             Mean action noise std: 3.19
          Mean value_function loss: 99.2051
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.1324
                       Mean reward: 865.75
               Mean episode length: 229.22
    Episode_Reward/reaching_object: 1.3720
     Episode_Reward/lifting_object: 173.2436
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 2.05s
                      Time elapsed: 01:05:48
                               ETA: 00:09:05

################################################################################
                     [1m Learning iteration 1758/2000 [0m                     

                       Computation: 47872 steps/s (collection: 1.954s, learning 0.100s)
             Mean action noise std: 3.19
          Mean value_function loss: 86.6027
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 56.1408
                       Mean reward: 883.60
               Mean episode length: 231.91
    Episode_Reward/reaching_object: 1.4188
     Episode_Reward/lifting_object: 178.9642
      Episode_Reward/object_height: 0.0274
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 2.05s
                      Time elapsed: 01:05:50
                               ETA: 00:09:03

################################################################################
                     [1m Learning iteration 1759/2000 [0m                     

                       Computation: 48440 steps/s (collection: 1.943s, learning 0.086s)
             Mean action noise std: 3.19
          Mean value_function loss: 98.9610
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.1489
                       Mean reward: 863.91
               Mean episode length: 228.88
    Episode_Reward/reaching_object: 1.4174
     Episode_Reward/lifting_object: 178.0061
      Episode_Reward/object_height: 0.0268
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 2.03s
                      Time elapsed: 01:05:52
                               ETA: 00:09:01

################################################################################
                     [1m Learning iteration 1760/2000 [0m                     

                       Computation: 48076 steps/s (collection: 1.948s, learning 0.097s)
             Mean action noise std: 3.20
          Mean value_function loss: 99.5408
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.1562
                       Mean reward: 894.78
               Mean episode length: 236.34
    Episode_Reward/reaching_object: 1.4051
     Episode_Reward/lifting_object: 176.9857
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 2.04s
                      Time elapsed: 01:05:54
                               ETA: 00:08:58

################################################################################
                     [1m Learning iteration 1761/2000 [0m                     

                       Computation: 48184 steps/s (collection: 1.954s, learning 0.087s)
             Mean action noise std: 3.20
          Mean value_function loss: 99.3304
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.1655
                       Mean reward: 859.01
               Mean episode length: 226.70
    Episode_Reward/reaching_object: 1.3818
     Episode_Reward/lifting_object: 173.8937
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 2.04s
                      Time elapsed: 01:05:56
                               ETA: 00:08:56

################################################################################
                     [1m Learning iteration 1762/2000 [0m                     

                       Computation: 47494 steps/s (collection: 1.975s, learning 0.095s)
             Mean action noise std: 3.20
          Mean value_function loss: 80.8566
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 56.1752
                       Mean reward: 913.62
               Mean episode length: 241.21
    Episode_Reward/reaching_object: 1.4147
     Episode_Reward/lifting_object: 177.1359
      Episode_Reward/object_height: 0.0268
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 2.07s
                      Time elapsed: 01:05:58
                               ETA: 00:08:54

################################################################################
                     [1m Learning iteration 1763/2000 [0m                     

                       Computation: 47760 steps/s (collection: 1.972s, learning 0.087s)
             Mean action noise std: 3.20
          Mean value_function loss: 87.0879
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.1816
                       Mean reward: 905.32
               Mean episode length: 238.87
    Episode_Reward/reaching_object: 1.4201
     Episode_Reward/lifting_object: 178.2635
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 2.06s
                      Time elapsed: 01:06:00
                               ETA: 00:08:52

################################################################################
                     [1m Learning iteration 1764/2000 [0m                     

                       Computation: 47294 steps/s (collection: 1.981s, learning 0.098s)
             Mean action noise std: 3.20
          Mean value_function loss: 84.3556
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.1929
                       Mean reward: 906.23
               Mean episode length: 238.63
    Episode_Reward/reaching_object: 1.4217
     Episode_Reward/lifting_object: 179.0326
      Episode_Reward/object_height: 0.0271
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 2.08s
                      Time elapsed: 01:06:02
                               ETA: 00:08:49

################################################################################
                     [1m Learning iteration 1765/2000 [0m                     

                       Computation: 45214 steps/s (collection: 2.077s, learning 0.098s)
             Mean action noise std: 3.20
          Mean value_function loss: 100.4446
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.2046
                       Mean reward: 887.54
               Mean episode length: 233.61
    Episode_Reward/reaching_object: 1.3944
     Episode_Reward/lifting_object: 175.3475
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 2.17s
                      Time elapsed: 01:06:05
                               ETA: 00:08:47

################################################################################
                     [1m Learning iteration 1766/2000 [0m                     

                       Computation: 46431 steps/s (collection: 2.002s, learning 0.115s)
             Mean action noise std: 3.20
          Mean value_function loss: 87.8371
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.2165
                       Mean reward: 896.92
               Mean episode length: 237.75
    Episode_Reward/reaching_object: 1.4135
     Episode_Reward/lifting_object: 177.5225
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 2.12s
                      Time elapsed: 01:06:07
                               ETA: 00:08:45

################################################################################
                     [1m Learning iteration 1767/2000 [0m                     

                       Computation: 44960 steps/s (collection: 2.071s, learning 0.115s)
             Mean action noise std: 3.21
          Mean value_function loss: 85.6673
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.2247
                       Mean reward: 869.15
               Mean episode length: 229.24
    Episode_Reward/reaching_object: 1.4219
     Episode_Reward/lifting_object: 178.9141
      Episode_Reward/object_height: 0.0274
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 2.19s
                      Time elapsed: 01:06:09
                               ETA: 00:08:43

################################################################################
                     [1m Learning iteration 1768/2000 [0m                     

                       Computation: 46531 steps/s (collection: 2.000s, learning 0.113s)
             Mean action noise std: 3.21
          Mean value_function loss: 88.7716
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.2292
                       Mean reward: 923.05
               Mean episode length: 242.63
    Episode_Reward/reaching_object: 1.4389
     Episode_Reward/lifting_object: 181.0429
      Episode_Reward/object_height: 0.0281
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 2.11s
                      Time elapsed: 01:06:11
                               ETA: 00:08:40

################################################################################
                     [1m Learning iteration 1769/2000 [0m                     

                       Computation: 47393 steps/s (collection: 1.988s, learning 0.087s)
             Mean action noise std: 3.21
          Mean value_function loss: 104.2572
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.2333
                       Mean reward: 864.22
               Mean episode length: 229.96
    Episode_Reward/reaching_object: 1.4256
     Episode_Reward/lifting_object: 178.7304
      Episode_Reward/object_height: 0.0280
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 2.07s
                      Time elapsed: 01:06:13
                               ETA: 00:08:38

################################################################################
                     [1m Learning iteration 1770/2000 [0m                     

                       Computation: 46733 steps/s (collection: 2.008s, learning 0.096s)
             Mean action noise std: 3.21
          Mean value_function loss: 95.5369
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.2396
                       Mean reward: 877.38
               Mean episode length: 232.20
    Episode_Reward/reaching_object: 1.4235
     Episode_Reward/lifting_object: 178.8824
      Episode_Reward/object_height: 0.0282
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 2.10s
                      Time elapsed: 01:06:15
                               ETA: 00:08:36

################################################################################
                     [1m Learning iteration 1771/2000 [0m                     

                       Computation: 46921 steps/s (collection: 2.003s, learning 0.092s)
             Mean action noise std: 3.21
          Mean value_function loss: 107.7446
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.2449
                       Mean reward: 896.59
               Mean episode length: 236.23
    Episode_Reward/reaching_object: 1.4043
     Episode_Reward/lifting_object: 176.6970
      Episode_Reward/object_height: 0.0282
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 2.10s
                      Time elapsed: 01:06:17
                               ETA: 00:08:34

################################################################################
                     [1m Learning iteration 1772/2000 [0m                     

                       Computation: 47704 steps/s (collection: 1.963s, learning 0.098s)
             Mean action noise std: 3.21
          Mean value_function loss: 105.2088
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.2553
                       Mean reward: 884.92
               Mean episode length: 233.01
    Episode_Reward/reaching_object: 1.4218
     Episode_Reward/lifting_object: 178.8865
      Episode_Reward/object_height: 0.0288
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 2.06s
                      Time elapsed: 01:06:19
                               ETA: 00:08:31

################################################################################
                     [1m Learning iteration 1773/2000 [0m                     

                       Computation: 47242 steps/s (collection: 1.990s, learning 0.091s)
             Mean action noise std: 3.21
          Mean value_function loss: 145.0234
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.2679
                       Mean reward: 870.34
               Mean episode length: 233.45
    Episode_Reward/reaching_object: 1.4001
     Episode_Reward/lifting_object: 176.4940
      Episode_Reward/object_height: 0.0284
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 2.08s
                      Time elapsed: 01:06:21
                               ETA: 00:08:29

################################################################################
                     [1m Learning iteration 1774/2000 [0m                     

                       Computation: 46460 steps/s (collection: 1.996s, learning 0.119s)
             Mean action noise std: 3.21
          Mean value_function loss: 110.2742
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.2781
                       Mean reward: 874.62
               Mean episode length: 230.82
    Episode_Reward/reaching_object: 1.4057
     Episode_Reward/lifting_object: 177.3396
      Episode_Reward/object_height: 0.0283
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 2.12s
                      Time elapsed: 01:06:24
                               ETA: 00:08:27

################################################################################
                     [1m Learning iteration 1775/2000 [0m                     

                       Computation: 47135 steps/s (collection: 1.985s, learning 0.101s)
             Mean action noise std: 3.21
          Mean value_function loss: 137.7198
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 56.2867
                       Mean reward: 876.49
               Mean episode length: 233.33
    Episode_Reward/reaching_object: 1.4059
     Episode_Reward/lifting_object: 176.2308
      Episode_Reward/object_height: 0.0283
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 2.09s
                      Time elapsed: 01:06:26
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1776/2000 [0m                     

                       Computation: 47225 steps/s (collection: 1.985s, learning 0.097s)
             Mean action noise std: 3.22
          Mean value_function loss: 112.3541
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 56.2920
                       Mean reward: 896.96
               Mean episode length: 236.76
    Episode_Reward/reaching_object: 1.3882
     Episode_Reward/lifting_object: 174.3501
      Episode_Reward/object_height: 0.0281
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 2.08s
                      Time elapsed: 01:06:28
                               ETA: 00:08:22

################################################################################
                     [1m Learning iteration 1777/2000 [0m                     

                       Computation: 46784 steps/s (collection: 2.000s, learning 0.101s)
             Mean action noise std: 3.22
          Mean value_function loss: 91.1017
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.2975
                       Mean reward: 892.98
               Mean episode length: 236.58
    Episode_Reward/reaching_object: 1.4174
     Episode_Reward/lifting_object: 178.9002
      Episode_Reward/object_height: 0.0288
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 2.10s
                      Time elapsed: 01:06:30
                               ETA: 00:08:20

################################################################################
                     [1m Learning iteration 1778/2000 [0m                     

                       Computation: 47160 steps/s (collection: 1.984s, learning 0.101s)
             Mean action noise std: 3.22
          Mean value_function loss: 91.3460
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.3053
                       Mean reward: 881.10
               Mean episode length: 233.67
    Episode_Reward/reaching_object: 1.3989
     Episode_Reward/lifting_object: 175.9350
      Episode_Reward/object_height: 0.0281
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 2.08s
                      Time elapsed: 01:06:32
                               ETA: 00:08:18

################################################################################
                     [1m Learning iteration 1779/2000 [0m                     

                       Computation: 47812 steps/s (collection: 1.957s, learning 0.099s)
             Mean action noise std: 3.22
          Mean value_function loss: 96.3037
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.3146
                       Mean reward: 893.64
               Mean episode length: 237.26
    Episode_Reward/reaching_object: 1.4353
     Episode_Reward/lifting_object: 180.5068
      Episode_Reward/object_height: 0.0288
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 2.06s
                      Time elapsed: 01:06:34
                               ETA: 00:08:15

################################################################################
                     [1m Learning iteration 1780/2000 [0m                     

                       Computation: 46970 steps/s (collection: 2.002s, learning 0.091s)
             Mean action noise std: 3.22
          Mean value_function loss: 115.5403
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 56.3219
                       Mean reward: 878.85
               Mean episode length: 231.99
    Episode_Reward/reaching_object: 1.4097
     Episode_Reward/lifting_object: 177.2201
      Episode_Reward/object_height: 0.0281
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 2.09s
                      Time elapsed: 01:06:36
                               ETA: 00:08:13

################################################################################
                     [1m Learning iteration 1781/2000 [0m                     

                       Computation: 47916 steps/s (collection: 1.966s, learning 0.085s)
             Mean action noise std: 3.22
          Mean value_function loss: 68.4680
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.3259
                       Mean reward: 921.41
               Mean episode length: 242.82
    Episode_Reward/reaching_object: 1.4448
     Episode_Reward/lifting_object: 181.8694
      Episode_Reward/object_height: 0.0284
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 2.05s
                      Time elapsed: 01:06:38
                               ETA: 00:08:11

################################################################################
                     [1m Learning iteration 1782/2000 [0m                     

                       Computation: 47080 steps/s (collection: 1.999s, learning 0.089s)
             Mean action noise std: 3.22
          Mean value_function loss: 75.6158
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.3325
                       Mean reward: 938.27
               Mean episode length: 245.64
    Episode_Reward/reaching_object: 1.4270
     Episode_Reward/lifting_object: 180.1390
      Episode_Reward/object_height: 0.0279
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 2.09s
                      Time elapsed: 01:06:40
                               ETA: 00:08:09

################################################################################
                     [1m Learning iteration 1783/2000 [0m                     

                       Computation: 47254 steps/s (collection: 1.977s, learning 0.104s)
             Mean action noise std: 3.22
          Mean value_function loss: 80.7416
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.3380
                       Mean reward: 911.11
               Mean episode length: 240.57
    Episode_Reward/reaching_object: 1.4332
     Episode_Reward/lifting_object: 180.2398
      Episode_Reward/object_height: 0.0274
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 2.08s
                      Time elapsed: 01:06:42
                               ETA: 00:08:06

################################################################################
                     [1m Learning iteration 1784/2000 [0m                     

                       Computation: 47462 steps/s (collection: 1.977s, learning 0.095s)
             Mean action noise std: 3.22
          Mean value_function loss: 88.2611
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 56.3464
                       Mean reward: 895.01
               Mean episode length: 236.89
    Episode_Reward/reaching_object: 1.4038
     Episode_Reward/lifting_object: 176.5714
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 2.07s
                      Time elapsed: 01:06:44
                               ETA: 00:08:04

################################################################################
                     [1m Learning iteration 1785/2000 [0m                     

                       Computation: 47564 steps/s (collection: 1.976s, learning 0.091s)
             Mean action noise std: 3.22
          Mean value_function loss: 95.4688
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.3503
                       Mean reward: 882.83
               Mean episode length: 234.70
    Episode_Reward/reaching_object: 1.4092
     Episode_Reward/lifting_object: 176.9379
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 2.07s
                      Time elapsed: 01:06:46
                               ETA: 00:08:02

################################################################################
                     [1m Learning iteration 1786/2000 [0m                     

                       Computation: 48302 steps/s (collection: 1.948s, learning 0.087s)
             Mean action noise std: 3.23
          Mean value_function loss: 77.6619
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 56.3545
                       Mean reward: 927.66
               Mean episode length: 244.68
    Episode_Reward/reaching_object: 1.4398
     Episode_Reward/lifting_object: 180.3719
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 2.04s
                      Time elapsed: 01:06:48
                               ETA: 00:08:00

################################################################################
                     [1m Learning iteration 1787/2000 [0m                     

                       Computation: 47418 steps/s (collection: 1.985s, learning 0.089s)
             Mean action noise std: 3.23
          Mean value_function loss: 107.2057
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 56.3651
                       Mean reward: 915.58
               Mean episode length: 241.03
    Episode_Reward/reaching_object: 1.4199
     Episode_Reward/lifting_object: 178.5245
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 2.07s
                      Time elapsed: 01:06:50
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1788/2000 [0m                     

                       Computation: 48044 steps/s (collection: 1.957s, learning 0.090s)
             Mean action noise std: 3.23
          Mean value_function loss: 70.6088
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 56.3805
                       Mean reward: 891.49
               Mean episode length: 235.73
    Episode_Reward/reaching_object: 1.4337
     Episode_Reward/lifting_object: 179.4329
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 2.05s
                      Time elapsed: 01:06:53
                               ETA: 00:07:55

################################################################################
                     [1m Learning iteration 1789/2000 [0m                     

                       Computation: 46758 steps/s (collection: 2.004s, learning 0.099s)
             Mean action noise std: 3.23
          Mean value_function loss: 75.2141
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.3845
                       Mean reward: 908.18
               Mean episode length: 239.04
    Episode_Reward/reaching_object: 1.4555
     Episode_Reward/lifting_object: 182.8842
      Episode_Reward/object_height: 0.0268
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 2.10s
                      Time elapsed: 01:06:55
                               ETA: 00:07:53

################################################################################
                     [1m Learning iteration 1790/2000 [0m                     

                       Computation: 46805 steps/s (collection: 1.988s, learning 0.112s)
             Mean action noise std: 3.23
          Mean value_function loss: 84.7234
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.3931
                       Mean reward: 883.64
               Mean episode length: 232.79
    Episode_Reward/reaching_object: 1.4287
     Episode_Reward/lifting_object: 179.0339
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 2.10s
                      Time elapsed: 01:06:57
                               ETA: 00:07:51

################################################################################
                     [1m Learning iteration 1791/2000 [0m                     

                       Computation: 47177 steps/s (collection: 1.971s, learning 0.113s)
             Mean action noise std: 3.23
          Mean value_function loss: 109.4566
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 56.4018
                       Mean reward: 883.64
               Mean episode length: 233.87
    Episode_Reward/reaching_object: 1.4318
     Episode_Reward/lifting_object: 179.8849
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 2.08s
                      Time elapsed: 01:06:59
                               ETA: 00:07:48

################################################################################
                     [1m Learning iteration 1792/2000 [0m                     

                       Computation: 46479 steps/s (collection: 2.019s, learning 0.096s)
             Mean action noise std: 3.23
          Mean value_function loss: 109.5600
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.4066
                       Mean reward: 865.51
               Mean episode length: 229.20
    Episode_Reward/reaching_object: 1.4069
     Episode_Reward/lifting_object: 175.4604
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 2.11s
                      Time elapsed: 01:07:01
                               ETA: 00:07:46

################################################################################
                     [1m Learning iteration 1793/2000 [0m                     

                       Computation: 47971 steps/s (collection: 1.949s, learning 0.100s)
             Mean action noise std: 3.23
          Mean value_function loss: 101.5574
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.4113
                       Mean reward: 888.33
               Mean episode length: 234.83
    Episode_Reward/reaching_object: 1.4037
     Episode_Reward/lifting_object: 175.4836
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 2.05s
                      Time elapsed: 01:07:03
                               ETA: 00:07:44

################################################################################
                     [1m Learning iteration 1794/2000 [0m                     

                       Computation: 47508 steps/s (collection: 1.979s, learning 0.091s)
             Mean action noise std: 3.24
          Mean value_function loss: 88.0764
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.4160
                       Mean reward: 898.02
               Mean episode length: 236.91
    Episode_Reward/reaching_object: 1.4282
     Episode_Reward/lifting_object: 178.6628
      Episode_Reward/object_height: 0.0273
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 2.07s
                      Time elapsed: 01:07:05
                               ETA: 00:07:41

################################################################################
                     [1m Learning iteration 1795/2000 [0m                     

                       Computation: 46955 steps/s (collection: 1.995s, learning 0.099s)
             Mean action noise std: 3.24
          Mean value_function loss: 115.7513
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 56.4238
                       Mean reward: 891.50
               Mean episode length: 235.87
    Episode_Reward/reaching_object: 1.3843
     Episode_Reward/lifting_object: 172.6988
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 2.09s
                      Time elapsed: 01:07:07
                               ETA: 00:07:39

################################################################################
                     [1m Learning iteration 1796/2000 [0m                     

                       Computation: 47405 steps/s (collection: 1.985s, learning 0.089s)
             Mean action noise std: 3.24
          Mean value_function loss: 121.0102
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.4308
                       Mean reward: 903.63
               Mean episode length: 238.04
    Episode_Reward/reaching_object: 1.4097
     Episode_Reward/lifting_object: 176.5833
      Episode_Reward/object_height: 0.0276
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 2.07s
                      Time elapsed: 01:07:09
                               ETA: 00:07:37

################################################################################
                     [1m Learning iteration 1797/2000 [0m                     

                       Computation: 47486 steps/s (collection: 1.982s, learning 0.089s)
             Mean action noise std: 3.24
          Mean value_function loss: 100.8550
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 56.4398
                       Mean reward: 892.02
               Mean episode length: 236.80
    Episode_Reward/reaching_object: 1.4230
     Episode_Reward/lifting_object: 177.7181
      Episode_Reward/object_height: 0.0280
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 2.07s
                      Time elapsed: 01:07:11
                               ETA: 00:07:35

################################################################################
                     [1m Learning iteration 1798/2000 [0m                     

                       Computation: 47445 steps/s (collection: 1.985s, learning 0.087s)
             Mean action noise std: 3.24
          Mean value_function loss: 98.2049
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 56.4485
                       Mean reward: 886.03
               Mean episode length: 235.39
    Episode_Reward/reaching_object: 1.4274
     Episode_Reward/lifting_object: 178.5337
      Episode_Reward/object_height: 0.0287
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 2.07s
                      Time elapsed: 01:07:13
                               ETA: 00:07:32

################################################################################
                     [1m Learning iteration 1799/2000 [0m                     

                       Computation: 48084 steps/s (collection: 1.957s, learning 0.088s)
             Mean action noise std: 3.24
          Mean value_function loss: 104.6997
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.4546
                       Mean reward: 889.30
               Mean episode length: 235.93
    Episode_Reward/reaching_object: 1.4028
     Episode_Reward/lifting_object: 175.2199
      Episode_Reward/object_height: 0.0283
        Episode_Reward/action_rate: -0.0549
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 2.04s
                      Time elapsed: 01:07:15
                               ETA: 00:07:30

################################################################################
                     [1m Learning iteration 1800/2000 [0m                     

                       Computation: 48269 steps/s (collection: 1.948s, learning 0.088s)
             Mean action noise std: 3.24
          Mean value_function loss: 94.9011
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 56.4633
                       Mean reward: 914.10
               Mean episode length: 240.38
    Episode_Reward/reaching_object: 1.4341
     Episode_Reward/lifting_object: 179.3870
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 2.04s
                      Time elapsed: 01:07:17
                               ETA: 00:07:28

################################################################################
                     [1m Learning iteration 1801/2000 [0m                     

                       Computation: 47196 steps/s (collection: 1.994s, learning 0.088s)
             Mean action noise std: 3.24
          Mean value_function loss: 106.5055
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.4743
                       Mean reward: 890.16
               Mean episode length: 235.34
    Episode_Reward/reaching_object: 1.4244
     Episode_Reward/lifting_object: 178.5667
      Episode_Reward/object_height: 0.0302
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 2.08s
                      Time elapsed: 01:07:20
                               ETA: 00:07:26

################################################################################
                     [1m Learning iteration 1802/2000 [0m                     

                       Computation: 46999 steps/s (collection: 1.988s, learning 0.103s)
             Mean action noise std: 3.25
          Mean value_function loss: 109.7434
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 56.4838
                       Mean reward: 891.90
               Mean episode length: 235.19
    Episode_Reward/reaching_object: 1.4095
     Episode_Reward/lifting_object: 176.4463
      Episode_Reward/object_height: 0.0309
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 2.09s
                      Time elapsed: 01:07:22
                               ETA: 00:07:23

################################################################################
                     [1m Learning iteration 1803/2000 [0m                     

                       Computation: 47237 steps/s (collection: 1.985s, learning 0.096s)
             Mean action noise std: 3.25
          Mean value_function loss: 92.6032
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 56.4938
                       Mean reward: 915.56
               Mean episode length: 240.81
    Episode_Reward/reaching_object: 1.4171
     Episode_Reward/lifting_object: 177.2978
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 2.08s
                      Time elapsed: 01:07:24
                               ETA: 00:07:21

################################################################################
                     [1m Learning iteration 1804/2000 [0m                     

                       Computation: 47677 steps/s (collection: 1.964s, learning 0.098s)
             Mean action noise std: 3.25
          Mean value_function loss: 83.4389
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 56.5032
                       Mean reward: 904.96
               Mean episode length: 239.45
    Episode_Reward/reaching_object: 1.4305
     Episode_Reward/lifting_object: 178.4093
      Episode_Reward/object_height: 0.0317
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 2.06s
                      Time elapsed: 01:07:26
                               ETA: 00:07:19

################################################################################
                     [1m Learning iteration 1805/2000 [0m                     

                       Computation: 47301 steps/s (collection: 1.975s, learning 0.104s)
             Mean action noise std: 3.25
          Mean value_function loss: 83.6564
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.5085
                       Mean reward: 915.32
               Mean episode length: 241.44
    Episode_Reward/reaching_object: 1.4279
     Episode_Reward/lifting_object: 178.9034
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 2.08s
                      Time elapsed: 01:07:28
                               ETA: 00:07:17

################################################################################
                     [1m Learning iteration 1806/2000 [0m                     

                       Computation: 47941 steps/s (collection: 1.958s, learning 0.093s)
             Mean action noise std: 3.25
          Mean value_function loss: 117.1914
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.5198
                       Mean reward: 888.38
               Mean episode length: 235.08
    Episode_Reward/reaching_object: 1.4101
     Episode_Reward/lifting_object: 177.1493
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 2.05s
                      Time elapsed: 01:07:30
                               ETA: 00:07:14

################################################################################
                     [1m Learning iteration 1807/2000 [0m                     

                       Computation: 47925 steps/s (collection: 1.962s, learning 0.089s)
             Mean action noise std: 3.25
          Mean value_function loss: 91.8391
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 56.5303
                       Mean reward: 921.28
               Mean episode length: 241.92
    Episode_Reward/reaching_object: 1.4070
     Episode_Reward/lifting_object: 175.9374
      Episode_Reward/object_height: 0.0309
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 2.05s
                      Time elapsed: 01:07:32
                               ETA: 00:07:12

################################################################################
                     [1m Learning iteration 1808/2000 [0m                     

                       Computation: 47623 steps/s (collection: 1.971s, learning 0.094s)
             Mean action noise std: 3.25
          Mean value_function loss: 77.2023
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.5409
                       Mean reward: 874.70
               Mean episode length: 231.54
    Episode_Reward/reaching_object: 1.4361
     Episode_Reward/lifting_object: 179.5328
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 2.06s
                      Time elapsed: 01:07:34
                               ETA: 00:07:10

################################################################################
                     [1m Learning iteration 1809/2000 [0m                     

                       Computation: 47973 steps/s (collection: 1.953s, learning 0.096s)
             Mean action noise std: 3.26
          Mean value_function loss: 105.4460
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.5495
                       Mean reward: 911.83
               Mean episode length: 239.50
    Episode_Reward/reaching_object: 1.4530
     Episode_Reward/lifting_object: 182.1885
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 2.05s
                      Time elapsed: 01:07:36
                               ETA: 00:07:08

################################################################################
                     [1m Learning iteration 1810/2000 [0m                     

                       Computation: 47497 steps/s (collection: 1.983s, learning 0.087s)
             Mean action noise std: 3.26
          Mean value_function loss: 93.3380
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.5564
                       Mean reward: 883.79
               Mean episode length: 233.74
    Episode_Reward/reaching_object: 1.4342
     Episode_Reward/lifting_object: 178.8204
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 2.07s
                      Time elapsed: 01:07:38
                               ETA: 00:07:05

################################################################################
                     [1m Learning iteration 1811/2000 [0m                     

                       Computation: 40701 steps/s (collection: 2.263s, learning 0.153s)
             Mean action noise std: 3.26
          Mean value_function loss: 102.9789
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.5642
                       Mean reward: 877.04
               Mean episode length: 231.94
    Episode_Reward/reaching_object: 1.4265
     Episode_Reward/lifting_object: 178.3667
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 2.42s
                      Time elapsed: 01:07:41
                               ETA: 00:07:03

################################################################################
                     [1m Learning iteration 1812/2000 [0m                     

                       Computation: 42874 steps/s (collection: 2.193s, learning 0.100s)
             Mean action noise std: 3.26
          Mean value_function loss: 78.7269
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.5755
                       Mean reward: 921.15
               Mean episode length: 243.18
    Episode_Reward/reaching_object: 1.4318
     Episode_Reward/lifting_object: 178.5919
      Episode_Reward/object_height: 0.0302
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 2.29s
                      Time elapsed: 01:07:43
                               ETA: 00:07:01

################################################################################
                     [1m Learning iteration 1813/2000 [0m                     

                       Computation: 44856 steps/s (collection: 2.081s, learning 0.111s)
             Mean action noise std: 3.26
          Mean value_function loss: 80.2762
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.5864
                       Mean reward: 911.78
               Mean episode length: 239.46
    Episode_Reward/reaching_object: 1.4460
     Episode_Reward/lifting_object: 180.2031
      Episode_Reward/object_height: 0.0308
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 2.19s
                      Time elapsed: 01:07:45
                               ETA: 00:06:59

################################################################################
                     [1m Learning iteration 1814/2000 [0m                     

                       Computation: 42216 steps/s (collection: 2.198s, learning 0.130s)
             Mean action noise std: 3.26
          Mean value_function loss: 102.8403
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 56.5905
                       Mean reward: 856.77
               Mean episode length: 228.04
    Episode_Reward/reaching_object: 1.3939
     Episode_Reward/lifting_object: 173.8629
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 2.33s
                      Time elapsed: 01:07:47
                               ETA: 00:06:56

################################################################################
                     [1m Learning iteration 1815/2000 [0m                     

                       Computation: 44335 steps/s (collection: 2.082s, learning 0.135s)
             Mean action noise std: 3.26
          Mean value_function loss: 121.2235
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 56.5975
                       Mean reward: 881.57
               Mean episode length: 232.57
    Episode_Reward/reaching_object: 1.3938
     Episode_Reward/lifting_object: 173.9061
      Episode_Reward/object_height: 0.0296
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 2.22s
                      Time elapsed: 01:07:50
                               ETA: 00:06:54

################################################################################
                     [1m Learning iteration 1816/2000 [0m                     

                       Computation: 46175 steps/s (collection: 2.041s, learning 0.088s)
             Mean action noise std: 3.27
          Mean value_function loss: 92.0566
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.6090
                       Mean reward: 922.26
               Mean episode length: 242.88
    Episode_Reward/reaching_object: 1.4230
     Episode_Reward/lifting_object: 178.5738
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 2.13s
                      Time elapsed: 01:07:52
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1817/2000 [0m                     

                       Computation: 46738 steps/s (collection: 1.995s, learning 0.108s)
             Mean action noise std: 3.27
          Mean value_function loss: 97.7476
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 56.6206
                       Mean reward: 871.79
               Mean episode length: 231.05
    Episode_Reward/reaching_object: 1.4171
     Episode_Reward/lifting_object: 177.1023
      Episode_Reward/object_height: 0.0296
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 2.10s
                      Time elapsed: 01:07:54
                               ETA: 00:06:50

################################################################################
                     [1m Learning iteration 1818/2000 [0m                     

                       Computation: 46411 steps/s (collection: 2.027s, learning 0.092s)
             Mean action noise std: 3.27
          Mean value_function loss: 95.4260
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.6330
                       Mean reward: 897.36
               Mean episode length: 237.06
    Episode_Reward/reaching_object: 1.4207
     Episode_Reward/lifting_object: 177.4301
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 2.12s
                      Time elapsed: 01:07:56
                               ETA: 00:06:47

################################################################################
                     [1m Learning iteration 1819/2000 [0m                     

                       Computation: 45879 steps/s (collection: 2.029s, learning 0.114s)
             Mean action noise std: 3.27
          Mean value_function loss: 108.3499
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 56.6412
                       Mean reward: 920.16
               Mean episode length: 242.48
    Episode_Reward/reaching_object: 1.4067
     Episode_Reward/lifting_object: 176.4070
      Episode_Reward/object_height: 0.0287
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 2.14s
                      Time elapsed: 01:07:58
                               ETA: 00:06:45

################################################################################
                     [1m Learning iteration 1820/2000 [0m                     

                       Computation: 44900 steps/s (collection: 2.080s, learning 0.110s)
             Mean action noise std: 3.27
          Mean value_function loss: 72.5304
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 56.6446
                       Mean reward: 916.84
               Mean episode length: 242.49
    Episode_Reward/reaching_object: 1.4301
     Episode_Reward/lifting_object: 178.7926
      Episode_Reward/object_height: 0.0288
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 2.19s
                      Time elapsed: 01:08:00
                               ETA: 00:06:43

################################################################################
                     [1m Learning iteration 1821/2000 [0m                     

                       Computation: 44522 steps/s (collection: 2.120s, learning 0.088s)
             Mean action noise std: 3.27
          Mean value_function loss: 97.0282
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 56.6473
                       Mean reward: 877.65
               Mean episode length: 232.73
    Episode_Reward/reaching_object: 1.3996
     Episode_Reward/lifting_object: 176.1929
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 2.21s
                      Time elapsed: 01:08:02
                               ETA: 00:06:41

################################################################################
                     [1m Learning iteration 1822/2000 [0m                     

                       Computation: 46167 steps/s (collection: 2.040s, learning 0.090s)
             Mean action noise std: 3.27
          Mean value_function loss: 125.3567
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.6561
                       Mean reward: 925.37
               Mean episode length: 243.43
    Episode_Reward/reaching_object: 1.4172
     Episode_Reward/lifting_object: 177.7752
      Episode_Reward/object_height: 0.0278
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 2.13s
                      Time elapsed: 01:08:05
                               ETA: 00:06:38

################################################################################
                     [1m Learning iteration 1823/2000 [0m                     

                       Computation: 46247 steps/s (collection: 2.037s, learning 0.089s)
             Mean action noise std: 3.27
          Mean value_function loss: 97.1141
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.6699
                       Mean reward: 860.40
               Mean episode length: 228.53
    Episode_Reward/reaching_object: 1.3956
     Episode_Reward/lifting_object: 175.1546
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 2.13s
                      Time elapsed: 01:08:07
                               ETA: 00:06:36

################################################################################
                     [1m Learning iteration 1824/2000 [0m                     

                       Computation: 41504 steps/s (collection: 2.224s, learning 0.144s)
             Mean action noise std: 3.28
          Mean value_function loss: 100.0846
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.6776
                       Mean reward: 925.24
               Mean episode length: 244.00
    Episode_Reward/reaching_object: 1.4108
     Episode_Reward/lifting_object: 176.9324
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 2.37s
                      Time elapsed: 01:08:09
                               ETA: 00:06:34

################################################################################
                     [1m Learning iteration 1825/2000 [0m                     

                       Computation: 41698 steps/s (collection: 2.265s, learning 0.093s)
             Mean action noise std: 3.28
          Mean value_function loss: 107.1430
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.6839
                       Mean reward: 891.97
               Mean episode length: 240.72
    Episode_Reward/reaching_object: 1.4009
     Episode_Reward/lifting_object: 174.6717
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0564
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 2.36s
                      Time elapsed: 01:08:11
                               ETA: 00:06:32

################################################################################
                     [1m Learning iteration 1826/2000 [0m                     

                       Computation: 45575 steps/s (collection: 2.045s, learning 0.112s)
             Mean action noise std: 3.28
          Mean value_function loss: 114.7500
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.6892
                       Mean reward: 896.78
               Mean episode length: 236.65
    Episode_Reward/reaching_object: 1.4048
     Episode_Reward/lifting_object: 176.0465
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0565
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 2.16s
                      Time elapsed: 01:08:14
                               ETA: 00:06:29

################################################################################
                     [1m Learning iteration 1827/2000 [0m                     

                       Computation: 45512 steps/s (collection: 2.046s, learning 0.114s)
             Mean action noise std: 3.28
          Mean value_function loss: 114.4608
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.6966
                       Mean reward: 857.87
               Mean episode length: 228.57
    Episode_Reward/reaching_object: 1.4055
     Episode_Reward/lifting_object: 176.7375
      Episode_Reward/object_height: 0.0268
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 2.16s
                      Time elapsed: 01:08:16
                               ETA: 00:06:27

################################################################################
                     [1m Learning iteration 1828/2000 [0m                     

                       Computation: 41631 steps/s (collection: 2.266s, learning 0.096s)
             Mean action noise std: 3.28
          Mean value_function loss: 118.5776
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 56.7065
                       Mean reward: 886.88
               Mean episode length: 234.31
    Episode_Reward/reaching_object: 1.3999
     Episode_Reward/lifting_object: 176.2091
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0565
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 2.36s
                      Time elapsed: 01:08:18
                               ETA: 00:06:25

################################################################################
                     [1m Learning iteration 1829/2000 [0m                     

                       Computation: 45801 steps/s (collection: 2.033s, learning 0.114s)
             Mean action noise std: 3.28
          Mean value_function loss: 111.0893
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.7125
                       Mean reward: 890.69
               Mean episode length: 234.27
    Episode_Reward/reaching_object: 1.4110
     Episode_Reward/lifting_object: 178.0515
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 2.15s
                      Time elapsed: 01:08:20
                               ETA: 00:06:23

################################################################################
                     [1m Learning iteration 1830/2000 [0m                     

                       Computation: 42321 steps/s (collection: 2.185s, learning 0.138s)
             Mean action noise std: 3.28
          Mean value_function loss: 89.6550
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.7181
                       Mean reward: 845.13
               Mean episode length: 225.66
    Episode_Reward/reaching_object: 1.3923
     Episode_Reward/lifting_object: 174.9645
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 2.32s
                      Time elapsed: 01:08:23
                               ETA: 00:06:20

################################################################################
                     [1m Learning iteration 1831/2000 [0m                     

                       Computation: 45776 steps/s (collection: 2.058s, learning 0.090s)
             Mean action noise std: 3.28
          Mean value_function loss: 59.6926
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.7217
                       Mean reward: 894.46
               Mean episode length: 235.37
    Episode_Reward/reaching_object: 1.4340
     Episode_Reward/lifting_object: 181.0950
      Episode_Reward/object_height: 0.0278
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 2.15s
                      Time elapsed: 01:08:25
                               ETA: 00:06:18

################################################################################
                     [1m Learning iteration 1832/2000 [0m                     

                       Computation: 45827 steps/s (collection: 2.060s, learning 0.086s)
             Mean action noise std: 3.28
          Mean value_function loss: 108.0628
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.7296
                       Mean reward: 922.26
               Mean episode length: 243.62
    Episode_Reward/reaching_object: 1.4359
     Episode_Reward/lifting_object: 181.3033
      Episode_Reward/object_height: 0.0283
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 2.15s
                      Time elapsed: 01:08:27
                               ETA: 00:06:16

################################################################################
                     [1m Learning iteration 1833/2000 [0m                     

                       Computation: 45641 steps/s (collection: 2.067s, learning 0.087s)
             Mean action noise std: 3.29
          Mean value_function loss: 101.7153
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 56.7370
                       Mean reward: 903.14
               Mean episode length: 238.48
    Episode_Reward/reaching_object: 1.4209
     Episode_Reward/lifting_object: 178.8332
      Episode_Reward/object_height: 0.0276
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 2.15s
                      Time elapsed: 01:08:29
                               ETA: 00:06:14

################################################################################
                     [1m Learning iteration 1834/2000 [0m                     

                       Computation: 46110 steps/s (collection: 2.040s, learning 0.092s)
             Mean action noise std: 3.29
          Mean value_function loss: 151.2350
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.7444
                       Mean reward: 888.98
               Mean episode length: 234.57
    Episode_Reward/reaching_object: 1.4054
     Episode_Reward/lifting_object: 176.9964
      Episode_Reward/object_height: 0.0276
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 2.13s
                      Time elapsed: 01:08:31
                               ETA: 00:06:11

################################################################################
                     [1m Learning iteration 1835/2000 [0m                     

                       Computation: 43655 steps/s (collection: 2.133s, learning 0.119s)
             Mean action noise std: 3.29
          Mean value_function loss: 82.2258
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.7567
                       Mean reward: 895.52
               Mean episode length: 236.81
    Episode_Reward/reaching_object: 1.4284
     Episode_Reward/lifting_object: 180.2231
      Episode_Reward/object_height: 0.0276
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 2.25s
                      Time elapsed: 01:08:33
                               ETA: 00:06:09

################################################################################
                     [1m Learning iteration 1836/2000 [0m                     

                       Computation: 44919 steps/s (collection: 2.083s, learning 0.106s)
             Mean action noise std: 3.29
          Mean value_function loss: 100.9759
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 56.7682
                       Mean reward: 852.07
               Mean episode length: 226.68
    Episode_Reward/reaching_object: 1.4045
     Episode_Reward/lifting_object: 176.7042
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0572
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 2.19s
                      Time elapsed: 01:08:36
                               ETA: 00:06:07

################################################################################
                     [1m Learning iteration 1837/2000 [0m                     

                       Computation: 39965 steps/s (collection: 2.299s, learning 0.161s)
             Mean action noise std: 3.29
          Mean value_function loss: 80.1027
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.7771
                       Mean reward: 880.68
               Mean episode length: 232.67
    Episode_Reward/reaching_object: 1.4054
     Episode_Reward/lifting_object: 176.9270
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 2.46s
                      Time elapsed: 01:08:38
                               ETA: 00:06:05

################################################################################
                     [1m Learning iteration 1838/2000 [0m                     

                       Computation: 42273 steps/s (collection: 2.212s, learning 0.114s)
             Mean action noise std: 3.29
          Mean value_function loss: 127.6194
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.7841
                       Mean reward: 868.01
               Mean episode length: 229.70
    Episode_Reward/reaching_object: 1.4066
     Episode_Reward/lifting_object: 176.9137
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 2.33s
                      Time elapsed: 01:08:40
                               ETA: 00:06:03

################################################################################
                     [1m Learning iteration 1839/2000 [0m                     

                       Computation: 44829 steps/s (collection: 2.048s, learning 0.145s)
             Mean action noise std: 3.29
          Mean value_function loss: 125.6762
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 56.7929
                       Mean reward: 883.43
               Mean episode length: 232.90
    Episode_Reward/reaching_object: 1.3828
     Episode_Reward/lifting_object: 174.2452
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 2.19s
                      Time elapsed: 01:08:43
                               ETA: 00:06:00

################################################################################
                     [1m Learning iteration 1840/2000 [0m                     

                       Computation: 45867 steps/s (collection: 2.040s, learning 0.103s)
             Mean action noise std: 3.30
          Mean value_function loss: 86.4749
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.8005
                       Mean reward: 915.03
               Mean episode length: 241.28
    Episode_Reward/reaching_object: 1.3943
     Episode_Reward/lifting_object: 175.6277
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 2.14s
                      Time elapsed: 01:08:45
                               ETA: 00:05:58

################################################################################
                     [1m Learning iteration 1841/2000 [0m                     

                       Computation: 40963 steps/s (collection: 2.237s, learning 0.163s)
             Mean action noise std: 3.30
          Mean value_function loss: 81.7751
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.8060
                       Mean reward: 928.74
               Mean episode length: 243.45
    Episode_Reward/reaching_object: 1.4229
     Episode_Reward/lifting_object: 180.2737
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 2.40s
                      Time elapsed: 01:08:47
                               ETA: 00:05:56

################################################################################
                     [1m Learning iteration 1842/2000 [0m                     

                       Computation: 39803 steps/s (collection: 2.268s, learning 0.202s)
             Mean action noise std: 3.30
          Mean value_function loss: 109.5210
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 56.8134
                       Mean reward: 884.46
               Mean episode length: 236.39
    Episode_Reward/reaching_object: 1.3801
     Episode_Reward/lifting_object: 173.4740
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0565
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 2.47s
                      Time elapsed: 01:08:50
                               ETA: 00:05:54

################################################################################
                     [1m Learning iteration 1843/2000 [0m                     

                       Computation: 42244 steps/s (collection: 2.204s, learning 0.123s)
             Mean action noise std: 3.30
          Mean value_function loss: 115.4410
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 56.8246
                       Mean reward: 907.77
               Mean episode length: 238.64
    Episode_Reward/reaching_object: 1.4264
     Episode_Reward/lifting_object: 179.6856
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 2.33s
                      Time elapsed: 01:08:52
                               ETA: 00:05:51

################################################################################
                     [1m Learning iteration 1844/2000 [0m                     

                       Computation: 46067 steps/s (collection: 2.033s, learning 0.101s)
             Mean action noise std: 3.30
          Mean value_function loss: 89.3492
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.8365
                       Mean reward: 869.77
               Mean episode length: 230.67
    Episode_Reward/reaching_object: 1.3861
     Episode_Reward/lifting_object: 173.9734
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 2.13s
                      Time elapsed: 01:08:54
                               ETA: 00:05:49

################################################################################
                     [1m Learning iteration 1845/2000 [0m                     

                       Computation: 47518 steps/s (collection: 1.984s, learning 0.085s)
             Mean action noise std: 3.30
          Mean value_function loss: 93.4467
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.8431
                       Mean reward: 908.77
               Mean episode length: 239.54
    Episode_Reward/reaching_object: 1.4156
     Episode_Reward/lifting_object: 178.6192
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 2.07s
                      Time elapsed: 01:08:56
                               ETA: 00:05:47

################################################################################
                     [1m Learning iteration 1846/2000 [0m                     

                       Computation: 46894 steps/s (collection: 1.992s, learning 0.105s)
             Mean action noise std: 3.30
          Mean value_function loss: 109.2257
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.8549
                       Mean reward: 888.18
               Mean episode length: 234.44
    Episode_Reward/reaching_object: 1.4069
     Episode_Reward/lifting_object: 177.4440
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 2.10s
                      Time elapsed: 01:08:58
                               ETA: 00:05:45

################################################################################
                     [1m Learning iteration 1847/2000 [0m                     

                       Computation: 45506 steps/s (collection: 2.021s, learning 0.139s)
             Mean action noise std: 3.31
          Mean value_function loss: 100.8134
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.8631
                       Mean reward: 856.77
               Mean episode length: 227.08
    Episode_Reward/reaching_object: 1.3896
     Episode_Reward/lifting_object: 174.6382
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 2.16s
                      Time elapsed: 01:09:00
                               ETA: 00:05:42

################################################################################
                     [1m Learning iteration 1848/2000 [0m                     

                       Computation: 46454 steps/s (collection: 1.988s, learning 0.128s)
             Mean action noise std: 3.31
          Mean value_function loss: 104.2893
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.8720
                       Mean reward: 850.52
               Mean episode length: 226.42
    Episode_Reward/reaching_object: 1.4190
     Episode_Reward/lifting_object: 178.6139
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 2.12s
                      Time elapsed: 01:09:03
                               ETA: 00:05:40

################################################################################
                     [1m Learning iteration 1849/2000 [0m                     

                       Computation: 47565 steps/s (collection: 1.973s, learning 0.094s)
             Mean action noise std: 3.31
          Mean value_function loss: 113.4283
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 56.8855
                       Mean reward: 877.53
               Mean episode length: 232.30
    Episode_Reward/reaching_object: 1.3917
     Episode_Reward/lifting_object: 174.9841
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 2.07s
                      Time elapsed: 01:09:05
                               ETA: 00:05:38

################################################################################
                     [1m Learning iteration 1850/2000 [0m                     

                       Computation: 46484 steps/s (collection: 2.013s, learning 0.102s)
             Mean action noise std: 3.31
          Mean value_function loss: 77.9251
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.8944
                       Mean reward: 900.48
               Mean episode length: 237.23
    Episode_Reward/reaching_object: 1.4129
     Episode_Reward/lifting_object: 178.6548
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 2.11s
                      Time elapsed: 01:09:07
                               ETA: 00:05:36

################################################################################
                     [1m Learning iteration 1851/2000 [0m                     

                       Computation: 46242 steps/s (collection: 2.032s, learning 0.094s)
             Mean action noise std: 3.31
          Mean value_function loss: 81.1106
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.9029
                       Mean reward: 886.11
               Mean episode length: 234.07
    Episode_Reward/reaching_object: 1.4037
     Episode_Reward/lifting_object: 177.4021
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 2.13s
                      Time elapsed: 01:09:09
                               ETA: 00:05:33

################################################################################
                     [1m Learning iteration 1852/2000 [0m                     

                       Computation: 45412 steps/s (collection: 2.066s, learning 0.099s)
             Mean action noise std: 3.31
          Mean value_function loss: 108.6269
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.9128
                       Mean reward: 902.67
               Mean episode length: 238.05
    Episode_Reward/reaching_object: 1.4099
     Episode_Reward/lifting_object: 177.7895
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 2.16s
                      Time elapsed: 01:09:11
                               ETA: 00:05:31

################################################################################
                     [1m Learning iteration 1853/2000 [0m                     

                       Computation: 43722 steps/s (collection: 2.110s, learning 0.139s)
             Mean action noise std: 3.31
          Mean value_function loss: 105.0619
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.9234
                       Mean reward: 842.34
               Mean episode length: 225.26
    Episode_Reward/reaching_object: 1.4006
     Episode_Reward/lifting_object: 176.5486
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 2.25s
                      Time elapsed: 01:09:13
                               ETA: 00:05:29

################################################################################
                     [1m Learning iteration 1854/2000 [0m                     

                       Computation: 40929 steps/s (collection: 2.210s, learning 0.192s)
             Mean action noise std: 3.32
          Mean value_function loss: 122.7857
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.9339
                       Mean reward: 887.86
               Mean episode length: 234.60
    Episode_Reward/reaching_object: 1.4029
     Episode_Reward/lifting_object: 177.2115
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 2.40s
                      Time elapsed: 01:09:16
                               ETA: 00:05:27

################################################################################
                     [1m Learning iteration 1855/2000 [0m                     

                       Computation: 44605 steps/s (collection: 2.118s, learning 0.086s)
             Mean action noise std: 3.32
          Mean value_function loss: 90.6242
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.9433
                       Mean reward: 889.18
               Mean episode length: 235.25
    Episode_Reward/reaching_object: 1.4071
     Episode_Reward/lifting_object: 177.5587
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 2.20s
                      Time elapsed: 01:09:18
                               ETA: 00:05:24

################################################################################
                     [1m Learning iteration 1856/2000 [0m                     

                       Computation: 46654 steps/s (collection: 2.019s, learning 0.088s)
             Mean action noise std: 3.32
          Mean value_function loss: 97.6171
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 56.9484
                       Mean reward: 922.19
               Mean episode length: 243.35
    Episode_Reward/reaching_object: 1.4056
     Episode_Reward/lifting_object: 177.7108
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 2.11s
                      Time elapsed: 01:09:20
                               ETA: 00:05:22

################################################################################
                     [1m Learning iteration 1857/2000 [0m                     

                       Computation: 43614 steps/s (collection: 2.169s, learning 0.085s)
             Mean action noise std: 3.32
          Mean value_function loss: 129.8305
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 56.9548
                       Mean reward: 881.14
               Mean episode length: 233.15
    Episode_Reward/reaching_object: 1.3983
     Episode_Reward/lifting_object: 176.4281
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 2.25s
                      Time elapsed: 01:09:22
                               ETA: 00:05:20

################################################################################
                     [1m Learning iteration 1858/2000 [0m                     

                       Computation: 46402 steps/s (collection: 2.029s, learning 0.090s)
             Mean action noise std: 3.32
          Mean value_function loss: 104.8073
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.9638
                       Mean reward: 908.02
               Mean episode length: 239.25
    Episode_Reward/reaching_object: 1.4256
     Episode_Reward/lifting_object: 180.2199
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.0590
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 2.12s
                      Time elapsed: 01:09:24
                               ETA: 00:05:18

################################################################################
                     [1m Learning iteration 1859/2000 [0m                     

                       Computation: 45417 steps/s (collection: 2.061s, learning 0.104s)
             Mean action noise std: 3.32
          Mean value_function loss: 102.0315
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.9714
                       Mean reward: 884.48
               Mean episode length: 234.64
    Episode_Reward/reaching_object: 1.4262
     Episode_Reward/lifting_object: 179.6795
      Episode_Reward/object_height: 0.0272
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 2.16s
                      Time elapsed: 01:09:26
                               ETA: 00:05:15

################################################################################
                     [1m Learning iteration 1860/2000 [0m                     

                       Computation: 45663 steps/s (collection: 2.020s, learning 0.133s)
             Mean action noise std: 3.32
          Mean value_function loss: 131.3466
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 56.9806
                       Mean reward: 868.08
               Mean episode length: 229.40
    Episode_Reward/reaching_object: 1.3759
     Episode_Reward/lifting_object: 174.1758
      Episode_Reward/object_height: 0.0273
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 2.15s
                      Time elapsed: 01:09:29
                               ETA: 00:05:13

################################################################################
                     [1m Learning iteration 1861/2000 [0m                     

                       Computation: 47018 steps/s (collection: 1.999s, learning 0.092s)
             Mean action noise std: 3.32
          Mean value_function loss: 112.7871
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 56.9892
                       Mean reward: 869.03
               Mean episode length: 230.84
    Episode_Reward/reaching_object: 1.3686
     Episode_Reward/lifting_object: 172.5212
      Episode_Reward/object_height: 0.0275
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 2.09s
                      Time elapsed: 01:09:31
                               ETA: 00:05:11

################################################################################
                     [1m Learning iteration 1862/2000 [0m                     

                       Computation: 47077 steps/s (collection: 1.984s, learning 0.104s)
             Mean action noise std: 3.32
          Mean value_function loss: 106.1540
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.9922
                       Mean reward: 851.12
               Mean episode length: 227.24
    Episode_Reward/reaching_object: 1.3778
     Episode_Reward/lifting_object: 173.3620
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 2.09s
                      Time elapsed: 01:09:33
                               ETA: 00:05:09

################################################################################
                     [1m Learning iteration 1863/2000 [0m                     

                       Computation: 46711 steps/s (collection: 2.019s, learning 0.085s)
             Mean action noise std: 3.33
          Mean value_function loss: 110.5455
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.9968
                       Mean reward: 876.86
               Mean episode length: 232.05
    Episode_Reward/reaching_object: 1.4041
     Episode_Reward/lifting_object: 176.7551
      Episode_Reward/object_height: 0.0285
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 2.10s
                      Time elapsed: 01:09:35
                               ETA: 00:05:06

################################################################################
                     [1m Learning iteration 1864/2000 [0m                     

                       Computation: 47134 steps/s (collection: 2.000s, learning 0.086s)
             Mean action noise std: 3.33
          Mean value_function loss: 118.9377
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 57.0070
                       Mean reward: 900.57
               Mean episode length: 237.93
    Episode_Reward/reaching_object: 1.3836
     Episode_Reward/lifting_object: 173.6516
      Episode_Reward/object_height: 0.0278
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 2.09s
                      Time elapsed: 01:09:37
                               ETA: 00:05:04

################################################################################
                     [1m Learning iteration 1865/2000 [0m                     

                       Computation: 47555 steps/s (collection: 1.976s, learning 0.092s)
             Mean action noise std: 3.33
          Mean value_function loss: 118.3112
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.0195
                       Mean reward: 889.76
               Mean episode length: 234.64
    Episode_Reward/reaching_object: 1.3770
     Episode_Reward/lifting_object: 174.0815
      Episode_Reward/object_height: 0.0276
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 2.07s
                      Time elapsed: 01:09:39
                               ETA: 00:05:02

################################################################################
                     [1m Learning iteration 1866/2000 [0m                     

                       Computation: 46581 steps/s (collection: 2.021s, learning 0.089s)
             Mean action noise std: 3.33
          Mean value_function loss: 103.6822
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.0283
                       Mean reward: 845.87
               Mean episode length: 225.48
    Episode_Reward/reaching_object: 1.3721
     Episode_Reward/lifting_object: 173.3296
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 2.11s
                      Time elapsed: 01:09:41
                               ETA: 00:05:00

################################################################################
                     [1m Learning iteration 1867/2000 [0m                     

                       Computation: 48127 steps/s (collection: 1.956s, learning 0.087s)
             Mean action noise std: 3.33
          Mean value_function loss: 132.5032
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 57.0370
                       Mean reward: 905.19
               Mean episode length: 239.60
    Episode_Reward/reaching_object: 1.3745
     Episode_Reward/lifting_object: 172.4740
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0576
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 2.04s
                      Time elapsed: 01:09:43
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 1868/2000 [0m                     

                       Computation: 47043 steps/s (collection: 2.003s, learning 0.087s)
             Mean action noise std: 3.33
          Mean value_function loss: 100.7630
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.0492
                       Mean reward: 917.75
               Mean episode length: 241.35
    Episode_Reward/reaching_object: 1.4094
     Episode_Reward/lifting_object: 177.9935
      Episode_Reward/object_height: 0.0271
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 2.09s
                      Time elapsed: 01:09:45
                               ETA: 00:04:55

################################################################################
                     [1m Learning iteration 1869/2000 [0m                     

                       Computation: 42442 steps/s (collection: 2.182s, learning 0.134s)
             Mean action noise std: 3.34
          Mean value_function loss: 103.7648
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 57.0566
                       Mean reward: 899.69
               Mean episode length: 236.98
    Episode_Reward/reaching_object: 1.3985
     Episode_Reward/lifting_object: 176.5008
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 2.32s
                      Time elapsed: 01:09:48
                               ETA: 00:04:53

################################################################################
                     [1m Learning iteration 1870/2000 [0m                     

                       Computation: 46695 steps/s (collection: 2.016s, learning 0.089s)
             Mean action noise std: 3.34
          Mean value_function loss: 91.7591
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.0674
                       Mean reward: 858.88
               Mean episode length: 228.44
    Episode_Reward/reaching_object: 1.4000
     Episode_Reward/lifting_object: 176.5498
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 2.11s
                      Time elapsed: 01:09:50
                               ETA: 00:04:51

################################################################################
                     [1m Learning iteration 1871/2000 [0m                     

                       Computation: 44296 steps/s (collection: 2.111s, learning 0.108s)
             Mean action noise std: 3.34
          Mean value_function loss: 119.8456
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.0785
                       Mean reward: 848.98
               Mean episode length: 224.82
    Episode_Reward/reaching_object: 1.4022
     Episode_Reward/lifting_object: 177.2888
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 2.22s
                      Time elapsed: 01:09:52
                               ETA: 00:04:48

################################################################################
                     [1m Learning iteration 1872/2000 [0m                     

                       Computation: 45836 steps/s (collection: 2.058s, learning 0.087s)
             Mean action noise std: 3.34
          Mean value_function loss: 100.7544
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.0891
                       Mean reward: 921.42
               Mean episode length: 242.79
    Episode_Reward/reaching_object: 1.4018
     Episode_Reward/lifting_object: 176.7440
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 2.14s
                      Time elapsed: 01:09:54
                               ETA: 00:04:46

################################################################################
                     [1m Learning iteration 1873/2000 [0m                     

                       Computation: 47808 steps/s (collection: 1.968s, learning 0.088s)
             Mean action noise std: 3.34
          Mean value_function loss: 106.1943
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.0978
                       Mean reward: 891.88
               Mean episode length: 235.60
    Episode_Reward/reaching_object: 1.4076
     Episode_Reward/lifting_object: 176.8617
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 2.06s
                      Time elapsed: 01:09:56
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 1874/2000 [0m                     

                       Computation: 47068 steps/s (collection: 2.000s, learning 0.089s)
             Mean action noise std: 3.34
          Mean value_function loss: 122.3932
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 57.1040
                       Mean reward: 889.37
               Mean episode length: 236.27
    Episode_Reward/reaching_object: 1.3819
     Episode_Reward/lifting_object: 174.2703
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0584
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 2.09s
                      Time elapsed: 01:09:58
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 1875/2000 [0m                     

                       Computation: 47075 steps/s (collection: 1.994s, learning 0.094s)
             Mean action noise std: 3.35
          Mean value_function loss: 134.2067
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 57.1152
                       Mean reward: 855.33
               Mean episode length: 227.16
    Episode_Reward/reaching_object: 1.3731
     Episode_Reward/lifting_object: 173.5933
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 2.09s
                      Time elapsed: 01:10:00
                               ETA: 00:04:39

################################################################################
                     [1m Learning iteration 1876/2000 [0m                     

                       Computation: 46334 steps/s (collection: 2.012s, learning 0.110s)
             Mean action noise std: 3.35
          Mean value_function loss: 130.7980
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.1295
                       Mean reward: 913.23
               Mean episode length: 240.85
    Episode_Reward/reaching_object: 1.3835
     Episode_Reward/lifting_object: 174.7771
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 2.12s
                      Time elapsed: 01:10:02
                               ETA: 00:04:37

################################################################################
                     [1m Learning iteration 1877/2000 [0m                     

                       Computation: 45934 steps/s (collection: 2.021s, learning 0.119s)
             Mean action noise std: 3.35
          Mean value_function loss: 126.3810
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.1410
                       Mean reward: 890.50
               Mean episode length: 235.59
    Episode_Reward/reaching_object: 1.3726
     Episode_Reward/lifting_object: 173.3554
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 2.14s
                      Time elapsed: 01:10:05
                               ETA: 00:04:35

################################################################################
                     [1m Learning iteration 1878/2000 [0m                     

                       Computation: 46251 steps/s (collection: 1.987s, learning 0.138s)
             Mean action noise std: 3.35
          Mean value_function loss: 125.1775
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 57.1486
                       Mean reward: 872.73
               Mean episode length: 231.55
    Episode_Reward/reaching_object: 1.4131
     Episode_Reward/lifting_object: 178.5682
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 2.13s
                      Time elapsed: 01:10:07
                               ETA: 00:04:33

################################################################################
                     [1m Learning iteration 1879/2000 [0m                     

                       Computation: 46578 steps/s (collection: 1.969s, learning 0.142s)
             Mean action noise std: 3.35
          Mean value_function loss: 99.3649
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.1615
                       Mean reward: 892.86
               Mean episode length: 235.07
    Episode_Reward/reaching_object: 1.4079
     Episode_Reward/lifting_object: 177.7661
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 2.11s
                      Time elapsed: 01:10:09
                               ETA: 00:04:30

################################################################################
                     [1m Learning iteration 1880/2000 [0m                     

                       Computation: 46555 steps/s (collection: 2.026s, learning 0.086s)
             Mean action noise std: 3.35
          Mean value_function loss: 113.7278
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.1720
                       Mean reward: 900.98
               Mean episode length: 238.60
    Episode_Reward/reaching_object: 1.3883
     Episode_Reward/lifting_object: 174.7896
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 2.11s
                      Time elapsed: 01:10:11
                               ETA: 00:04:28

################################################################################
                     [1m Learning iteration 1881/2000 [0m                     

                       Computation: 46444 steps/s (collection: 2.026s, learning 0.091s)
             Mean action noise std: 3.36
          Mean value_function loss: 102.7721
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.1818
                       Mean reward: 913.19
               Mean episode length: 240.11
    Episode_Reward/reaching_object: 1.4164
     Episode_Reward/lifting_object: 178.6719
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 2.12s
                      Time elapsed: 01:10:13
                               ETA: 00:04:26

################################################################################
                     [1m Learning iteration 1882/2000 [0m                     

                       Computation: 46782 steps/s (collection: 2.015s, learning 0.087s)
             Mean action noise std: 3.36
          Mean value_function loss: 99.3411
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.1931
                       Mean reward: 903.57
               Mean episode length: 237.14
    Episode_Reward/reaching_object: 1.4117
     Episode_Reward/lifting_object: 178.2574
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 2.10s
                      Time elapsed: 01:10:15
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1883/2000 [0m                     

                       Computation: 46598 steps/s (collection: 2.021s, learning 0.089s)
             Mean action noise std: 3.36
          Mean value_function loss: 118.3217
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.2033
                       Mean reward: 884.52
               Mean episode length: 235.77
    Episode_Reward/reaching_object: 1.3955
     Episode_Reward/lifting_object: 175.6292
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 2.11s
                      Time elapsed: 01:10:17
                               ETA: 00:04:21

################################################################################
                     [1m Learning iteration 1884/2000 [0m                     

                       Computation: 47346 steps/s (collection: 1.981s, learning 0.096s)
             Mean action noise std: 3.36
          Mean value_function loss: 115.9356
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.2105
                       Mean reward: 852.83
               Mean episode length: 226.73
    Episode_Reward/reaching_object: 1.3863
     Episode_Reward/lifting_object: 174.6768
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 2.08s
                      Time elapsed: 01:10:19
                               ETA: 00:04:19

################################################################################
                     [1m Learning iteration 1885/2000 [0m                     

                       Computation: 45669 steps/s (collection: 2.052s, learning 0.101s)
             Mean action noise std: 3.36
          Mean value_function loss: 109.3757
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 57.2189
                       Mean reward: 889.51
               Mean episode length: 236.14
    Episode_Reward/reaching_object: 1.3892
     Episode_Reward/lifting_object: 174.4613
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0590
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 2.15s
                      Time elapsed: 01:10:21
                               ETA: 00:04:17

################################################################################
                     [1m Learning iteration 1886/2000 [0m                     

                       Computation: 46421 steps/s (collection: 2.002s, learning 0.116s)
             Mean action noise std: 3.36
          Mean value_function loss: 106.0090
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.2256
                       Mean reward: 896.62
               Mean episode length: 237.13
    Episode_Reward/reaching_object: 1.3931
     Episode_Reward/lifting_object: 175.2737
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 2.12s
                      Time elapsed: 01:10:24
                               ETA: 00:04:15

################################################################################
                     [1m Learning iteration 1887/2000 [0m                     

                       Computation: 45305 steps/s (collection: 2.035s, learning 0.135s)
             Mean action noise std: 3.36
          Mean value_function loss: 136.5183
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.2287
                       Mean reward: 901.16
               Mean episode length: 237.12
    Episode_Reward/reaching_object: 1.3985
     Episode_Reward/lifting_object: 175.7886
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 2.17s
                      Time elapsed: 01:10:26
                               ETA: 00:04:12

################################################################################
                     [1m Learning iteration 1888/2000 [0m                     

                       Computation: 46444 steps/s (collection: 2.023s, learning 0.094s)
             Mean action noise std: 3.36
          Mean value_function loss: 110.0844
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.2314
                       Mean reward: 875.51
               Mean episode length: 231.48
    Episode_Reward/reaching_object: 1.4074
     Episode_Reward/lifting_object: 177.0339
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 2.12s
                      Time elapsed: 01:10:28
                               ETA: 00:04:10

################################################################################
                     [1m Learning iteration 1889/2000 [0m                     

                       Computation: 47645 steps/s (collection: 1.970s, learning 0.094s)
             Mean action noise std: 3.36
          Mean value_function loss: 96.8104
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.2383
                       Mean reward: 881.37
               Mean episode length: 232.00
    Episode_Reward/reaching_object: 1.3925
     Episode_Reward/lifting_object: 175.0206
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 2.06s
                      Time elapsed: 01:10:30
                               ETA: 00:04:08

################################################################################
                     [1m Learning iteration 1890/2000 [0m                     

                       Computation: 45610 steps/s (collection: 2.018s, learning 0.138s)
             Mean action noise std: 3.37
          Mean value_function loss: 127.9270
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.2485
                       Mean reward: 867.22
               Mean episode length: 229.05
    Episode_Reward/reaching_object: 1.3753
     Episode_Reward/lifting_object: 172.4961
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 2.16s
                      Time elapsed: 01:10:32
                               ETA: 00:04:06

################################################################################
                     [1m Learning iteration 1891/2000 [0m                     

                       Computation: 46303 steps/s (collection: 2.029s, learning 0.095s)
             Mean action noise std: 3.37
          Mean value_function loss: 120.8352
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.2578
                       Mean reward: 923.40
               Mean episode length: 244.23
    Episode_Reward/reaching_object: 1.3932
     Episode_Reward/lifting_object: 174.8680
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 2.12s
                      Time elapsed: 01:10:34
                               ETA: 00:04:03

################################################################################
                     [1m Learning iteration 1892/2000 [0m                     

                       Computation: 46639 steps/s (collection: 1.986s, learning 0.122s)
             Mean action noise std: 3.37
          Mean value_function loss: 88.4185
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 57.2674
                       Mean reward: 916.15
               Mean episode length: 240.98
    Episode_Reward/reaching_object: 1.4159
     Episode_Reward/lifting_object: 178.4797
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 2.11s
                      Time elapsed: 01:10:36
                               ETA: 00:04:01

################################################################################
                     [1m Learning iteration 1893/2000 [0m                     

                       Computation: 45233 steps/s (collection: 2.047s, learning 0.127s)
             Mean action noise std: 3.37
          Mean value_function loss: 137.3864
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 57.2753
                       Mean reward: 848.97
               Mean episode length: 224.75
    Episode_Reward/reaching_object: 1.3588
     Episode_Reward/lifting_object: 170.4572
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 2.17s
                      Time elapsed: 01:10:39
                               ETA: 00:03:59

################################################################################
                     [1m Learning iteration 1894/2000 [0m                     

                       Computation: 46132 steps/s (collection: 2.038s, learning 0.093s)
             Mean action noise std: 3.37
          Mean value_function loss: 126.1354
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.2841
                       Mean reward: 881.88
               Mean episode length: 234.88
    Episode_Reward/reaching_object: 1.3594
     Episode_Reward/lifting_object: 170.6855
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 2.13s
                      Time elapsed: 01:10:41
                               ETA: 00:03:57

################################################################################
                     [1m Learning iteration 1895/2000 [0m                     

                       Computation: 46117 steps/s (collection: 2.033s, learning 0.099s)
             Mean action noise std: 3.37
          Mean value_function loss: 94.4654
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.2973
                       Mean reward: 894.75
               Mean episode length: 237.37
    Episode_Reward/reaching_object: 1.4202
     Episode_Reward/lifting_object: 179.0767
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 2.13s
                      Time elapsed: 01:10:43
                               ETA: 00:03:54

################################################################################
                     [1m Learning iteration 1896/2000 [0m                     

                       Computation: 46622 steps/s (collection: 1.986s, learning 0.123s)
             Mean action noise std: 3.37
          Mean value_function loss: 119.2745
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 57.3078
                       Mean reward: 899.51
               Mean episode length: 237.67
    Episode_Reward/reaching_object: 1.4163
     Episode_Reward/lifting_object: 178.3984
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 2.11s
                      Time elapsed: 01:10:45
                               ETA: 00:03:52

################################################################################
                     [1m Learning iteration 1897/2000 [0m                     

                       Computation: 45968 steps/s (collection: 2.036s, learning 0.102s)
             Mean action noise std: 3.38
          Mean value_function loss: 123.4707
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 57.3143
                       Mean reward: 898.99
               Mean episode length: 237.87
    Episode_Reward/reaching_object: 1.4022
     Episode_Reward/lifting_object: 176.4610
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 2.14s
                      Time elapsed: 01:10:47
                               ETA: 00:03:50

################################################################################
                     [1m Learning iteration 1898/2000 [0m                     

                       Computation: 47334 steps/s (collection: 1.987s, learning 0.090s)
             Mean action noise std: 3.38
          Mean value_function loss: 124.2742
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.3236
                       Mean reward: 903.03
               Mean episode length: 237.11
    Episode_Reward/reaching_object: 1.3867
     Episode_Reward/lifting_object: 174.3751
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 2.08s
                      Time elapsed: 01:10:49
                               ETA: 00:03:48

################################################################################
                     [1m Learning iteration 1899/2000 [0m                     

                       Computation: 46468 steps/s (collection: 2.031s, learning 0.084s)
             Mean action noise std: 3.38
          Mean value_function loss: 111.5291
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.3315
                       Mean reward: 878.31
               Mean episode length: 232.96
    Episode_Reward/reaching_object: 1.4038
     Episode_Reward/lifting_object: 176.0242
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 2.12s
                      Time elapsed: 01:10:51
                               ETA: 00:03:46

################################################################################
                     [1m Learning iteration 1900/2000 [0m                     

                       Computation: 46878 steps/s (collection: 2.005s, learning 0.092s)
             Mean action noise std: 3.38
          Mean value_function loss: 121.5381
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.3381
                       Mean reward: 852.77
               Mean episode length: 227.08
    Episode_Reward/reaching_object: 1.3669
     Episode_Reward/lifting_object: 171.8163
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0590
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 2.10s
                      Time elapsed: 01:10:53
                               ETA: 00:03:43

################################################################################
                     [1m Learning iteration 1901/2000 [0m                     

                       Computation: 44564 steps/s (collection: 2.109s, learning 0.097s)
             Mean action noise std: 3.38
          Mean value_function loss: 129.7329
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.3475
                       Mean reward: 878.36
               Mean episode length: 232.11
    Episode_Reward/reaching_object: 1.4146
     Episode_Reward/lifting_object: 178.0205
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 2.21s
                      Time elapsed: 01:10:56
                               ETA: 00:03:41

################################################################################
                     [1m Learning iteration 1902/2000 [0m                     

                       Computation: 45520 steps/s (collection: 2.065s, learning 0.095s)
             Mean action noise std: 3.38
          Mean value_function loss: 123.4931
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.3545
                       Mean reward: 877.01
               Mean episode length: 232.50
    Episode_Reward/reaching_object: 1.3930
     Episode_Reward/lifting_object: 174.6137
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 2.16s
                      Time elapsed: 01:10:58
                               ETA: 00:03:39

################################################################################
                     [1m Learning iteration 1903/2000 [0m                     

                       Computation: 44786 steps/s (collection: 2.099s, learning 0.096s)
             Mean action noise std: 3.38
          Mean value_function loss: 136.5929
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 57.3617
                       Mean reward: 886.71
               Mean episode length: 235.77
    Episode_Reward/reaching_object: 1.3539
     Episode_Reward/lifting_object: 169.5518
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 2.19s
                      Time elapsed: 01:11:00
                               ETA: 00:03:37

################################################################################
                     [1m Learning iteration 1904/2000 [0m                     

                       Computation: 45889 steps/s (collection: 2.041s, learning 0.102s)
             Mean action noise std: 3.38
          Mean value_function loss: 112.4548
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.3668
                       Mean reward: 896.05
               Mean episode length: 236.17
    Episode_Reward/reaching_object: 1.3778
     Episode_Reward/lifting_object: 172.8251
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 2.14s
                      Time elapsed: 01:11:02
                               ETA: 00:03:34

################################################################################
                     [1m Learning iteration 1905/2000 [0m                     

                       Computation: 46209 steps/s (collection: 2.036s, learning 0.092s)
             Mean action noise std: 3.39
          Mean value_function loss: 106.2715
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.3759
                       Mean reward: 884.16
               Mean episode length: 234.48
    Episode_Reward/reaching_object: 1.3999
     Episode_Reward/lifting_object: 175.5425
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 2.13s
                      Time elapsed: 01:11:04
                               ETA: 00:03:32

################################################################################
                     [1m Learning iteration 1906/2000 [0m                     

                       Computation: 46407 steps/s (collection: 2.025s, learning 0.093s)
             Mean action noise std: 3.39
          Mean value_function loss: 102.7052
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.3836
                       Mean reward: 879.67
               Mean episode length: 233.32
    Episode_Reward/reaching_object: 1.4078
     Episode_Reward/lifting_object: 176.6294
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 2.12s
                      Time elapsed: 01:11:06
                               ETA: 00:03:30

################################################################################
                     [1m Learning iteration 1907/2000 [0m                     

                       Computation: 46531 steps/s (collection: 2.024s, learning 0.089s)
             Mean action noise std: 3.39
          Mean value_function loss: 135.0037
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 57.3905
                       Mean reward: 866.05
               Mean episode length: 229.66
    Episode_Reward/reaching_object: 1.3508
     Episode_Reward/lifting_object: 169.4786
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 2.11s
                      Time elapsed: 01:11:08
                               ETA: 00:03:28

################################################################################
                     [1m Learning iteration 1908/2000 [0m                     

                       Computation: 44490 steps/s (collection: 2.117s, learning 0.093s)
             Mean action noise std: 3.39
          Mean value_function loss: 116.5469
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.3975
                       Mean reward: 886.84
               Mean episode length: 236.60
    Episode_Reward/reaching_object: 1.4085
     Episode_Reward/lifting_object: 176.5945
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 2.21s
                      Time elapsed: 01:11:11
                               ETA: 00:03:25

################################################################################
                     [1m Learning iteration 1909/2000 [0m                     

                       Computation: 45732 steps/s (collection: 2.054s, learning 0.096s)
             Mean action noise std: 3.39
          Mean value_function loss: 115.6176
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.4058
                       Mean reward: 889.78
               Mean episode length: 235.85
    Episode_Reward/reaching_object: 1.3955
     Episode_Reward/lifting_object: 174.8828
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 2.15s
                      Time elapsed: 01:11:13
                               ETA: 00:03:23

################################################################################
                     [1m Learning iteration 1910/2000 [0m                     

                       Computation: 44547 steps/s (collection: 2.083s, learning 0.124s)
             Mean action noise std: 3.39
          Mean value_function loss: 133.3922
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 57.4114
                       Mean reward: 876.34
               Mean episode length: 231.61
    Episode_Reward/reaching_object: 1.3784
     Episode_Reward/lifting_object: 172.7126
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0596
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 2.21s
                      Time elapsed: 01:11:15
                               ETA: 00:03:21

################################################################################
                     [1m Learning iteration 1911/2000 [0m                     

                       Computation: 45679 steps/s (collection: 2.044s, learning 0.108s)
             Mean action noise std: 3.39
          Mean value_function loss: 130.3294
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.4146
                       Mean reward: 900.34
               Mean episode length: 237.72
    Episode_Reward/reaching_object: 1.3853
     Episode_Reward/lifting_object: 173.9355
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 2.15s
                      Time elapsed: 01:11:17
                               ETA: 00:03:19

################################################################################
                     [1m Learning iteration 1912/2000 [0m                     

                       Computation: 45663 steps/s (collection: 2.040s, learning 0.113s)
             Mean action noise std: 3.39
          Mean value_function loss: 128.9213
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.4235
                       Mean reward: 868.54
               Mean episode length: 230.89
    Episode_Reward/reaching_object: 1.3833
     Episode_Reward/lifting_object: 172.8388
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 2.15s
                      Time elapsed: 01:11:19
                               ETA: 00:03:16

################################################################################
                     [1m Learning iteration 1913/2000 [0m                     

                       Computation: 45994 steps/s (collection: 2.039s, learning 0.099s)
             Mean action noise std: 3.40
          Mean value_function loss: 97.3722
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.4329
                       Mean reward: 909.03
               Mean episode length: 240.03
    Episode_Reward/reaching_object: 1.4271
     Episode_Reward/lifting_object: 178.9334
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 2.14s
                      Time elapsed: 01:11:21
                               ETA: 00:03:14

################################################################################
                     [1m Learning iteration 1914/2000 [0m                     

                       Computation: 45879 steps/s (collection: 2.053s, learning 0.090s)
             Mean action noise std: 3.40
          Mean value_function loss: 113.0385
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.4386
                       Mean reward: 888.12
               Mean episode length: 235.38
    Episode_Reward/reaching_object: 1.3699
     Episode_Reward/lifting_object: 171.4984
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 2.14s
                      Time elapsed: 01:11:24
                               ETA: 00:03:12

################################################################################
                     [1m Learning iteration 1915/2000 [0m                     

                       Computation: 45557 steps/s (collection: 2.065s, learning 0.093s)
             Mean action noise std: 3.40
          Mean value_function loss: 113.2028
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.4470
                       Mean reward: 883.79
               Mean episode length: 232.70
    Episode_Reward/reaching_object: 1.3996
     Episode_Reward/lifting_object: 175.8059
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 2.16s
                      Time elapsed: 01:11:26
                               ETA: 00:03:10

################################################################################
                     [1m Learning iteration 1916/2000 [0m                     

                       Computation: 45762 steps/s (collection: 2.056s, learning 0.093s)
             Mean action noise std: 3.40
          Mean value_function loss: 101.7617
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 57.4561
                       Mean reward: 885.55
               Mean episode length: 234.32
    Episode_Reward/reaching_object: 1.4262
     Episode_Reward/lifting_object: 179.0465
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 2.15s
                      Time elapsed: 01:11:28
                               ETA: 00:03:07

################################################################################
                     [1m Learning iteration 1917/2000 [0m                     

                       Computation: 45421 steps/s (collection: 2.049s, learning 0.116s)
             Mean action noise std: 3.40
          Mean value_function loss: 105.3821
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 57.4647
                       Mean reward: 867.06
               Mean episode length: 230.12
    Episode_Reward/reaching_object: 1.4054
     Episode_Reward/lifting_object: 176.2266
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 2.16s
                      Time elapsed: 01:11:30
                               ETA: 00:03:05

################################################################################
                     [1m Learning iteration 1918/2000 [0m                     

                       Computation: 45436 steps/s (collection: 2.065s, learning 0.099s)
             Mean action noise std: 3.40
          Mean value_function loss: 96.0808
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.4751
                       Mean reward: 860.85
               Mean episode length: 228.24
    Episode_Reward/reaching_object: 1.4101
     Episode_Reward/lifting_object: 175.9447
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0612
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 2.16s
                      Time elapsed: 01:11:32
                               ETA: 00:03:03

################################################################################
                     [1m Learning iteration 1919/2000 [0m                     

                       Computation: 45267 steps/s (collection: 2.074s, learning 0.098s)
             Mean action noise std: 3.40
          Mean value_function loss: 110.6329
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.4789
                       Mean reward: 910.24
               Mean episode length: 239.79
    Episode_Reward/reaching_object: 1.4013
     Episode_Reward/lifting_object: 175.7364
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 2.17s
                      Time elapsed: 01:11:34
                               ETA: 00:03:01

################################################################################
                     [1m Learning iteration 1920/2000 [0m                     

                       Computation: 44946 steps/s (collection: 2.075s, learning 0.112s)
             Mean action noise std: 3.40
          Mean value_function loss: 82.6358
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.4839
                       Mean reward: 896.16
               Mean episode length: 236.16
    Episode_Reward/reaching_object: 1.3978
     Episode_Reward/lifting_object: 175.6805
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 2.19s
                      Time elapsed: 01:11:37
                               ETA: 00:02:58

################################################################################
                     [1m Learning iteration 1921/2000 [0m                     

                       Computation: 45475 steps/s (collection: 2.047s, learning 0.115s)
             Mean action noise std: 3.40
          Mean value_function loss: 106.3591
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.4909
                       Mean reward: 891.84
               Mean episode length: 236.09
    Episode_Reward/reaching_object: 1.4199
     Episode_Reward/lifting_object: 178.1667
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 2.16s
                      Time elapsed: 01:11:39
                               ETA: 00:02:56

################################################################################
                     [1m Learning iteration 1922/2000 [0m                     

                       Computation: 45385 steps/s (collection: 2.070s, learning 0.096s)
             Mean action noise std: 3.41
          Mean value_function loss: 94.3012
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.5003
                       Mean reward: 896.21
               Mean episode length: 237.51
    Episode_Reward/reaching_object: 1.4237
     Episode_Reward/lifting_object: 178.6331
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 2.17s
                      Time elapsed: 01:11:41
                               ETA: 00:02:54

################################################################################
                     [1m Learning iteration 1923/2000 [0m                     

                       Computation: 44468 steps/s (collection: 2.067s, learning 0.144s)
             Mean action noise std: 3.41
          Mean value_function loss: 102.1059
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 57.5120
                       Mean reward: 891.22
               Mean episode length: 234.67
    Episode_Reward/reaching_object: 1.4269
     Episode_Reward/lifting_object: 178.9314
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 2.21s
                      Time elapsed: 01:11:43
                               ETA: 00:02:52

################################################################################
                     [1m Learning iteration 1924/2000 [0m                     

                       Computation: 44530 steps/s (collection: 2.111s, learning 0.097s)
             Mean action noise std: 3.41
          Mean value_function loss: 114.1965
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.5244
                       Mean reward: 848.49
               Mean episode length: 225.60
    Episode_Reward/reaching_object: 1.3490
     Episode_Reward/lifting_object: 168.9963
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 2.21s
                      Time elapsed: 01:11:45
                               ETA: 00:02:49

################################################################################
                     [1m Learning iteration 1925/2000 [0m                     

                       Computation: 44619 steps/s (collection: 2.112s, learning 0.091s)
             Mean action noise std: 3.41
          Mean value_function loss: 117.2671
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.5374
                       Mean reward: 860.56
               Mean episode length: 228.26
    Episode_Reward/reaching_object: 1.3863
     Episode_Reward/lifting_object: 173.2618
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 2.20s
                      Time elapsed: 01:11:47
                               ETA: 00:02:47

################################################################################
                     [1m Learning iteration 1926/2000 [0m                     

                       Computation: 45004 steps/s (collection: 2.090s, learning 0.095s)
             Mean action noise std: 3.41
          Mean value_function loss: 133.2150
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 57.5418
                       Mean reward: 840.90
               Mean episode length: 223.86
    Episode_Reward/reaching_object: 1.3581
     Episode_Reward/lifting_object: 170.3784
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 2.18s
                      Time elapsed: 01:11:50
                               ETA: 00:02:45

################################################################################
                     [1m Learning iteration 1927/2000 [0m                     

                       Computation: 46007 steps/s (collection: 2.034s, learning 0.103s)
             Mean action noise std: 3.41
          Mean value_function loss: 114.4766
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 57.5447
                       Mean reward: 891.40
               Mean episode length: 234.98
    Episode_Reward/reaching_object: 1.4016
     Episode_Reward/lifting_object: 176.0663
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 2.14s
                      Time elapsed: 01:11:52
                               ETA: 00:02:43

################################################################################
                     [1m Learning iteration 1928/2000 [0m                     

                       Computation: 45183 steps/s (collection: 2.043s, learning 0.133s)
             Mean action noise std: 3.41
          Mean value_function loss: 104.5391
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 57.5468
                       Mean reward: 856.53
               Mean episode length: 226.84
    Episode_Reward/reaching_object: 1.4008
     Episode_Reward/lifting_object: 176.2232
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0617
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 2.18s
                      Time elapsed: 01:11:54
                               ETA: 00:02:41

################################################################################
                     [1m Learning iteration 1929/2000 [0m                     

                       Computation: 44963 steps/s (collection: 2.084s, learning 0.102s)
             Mean action noise std: 3.41
          Mean value_function loss: 113.2753
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 57.5477
                       Mean reward: 915.09
               Mean episode length: 242.08
    Episode_Reward/reaching_object: 1.4049
     Episode_Reward/lifting_object: 176.5195
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 2.19s
                      Time elapsed: 01:11:56
                               ETA: 00:02:38

################################################################################
                     [1m Learning iteration 1930/2000 [0m                     

                       Computation: 43254 steps/s (collection: 2.178s, learning 0.095s)
             Mean action noise std: 3.41
          Mean value_function loss: 148.0935
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 57.5501
                       Mean reward: 842.54
               Mean episode length: 223.40
    Episode_Reward/reaching_object: 1.3613
     Episode_Reward/lifting_object: 170.6218
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0605
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 2.27s
                      Time elapsed: 01:11:58
                               ETA: 00:02:36

################################################################################
                     [1m Learning iteration 1931/2000 [0m                     

                       Computation: 44149 steps/s (collection: 2.105s, learning 0.122s)
             Mean action noise std: 3.42
          Mean value_function loss: 124.0376
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.5567
                       Mean reward: 875.97
               Mean episode length: 231.87
    Episode_Reward/reaching_object: 1.3850
     Episode_Reward/lifting_object: 174.1694
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 2.23s
                      Time elapsed: 01:12:01
                               ETA: 00:02:34

################################################################################
                     [1m Learning iteration 1932/2000 [0m                     

                       Computation: 45767 steps/s (collection: 2.055s, learning 0.093s)
             Mean action noise std: 3.42
          Mean value_function loss: 141.4655
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.5657
                       Mean reward: 835.12
               Mean episode length: 223.40
    Episode_Reward/reaching_object: 1.3926
     Episode_Reward/lifting_object: 175.3338
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 2.15s
                      Time elapsed: 01:12:03
                               ETA: 00:02:32

################################################################################
                     [1m Learning iteration 1933/2000 [0m                     

                       Computation: 44941 steps/s (collection: 2.060s, learning 0.128s)
             Mean action noise std: 3.42
          Mean value_function loss: 125.0771
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 57.5720
                       Mean reward: 858.81
               Mean episode length: 227.79
    Episode_Reward/reaching_object: 1.3915
     Episode_Reward/lifting_object: 175.3687
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 2.19s
                      Time elapsed: 01:12:05
                               ETA: 00:02:29

################################################################################
                     [1m Learning iteration 1934/2000 [0m                     

                       Computation: 43219 steps/s (collection: 2.134s, learning 0.140s)
             Mean action noise std: 3.42
          Mean value_function loss: 128.6822
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.5754
                       Mean reward: 811.26
               Mean episode length: 216.83
    Episode_Reward/reaching_object: 1.3525
     Episode_Reward/lifting_object: 170.8829
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 2.27s
                      Time elapsed: 01:12:07
                               ETA: 00:02:27

################################################################################
                     [1m Learning iteration 1935/2000 [0m                     

                       Computation: 42878 steps/s (collection: 2.180s, learning 0.113s)
             Mean action noise std: 3.42
          Mean value_function loss: 142.0415
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 57.5805
                       Mean reward: 835.21
               Mean episode length: 222.74
    Episode_Reward/reaching_object: 1.3624
     Episode_Reward/lifting_object: 171.6254
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 2.29s
                      Time elapsed: 01:12:10
                               ETA: 00:02:25

################################################################################
                     [1m Learning iteration 1936/2000 [0m                     

                       Computation: 45132 steps/s (collection: 2.067s, learning 0.112s)
             Mean action noise std: 3.42
          Mean value_function loss: 121.9593
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 57.5832
                       Mean reward: 891.35
               Mean episode length: 234.62
    Episode_Reward/reaching_object: 1.3787
     Episode_Reward/lifting_object: 173.9152
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0613
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 2.18s
                      Time elapsed: 01:12:12
                               ETA: 00:02:23

################################################################################
                     [1m Learning iteration 1937/2000 [0m                     

                       Computation: 44187 steps/s (collection: 2.118s, learning 0.107s)
             Mean action noise std: 3.42
          Mean value_function loss: 149.8973
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 57.5873
                       Mean reward: 856.17
               Mean episode length: 227.68
    Episode_Reward/reaching_object: 1.3600
     Episode_Reward/lifting_object: 171.7335
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 2.22s
                      Time elapsed: 01:12:14
                               ETA: 00:02:20

################################################################################
                     [1m Learning iteration 1938/2000 [0m                     

                       Computation: 44431 steps/s (collection: 2.114s, learning 0.099s)
             Mean action noise std: 3.42
          Mean value_function loss: 110.8824
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.5973
                       Mean reward: 892.03
               Mean episode length: 235.96
    Episode_Reward/reaching_object: 1.3855
     Episode_Reward/lifting_object: 174.9901
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 2.21s
                      Time elapsed: 01:12:16
                               ETA: 00:02:18

################################################################################
                     [1m Learning iteration 1939/2000 [0m                     

                       Computation: 41794 steps/s (collection: 2.231s, learning 0.121s)
             Mean action noise std: 3.42
          Mean value_function loss: 125.5329
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 57.6066
                       Mean reward: 871.91
               Mean episode length: 231.93
    Episode_Reward/reaching_object: 1.4003
     Episode_Reward/lifting_object: 176.8262
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 2.35s
                      Time elapsed: 01:12:19
                               ETA: 00:02:16

################################################################################
                     [1m Learning iteration 1940/2000 [0m                     

                       Computation: 44237 steps/s (collection: 2.114s, learning 0.109s)
             Mean action noise std: 3.42
          Mean value_function loss: 142.2441
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.6096
                       Mean reward: 837.23
               Mean episode length: 223.66
    Episode_Reward/reaching_object: 1.3337
     Episode_Reward/lifting_object: 167.4087
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 2.22s
                      Time elapsed: 01:12:21
                               ETA: 00:02:14

################################################################################
                     [1m Learning iteration 1941/2000 [0m                     

                       Computation: 44217 steps/s (collection: 2.134s, learning 0.090s)
             Mean action noise std: 3.43
          Mean value_function loss: 105.8426
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.6148
                       Mean reward: 909.49
               Mean episode length: 240.00
    Episode_Reward/reaching_object: 1.3917
     Episode_Reward/lifting_object: 175.4155
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 2.22s
                      Time elapsed: 01:12:23
                               ETA: 00:02:11

################################################################################
                     [1m Learning iteration 1942/2000 [0m                     

                       Computation: 45001 steps/s (collection: 2.092s, learning 0.093s)
             Mean action noise std: 3.43
          Mean value_function loss: 118.7765
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.6240
                       Mean reward: 861.33
               Mean episode length: 228.01
    Episode_Reward/reaching_object: 1.3825
     Episode_Reward/lifting_object: 174.2106
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 2.18s
                      Time elapsed: 01:12:25
                               ETA: 00:02:09

################################################################################
                     [1m Learning iteration 1943/2000 [0m                     

                       Computation: 45489 steps/s (collection: 2.069s, learning 0.092s)
             Mean action noise std: 3.43
          Mean value_function loss: 102.8401
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 57.6316
                       Mean reward: 880.34
               Mean episode length: 232.32
    Episode_Reward/reaching_object: 1.3941
     Episode_Reward/lifting_object: 175.9221
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 2.16s
                      Time elapsed: 01:12:27
                               ETA: 00:02:07

################################################################################
                     [1m Learning iteration 1944/2000 [0m                     

                       Computation: 45184 steps/s (collection: 2.062s, learning 0.114s)
             Mean action noise std: 3.43
          Mean value_function loss: 137.8801
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 57.6411
                       Mean reward: 888.45
               Mean episode length: 235.50
    Episode_Reward/reaching_object: 1.4106
     Episode_Reward/lifting_object: 178.0643
      Episode_Reward/object_height: 0.0272
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 2.18s
                      Time elapsed: 01:12:29
                               ETA: 00:02:05

################################################################################
                     [1m Learning iteration 1945/2000 [0m                     

                       Computation: 39291 steps/s (collection: 2.311s, learning 0.191s)
             Mean action noise std: 3.43
          Mean value_function loss: 105.2246
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 57.6495
                       Mean reward: 895.32
               Mean episode length: 237.13
    Episode_Reward/reaching_object: 1.3913
     Episode_Reward/lifting_object: 175.4756
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 2.50s
                      Time elapsed: 01:12:32
                               ETA: 00:02:03

################################################################################
                     [1m Learning iteration 1946/2000 [0m                     

                       Computation: 38365 steps/s (collection: 2.441s, learning 0.122s)
             Mean action noise std: 3.43
          Mean value_function loss: 112.4929
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 57.6564
                       Mean reward: 888.99
               Mean episode length: 235.53
    Episode_Reward/reaching_object: 1.4087
     Episode_Reward/lifting_object: 177.2507
      Episode_Reward/object_height: 0.0275
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 2.56s
                      Time elapsed: 01:12:35
                               ETA: 00:02:00

################################################################################
                     [1m Learning iteration 1947/2000 [0m                     

                       Computation: 35202 steps/s (collection: 2.532s, learning 0.261s)
             Mean action noise std: 3.43
          Mean value_function loss: 107.9695
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 57.6608
                       Mean reward: 910.95
               Mean episode length: 239.73
    Episode_Reward/reaching_object: 1.4101
     Episode_Reward/lifting_object: 177.6686
      Episode_Reward/object_height: 0.0274
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 2.79s
                      Time elapsed: 01:12:37
                               ETA: 00:01:58

################################################################################
                     [1m Learning iteration 1948/2000 [0m                     

                       Computation: 39423 steps/s (collection: 2.383s, learning 0.111s)
             Mean action noise std: 3.43
          Mean value_function loss: 94.6304
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.6692
                       Mean reward: 929.50
               Mean episode length: 243.98
    Episode_Reward/reaching_object: 1.3995
     Episode_Reward/lifting_object: 175.9298
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 2.49s
                      Time elapsed: 01:12:40
                               ETA: 00:01:56

################################################################################
                     [1m Learning iteration 1949/2000 [0m                     

                       Computation: 34599 steps/s (collection: 2.602s, learning 0.239s)
             Mean action noise std: 3.43
          Mean value_function loss: 103.5528
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.6748
                       Mean reward: 895.44
               Mean episode length: 236.64
    Episode_Reward/reaching_object: 1.3938
     Episode_Reward/lifting_object: 175.4323
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 2.84s
                      Time elapsed: 01:12:43
                               ETA: 00:01:54

################################################################################
                     [1m Learning iteration 1950/2000 [0m                     

                       Computation: 33725 steps/s (collection: 2.737s, learning 0.178s)
             Mean action noise std: 3.44
          Mean value_function loss: 98.6781
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.6823
                       Mean reward: 919.30
               Mean episode length: 241.34
    Episode_Reward/reaching_object: 1.4252
     Episode_Reward/lifting_object: 179.6719
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 2.91s
                      Time elapsed: 01:12:46
                               ETA: 00:01:51

################################################################################
                     [1m Learning iteration 1951/2000 [0m                     

                       Computation: 36678 steps/s (collection: 2.473s, learning 0.208s)
             Mean action noise std: 3.44
          Mean value_function loss: 107.3235
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.6902
                       Mean reward: 903.54
               Mean episode length: 238.11
    Episode_Reward/reaching_object: 1.4137
     Episode_Reward/lifting_object: 177.9602
      Episode_Reward/object_height: 0.0271
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 2.68s
                      Time elapsed: 01:12:48
                               ETA: 00:01:49

################################################################################
                     [1m Learning iteration 1952/2000 [0m                     

                       Computation: 43092 steps/s (collection: 2.167s, learning 0.114s)
             Mean action noise std: 3.44
          Mean value_function loss: 94.2658
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 57.6962
                       Mean reward: 923.00
               Mean episode length: 242.66
    Episode_Reward/reaching_object: 1.4082
     Episode_Reward/lifting_object: 177.2801
      Episode_Reward/object_height: 0.0273
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 2.28s
                      Time elapsed: 01:12:51
                               ETA: 00:01:47

################################################################################
                     [1m Learning iteration 1953/2000 [0m                     

                       Computation: 45605 steps/s (collection: 2.053s, learning 0.103s)
             Mean action noise std: 3.44
          Mean value_function loss: 106.8206
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.7014
                       Mean reward: 923.19
               Mean episode length: 241.69
    Episode_Reward/reaching_object: 1.3982
     Episode_Reward/lifting_object: 176.1213
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0622
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 2.16s
                      Time elapsed: 01:12:53
                               ETA: 00:01:45

################################################################################
                     [1m Learning iteration 1954/2000 [0m                     

                       Computation: 45623 steps/s (collection: 2.042s, learning 0.113s)
             Mean action noise std: 3.44
          Mean value_function loss: 89.9052
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 57.7066
                       Mean reward: 894.46
               Mean episode length: 235.78
    Episode_Reward/reaching_object: 1.4233
     Episode_Reward/lifting_object: 178.9609
      Episode_Reward/object_height: 0.0278
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 2.15s
                      Time elapsed: 01:12:55
                               ETA: 00:01:42

################################################################################
                     [1m Learning iteration 1955/2000 [0m                     

                       Computation: 44976 steps/s (collection: 2.065s, learning 0.121s)
             Mean action noise std: 3.44
          Mean value_function loss: 91.8428
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 57.7139
                       Mean reward: 908.02
               Mean episode length: 240.38
    Episode_Reward/reaching_object: 1.4347
     Episode_Reward/lifting_object: 180.0262
      Episode_Reward/object_height: 0.0280
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 2.19s
                      Time elapsed: 01:12:57
                               ETA: 00:01:40

################################################################################
                     [1m Learning iteration 1956/2000 [0m                     

                       Computation: 45049 steps/s (collection: 2.045s, learning 0.137s)
             Mean action noise std: 3.44
          Mean value_function loss: 95.5367
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 57.7231
                       Mean reward: 874.42
               Mean episode length: 231.38
    Episode_Reward/reaching_object: 1.3977
     Episode_Reward/lifting_object: 175.6125
      Episode_Reward/object_height: 0.0274
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 2.18s
                      Time elapsed: 01:12:59
                               ETA: 00:01:38

################################################################################
                     [1m Learning iteration 1957/2000 [0m                     

                       Computation: 45112 steps/s (collection: 2.054s, learning 0.125s)
             Mean action noise std: 3.44
          Mean value_function loss: 81.7759
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 57.7262
                       Mean reward: 862.37
               Mean episode length: 228.77
    Episode_Reward/reaching_object: 1.4156
     Episode_Reward/lifting_object: 178.1078
      Episode_Reward/object_height: 0.0283
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 2.18s
                      Time elapsed: 01:13:01
                               ETA: 00:01:36

################################################################################
                     [1m Learning iteration 1958/2000 [0m                     

                       Computation: 43692 steps/s (collection: 2.133s, learning 0.117s)
             Mean action noise std: 3.44
          Mean value_function loss: 91.5798
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.7289
                       Mean reward: 881.70
               Mean episode length: 232.95
    Episode_Reward/reaching_object: 1.3860
     Episode_Reward/lifting_object: 174.8293
      Episode_Reward/object_height: 0.0275
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 2.25s
                      Time elapsed: 01:13:04
                               ETA: 00:01:33

################################################################################
                     [1m Learning iteration 1959/2000 [0m                     

                       Computation: 44874 steps/s (collection: 2.074s, learning 0.117s)
             Mean action noise std: 3.45
          Mean value_function loss: 77.1258
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.7341
                       Mean reward: 900.54
               Mean episode length: 238.38
    Episode_Reward/reaching_object: 1.4351
     Episode_Reward/lifting_object: 180.5813
      Episode_Reward/object_height: 0.0284
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 2.19s
                      Time elapsed: 01:13:06
                               ETA: 00:01:31

################################################################################
                     [1m Learning iteration 1960/2000 [0m                     

                       Computation: 44955 steps/s (collection: 2.061s, learning 0.126s)
             Mean action noise std: 3.45
          Mean value_function loss: 76.4570
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.7461
                       Mean reward: 895.81
               Mean episode length: 236.42
    Episode_Reward/reaching_object: 1.4175
     Episode_Reward/lifting_object: 177.9316
      Episode_Reward/object_height: 0.0279
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 2.19s
                      Time elapsed: 01:13:08
                               ETA: 00:01:29

################################################################################
                     [1m Learning iteration 1961/2000 [0m                     

                       Computation: 44012 steps/s (collection: 2.105s, learning 0.129s)
             Mean action noise std: 3.45
          Mean value_function loss: 96.4919
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.7564
                       Mean reward: 906.08
               Mean episode length: 239.10
    Episode_Reward/reaching_object: 1.4199
     Episode_Reward/lifting_object: 177.9052
      Episode_Reward/object_height: 0.0274
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 2.23s
                      Time elapsed: 01:13:10
                               ETA: 00:01:27

################################################################################
                     [1m Learning iteration 1962/2000 [0m                     

                       Computation: 44101 steps/s (collection: 2.097s, learning 0.132s)
             Mean action noise std: 3.45
          Mean value_function loss: 87.0050
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 57.7615
                       Mean reward: 882.46
               Mean episode length: 235.39
    Episode_Reward/reaching_object: 1.4248
     Episode_Reward/lifting_object: 178.3848
      Episode_Reward/object_height: 0.0271
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 2.23s
                      Time elapsed: 01:13:13
                               ETA: 00:01:25

################################################################################
                     [1m Learning iteration 1963/2000 [0m                     

                       Computation: 44756 steps/s (collection: 2.097s, learning 0.099s)
             Mean action noise std: 3.45
          Mean value_function loss: 82.6812
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.7717
                       Mean reward: 916.60
               Mean episode length: 241.58
    Episode_Reward/reaching_object: 1.4153
     Episode_Reward/lifting_object: 178.0225
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 2.20s
                      Time elapsed: 01:13:15
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 1964/2000 [0m                     

                       Computation: 46020 steps/s (collection: 2.029s, learning 0.107s)
             Mean action noise std: 3.45
          Mean value_function loss: 85.8246
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.7823
                       Mean reward: 897.34
               Mean episode length: 236.95
    Episode_Reward/reaching_object: 1.4229
     Episode_Reward/lifting_object: 178.0755
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 2.14s
                      Time elapsed: 01:13:17
                               ETA: 00:01:20

################################################################################
                     [1m Learning iteration 1965/2000 [0m                     

                       Computation: 44404 steps/s (collection: 2.112s, learning 0.102s)
             Mean action noise std: 3.46
          Mean value_function loss: 102.4382
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.7926
                       Mean reward: 905.24
               Mean episode length: 239.65
    Episode_Reward/reaching_object: 1.4310
     Episode_Reward/lifting_object: 179.7761
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 2.21s
                      Time elapsed: 01:13:19
                               ETA: 00:01:18

################################################################################
                     [1m Learning iteration 1966/2000 [0m                     

                       Computation: 45429 steps/s (collection: 2.048s, learning 0.116s)
             Mean action noise std: 3.46
          Mean value_function loss: 87.8174
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 57.8042
                       Mean reward: 873.15
               Mean episode length: 232.51
    Episode_Reward/reaching_object: 1.4111
     Episode_Reward/lifting_object: 176.8203
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 2.16s
                      Time elapsed: 01:13:21
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 1967/2000 [0m                     

                       Computation: 45046 steps/s (collection: 2.053s, learning 0.129s)
             Mean action noise std: 3.46
          Mean value_function loss: 92.2059
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.8118
                       Mean reward: 883.10
               Mean episode length: 234.57
    Episode_Reward/reaching_object: 1.3992
     Episode_Reward/lifting_object: 174.4748
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 2.18s
                      Time elapsed: 01:13:23
                               ETA: 00:01:13

################################################################################
                     [1m Learning iteration 1968/2000 [0m                     

                       Computation: 44959 steps/s (collection: 2.066s, learning 0.121s)
             Mean action noise std: 3.46
          Mean value_function loss: 79.2041
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 57.8234
                       Mean reward: 919.03
               Mean episode length: 242.41
    Episode_Reward/reaching_object: 1.4239
     Episode_Reward/lifting_object: 178.6910
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 2.19s
                      Time elapsed: 01:13:26
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 1969/2000 [0m                     

                       Computation: 45566 steps/s (collection: 2.047s, learning 0.111s)
             Mean action noise std: 3.46
          Mean value_function loss: 91.5131
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.8349
                       Mean reward: 853.01
               Mean episode length: 227.80
    Episode_Reward/reaching_object: 1.4021
     Episode_Reward/lifting_object: 176.1769
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 2.16s
                      Time elapsed: 01:13:28
                               ETA: 00:01:09

################################################################################
                     [1m Learning iteration 1970/2000 [0m                     

                       Computation: 45152 steps/s (collection: 2.045s, learning 0.132s)
             Mean action noise std: 3.46
          Mean value_function loss: 117.0960
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.8421
                       Mean reward: 909.02
               Mean episode length: 239.74
    Episode_Reward/reaching_object: 1.3745
     Episode_Reward/lifting_object: 172.3947
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 2.18s
                      Time elapsed: 01:13:30
                               ETA: 00:01:07

################################################################################
                     [1m Learning iteration 1971/2000 [0m                     

                       Computation: 43874 steps/s (collection: 2.124s, learning 0.117s)
             Mean action noise std: 3.46
          Mean value_function loss: 104.9303
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.8506
                       Mean reward: 884.48
               Mean episode length: 233.05
    Episode_Reward/reaching_object: 1.3934
     Episode_Reward/lifting_object: 175.6156
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 2.24s
                      Time elapsed: 01:13:32
                               ETA: 00:01:04

################################################################################
                     [1m Learning iteration 1972/2000 [0m                     

                       Computation: 44320 steps/s (collection: 2.106s, learning 0.112s)
             Mean action noise std: 3.47
          Mean value_function loss: 121.9426
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.8583
                       Mean reward: 886.71
               Mean episode length: 235.19
    Episode_Reward/reaching_object: 1.3951
     Episode_Reward/lifting_object: 174.9468
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 2.22s
                      Time elapsed: 01:13:34
                               ETA: 00:01:02

################################################################################
                     [1m Learning iteration 1973/2000 [0m                     

                       Computation: 45133 steps/s (collection: 2.063s, learning 0.115s)
             Mean action noise std: 3.47
          Mean value_function loss: 89.7874
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 57.8636
                       Mean reward: 906.03
               Mean episode length: 238.91
    Episode_Reward/reaching_object: 1.4058
     Episode_Reward/lifting_object: 176.8468
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 2.18s
                      Time elapsed: 01:13:37
                               ETA: 00:01:00

################################################################################
                     [1m Learning iteration 1974/2000 [0m                     

                       Computation: 44789 steps/s (collection: 2.054s, learning 0.141s)
             Mean action noise std: 3.47
          Mean value_function loss: 90.6816
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.8682
                       Mean reward: 922.11
               Mean episode length: 242.20
    Episode_Reward/reaching_object: 1.4361
     Episode_Reward/lifting_object: 180.3151
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 2.19s
                      Time elapsed: 01:13:39
                               ETA: 00:00:58

################################################################################
                     [1m Learning iteration 1975/2000 [0m                     

                       Computation: 44948 steps/s (collection: 2.068s, learning 0.119s)
             Mean action noise std: 3.47
          Mean value_function loss: 89.1967
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.8748
                       Mean reward: 897.15
               Mean episode length: 236.10
    Episode_Reward/reaching_object: 1.4158
     Episode_Reward/lifting_object: 178.7488
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 2.19s
                      Time elapsed: 01:13:41
                               ETA: 00:00:55

################################################################################
                     [1m Learning iteration 1976/2000 [0m                     

                       Computation: 44355 steps/s (collection: 2.101s, learning 0.116s)
             Mean action noise std: 3.47
          Mean value_function loss: 80.6041
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 57.8809
                       Mean reward: 885.84
               Mean episode length: 232.85
    Episode_Reward/reaching_object: 1.4047
     Episode_Reward/lifting_object: 176.5428
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 2.22s
                      Time elapsed: 01:13:43
                               ETA: 00:00:53

################################################################################
                     [1m Learning iteration 1977/2000 [0m                     

                       Computation: 44559 steps/s (collection: 2.090s, learning 0.116s)
             Mean action noise std: 3.47
          Mean value_function loss: 92.9834
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 57.8913
                       Mean reward: 831.49
               Mean episode length: 222.83
    Episode_Reward/reaching_object: 1.4106
     Episode_Reward/lifting_object: 176.7276
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 2.21s
                      Time elapsed: 01:13:45
                               ETA: 00:00:51

################################################################################
                     [1m Learning iteration 1978/2000 [0m                     

                       Computation: 45958 steps/s (collection: 2.025s, learning 0.114s)
             Mean action noise std: 3.47
          Mean value_function loss: 111.7459
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 57.8972
                       Mean reward: 885.04
               Mean episode length: 233.98
    Episode_Reward/reaching_object: 1.3932
     Episode_Reward/lifting_object: 174.2822
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 2.14s
                      Time elapsed: 01:13:47
                               ETA: 00:00:49

################################################################################
                     [1m Learning iteration 1979/2000 [0m                     

                       Computation: 45252 steps/s (collection: 2.052s, learning 0.121s)
             Mean action noise std: 3.47
          Mean value_function loss: 109.6143
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 57.8999
                       Mean reward: 864.51
               Mean episode length: 229.26
    Episode_Reward/reaching_object: 1.3972
     Episode_Reward/lifting_object: 175.3353
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0634
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 2.17s
                      Time elapsed: 01:13:50
                               ETA: 00:00:46

################################################################################
                     [1m Learning iteration 1980/2000 [0m                     

                       Computation: 45591 steps/s (collection: 2.047s, learning 0.110s)
             Mean action noise std: 3.47
          Mean value_function loss: 80.5351
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.9029
                       Mean reward: 906.41
               Mean episode length: 240.19
    Episode_Reward/reaching_object: 1.4147
     Episode_Reward/lifting_object: 177.6703
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0637
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 2.16s
                      Time elapsed: 01:13:52
                               ETA: 00:00:44

################################################################################
                     [1m Learning iteration 1981/2000 [0m                     

                       Computation: 46203 steps/s (collection: 2.024s, learning 0.104s)
             Mean action noise std: 3.47
          Mean value_function loss: 91.0159
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.9077
                       Mean reward: 906.66
               Mean episode length: 239.55
    Episode_Reward/reaching_object: 1.4069
     Episode_Reward/lifting_object: 176.3343
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 2.13s
                      Time elapsed: 01:13:54
                               ETA: 00:00:42

################################################################################
                     [1m Learning iteration 1982/2000 [0m                     

                       Computation: 45243 steps/s (collection: 2.052s, learning 0.121s)
             Mean action noise std: 3.47
          Mean value_function loss: 70.2220
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 57.9108
                       Mean reward: 882.21
               Mean episode length: 234.09
    Episode_Reward/reaching_object: 1.4339
     Episode_Reward/lifting_object: 179.7865
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.0648
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 2.17s
                      Time elapsed: 01:13:56
                               ETA: 00:00:40

################################################################################
                     [1m Learning iteration 1983/2000 [0m                     

                       Computation: 45478 steps/s (collection: 2.030s, learning 0.132s)
             Mean action noise std: 3.47
          Mean value_function loss: 89.6626
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 57.9138
                       Mean reward: 882.71
               Mean episode length: 233.51
    Episode_Reward/reaching_object: 1.4047
     Episode_Reward/lifting_object: 175.5089
      Episode_Reward/object_height: 0.0273
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 2.16s
                      Time elapsed: 01:13:58
                               ETA: 00:00:38

################################################################################
                     [1m Learning iteration 1984/2000 [0m                     

                       Computation: 45616 steps/s (collection: 2.026s, learning 0.129s)
             Mean action noise std: 3.47
          Mean value_function loss: 97.0444
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.9156
                       Mean reward: 872.94
               Mean episode length: 231.63
    Episode_Reward/reaching_object: 1.3943
     Episode_Reward/lifting_object: 174.2807
      Episode_Reward/object_height: 0.0273
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 2.16s
                      Time elapsed: 01:14:00
                               ETA: 00:00:35

################################################################################
                     [1m Learning iteration 1985/2000 [0m                     

                       Computation: 45130 steps/s (collection: 2.050s, learning 0.128s)
             Mean action noise std: 3.48
          Mean value_function loss: 94.8657
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.9215
                       Mean reward: 894.89
               Mean episode length: 236.42
    Episode_Reward/reaching_object: 1.4312
     Episode_Reward/lifting_object: 179.3663
      Episode_Reward/object_height: 0.0282
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 2.18s
                      Time elapsed: 01:14:03
                               ETA: 00:00:33

################################################################################
                     [1m Learning iteration 1986/2000 [0m                     

                       Computation: 44678 steps/s (collection: 2.089s, learning 0.112s)
             Mean action noise std: 3.48
          Mean value_function loss: 82.1416
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.9291
                       Mean reward: 899.81
               Mean episode length: 237.82
    Episode_Reward/reaching_object: 1.4197
     Episode_Reward/lifting_object: 177.9883
      Episode_Reward/object_height: 0.0281
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 2.20s
                      Time elapsed: 01:14:05
                               ETA: 00:00:31

################################################################################
                     [1m Learning iteration 1987/2000 [0m                     

                       Computation: 43869 steps/s (collection: 2.131s, learning 0.110s)
             Mean action noise std: 3.48
          Mean value_function loss: 88.1039
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 57.9338
                       Mean reward: 918.67
               Mean episode length: 241.73
    Episode_Reward/reaching_object: 1.4315
     Episode_Reward/lifting_object: 179.2192
      Episode_Reward/object_height: 0.0280
        Episode_Reward/action_rate: -0.0642
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 2.24s
                      Time elapsed: 01:14:07
                               ETA: 00:00:29

################################################################################
                     [1m Learning iteration 1988/2000 [0m                     

                       Computation: 43640 steps/s (collection: 2.152s, learning 0.101s)
             Mean action noise std: 3.48
          Mean value_function loss: 128.0864
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 57.9417
                       Mean reward: 867.34
               Mean episode length: 229.92
    Episode_Reward/reaching_object: 1.3751
     Episode_Reward/lifting_object: 171.9556
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 2.25s
                      Time elapsed: 01:14:09
                               ETA: 00:00:26

################################################################################
                     [1m Learning iteration 1989/2000 [0m                     

                       Computation: 44384 steps/s (collection: 2.090s, learning 0.125s)
             Mean action noise std: 3.48
          Mean value_function loss: 78.2937
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.9479
                       Mean reward: 886.13
               Mean episode length: 234.26
    Episode_Reward/reaching_object: 1.4306
     Episode_Reward/lifting_object: 179.0706
      Episode_Reward/object_height: 0.0283
        Episode_Reward/action_rate: -0.0642
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 2.21s
                      Time elapsed: 01:14:12
                               ETA: 00:00:24

################################################################################
                     [1m Learning iteration 1990/2000 [0m                     

                       Computation: 42625 steps/s (collection: 2.180s, learning 0.127s)
             Mean action noise std: 3.48
          Mean value_function loss: 88.4343
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 57.9531
                       Mean reward: 900.49
               Mean episode length: 239.27
    Episode_Reward/reaching_object: 1.4202
     Episode_Reward/lifting_object: 177.5544
      Episode_Reward/object_height: 0.0278
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 2.31s
                      Time elapsed: 01:14:14
                               ETA: 00:00:22

################################################################################
                     [1m Learning iteration 1991/2000 [0m                     

                       Computation: 45161 steps/s (collection: 2.058s, learning 0.119s)
             Mean action noise std: 3.48
          Mean value_function loss: 97.4625
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.9618
                       Mean reward: 884.72
               Mean episode length: 234.29
    Episode_Reward/reaching_object: 1.4195
     Episode_Reward/lifting_object: 177.4699
      Episode_Reward/object_height: 0.0276
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 2.18s
                      Time elapsed: 01:14:16
                               ETA: 00:00:20

################################################################################
                     [1m Learning iteration 1992/2000 [0m                     

                       Computation: 44021 steps/s (collection: 2.125s, learning 0.108s)
             Mean action noise std: 3.48
          Mean value_function loss: 116.8033
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.9680
                       Mean reward: 859.86
               Mean episode length: 227.06
    Episode_Reward/reaching_object: 1.4039
     Episode_Reward/lifting_object: 175.7361
      Episode_Reward/object_height: 0.0276
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 2.23s
                      Time elapsed: 01:14:18
                               ETA: 00:00:17

################################################################################
                     [1m Learning iteration 1993/2000 [0m                     

                       Computation: 44895 steps/s (collection: 2.061s, learning 0.129s)
             Mean action noise std: 3.48
          Mean value_function loss: 94.0499
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.9758
                       Mean reward: 898.54
               Mean episode length: 237.75
    Episode_Reward/reaching_object: 1.4233
     Episode_Reward/lifting_object: 178.5270
      Episode_Reward/object_height: 0.0279
        Episode_Reward/action_rate: -0.0642
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 2.19s
                      Time elapsed: 01:14:20
                               ETA: 00:00:15

################################################################################
                     [1m Learning iteration 1994/2000 [0m                     

                       Computation: 45645 steps/s (collection: 2.046s, learning 0.108s)
             Mean action noise std: 3.49
          Mean value_function loss: 91.3292
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.9869
                       Mean reward: 875.18
               Mean episode length: 232.32
    Episode_Reward/reaching_object: 1.4147
     Episode_Reward/lifting_object: 178.0506
      Episode_Reward/object_height: 0.0273
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 2.15s
                      Time elapsed: 01:14:23
                               ETA: 00:00:13

################################################################################
                     [1m Learning iteration 1995/2000 [0m                     

                       Computation: 44500 steps/s (collection: 2.084s, learning 0.125s)
             Mean action noise std: 3.49
          Mean value_function loss: 101.9709
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 57.9980
                       Mean reward: 903.76
               Mean episode length: 238.74
    Episode_Reward/reaching_object: 1.4151
     Episode_Reward/lifting_object: 177.1130
      Episode_Reward/object_height: 0.0271
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 2.21s
                      Time elapsed: 01:14:25
                               ETA: 00:00:11

################################################################################
                     [1m Learning iteration 1996/2000 [0m                     

                       Computation: 45405 steps/s (collection: 2.054s, learning 0.112s)
             Mean action noise std: 3.49
          Mean value_function loss: 115.4941
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 58.0072
                       Mean reward: 867.47
               Mean episode length: 229.01
    Episode_Reward/reaching_object: 1.3796
     Episode_Reward/lifting_object: 172.6357
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 2.17s
                      Time elapsed: 01:14:27
                               ETA: 00:00:08

################################################################################
                     [1m Learning iteration 1997/2000 [0m                     

                       Computation: 45352 steps/s (collection: 2.046s, learning 0.122s)
             Mean action noise std: 3.49
          Mean value_function loss: 86.3743
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 58.0211
                       Mean reward: 929.83
               Mean episode length: 243.71
    Episode_Reward/reaching_object: 1.4073
     Episode_Reward/lifting_object: 176.1310
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 2.17s
                      Time elapsed: 01:14:29
                               ETA: 00:00:06

################################################################################
                     [1m Learning iteration 1998/2000 [0m                     

                       Computation: 44996 steps/s (collection: 2.072s, learning 0.113s)
             Mean action noise std: 3.49
          Mean value_function loss: 118.5390
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 58.0300
                       Mean reward: 880.19
               Mean episode length: 232.77
    Episode_Reward/reaching_object: 1.3893
     Episode_Reward/lifting_object: 174.2187
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 2.18s
                      Time elapsed: 01:14:31
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1999/2000 [0m                     

                       Computation: 45149 steps/s (collection: 2.066s, learning 0.112s)
             Mean action noise std: 3.49
          Mean value_function loss: 127.6917
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.0357
                       Mean reward: 899.57
               Mean episode length: 237.30
    Episode_Reward/reaching_object: 1.3848
     Episode_Reward/lifting_object: 173.8810
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 2.18s
                      Time elapsed: 01:14:33
                               ETA: 00:00:02

