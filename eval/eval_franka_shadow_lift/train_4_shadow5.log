################################################################################
                      [1m Learning iteration 0/2000 [0m                       

                       Computation: 10175 steps/s (collection: 9.390s, learning 0.271s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0058
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 44.0332
                       Mean reward: 0.01
               Mean episode length: 21.94
    Episode_Reward/reaching_object: 0.0013
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0003
          Episode_Reward/joint_vel: -0.0004
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 9.66s
                      Time elapsed: 00:00:09
                               ETA: 05:22:00

################################################################################
                      [1m Learning iteration 1/2000 [0m                       

                       Computation: 13854 steps/s (collection: 6.956s, learning 0.139s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 44.1580
                       Mean reward: 0.01
               Mean episode length: 45.00
    Episode_Reward/reaching_object: 0.0037
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0009
          Episode_Reward/joint_vel: -0.0013
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 7.10s
                      Time elapsed: 00:00:16
                               ETA: 04:39:07

################################################################################
                      [1m Learning iteration 2/2000 [0m                       

                       Computation: 14229 steps/s (collection: 6.728s, learning 0.181s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 44.2381
                       Mean reward: 0.02
               Mean episode length: 69.37
    Episode_Reward/reaching_object: 0.0056
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0015
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 6.91s
                      Time elapsed: 00:00:23
                               ETA: 04:22:40

################################################################################
                      [1m Learning iteration 3/2000 [0m                       

                       Computation: 14240 steps/s (collection: 6.745s, learning 0.158s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 44.3144
                       Mean reward: 0.02
               Mean episode length: 93.77
    Episode_Reward/reaching_object: 0.0082
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0031
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 6.90s
                      Time elapsed: 00:00:30
                               ETA: 04:14:20

################################################################################
                      [1m Learning iteration 4/2000 [0m                       

                       Computation: 13949 steps/s (collection: 6.869s, learning 0.179s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 44.3673
                       Mean reward: 0.02
               Mean episode length: 117.34
    Episode_Reward/reaching_object: 0.0105
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0040
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 7.05s
                      Time elapsed: 00:00:37
                               ETA: 04:10:15

################################################################################
                      [1m Learning iteration 5/2000 [0m                       

                       Computation: 13464 steps/s (collection: 7.125s, learning 0.176s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 44.3935
                       Mean reward: 0.03
               Mean episode length: 141.43
    Episode_Reward/reaching_object: 0.0129
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0050
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 7.30s
                      Time elapsed: 00:00:44
                               ETA: 04:08:54

################################################################################
                      [1m Learning iteration 6/2000 [0m                       

                       Computation: 14668 steps/s (collection: 6.546s, learning 0.155s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 44.4137
                       Mean reward: 0.05
               Mean episode length: 165.01
    Episode_Reward/reaching_object: 0.0168
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0059
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 6.70s
                      Time elapsed: 00:00:51
                               ETA: 04:05:03

################################################################################
                      [1m Learning iteration 7/2000 [0m                       

                       Computation: 14527 steps/s (collection: 6.632s, learning 0.135s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0079
                 Mean entropy loss: 44.4537
                       Mean reward: 0.06
               Mean episode length: 189.52
    Episode_Reward/reaching_object: 0.0194
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 6.77s
                      Time elapsed: 00:00:58
                               ETA: 04:02:24

################################################################################
                      [1m Learning iteration 8/2000 [0m                       

                       Computation: 17472 steps/s (collection: 5.504s, learning 0.122s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0079
                 Mean entropy loss: 44.4590
                       Mean reward: 0.07
               Mean episode length: 213.81
    Episode_Reward/reaching_object: 0.0247
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 5.63s
                      Time elapsed: 00:01:04
                               ETA: 03:56:07

################################################################################
                      [1m Learning iteration 9/2000 [0m                       

                       Computation: 56554 steps/s (collection: 1.629s, learning 0.109s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 44.4694
                       Mean reward: 0.08
               Mean episode length: 237.09
    Episode_Reward/reaching_object: 0.0291
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 1.74s
                      Time elapsed: 00:01:05
                               ETA: 03:38:10

################################################################################
                      [1m Learning iteration 10/2000 [0m                      

                       Computation: 55757 steps/s (collection: 1.643s, learning 0.120s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 44.4674
                       Mean reward: 0.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0318
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 1.76s
                      Time elapsed: 00:01:07
                               ETA: 03:23:33

################################################################################
                      [1m Learning iteration 11/2000 [0m                      

                       Computation: 54766 steps/s (collection: 1.686s, learning 0.108s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 44.5231
                       Mean reward: 0.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0378
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 1.79s
                      Time elapsed: 00:01:09
                               ETA: 03:11:27

################################################################################
                      [1m Learning iteration 12/2000 [0m                      

                       Computation: 55762 steps/s (collection: 1.661s, learning 0.101s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 44.5670
                       Mean reward: 0.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0462
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 1.76s
                      Time elapsed: 00:01:11
                               ETA: 03:01:08

################################################################################
                      [1m Learning iteration 13/2000 [0m                      

                       Computation: 56249 steps/s (collection: 1.647s, learning 0.101s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0082
                 Mean entropy loss: 44.6094
                       Mean reward: 0.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0539
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 1.75s
                      Time elapsed: 00:01:12
                               ETA: 02:52:14

################################################################################
                      [1m Learning iteration 14/2000 [0m                      

                       Computation: 52736 steps/s (collection: 1.765s, learning 0.100s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0076
                 Mean entropy loss: 44.6681
                       Mean reward: 0.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0670
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 1.86s
                      Time elapsed: 00:01:14
                               ETA: 02:44:47

################################################################################
                      [1m Learning iteration 15/2000 [0m                      

                       Computation: 56945 steps/s (collection: 1.638s, learning 0.089s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 44.7567
                       Mean reward: 0.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0842
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 1.73s
                      Time elapsed: 00:01:16
                               ETA: 02:37:59

################################################################################
                      [1m Learning iteration 16/2000 [0m                      

                       Computation: 56636 steps/s (collection: 1.649s, learning 0.087s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0084
                 Mean entropy loss: 44.7632
                       Mean reward: 0.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1108
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 1.74s
                      Time elapsed: 00:01:18
                               ETA: 02:31:59

################################################################################
                      [1m Learning iteration 17/2000 [0m                      

                       Computation: 54654 steps/s (collection: 1.690s, learning 0.109s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.6093
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 44.8499
                       Mean reward: -0.87
               Mean episode length: 249.73
    Episode_Reward/reaching_object: 0.1331
     Episode_Reward/lifting_object: -0.0997
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 1.80s
                      Time elapsed: 00:01:19
                               ETA: 02:26:46

################################################################################
                      [1m Learning iteration 18/2000 [0m                      

                       Computation: 55274 steps/s (collection: 1.687s, learning 0.091s)
             Mean action noise std: 1.03
          Mean value_function loss: 12.4103
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 44.9382
                       Mean reward: -8.32
               Mean episode length: 249.24
    Episode_Reward/reaching_object: 0.1610
     Episode_Reward/lifting_object: -0.4919
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 1.78s
                      Time elapsed: 00:01:21
                               ETA: 02:22:04

################################################################################
                      [1m Learning iteration 19/2000 [0m                      

                       Computation: 51049 steps/s (collection: 1.813s, learning 0.113s)
             Mean action noise std: 1.04
          Mean value_function loss: 13.5569
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 45.1619
                       Mean reward: -4.05
               Mean episode length: 249.83
    Episode_Reward/reaching_object: 0.2067
     Episode_Reward/lifting_object: -0.9145
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 1.93s
                      Time elapsed: 00:01:23
                               ETA: 02:18:05

################################################################################
                      [1m Learning iteration 20/2000 [0m                      

                       Computation: 52831 steps/s (collection: 1.764s, learning 0.097s)
             Mean action noise std: 1.05
          Mean value_function loss: 16.6759
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 45.2985
                       Mean reward: -0.78
               Mean episode length: 249.78
    Episode_Reward/reaching_object: 0.2384
     Episode_Reward/lifting_object: -1.3645
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 1.86s
                      Time elapsed: 00:01:25
                               ETA: 02:14:22

################################################################################
                      [1m Learning iteration 21/2000 [0m                      

                       Computation: 51853 steps/s (collection: 1.799s, learning 0.097s)
             Mean action noise std: 1.05
          Mean value_function loss: 25.2358
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 45.4879
                       Mean reward: -10.83
               Mean episode length: 248.32
    Episode_Reward/reaching_object: 0.2766
     Episode_Reward/lifting_object: -1.7364
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 1.90s
                      Time elapsed: 00:01:27
                               ETA: 02:11:02

################################################################################
                      [1m Learning iteration 22/2000 [0m                      

                       Computation: 52385 steps/s (collection: 1.789s, learning 0.088s)
             Mean action noise std: 1.06
          Mean value_function loss: 21.3355
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 45.6151
                       Mean reward: -6.29
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.3176
     Episode_Reward/lifting_object: -1.8788
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 1.88s
                      Time elapsed: 00:01:29
                               ETA: 02:07:57

################################################################################
                      [1m Learning iteration 23/2000 [0m                      

                       Computation: 51061 steps/s (collection: 1.826s, learning 0.100s)
             Mean action noise std: 1.06
          Mean value_function loss: 10.7964
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 45.7786
                       Mean reward: -1.94
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 0.3182
     Episode_Reward/lifting_object: -1.5747
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 1.93s
                      Time elapsed: 00:01:31
                               ETA: 02:05:12

################################################################################
                      [1m Learning iteration 24/2000 [0m                      

                       Computation: 51935 steps/s (collection: 1.757s, learning 0.136s)
             Mean action noise std: 1.07
          Mean value_function loss: 1.9959
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 45.9127
                       Mean reward: -0.38
               Mean episode length: 249.71
    Episode_Reward/reaching_object: 0.3407
     Episode_Reward/lifting_object: -0.8228
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 1.89s
                      Time elapsed: 00:01:33
                               ETA: 02:02:38

################################################################################
                      [1m Learning iteration 25/2000 [0m                      

                       Computation: 55021 steps/s (collection: 1.697s, learning 0.090s)
             Mean action noise std: 1.07
          Mean value_function loss: 2.4720
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 46.0438
                       Mean reward: -0.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3372
     Episode_Reward/lifting_object: -0.4254
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 1.79s
                      Time elapsed: 00:01:34
                               ETA: 02:00:07

################################################################################
                      [1m Learning iteration 26/2000 [0m                      

                       Computation: 53867 steps/s (collection: 1.716s, learning 0.109s)
             Mean action noise std: 1.08
          Mean value_function loss: 1.6576
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 46.1989
                       Mean reward: 0.49
               Mean episode length: 249.36
    Episode_Reward/reaching_object: 0.2998
     Episode_Reward/lifting_object: -0.3412
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 1.82s
                      Time elapsed: 00:01:36
                               ETA: 01:57:50

################################################################################
                      [1m Learning iteration 27/2000 [0m                      

                       Computation: 53460 steps/s (collection: 1.726s, learning 0.113s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.1556
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 46.3742
                       Mean reward: 1.22
               Mean episode length: 249.94
    Episode_Reward/reaching_object: 0.2834
     Episode_Reward/lifting_object: -0.0429
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 1.84s
                      Time elapsed: 00:01:38
                               ETA: 01:55:44

################################################################################
                      [1m Learning iteration 28/2000 [0m                      

                       Computation: 55627 steps/s (collection: 1.672s, learning 0.095s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0045
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 46.4846
                       Mean reward: 1.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2456
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 1.77s
                      Time elapsed: 00:01:40
                               ETA: 01:53:41

################################################################################
                      [1m Learning iteration 29/2000 [0m                      

                       Computation: 57134 steps/s (collection: 1.629s, learning 0.092s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0018
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 46.6263
                       Mean reward: 0.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2062
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 1.72s
                      Time elapsed: 00:01:42
                               ETA: 01:51:43

################################################################################
                      [1m Learning iteration 30/2000 [0m                      

                       Computation: 55742 steps/s (collection: 1.665s, learning 0.099s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 46.7431
                       Mean reward: 0.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1577
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 1.76s
                      Time elapsed: 00:01:43
                               ETA: 01:49:56

################################################################################
                      [1m Learning iteration 31/2000 [0m                      

                       Computation: 51781 steps/s (collection: 1.765s, learning 0.133s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 46.8773
                       Mean reward: 0.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1404
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 1.90s
                      Time elapsed: 00:01:45
                               ETA: 01:48:23

################################################################################
                      [1m Learning iteration 32/2000 [0m                      

                       Computation: 55442 steps/s (collection: 1.657s, learning 0.116s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 46.9540
                       Mean reward: 0.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1107
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 1.77s
                      Time elapsed: 00:01:47
                               ETA: 01:46:49

################################################################################
                      [1m Learning iteration 33/2000 [0m                      

                       Computation: 54301 steps/s (collection: 1.662s, learning 0.149s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 46.9960
                       Mean reward: 0.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0959
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 1.81s
                      Time elapsed: 00:01:49
                               ETA: 01:45:22

################################################################################
                      [1m Learning iteration 34/2000 [0m                      

                       Computation: 53305 steps/s (collection: 1.709s, learning 0.135s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0087
                 Mean entropy loss: 47.0654
                       Mean reward: 0.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0865
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 1.84s
                      Time elapsed: 00:01:51
                               ETA: 01:44:02

################################################################################
                      [1m Learning iteration 35/2000 [0m                      

                       Computation: 56554 steps/s (collection: 1.636s, learning 0.102s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 47.1292
                       Mean reward: 0.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0826
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 1.74s
                      Time elapsed: 00:01:52
                               ETA: 01:42:40

################################################################################
                      [1m Learning iteration 36/2000 [0m                      

                       Computation: 50913 steps/s (collection: 1.833s, learning 0.098s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 47.2299
                       Mean reward: 0.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0793
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 1.93s
                      Time elapsed: 00:01:54
                               ETA: 01:41:33

################################################################################
                      [1m Learning iteration 37/2000 [0m                      

                       Computation: 56570 steps/s (collection: 1.640s, learning 0.098s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 47.2967
                       Mean reward: 0.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0742
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 1.74s
                      Time elapsed: 00:01:56
                               ETA: 01:40:19

################################################################################
                      [1m Learning iteration 38/2000 [0m                      

                       Computation: 55992 steps/s (collection: 1.634s, learning 0.122s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 47.3658
                       Mean reward: 0.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0789
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 1.76s
                      Time elapsed: 00:01:58
                               ETA: 01:39:10

################################################################################
                      [1m Learning iteration 39/2000 [0m                      

                       Computation: 56532 steps/s (collection: 1.642s, learning 0.097s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 47.4516
                       Mean reward: 0.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0838
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 1.74s
                      Time elapsed: 00:02:00
                               ETA: 01:38:04

################################################################################
                      [1m Learning iteration 40/2000 [0m                      

                       Computation: 54878 steps/s (collection: 1.680s, learning 0.112s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 47.5467
                       Mean reward: 0.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1026
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 1.79s
                      Time elapsed: 00:02:01
                               ETA: 01:37:03

################################################################################
                      [1m Learning iteration 41/2000 [0m                      

                       Computation: 54635 steps/s (collection: 1.704s, learning 0.095s)
             Mean action noise std: 1.13
          Mean value_function loss: 3.9752
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 47.6326
                       Mean reward: -1.47
               Mean episode length: 249.75
    Episode_Reward/reaching_object: 0.1152
     Episode_Reward/lifting_object: -0.0926
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 1.80s
                      Time elapsed: 00:02:03
                               ETA: 01:36:05

################################################################################
                      [1m Learning iteration 42/2000 [0m                      

                       Computation: 54203 steps/s (collection: 1.699s, learning 0.115s)
             Mean action noise std: 1.13
          Mean value_function loss: 4.1613
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 47.6956
                       Mean reward: -1.53
               Mean episode length: 249.97
    Episode_Reward/reaching_object: 0.1387
     Episode_Reward/lifting_object: -0.3283
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 1.81s
                      Time elapsed: 00:02:05
                               ETA: 01:35:11

################################################################################
                      [1m Learning iteration 43/2000 [0m                      

                       Computation: 50901 steps/s (collection: 1.810s, learning 0.122s)
             Mean action noise std: 1.14
          Mean value_function loss: 1.7454
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 47.8953
                       Mean reward: -1.42
               Mean episode length: 249.44
    Episode_Reward/reaching_object: 0.1648
     Episode_Reward/lifting_object: -0.1012
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 1.93s
                      Time elapsed: 00:02:07
                               ETA: 01:34:24

################################################################################
                      [1m Learning iteration 44/2000 [0m                      

                       Computation: 52285 steps/s (collection: 1.769s, learning 0.112s)
             Mean action noise std: 1.14
          Mean value_function loss: 1.8523
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.0211
                       Mean reward: 0.54
               Mean episode length: 248.52
    Episode_Reward/reaching_object: 0.1931
     Episode_Reward/lifting_object: -0.1959
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 1.88s
                      Time elapsed: 00:02:09
                               ETA: 01:33:37

################################################################################
                      [1m Learning iteration 45/2000 [0m                      

                       Computation: 53617 steps/s (collection: 1.739s, learning 0.094s)
             Mean action noise std: 1.15
          Mean value_function loss: 2.8924
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 48.1646
                       Mean reward: -0.61
               Mean episode length: 248.94
    Episode_Reward/reaching_object: 0.2337
     Episode_Reward/lifting_object: -0.3615
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 1.83s
                      Time elapsed: 00:02:11
                               ETA: 01:32:50

################################################################################
                      [1m Learning iteration 46/2000 [0m                      

                       Computation: 55404 steps/s (collection: 1.669s, learning 0.105s)
             Mean action noise std: 1.15
          Mean value_function loss: 7.4443
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 48.2815
                       Mean reward: -0.42
               Mean episode length: 249.11
    Episode_Reward/reaching_object: 0.2426
     Episode_Reward/lifting_object: -0.1624
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 1.77s
                      Time elapsed: 00:02:12
                               ETA: 01:32:03

################################################################################
                      [1m Learning iteration 47/2000 [0m                      

                       Computation: 53506 steps/s (collection: 1.745s, learning 0.093s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.2993
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.4047
                       Mean reward: 1.14
               Mean episode length: 249.46
    Episode_Reward/reaching_object: 0.2598
     Episode_Reward/lifting_object: -0.4504
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 1.84s
                      Time elapsed: 00:02:14
                               ETA: 01:31:20

################################################################################
                      [1m Learning iteration 48/2000 [0m                      

                       Computation: 53271 steps/s (collection: 1.747s, learning 0.098s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.5041
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 48.5392
                       Mean reward: 1.23
               Mean episode length: 249.63
    Episode_Reward/reaching_object: 0.2715
     Episode_Reward/lifting_object: -0.2636
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 1.85s
                      Time elapsed: 00:02:16
                               ETA: 01:30:38

################################################################################
                      [1m Learning iteration 49/2000 [0m                      

                       Computation: 52531 steps/s (collection: 1.773s, learning 0.099s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.8749
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.6755
                       Mean reward: 1.34
               Mean episode length: 248.95
    Episode_Reward/reaching_object: 0.2825
     Episode_Reward/lifting_object: -0.0088
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0119
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 1.87s
                      Time elapsed: 00:02:18
                               ETA: 01:30:00

################################################################################
                      [1m Learning iteration 50/2000 [0m                      

                       Computation: 55626 steps/s (collection: 1.670s, learning 0.097s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.4785
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 48.8080
                       Mean reward: 0.77
               Mean episode length: 249.61
    Episode_Reward/reaching_object: 0.2708
     Episode_Reward/lifting_object: -0.1787
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 1.77s
                      Time elapsed: 00:02:20
                               ETA: 01:29:19

################################################################################
                      [1m Learning iteration 51/2000 [0m                      

                       Computation: 55669 steps/s (collection: 1.667s, learning 0.099s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0889
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 48.9443
                       Mean reward: 1.23
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 0.2562
     Episode_Reward/lifting_object: -0.0446
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 1.77s
                      Time elapsed: 00:02:21
                               ETA: 01:28:39

################################################################################
                      [1m Learning iteration 52/2000 [0m                      

                       Computation: 55895 steps/s (collection: 1.655s, learning 0.104s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0343
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 49.1010
                       Mean reward: 1.15
               Mean episode length: 249.77
    Episode_Reward/reaching_object: 0.2308
     Episode_Reward/lifting_object: -0.0175
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 1.76s
                      Time elapsed: 00:02:23
                               ETA: 01:28:01

################################################################################
                      [1m Learning iteration 53/2000 [0m                      

                       Computation: 55431 steps/s (collection: 1.681s, learning 0.093s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 49.3808
                       Mean reward: 0.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2193
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 1.77s
                      Time elapsed: 00:02:25
                               ETA: 01:27:24

################################################################################
                      [1m Learning iteration 54/2000 [0m                      

                       Computation: 53409 steps/s (collection: 1.727s, learning 0.114s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0087
                 Mean entropy loss: 49.4006
                       Mean reward: 0.96
               Mean episode length: 249.75
    Episode_Reward/reaching_object: 0.1995
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 1.84s
                      Time elapsed: 00:02:27
                               ETA: 01:26:52

################################################################################
                      [1m Learning iteration 55/2000 [0m                      

                       Computation: 54884 steps/s (collection: 1.674s, learning 0.117s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0092
                 Mean entropy loss: 49.5366
                       Mean reward: 0.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1992
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 1.79s
                      Time elapsed: 00:02:29
                               ETA: 01:26:18

################################################################################
                      [1m Learning iteration 56/2000 [0m                      

                       Computation: 55163 steps/s (collection: 1.689s, learning 0.093s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 49.6300
                       Mean reward: 0.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1949
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 1.78s
                      Time elapsed: 00:02:30
                               ETA: 01:25:45

################################################################################
                      [1m Learning iteration 57/2000 [0m                      

                       Computation: 54892 steps/s (collection: 1.704s, learning 0.087s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0309
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 49.7078
                       Mean reward: 0.93
               Mean episode length: 249.55
    Episode_Reward/reaching_object: 0.2095
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0131
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 1.79s
                      Time elapsed: 00:02:32
                               ETA: 01:25:14

################################################################################
                      [1m Learning iteration 58/2000 [0m                      

                       Computation: 53832 steps/s (collection: 1.738s, learning 0.088s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0813
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 49.7694
                       Mean reward: 0.95
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 0.2106
     Episode_Reward/lifting_object: -0.0575
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0133
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 1.83s
                      Time elapsed: 00:02:34
                               ETA: 01:24:45

################################################################################
                      [1m Learning iteration 59/2000 [0m                      

                       Computation: 49976 steps/s (collection: 1.809s, learning 0.158s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.9275
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 49.9212
                       Mean reward: 0.51
               Mean episode length: 249.27
    Episode_Reward/reaching_object: 0.2275
     Episode_Reward/lifting_object: -0.1071
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 1.97s
                      Time elapsed: 00:02:36
                               ETA: 01:24:21

################################################################################
                      [1m Learning iteration 60/2000 [0m                      

                       Computation: 52159 steps/s (collection: 1.772s, learning 0.113s)
             Mean action noise std: 1.22
          Mean value_function loss: 1.7445
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 50.0207
                       Mean reward: -0.01
               Mean episode length: 249.35
    Episode_Reward/reaching_object: 0.2496
     Episode_Reward/lifting_object: -0.2093
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 1.88s
                      Time elapsed: 00:02:38
                               ETA: 01:23:56

################################################################################
                      [1m Learning iteration 61/2000 [0m                      

                       Computation: 52309 steps/s (collection: 1.755s, learning 0.125s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.5215
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.1331
                       Mean reward: 0.84
               Mean episode length: 246.79
    Episode_Reward/reaching_object: 0.2752
     Episode_Reward/lifting_object: -0.1953
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 1.88s
                      Time elapsed: 00:02:40
                               ETA: 01:23:31

################################################################################
                      [1m Learning iteration 62/2000 [0m                      

                       Computation: 51622 steps/s (collection: 1.811s, learning 0.093s)
             Mean action noise std: 1.23
          Mean value_function loss: 4.8472
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 50.2375
                       Mean reward: 1.31
               Mean episode length: 246.61
    Episode_Reward/reaching_object: 0.3181
     Episode_Reward/lifting_object: -0.3444
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 1.90s
                      Time elapsed: 00:02:42
                               ETA: 01:23:07

################################################################################
                      [1m Learning iteration 63/2000 [0m                      

                       Computation: 50461 steps/s (collection: 1.845s, learning 0.103s)
             Mean action noise std: 1.23
          Mean value_function loss: 1.0768
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.3409
                       Mean reward: 1.11
               Mean episode length: 244.74
    Episode_Reward/reaching_object: 0.3394
     Episode_Reward/lifting_object: -0.3789
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 1.95s
                      Time elapsed: 00:02:44
                               ETA: 01:22:46

################################################################################
                      [1m Learning iteration 64/2000 [0m                      

                       Computation: 52366 steps/s (collection: 1.779s, learning 0.099s)
             Mean action noise std: 1.23
          Mean value_function loss: 1.1215
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.4583
                       Mean reward: 1.95
               Mean episode length: 243.57
    Episode_Reward/reaching_object: 0.3735
     Episode_Reward/lifting_object: -0.1282
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 1.88s
                      Time elapsed: 00:02:45
                               ETA: 01:22:23

################################################################################
                      [1m Learning iteration 65/2000 [0m                      

                       Computation: 53052 steps/s (collection: 1.756s, learning 0.097s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.9409
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.5568
                       Mean reward: 1.40
               Mean episode length: 244.81
    Episode_Reward/reaching_object: 0.3721
     Episode_Reward/lifting_object: -0.2353
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 1.85s
                      Time elapsed: 00:02:47
                               ETA: 01:21:59

################################################################################
                      [1m Learning iteration 66/2000 [0m                      

                       Computation: 50802 steps/s (collection: 1.825s, learning 0.110s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.3977
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 50.6562
                       Mean reward: 0.97
               Mean episode length: 245.20
    Episode_Reward/reaching_object: 0.3912
     Episode_Reward/lifting_object: -0.0637
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 1.94s
                      Time elapsed: 00:02:49
                               ETA: 01:21:39

################################################################################
                      [1m Learning iteration 67/2000 [0m                      

                       Computation: 53359 steps/s (collection: 1.734s, learning 0.108s)
             Mean action noise std: 1.25
          Mean value_function loss: 2.4257
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 50.7407
                       Mean reward: 1.93
               Mean episode length: 243.09
    Episode_Reward/reaching_object: 0.4003
     Episode_Reward/lifting_object: -0.1780
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 1.84s
                      Time elapsed: 00:02:51
                               ETA: 01:21:17

################################################################################
                      [1m Learning iteration 68/2000 [0m                      

                       Computation: 52755 steps/s (collection: 1.751s, learning 0.113s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0630
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.8520
                       Mean reward: 1.44
               Mean episode length: 244.23
    Episode_Reward/reaching_object: 0.3875
     Episode_Reward/lifting_object: -0.0474
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 1.86s
                      Time elapsed: 00:02:53
                               ETA: 01:20:56

################################################################################
                      [1m Learning iteration 69/2000 [0m                      

                       Computation: 53855 steps/s (collection: 1.704s, learning 0.121s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.1611
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 51.0034
                       Mean reward: 1.30
               Mean episode length: 242.57
    Episode_Reward/reaching_object: 0.3892
     Episode_Reward/lifting_object: -0.0441
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 1.83s
                      Time elapsed: 00:02:55
                               ETA: 01:20:35

################################################################################
                      [1m Learning iteration 70/2000 [0m                      

                       Computation: 54289 steps/s (collection: 1.713s, learning 0.098s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0341
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 51.0653
                       Mean reward: 1.42
               Mean episode length: 246.02
    Episode_Reward/reaching_object: 0.3695
     Episode_Reward/lifting_object: -0.0104
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 1.81s
                      Time elapsed: 00:02:57
                               ETA: 01:20:13

################################################################################
                      [1m Learning iteration 71/2000 [0m                      

                       Computation: 54180 steps/s (collection: 1.698s, learning 0.116s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0041
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 51.2546
                       Mean reward: 1.53
               Mean episode length: 244.01
    Episode_Reward/reaching_object: 0.3469
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 1.81s
                      Time elapsed: 00:02:58
                               ETA: 01:19:53

################################################################################
                      [1m Learning iteration 72/2000 [0m                      

                       Computation: 53465 steps/s (collection: 1.742s, learning 0.097s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0638
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 51.2141
                       Mean reward: 1.39
               Mean episode length: 244.05
    Episode_Reward/reaching_object: 0.3215
     Episode_Reward/lifting_object: -0.0286
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 1.84s
                      Time elapsed: 00:03:00
                               ETA: 01:19:33

################################################################################
                      [1m Learning iteration 73/2000 [0m                      

                       Computation: 50374 steps/s (collection: 1.858s, learning 0.093s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0093
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 51.2389
                       Mean reward: 1.55
               Mean episode length: 244.55
    Episode_Reward/reaching_object: 0.3317
     Episode_Reward/lifting_object: -0.0104
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 1.95s
                      Time elapsed: 00:03:02
                               ETA: 01:19:17

################################################################################
                      [1m Learning iteration 74/2000 [0m                      

                       Computation: 52696 steps/s (collection: 1.774s, learning 0.091s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0321
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.3563
                       Mean reward: 1.42
               Mean episode length: 242.61
    Episode_Reward/reaching_object: 0.3338
     Episode_Reward/lifting_object: -0.0080
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 1.87s
                      Time elapsed: 00:03:04
                               ETA: 01:18:59

################################################################################
                      [1m Learning iteration 75/2000 [0m                      

                       Computation: 51338 steps/s (collection: 1.777s, learning 0.138s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.6450
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 51.4675
                       Mean reward: 1.28
               Mean episode length: 236.97
    Episode_Reward/reaching_object: 0.3461
     Episode_Reward/lifting_object: -0.0617
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 1.91s
                      Time elapsed: 00:03:06
                               ETA: 01:18:43

################################################################################
                      [1m Learning iteration 76/2000 [0m                      

                       Computation: 51979 steps/s (collection: 1.764s, learning 0.128s)
             Mean action noise std: 1.28
          Mean value_function loss: 1.2974
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 51.5186
                       Mean reward: 1.02
               Mean episode length: 238.18
    Episode_Reward/reaching_object: 0.3514
     Episode_Reward/lifting_object: -0.2063
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 1.89s
                      Time elapsed: 00:03:08
                               ETA: 01:18:26

################################################################################
                      [1m Learning iteration 77/2000 [0m                      

                       Computation: 53365 steps/s (collection: 1.733s, learning 0.109s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.8239
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.6055
                       Mean reward: 0.39
               Mean episode length: 235.13
    Episode_Reward/reaching_object: 0.3652
     Episode_Reward/lifting_object: -0.2538
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 1.84s
                      Time elapsed: 00:03:10
                               ETA: 01:18:09

################################################################################
                      [1m Learning iteration 78/2000 [0m                      

                       Computation: 50134 steps/s (collection: 1.808s, learning 0.153s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.2219
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 51.6910
                       Mean reward: 1.52
               Mean episode length: 237.60
    Episode_Reward/reaching_object: 0.3725
     Episode_Reward/lifting_object: -0.0455
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 1.96s
                      Time elapsed: 00:03:12
                               ETA: 01:17:55

################################################################################
                      [1m Learning iteration 79/2000 [0m                      

                       Computation: 51486 steps/s (collection: 1.813s, learning 0.096s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.6481
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.8068
                       Mean reward: 0.46
               Mean episode length: 229.26
    Episode_Reward/reaching_object: 0.3947
     Episode_Reward/lifting_object: -0.0781
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 1.91s
                      Time elapsed: 00:03:14
                               ETA: 01:17:40

################################################################################
                      [1m Learning iteration 80/2000 [0m                      

                       Computation: 52665 steps/s (collection: 1.770s, learning 0.097s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.7828
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.9025
                       Mean reward: 1.24
               Mean episode length: 230.07
    Episode_Reward/reaching_object: 0.3958
     Episode_Reward/lifting_object: -0.1426
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 1.87s
                      Time elapsed: 00:03:15
                               ETA: 01:17:24

################################################################################
                      [1m Learning iteration 81/2000 [0m                      

                       Computation: 53487 steps/s (collection: 1.750s, learning 0.088s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.3128
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 52.0011
                       Mean reward: 2.00
               Mean episode length: 235.80
    Episode_Reward/reaching_object: 0.3999
     Episode_Reward/lifting_object: -0.0696
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 1.84s
                      Time elapsed: 00:03:17
                               ETA: 01:17:08

################################################################################
                      [1m Learning iteration 82/2000 [0m                      

                       Computation: 52322 steps/s (collection: 1.789s, learning 0.090s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.2295
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.1091
                       Mean reward: 1.61
               Mean episode length: 230.35
    Episode_Reward/reaching_object: 0.4208
     Episode_Reward/lifting_object: -0.0381
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 1.88s
                      Time elapsed: 00:03:19
                               ETA: 01:16:53

################################################################################
                      [1m Learning iteration 83/2000 [0m                      

                       Computation: 52144 steps/s (collection: 1.778s, learning 0.107s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.1793
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 52.2080
                       Mean reward: 1.89
               Mean episode length: 235.64
    Episode_Reward/reaching_object: 0.4039
     Episode_Reward/lifting_object: -0.0745
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 1.89s
                      Time elapsed: 00:03:21
                               ETA: 01:16:39

################################################################################
                      [1m Learning iteration 84/2000 [0m                      

                       Computation: 50204 steps/s (collection: 1.863s, learning 0.095s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0600
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.3100
                       Mean reward: 1.63
               Mean episode length: 237.21
    Episode_Reward/reaching_object: 0.4063
     Episode_Reward/lifting_object: -0.0098
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 1.96s
                      Time elapsed: 00:03:23
                               ETA: 01:16:27

################################################################################
                      [1m Learning iteration 85/2000 [0m                      

                       Computation: 51617 steps/s (collection: 1.780s, learning 0.125s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.2283
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 52.4061
                       Mean reward: 1.44
               Mean episode length: 240.80
    Episode_Reward/reaching_object: 0.4027
     Episode_Reward/lifting_object: -0.0655
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 1.90s
                      Time elapsed: 00:03:25
                               ETA: 01:16:13

################################################################################
                      [1m Learning iteration 86/2000 [0m                      

                       Computation: 51239 steps/s (collection: 1.776s, learning 0.142s)
             Mean action noise std: 1.32
          Mean value_function loss: 2.1561
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.4799
                       Mean reward: 0.47
               Mean episode length: 239.79
    Episode_Reward/reaching_object: 0.3975
     Episode_Reward/lifting_object: -0.2572
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 1.92s
                      Time elapsed: 00:03:27
                               ETA: 01:16:01

################################################################################
                      [1m Learning iteration 87/2000 [0m                      

                       Computation: 52073 steps/s (collection: 1.786s, learning 0.101s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0191
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 52.5737
                       Mean reward: 1.79
               Mean episode length: 240.65
    Episode_Reward/reaching_object: 0.3936
     Episode_Reward/lifting_object: -0.0238
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 1.89s
                      Time elapsed: 00:03:29
                               ETA: 01:15:48

################################################################################
                      [1m Learning iteration 88/2000 [0m                      

                       Computation: 53223 steps/s (collection: 1.752s, learning 0.095s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.1152
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 52.7341
                       Mean reward: 1.91
               Mean episode length: 239.05
    Episode_Reward/reaching_object: 0.3912
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 1.85s
                      Time elapsed: 00:03:31
                               ETA: 01:15:34

################################################################################
                      [1m Learning iteration 89/2000 [0m                      

                       Computation: 52768 steps/s (collection: 1.774s, learning 0.089s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.2840
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.7855
                       Mean reward: 1.22
               Mean episode length: 242.28
    Episode_Reward/reaching_object: 0.3658
     Episode_Reward/lifting_object: -0.1022
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 1.86s
                      Time elapsed: 00:03:32
                               ETA: 01:15:21

################################################################################
                      [1m Learning iteration 90/2000 [0m                      

                       Computation: 53226 steps/s (collection: 1.751s, learning 0.096s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0394
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.8815
                       Mean reward: 1.63
               Mean episode length: 244.54
    Episode_Reward/reaching_object: 0.3706
     Episode_Reward/lifting_object: -0.0472
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 1.85s
                      Time elapsed: 00:03:34
                               ETA: 01:15:07

################################################################################
                      [1m Learning iteration 91/2000 [0m                      

                       Computation: 52789 steps/s (collection: 1.755s, learning 0.108s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.1048
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 53.0480
                       Mean reward: 1.78
               Mean episode length: 243.13
    Episode_Reward/reaching_object: 0.3763
     Episode_Reward/lifting_object: -0.0385
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 1.86s
                      Time elapsed: 00:03:36
                               ETA: 01:14:55

################################################################################
                      [1m Learning iteration 92/2000 [0m                      

                       Computation: 50451 steps/s (collection: 1.805s, learning 0.143s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0455
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.0935
                       Mean reward: 1.59
               Mean episode length: 241.20
    Episode_Reward/reaching_object: 0.3690
     Episode_Reward/lifting_object: -0.0165
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 1.95s
                      Time elapsed: 00:03:38
                               ETA: 01:14:44

################################################################################
                      [1m Learning iteration 93/2000 [0m                      

                       Computation: 46507 steps/s (collection: 1.998s, learning 0.116s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.9221
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.1884
                       Mean reward: -0.41
               Mean episode length: 240.70
    Episode_Reward/reaching_object: 0.3608
     Episode_Reward/lifting_object: -0.1194
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 2.11s
                      Time elapsed: 00:03:40
                               ETA: 01:14:37

################################################################################
                      [1m Learning iteration 94/2000 [0m                      

                       Computation: 48823 steps/s (collection: 1.884s, learning 0.129s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.8094
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.2532
                       Mean reward: 1.69
               Mean episode length: 241.02
    Episode_Reward/reaching_object: 0.3537
     Episode_Reward/lifting_object: -0.0841
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 2.01s
                      Time elapsed: 00:03:42
                               ETA: 01:14:28

################################################################################
                      [1m Learning iteration 95/2000 [0m                      

                       Computation: 50461 steps/s (collection: 1.834s, learning 0.114s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.3097
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 53.3593
                       Mean reward: 1.80
               Mean episode length: 237.00
    Episode_Reward/reaching_object: 0.3665
     Episode_Reward/lifting_object: -0.1867
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 1.95s
                      Time elapsed: 00:03:44
                               ETA: 01:14:18

################################################################################
                      [1m Learning iteration 96/2000 [0m                      

                       Computation: 52241 steps/s (collection: 1.791s, learning 0.091s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.3541
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.4511
                       Mean reward: 1.64
               Mean episode length: 237.88
    Episode_Reward/reaching_object: 0.3614
     Episode_Reward/lifting_object: -0.0402
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 1.88s
                      Time elapsed: 00:03:46
                               ETA: 01:14:06

################################################################################
                      [1m Learning iteration 97/2000 [0m                      

                       Computation: 52138 steps/s (collection: 1.783s, learning 0.103s)
             Mean action noise std: 1.36
          Mean value_function loss: 1.0398
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.5220
                       Mean reward: -0.26
               Mean episode length: 234.06
    Episode_Reward/reaching_object: 0.3729
     Episode_Reward/lifting_object: -0.1911
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 1.89s
                      Time elapsed: 00:03:48
                               ETA: 01:13:55

################################################################################
                      [1m Learning iteration 98/2000 [0m                      

                       Computation: 52719 steps/s (collection: 1.764s, learning 0.101s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.7128
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.5871
                       Mean reward: 1.71
               Mean episode length: 229.97
    Episode_Reward/reaching_object: 0.3869
     Episode_Reward/lifting_object: -0.1437
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 1.86s
                      Time elapsed: 00:03:50
                               ETA: 01:13:44

################################################################################
                      [1m Learning iteration 99/2000 [0m                      

                       Computation: 51767 steps/s (collection: 1.809s, learning 0.090s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.6454
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 53.6930
                       Mean reward: 1.74
               Mean episode length: 233.70
    Episode_Reward/reaching_object: 0.3991
     Episode_Reward/lifting_object: -0.0758
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 1.90s
                      Time elapsed: 00:03:52
                               ETA: 01:13:33

################################################################################
                     [1m Learning iteration 100/2000 [0m                      

                       Computation: 49933 steps/s (collection: 1.862s, learning 0.107s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.1107
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.7675
                       Mean reward: 1.67
               Mean episode length: 229.58
    Episode_Reward/reaching_object: 0.3955
     Episode_Reward/lifting_object: -0.0620
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 1.97s
                      Time elapsed: 00:03:54
                               ETA: 01:13:24

################################################################################
                     [1m Learning iteration 101/2000 [0m                      

                       Computation: 51726 steps/s (collection: 1.790s, learning 0.110s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.0529
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 53.9024
                       Mean reward: 1.94
               Mean episode length: 233.15
    Episode_Reward/reaching_object: 0.3985
     Episode_Reward/lifting_object: -0.0136
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 1.90s
                      Time elapsed: 00:03:56
                               ETA: 01:13:14

################################################################################
                     [1m Learning iteration 102/2000 [0m                      

                       Computation: 52763 steps/s (collection: 1.770s, learning 0.093s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.6035
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 54.0825
                       Mean reward: 1.01
               Mean episode length: 227.11
    Episode_Reward/reaching_object: 0.3966
     Episode_Reward/lifting_object: -0.1086
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 1.86s
                      Time elapsed: 00:03:57
                               ETA: 01:13:04

################################################################################
                     [1m Learning iteration 103/2000 [0m                      

                       Computation: 51365 steps/s (collection: 1.805s, learning 0.109s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.1993
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 54.1250
                       Mean reward: 1.97
               Mean episode length: 230.67
    Episode_Reward/reaching_object: 0.4003
     Episode_Reward/lifting_object: -0.0385
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 1.91s
                      Time elapsed: 00:03:59
                               ETA: 01:12:54

################################################################################
                     [1m Learning iteration 104/2000 [0m                      

                       Computation: 52443 steps/s (collection: 1.780s, learning 0.094s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.7355
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.2009
                       Mean reward: 1.03
               Mean episode length: 234.26
    Episode_Reward/reaching_object: 0.4079
     Episode_Reward/lifting_object: -0.0807
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 1.87s
                      Time elapsed: 00:04:01
                               ETA: 01:12:44

################################################################################
                     [1m Learning iteration 105/2000 [0m                      

                       Computation: 52751 steps/s (collection: 1.749s, learning 0.114s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.1483
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 54.2727
                       Mean reward: 1.79
               Mean episode length: 234.55
    Episode_Reward/reaching_object: 0.3759
     Episode_Reward/lifting_object: -0.0957
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 1.86s
                      Time elapsed: 00:04:03
                               ETA: 01:12:34

################################################################################
                     [1m Learning iteration 106/2000 [0m                      

                       Computation: 51033 steps/s (collection: 1.810s, learning 0.116s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.2637
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 54.4419
                       Mean reward: 1.56
               Mean episode length: 236.76
    Episode_Reward/reaching_object: 0.3822
     Episode_Reward/lifting_object: -0.0906
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 1.93s
                      Time elapsed: 00:04:05
                               ETA: 01:12:25

################################################################################
                     [1m Learning iteration 107/2000 [0m                      

                       Computation: 52644 steps/s (collection: 1.769s, learning 0.098s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.7862
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.4968
                       Mean reward: 1.71
               Mean episode length: 233.55
    Episode_Reward/reaching_object: 0.3926
     Episode_Reward/lifting_object: -0.1526
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 1.87s
                      Time elapsed: 00:04:07
                               ETA: 01:12:15

################################################################################
                     [1m Learning iteration 108/2000 [0m                      

                       Computation: 53768 steps/s (collection: 1.737s, learning 0.091s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.0053
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 54.5757
                       Mean reward: 1.68
               Mean episode length: 236.91
    Episode_Reward/reaching_object: 0.3631
     Episode_Reward/lifting_object: -0.0224
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 1.83s
                      Time elapsed: 00:04:09
                               ETA: 01:12:05

################################################################################
                     [1m Learning iteration 109/2000 [0m                      

                       Computation: 53019 steps/s (collection: 1.761s, learning 0.093s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.1550
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 54.6725
                       Mean reward: 1.81
               Mean episode length: 236.49
    Episode_Reward/reaching_object: 0.3665
     Episode_Reward/lifting_object: -0.0484
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 1.85s
                      Time elapsed: 00:04:11
                               ETA: 01:11:55

################################################################################
                     [1m Learning iteration 110/2000 [0m                      

                       Computation: 52170 steps/s (collection: 1.769s, learning 0.116s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0450
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.7144
                       Mean reward: 1.39
               Mean episode length: 235.41
    Episode_Reward/reaching_object: 0.3450
     Episode_Reward/lifting_object: -0.0212
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 1.88s
                      Time elapsed: 00:04:12
                               ETA: 01:11:46

################################################################################
                     [1m Learning iteration 111/2000 [0m                      

                       Computation: 52032 steps/s (collection: 1.803s, learning 0.087s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0293
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 54.8630
                       Mean reward: 1.50
               Mean episode length: 235.73
    Episode_Reward/reaching_object: 0.3409
     Episode_Reward/lifting_object: -0.0088
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 1.89s
                      Time elapsed: 00:04:14
                               ETA: 01:11:37

################################################################################
                     [1m Learning iteration 112/2000 [0m                      

                       Computation: 51937 steps/s (collection: 1.787s, learning 0.106s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.1492
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 54.9271
                       Mean reward: 1.25
               Mean episode length: 231.82
    Episode_Reward/reaching_object: 0.3459
     Episode_Reward/lifting_object: -0.0374
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 1.89s
                      Time elapsed: 00:04:16
                               ETA: 01:11:29

################################################################################
                     [1m Learning iteration 113/2000 [0m                      

                       Computation: 51634 steps/s (collection: 1.811s, learning 0.093s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.0058
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 54.9609
                       Mean reward: 1.60
               Mean episode length: 229.93
    Episode_Reward/reaching_object: 0.3483
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 1.90s
                      Time elapsed: 00:04:18
                               ETA: 01:11:20

################################################################################
                     [1m Learning iteration 114/2000 [0m                      

                       Computation: 52229 steps/s (collection: 1.785s, learning 0.097s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.4991
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 55.0286
                       Mean reward: 0.93
               Mean episode length: 232.51
    Episode_Reward/reaching_object: 0.3364
     Episode_Reward/lifting_object: -0.0655
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 1.88s
                      Time elapsed: 00:04:20
                               ETA: 01:11:12

################################################################################
                     [1m Learning iteration 115/2000 [0m                      

                       Computation: 52737 steps/s (collection: 1.769s, learning 0.095s)
             Mean action noise std: 1.43
          Mean value_function loss: 1.7745
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.0681
                       Mean reward: 0.63
               Mean episode length: 226.77
    Episode_Reward/reaching_object: 0.3451
     Episode_Reward/lifting_object: -0.2881
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 1.86s
                      Time elapsed: 00:04:22
                               ETA: 01:11:03

################################################################################
                     [1m Learning iteration 116/2000 [0m                      

                       Computation: 49193 steps/s (collection: 1.905s, learning 0.093s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.5920
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.1375
                       Mean reward: 1.62
               Mean episode length: 232.67
    Episode_Reward/reaching_object: 0.3565
     Episode_Reward/lifting_object: -0.1143
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 2.00s
                      Time elapsed: 00:04:24
                               ETA: 01:10:56

################################################################################
                     [1m Learning iteration 117/2000 [0m                      

                       Computation: 51821 steps/s (collection: 1.802s, learning 0.095s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.2854
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.2099
                       Mean reward: 1.11
               Mean episode length: 222.91
    Episode_Reward/reaching_object: 0.3571
     Episode_Reward/lifting_object: -0.0833
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 1.90s
                      Time elapsed: 00:04:26
                               ETA: 01:10:48

################################################################################
                     [1m Learning iteration 118/2000 [0m                      

                       Computation: 49455 steps/s (collection: 1.824s, learning 0.164s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.0611
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.3087
                       Mean reward: 1.58
               Mean episode length: 225.66
    Episode_Reward/reaching_object: 0.3553
     Episode_Reward/lifting_object: -0.0148
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 1.99s
                      Time elapsed: 00:04:28
                               ETA: 01:10:42

################################################################################
                     [1m Learning iteration 119/2000 [0m                      

                       Computation: 52836 steps/s (collection: 1.763s, learning 0.098s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.0770
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 55.4810
                       Mean reward: 1.67
               Mean episode length: 231.64
    Episode_Reward/reaching_object: 0.3587
     Episode_Reward/lifting_object: -0.0375
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 1.86s
                      Time elapsed: 00:04:30
                               ETA: 01:10:33

################################################################################
                     [1m Learning iteration 120/2000 [0m                      

                       Computation: 52637 steps/s (collection: 1.771s, learning 0.097s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.2467
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 55.5157
                       Mean reward: 1.38
               Mean episode length: 228.59
    Episode_Reward/reaching_object: 0.3741
     Episode_Reward/lifting_object: -0.0142
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 1.87s
                      Time elapsed: 00:04:31
                               ETA: 01:10:25

################################################################################
                     [1m Learning iteration 121/2000 [0m                      

                       Computation: 51379 steps/s (collection: 1.817s, learning 0.096s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.1705
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.5623
                       Mean reward: 1.56
               Mean episode length: 229.58
    Episode_Reward/reaching_object: 0.3987
     Episode_Reward/lifting_object: -0.0543
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 1.91s
                      Time elapsed: 00:04:33
                               ETA: 01:10:18

################################################################################
                     [1m Learning iteration 122/2000 [0m                      

                       Computation: 51637 steps/s (collection: 1.792s, learning 0.112s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.1179
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 55.6345
                       Mean reward: 0.92
               Mean episode length: 233.87
    Episode_Reward/reaching_object: 0.3771
     Episode_Reward/lifting_object: -0.0355
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 1.90s
                      Time elapsed: 00:04:35
                               ETA: 01:10:10

################################################################################
                     [1m Learning iteration 123/2000 [0m                      

                       Computation: 50616 steps/s (collection: 1.831s, learning 0.112s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.0544
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 55.7617
                       Mean reward: 1.66
               Mean episode length: 222.98
    Episode_Reward/reaching_object: 0.3948
     Episode_Reward/lifting_object: -0.0320
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 1.94s
                      Time elapsed: 00:04:37
                               ETA: 01:10:04

################################################################################
                     [1m Learning iteration 124/2000 [0m                      

                       Computation: 50354 steps/s (collection: 1.804s, learning 0.149s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.1570
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 55.9203
                       Mean reward: 1.67
               Mean episode length: 220.24
    Episode_Reward/reaching_object: 0.3604
     Episode_Reward/lifting_object: -0.0322
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 1.95s
                      Time elapsed: 00:04:39
                               ETA: 01:09:57

################################################################################
                     [1m Learning iteration 125/2000 [0m                      

                       Computation: 49581 steps/s (collection: 1.833s, learning 0.150s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.7572
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.9544
                       Mean reward: 1.53
               Mean episode length: 221.14
    Episode_Reward/reaching_object: 0.3603
     Episode_Reward/lifting_object: -0.1234
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 1.98s
                      Time elapsed: 00:04:41
                               ETA: 01:09:51

################################################################################
                     [1m Learning iteration 126/2000 [0m                      

                       Computation: 49550 steps/s (collection: 1.890s, learning 0.094s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.3784
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.0083
                       Mean reward: 0.91
               Mean episode length: 217.43
    Episode_Reward/reaching_object: 0.3532
     Episode_Reward/lifting_object: -0.0548
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 1.98s
                      Time elapsed: 00:04:43
                               ETA: 01:09:45

################################################################################
                     [1m Learning iteration 127/2000 [0m                      

                       Computation: 51646 steps/s (collection: 1.816s, learning 0.087s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.4791
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 56.0676
                       Mean reward: 1.49
               Mean episode length: 217.87
    Episode_Reward/reaching_object: 0.3659
     Episode_Reward/lifting_object: -0.0592
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 1.90s
                      Time elapsed: 00:04:45
                               ETA: 01:09:38

################################################################################
                     [1m Learning iteration 128/2000 [0m                      

                       Computation: 48858 steps/s (collection: 1.815s, learning 0.197s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.3019
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.1349
                       Mean reward: 1.07
               Mean episode length: 205.11
    Episode_Reward/reaching_object: 0.3684
     Episode_Reward/lifting_object: -0.1085
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.0833
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 2.01s
                      Time elapsed: 00:04:47
                               ETA: 01:09:33

################################################################################
                     [1m Learning iteration 129/2000 [0m                      

                       Computation: 51012 steps/s (collection: 1.836s, learning 0.091s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.1412
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.1855
                       Mean reward: 1.77
               Mean episode length: 201.37
    Episode_Reward/reaching_object: 0.3680
     Episode_Reward/lifting_object: -0.0778
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 3.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 1.93s
                      Time elapsed: 00:04:49
                               ETA: 01:09:26

################################################################################
                     [1m Learning iteration 130/2000 [0m                      

                       Computation: 53071 steps/s (collection: 1.757s, learning 0.095s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.3513
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.2530
                       Mean reward: 1.85
               Mean episode length: 209.42
    Episode_Reward/reaching_object: 0.3921
     Episode_Reward/lifting_object: -0.0351
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 4.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 1.85s
                      Time elapsed: 00:04:51
                               ETA: 01:09:18

################################################################################
                     [1m Learning iteration 131/2000 [0m                      

                       Computation: 52543 steps/s (collection: 1.779s, learning 0.092s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.4859
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.3123
                       Mean reward: 1.91
               Mean episode length: 205.31
    Episode_Reward/reaching_object: 0.3914
     Episode_Reward/lifting_object: -0.0918
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 1.87s
                      Time elapsed: 00:04:53
                               ETA: 01:09:11

################################################################################
                     [1m Learning iteration 132/2000 [0m                      

                       Computation: 53395 steps/s (collection: 1.746s, learning 0.096s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.3832
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 56.3828
                       Mean reward: 1.87
               Mean episode length: 201.21
    Episode_Reward/reaching_object: 0.3895
     Episode_Reward/lifting_object: -0.0910
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 1.84s
                      Time elapsed: 00:04:55
                               ETA: 01:09:04

################################################################################
                     [1m Learning iteration 133/2000 [0m                      

                       Computation: 51237 steps/s (collection: 1.831s, learning 0.088s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.7099
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 56.4599
                       Mean reward: 1.26
               Mean episode length: 207.53
    Episode_Reward/reaching_object: 0.3912
     Episode_Reward/lifting_object: -0.0711
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.7083
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 1.92s
                      Time elapsed: 00:04:56
                               ETA: 01:08:57

################################################################################
                     [1m Learning iteration 134/2000 [0m                      

                       Computation: 52876 steps/s (collection: 1.769s, learning 0.091s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.0264
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.5228
                       Mean reward: 1.89
               Mean episode length: 212.06
    Episode_Reward/reaching_object: 0.4024
     Episode_Reward/lifting_object: -0.0303
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.7917
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 1.86s
                      Time elapsed: 00:04:58
                               ETA: 01:08:50

################################################################################
                     [1m Learning iteration 135/2000 [0m                      

                       Computation: 51724 steps/s (collection: 1.784s, learning 0.116s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.3434
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 56.6358
                       Mean reward: 1.93
               Mean episode length: 216.17
    Episode_Reward/reaching_object: 0.3946
     Episode_Reward/lifting_object: -0.0367
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 1.90s
                      Time elapsed: 00:05:00
                               ETA: 01:08:44

################################################################################
                     [1m Learning iteration 136/2000 [0m                      

                       Computation: 51057 steps/s (collection: 1.810s, learning 0.116s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.0713
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.6759
                       Mean reward: 1.67
               Mean episode length: 216.01
    Episode_Reward/reaching_object: 0.4036
     Episode_Reward/lifting_object: -0.0594
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 1.93s
                      Time elapsed: 00:05:02
                               ETA: 01:08:38

################################################################################
                     [1m Learning iteration 137/2000 [0m                      

                       Computation: 50225 steps/s (collection: 1.845s, learning 0.113s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.1386
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 56.7926
                       Mean reward: 1.75
               Mean episode length: 217.82
    Episode_Reward/reaching_object: 0.3817
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 1.96s
                      Time elapsed: 00:05:04
                               ETA: 01:08:32

################################################################################
                     [1m Learning iteration 138/2000 [0m                      

                       Computation: 51179 steps/s (collection: 1.827s, learning 0.094s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.1957
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.8399
                       Mean reward: 1.31
               Mean episode length: 224.41
    Episode_Reward/reaching_object: 0.3642
     Episode_Reward/lifting_object: -0.0566
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 1.92s
                      Time elapsed: 00:05:06
                               ETA: 01:08:26

################################################################################
                     [1m Learning iteration 139/2000 [0m                      

                       Computation: 47286 steps/s (collection: 1.977s, learning 0.102s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.6973
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.9047
                       Mean reward: 1.71
               Mean episode length: 216.20
    Episode_Reward/reaching_object: 0.3699
     Episode_Reward/lifting_object: -0.1359
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.7083
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 2.08s
                      Time elapsed: 00:05:08
                               ETA: 01:08:22

################################################################################
                     [1m Learning iteration 140/2000 [0m                      

                       Computation: 50706 steps/s (collection: 1.826s, learning 0.113s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.1388
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 56.9623
                       Mean reward: 1.61
               Mean episode length: 209.65
    Episode_Reward/reaching_object: 0.3579
     Episode_Reward/lifting_object: -0.0288
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 4.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.6667
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 1.94s
                      Time elapsed: 00:05:10
                               ETA: 01:08:16

################################################################################
                     [1m Learning iteration 141/2000 [0m                      

                       Computation: 49503 steps/s (collection: 1.880s, learning 0.106s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.9779
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.0586
                       Mean reward: 0.53
               Mean episode length: 212.70
    Episode_Reward/reaching_object: 0.3738
     Episode_Reward/lifting_object: -0.0785
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.5000
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 1.99s
                      Time elapsed: 00:05:12
                               ETA: 01:08:11

################################################################################
                     [1m Learning iteration 142/2000 [0m                      

                       Computation: 50650 steps/s (collection: 1.834s, learning 0.107s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.0601
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.1026
                       Mean reward: 1.40
               Mean episode length: 204.63
    Episode_Reward/reaching_object: 0.3754
     Episode_Reward/lifting_object: -0.0153
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.9583
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 1.94s
                      Time elapsed: 00:05:14
                               ETA: 01:08:06

################################################################################
                     [1m Learning iteration 143/2000 [0m                      

                       Computation: 52451 steps/s (collection: 1.779s, learning 0.095s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.6777
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 57.2268
                       Mean reward: 1.80
               Mean episode length: 206.45
    Episode_Reward/reaching_object: 0.3779
     Episode_Reward/lifting_object: -0.0397
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.3333
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 1.87s
                      Time elapsed: 00:05:16
                               ETA: 01:07:59

################################################################################
                     [1m Learning iteration 144/2000 [0m                      

                       Computation: 50951 steps/s (collection: 1.816s, learning 0.114s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.2108
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 57.2735
                       Mean reward: 1.75
               Mean episode length: 201.66
    Episode_Reward/reaching_object: 0.3720
     Episode_Reward/lifting_object: -0.0423
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.8333
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 1.93s
                      Time elapsed: 00:05:18
                               ETA: 01:07:54

################################################################################
                     [1m Learning iteration 145/2000 [0m                      

                       Computation: 51755 steps/s (collection: 1.803s, learning 0.096s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.0379
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 57.4468
                       Mean reward: 1.72
               Mean episode length: 198.93
    Episode_Reward/reaching_object: 0.3838
     Episode_Reward/lifting_object: -0.0062
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 19.7917
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 1.90s
                      Time elapsed: 00:05:20
                               ETA: 01:07:48

################################################################################
                     [1m Learning iteration 146/2000 [0m                      

                       Computation: 51016 steps/s (collection: 1.812s, learning 0.115s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.0644
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.5677
                       Mean reward: 1.44
               Mean episode length: 202.75
    Episode_Reward/reaching_object: 0.3543
     Episode_Reward/lifting_object: -0.0155
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 20.2917
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 1.93s
                      Time elapsed: 00:05:22
                               ETA: 01:07:42

################################################################################
                     [1m Learning iteration 147/2000 [0m                      

                       Computation: 47775 steps/s (collection: 1.891s, learning 0.167s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.0366
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 57.7007
                       Mean reward: 1.70
               Mean episode length: 192.04
    Episode_Reward/reaching_object: 0.3622
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 21.3750
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 2.06s
                      Time elapsed: 00:05:24
                               ETA: 01:07:38

################################################################################
                     [1m Learning iteration 148/2000 [0m                      

                       Computation: 49046 steps/s (collection: 1.892s, learning 0.112s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.0684
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.8213
                       Mean reward: 1.54
               Mean episode length: 189.53
    Episode_Reward/reaching_object: 0.3591
     Episode_Reward/lifting_object: -0.0172
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 19.6667
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 2.00s
                      Time elapsed: 00:05:26
                               ETA: 01:07:34

################################################################################
                     [1m Learning iteration 149/2000 [0m                      

                       Computation: 51773 steps/s (collection: 1.805s, learning 0.094s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.1673
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 57.8753
                       Mean reward: 1.44
               Mean episode length: 193.31
    Episode_Reward/reaching_object: 0.3557
     Episode_Reward/lifting_object: -0.0375
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 20.2500
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 1.90s
                      Time elapsed: 00:05:28
                               ETA: 01:07:28

################################################################################
                     [1m Learning iteration 150/2000 [0m                      

                       Computation: 48961 steps/s (collection: 1.912s, learning 0.096s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.8459
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.9331
                       Mean reward: 0.22
               Mean episode length: 192.87
    Episode_Reward/reaching_object: 0.3626
     Episode_Reward/lifting_object: -0.0983
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 19.7083
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 2.01s
                      Time elapsed: 00:05:30
                               ETA: 01:07:24

################################################################################
                     [1m Learning iteration 151/2000 [0m                      

                       Computation: 50889 steps/s (collection: 1.833s, learning 0.099s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.0590
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.9897
                       Mean reward: 1.55
               Mean episode length: 191.64
    Episode_Reward/reaching_object: 0.3703
     Episode_Reward/lifting_object: -0.0260
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 19.0417
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 1.93s
                      Time elapsed: 00:05:32
                               ETA: 01:07:18

################################################################################
                     [1m Learning iteration 152/2000 [0m                      

                       Computation: 52681 steps/s (collection: 1.753s, learning 0.113s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.0346
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 58.1538
                       Mean reward: 1.64
               Mean episode length: 188.22
    Episode_Reward/reaching_object: 0.3696
     Episode_Reward/lifting_object: -0.0079
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 18.8750
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 1.87s
                      Time elapsed: 00:05:33
                               ETA: 01:07:12

################################################################################
                     [1m Learning iteration 153/2000 [0m                      

                       Computation: 53169 steps/s (collection: 1.755s, learning 0.094s)
             Mean action noise std: 1.59
          Mean value_function loss: 1.2201
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 58.2536
                       Mean reward: 1.83
               Mean episode length: 189.49
    Episode_Reward/reaching_object: 0.3798
     Episode_Reward/lifting_object: -0.0867
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 21.2083
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 1.85s
                      Time elapsed: 00:05:35
                               ETA: 01:07:06

################################################################################
                     [1m Learning iteration 154/2000 [0m                      

                       Computation: 52658 steps/s (collection: 1.764s, learning 0.103s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.0281
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.2934
                       Mean reward: 1.76
               Mean episode length: 188.96
    Episode_Reward/reaching_object: 0.3919
     Episode_Reward/lifting_object: -0.0269
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 19.3750
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 1.87s
                      Time elapsed: 00:05:37
                               ETA: 01:07:00

################################################################################
                     [1m Learning iteration 155/2000 [0m                      

                       Computation: 51845 steps/s (collection: 1.786s, learning 0.110s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.0081
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 58.4318
                       Mean reward: 1.72
               Mean episode length: 198.68
    Episode_Reward/reaching_object: 0.4044
     Episode_Reward/lifting_object: -0.0098
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.3750
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 1.90s
                      Time elapsed: 00:05:39
                               ETA: 01:06:55

################################################################################
                     [1m Learning iteration 156/2000 [0m                      

                       Computation: 51686 steps/s (collection: 1.811s, learning 0.091s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.0378
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 58.4876
                       Mean reward: 1.76
               Mean episode length: 201.79
    Episode_Reward/reaching_object: 0.4127
     Episode_Reward/lifting_object: -0.0064
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.7083
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 1.90s
                      Time elapsed: 00:05:41
                               ETA: 01:06:49

################################################################################
                     [1m Learning iteration 157/2000 [0m                      

                       Computation: 51454 steps/s (collection: 1.803s, learning 0.108s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.0082
               Mean surrogate loss: -0.0081
                 Mean entropy loss: 58.5248
                       Mean reward: 1.97
               Mean episode length: 197.76
    Episode_Reward/reaching_object: 0.4137
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.5833
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 1.91s
                      Time elapsed: 00:05:43
                               ETA: 01:06:44

################################################################################
                     [1m Learning iteration 158/2000 [0m                      

                       Computation: 48039 steps/s (collection: 1.899s, learning 0.147s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.2359
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 58.5959
                       Mean reward: 1.64
               Mean episode length: 200.03
    Episode_Reward/reaching_object: 0.4105
     Episode_Reward/lifting_object: -0.0471
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 2.05s
                      Time elapsed: 00:05:45
                               ETA: 01:06:40

################################################################################
                     [1m Learning iteration 159/2000 [0m                      

                       Computation: 52799 steps/s (collection: 1.765s, learning 0.097s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.1199
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 58.6294
                       Mean reward: 1.76
               Mean episode length: 198.56
    Episode_Reward/reaching_object: 0.4014
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.4167
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 1.86s
                      Time elapsed: 00:05:47
                               ETA: 01:06:35

################################################################################
                     [1m Learning iteration 160/2000 [0m                      

                       Computation: 52930 steps/s (collection: 1.764s, learning 0.093s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.0468
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 58.7543
                       Mean reward: 1.87
               Mean episode length: 199.81
    Episode_Reward/reaching_object: 0.4115
     Episode_Reward/lifting_object: -0.0264
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 18.8333
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 1.86s
                      Time elapsed: 00:05:49
                               ETA: 01:06:29

################################################################################
                     [1m Learning iteration 161/2000 [0m                      

                       Computation: 52533 steps/s (collection: 1.784s, learning 0.087s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.9338
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 58.8971
                       Mean reward: 1.93
               Mean episode length: 200.66
    Episode_Reward/reaching_object: 0.4068
     Episode_Reward/lifting_object: -0.0076
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 20.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 1.87s
                      Time elapsed: 00:05:50
                               ETA: 01:06:23

################################################################################
                     [1m Learning iteration 162/2000 [0m                      

                       Computation: 48966 steps/s (collection: 1.871s, learning 0.137s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.0099
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 58.9241
                       Mean reward: 0.82
               Mean episode length: 205.79
    Episode_Reward/reaching_object: 0.4235
     Episode_Reward/lifting_object: -0.0556
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.6667
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 2.01s
                      Time elapsed: 00:05:52
                               ETA: 01:06:19

################################################################################
                     [1m Learning iteration 163/2000 [0m                      

                       Computation: 52518 steps/s (collection: 1.783s, learning 0.089s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.0568
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 58.9968
                       Mean reward: 2.22
               Mean episode length: 204.29
    Episode_Reward/reaching_object: 0.4472
     Episode_Reward/lifting_object: -0.0076
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.6667
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 1.87s
                      Time elapsed: 00:05:54
                               ETA: 01:06:14

################################################################################
                     [1m Learning iteration 164/2000 [0m                      

                       Computation: 52996 steps/s (collection: 1.754s, learning 0.101s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.0895
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.1423
                       Mean reward: 1.98
               Mean episode length: 200.99
    Episode_Reward/reaching_object: 0.4635
     Episode_Reward/lifting_object: -0.0328
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 4.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 1.85s
                      Time elapsed: 00:05:56
                               ETA: 01:06:08

################################################################################
                     [1m Learning iteration 165/2000 [0m                      

                       Computation: 52847 steps/s (collection: 1.761s, learning 0.100s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.5723
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 59.2708
                       Mean reward: 2.11
               Mean episode length: 202.38
    Episode_Reward/reaching_object: 0.4735
     Episode_Reward/lifting_object: -0.0398
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 1.86s
                      Time elapsed: 00:05:58
                               ETA: 01:06:03

################################################################################
                     [1m Learning iteration 166/2000 [0m                      

                       Computation: 50228 steps/s (collection: 1.808s, learning 0.149s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.0178
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 59.2980
                       Mean reward: 2.69
               Mean episode length: 213.44
    Episode_Reward/reaching_object: 0.5016
     Episode_Reward/lifting_object: 0.0077
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 1.96s
                      Time elapsed: 00:06:00
                               ETA: 01:05:58

################################################################################
                     [1m Learning iteration 167/2000 [0m                      

                       Computation: 53218 steps/s (collection: 1.738s, learning 0.109s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.0082
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 59.3554
                       Mean reward: 2.40
               Mean episode length: 223.88
    Episode_Reward/reaching_object: 0.5338
     Episode_Reward/lifting_object: -0.0833
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 1.85s
                      Time elapsed: 00:06:02
                               ETA: 01:05:53

################################################################################
                     [1m Learning iteration 168/2000 [0m                      

                       Computation: 51998 steps/s (collection: 1.788s, learning 0.102s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.0076
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 59.4067
                       Mean reward: 2.64
               Mean episode length: 230.18
    Episode_Reward/reaching_object: 0.5550
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 1.89s
                      Time elapsed: 00:06:04
                               ETA: 01:05:48

################################################################################
                     [1m Learning iteration 169/2000 [0m                      

                       Computation: 51150 steps/s (collection: 1.833s, learning 0.089s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.0076
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 59.4527
                       Mean reward: 2.61
               Mean episode length: 233.36
    Episode_Reward/reaching_object: 0.5683
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 1.92s
                      Time elapsed: 00:06:06
                               ETA: 01:05:43

################################################################################
                     [1m Learning iteration 170/2000 [0m                      

                       Computation: 53939 steps/s (collection: 1.728s, learning 0.094s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.1078
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 59.4931
                       Mean reward: 2.81
               Mean episode length: 237.54
    Episode_Reward/reaching_object: 0.5620
     Episode_Reward/lifting_object: -0.0123
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 1.82s
                      Time elapsed: 00:06:07
                               ETA: 01:05:38

################################################################################
                     [1m Learning iteration 171/2000 [0m                      

                       Computation: 53691 steps/s (collection: 1.718s, learning 0.113s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.0382
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 59.5786
                       Mean reward: 2.56
               Mean episode length: 225.07
    Episode_Reward/reaching_object: 0.5570
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 1.83s
                      Time elapsed: 00:06:09
                               ETA: 01:05:32

################################################################################
                     [1m Learning iteration 172/2000 [0m                      

                       Computation: 52950 steps/s (collection: 1.758s, learning 0.099s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.1150
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 59.7001
                       Mean reward: 2.53
               Mean episode length: 223.77
    Episode_Reward/reaching_object: 0.5739
     Episode_Reward/lifting_object: -0.0098
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 1.86s
                      Time elapsed: 00:06:11
                               ETA: 01:05:27

################################################################################
                     [1m Learning iteration 173/2000 [0m                      

                       Computation: 50404 steps/s (collection: 1.841s, learning 0.109s)
             Mean action noise std: 1.67
          Mean value_function loss: 2.0271
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 59.7611
                       Mean reward: 2.77
               Mean episode length: 222.12
    Episode_Reward/reaching_object: 0.5809
     Episode_Reward/lifting_object: -0.1216
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 1.95s
                      Time elapsed: 00:06:13
                               ETA: 01:05:22

################################################################################
                     [1m Learning iteration 174/2000 [0m                      

                       Computation: 51731 steps/s (collection: 1.811s, learning 0.090s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.0110
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 59.7860
                       Mean reward: 2.91
               Mean episode length: 224.79
    Episode_Reward/reaching_object: 0.6230
     Episode_Reward/lifting_object: -0.0208
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 1.90s
                      Time elapsed: 00:06:15
                               ETA: 01:05:18

################################################################################
                     [1m Learning iteration 175/2000 [0m                      

                       Computation: 52626 steps/s (collection: 1.774s, learning 0.094s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.1904
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 59.8435
                       Mean reward: 3.10
               Mean episode length: 224.00
    Episode_Reward/reaching_object: 0.6312
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 1.87s
                      Time elapsed: 00:06:17
                               ETA: 01:05:13

################################################################################
                     [1m Learning iteration 176/2000 [0m                      

                       Computation: 52646 steps/s (collection: 1.762s, learning 0.106s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.1509
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 59.8650
                       Mean reward: 3.32
               Mean episode length: 222.21
    Episode_Reward/reaching_object: 0.6530
     Episode_Reward/lifting_object: -0.0278
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 1.87s
                      Time elapsed: 00:06:19
                               ETA: 01:05:08

################################################################################
                     [1m Learning iteration 177/2000 [0m                      

                       Computation: 52712 steps/s (collection: 1.760s, learning 0.105s)
             Mean action noise std: 1.67
          Mean value_function loss: 2.9933
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 59.9143
                       Mean reward: -0.29
               Mean episode length: 208.43
    Episode_Reward/reaching_object: 0.6503
     Episode_Reward/lifting_object: -0.1494
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 1.86s
                      Time elapsed: 00:06:21
                               ETA: 01:05:03

################################################################################
                     [1m Learning iteration 178/2000 [0m                      

                       Computation: 51237 steps/s (collection: 1.812s, learning 0.106s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.7794
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 59.9409
                       Mean reward: 2.98
               Mean episode length: 207.37
    Episode_Reward/reaching_object: 0.6621
     Episode_Reward/lifting_object: -0.0400
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 1.92s
                      Time elapsed: 00:06:23
                               ETA: 01:04:58

################################################################################
                     [1m Learning iteration 179/2000 [0m                      

                       Computation: 52615 steps/s (collection: 1.777s, learning 0.091s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.0652
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 60.0266
                       Mean reward: 3.60
               Mean episode length: 207.10
    Episode_Reward/reaching_object: 0.7501
     Episode_Reward/lifting_object: -0.0630
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 1.87s
                      Time elapsed: 00:06:24
                               ETA: 01:04:53

################################################################################
                     [1m Learning iteration 180/2000 [0m                      

                       Computation: 52489 steps/s (collection: 1.781s, learning 0.092s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.0421
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 60.1373
                       Mean reward: 3.61
               Mean episode length: 214.40
    Episode_Reward/reaching_object: 0.7780
     Episode_Reward/lifting_object: -0.0069
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 1.87s
                      Time elapsed: 00:06:26
                               ETA: 01:04:49

################################################################################
                     [1m Learning iteration 181/2000 [0m                      

                       Computation: 52536 steps/s (collection: 1.775s, learning 0.096s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.1128
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 60.2268
                       Mean reward: 3.99
               Mean episode length: 226.17
    Episode_Reward/reaching_object: 0.8264
     Episode_Reward/lifting_object: -0.0083
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 1.87s
                      Time elapsed: 00:06:28
                               ETA: 01:04:44

################################################################################
                     [1m Learning iteration 182/2000 [0m                      

                       Computation: 50527 steps/s (collection: 1.854s, learning 0.092s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.3777
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 60.2986
                       Mean reward: 3.90
               Mean episode length: 219.15
    Episode_Reward/reaching_object: 0.8206
     Episode_Reward/lifting_object: -0.0249
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 1.95s
                      Time elapsed: 00:06:30
                               ETA: 01:04:40

################################################################################
                     [1m Learning iteration 183/2000 [0m                      

                       Computation: 51005 steps/s (collection: 1.835s, learning 0.093s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.6392
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 60.3582
                       Mean reward: 3.63
               Mean episode length: 219.80
    Episode_Reward/reaching_object: 0.8189
     Episode_Reward/lifting_object: -0.0910
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 1.93s
                      Time elapsed: 00:06:32
                               ETA: 01:04:36

################################################################################
                     [1m Learning iteration 184/2000 [0m                      

                       Computation: 50143 steps/s (collection: 1.859s, learning 0.102s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.2120
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 60.3765
                       Mean reward: 4.31
               Mean episode length: 223.62
    Episode_Reward/reaching_object: 0.8540
     Episode_Reward/lifting_object: -0.1221
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 1.96s
                      Time elapsed: 00:06:34
                               ETA: 01:04:32

################################################################################
                     [1m Learning iteration 185/2000 [0m                      

                       Computation: 48188 steps/s (collection: 1.923s, learning 0.117s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.5395
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 60.4332
                       Mean reward: 3.15
               Mean episode length: 225.15
    Episode_Reward/reaching_object: 0.8906
     Episode_Reward/lifting_object: -0.0828
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 2.04s
                      Time elapsed: 00:06:36
                               ETA: 01:04:29

################################################################################
                     [1m Learning iteration 186/2000 [0m                      

                       Computation: 50618 steps/s (collection: 1.829s, learning 0.113s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.4112
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 60.5286
                       Mean reward: 4.70
               Mean episode length: 234.53
    Episode_Reward/reaching_object: 0.9385
     Episode_Reward/lifting_object: -0.0064
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 1.94s
                      Time elapsed: 00:06:38
                               ETA: 01:04:25

################################################################################
                     [1m Learning iteration 187/2000 [0m                      

                       Computation: 48225 steps/s (collection: 1.926s, learning 0.112s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.1475
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 60.6138
                       Mean reward: 4.62
               Mean episode length: 226.24
    Episode_Reward/reaching_object: 0.9519
     Episode_Reward/lifting_object: -0.0797
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 2.04s
                      Time elapsed: 00:06:40
                               ETA: 01:04:22

################################################################################
                     [1m Learning iteration 188/2000 [0m                      

                       Computation: 50465 steps/s (collection: 1.844s, learning 0.104s)
             Mean action noise std: 1.72
          Mean value_function loss: 1.8655
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 60.7049
                       Mean reward: 4.45
               Mean episode length: 219.31
    Episode_Reward/reaching_object: 0.9719
     Episode_Reward/lifting_object: -0.0634
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 1.95s
                      Time elapsed: 00:06:42
                               ETA: 01:04:18

################################################################################
                     [1m Learning iteration 189/2000 [0m                      

                       Computation: 50440 steps/s (collection: 1.826s, learning 0.123s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.4420
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 60.7267
                       Mean reward: 4.97
               Mean episode length: 237.40
    Episode_Reward/reaching_object: 0.9829
     Episode_Reward/lifting_object: -0.0268
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 1.95s
                      Time elapsed: 00:06:44
                               ETA: 01:04:14

################################################################################
                     [1m Learning iteration 190/2000 [0m                      

                       Computation: 48021 steps/s (collection: 1.907s, learning 0.141s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.1503
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 60.7849
                       Mean reward: 5.02
               Mean episode length: 236.41
    Episode_Reward/reaching_object: 1.0157
     Episode_Reward/lifting_object: -0.0366
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 2.05s
                      Time elapsed: 00:06:46
                               ETA: 01:04:11

################################################################################
                     [1m Learning iteration 191/2000 [0m                      

                       Computation: 52079 steps/s (collection: 1.793s, learning 0.095s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.1421
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 60.8464
                       Mean reward: 2.75
               Mean episode length: 232.38
    Episode_Reward/reaching_object: 1.0441
     Episode_Reward/lifting_object: -0.1305
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 1.89s
                      Time elapsed: 00:06:48
                               ETA: 01:04:07

################################################################################
                     [1m Learning iteration 192/2000 [0m                      

                       Computation: 50542 steps/s (collection: 1.852s, learning 0.093s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.3534
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 60.8981
                       Mean reward: 3.56
               Mean episode length: 240.13
    Episode_Reward/reaching_object: 1.0330
     Episode_Reward/lifting_object: -0.0848
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 1.94s
                      Time elapsed: 00:06:50
                               ETA: 01:04:03

################################################################################
                     [1m Learning iteration 193/2000 [0m                      

                       Computation: 50881 steps/s (collection: 1.842s, learning 0.090s)
             Mean action noise std: 1.73
          Mean value_function loss: 1.2443
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 60.9235
                       Mean reward: 3.41
               Mean episode length: 229.72
    Episode_Reward/reaching_object: 1.0073
     Episode_Reward/lifting_object: -0.1337
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 19.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 1.93s
                      Time elapsed: 00:06:52
                               ETA: 01:03:59

################################################################################
                     [1m Learning iteration 194/2000 [0m                      

                       Computation: 52997 steps/s (collection: 1.766s, learning 0.089s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.4060
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 60.9419
                       Mean reward: 4.64
               Mean episode length: 232.98
    Episode_Reward/reaching_object: 1.0243
     Episode_Reward/lifting_object: -0.0893
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 1.85s
                      Time elapsed: 00:06:54
                               ETA: 01:03:54

################################################################################
                     [1m Learning iteration 195/2000 [0m                      

                       Computation: 51967 steps/s (collection: 1.802s, learning 0.090s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.0935
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 60.9993
                       Mean reward: 4.47
               Mean episode length: 232.37
    Episode_Reward/reaching_object: 1.0354
     Episode_Reward/lifting_object: -0.0514
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0268
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 1.89s
                      Time elapsed: 00:06:55
                               ETA: 01:03:50

################################################################################
                     [1m Learning iteration 196/2000 [0m                      

                       Computation: 51721 steps/s (collection: 1.795s, learning 0.106s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.1455
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 61.0684
                       Mean reward: 4.97
               Mean episode length: 235.62
    Episode_Reward/reaching_object: 1.0355
     Episode_Reward/lifting_object: -0.0241
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 1.90s
                      Time elapsed: 00:06:57
                               ETA: 01:03:46

################################################################################
                     [1m Learning iteration 197/2000 [0m                      

                       Computation: 50538 steps/s (collection: 1.856s, learning 0.089s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.4834
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 61.1054
                       Mean reward: 4.79
               Mean episode length: 219.24
    Episode_Reward/reaching_object: 1.0238
     Episode_Reward/lifting_object: -0.0724
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 1.95s
                      Time elapsed: 00:06:59
                               ETA: 01:03:42

################################################################################
                     [1m Learning iteration 198/2000 [0m                      

                       Computation: 50834 steps/s (collection: 1.826s, learning 0.108s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.9598
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 61.1304
                       Mean reward: 4.23
               Mean episode length: 222.31
    Episode_Reward/reaching_object: 0.9802
     Episode_Reward/lifting_object: -0.0613
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 1.93s
                      Time elapsed: 00:07:01
                               ETA: 01:03:38

################################################################################
                     [1m Learning iteration 199/2000 [0m                      

                       Computation: 50406 steps/s (collection: 1.841s, learning 0.109s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.5058
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 61.1840
                       Mean reward: 5.01
               Mean episode length: 226.66
    Episode_Reward/reaching_object: 0.9979
     Episode_Reward/lifting_object: -0.0586
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 1.95s
                      Time elapsed: 00:07:03
                               ETA: 01:03:35

################################################################################
                     [1m Learning iteration 200/2000 [0m                      

                       Computation: 49990 steps/s (collection: 1.850s, learning 0.117s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.1536
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 61.2832
                       Mean reward: 4.49
               Mean episode length: 230.53
    Episode_Reward/reaching_object: 1.0262
     Episode_Reward/lifting_object: -0.0950
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 1.97s
                      Time elapsed: 00:07:05
                               ETA: 01:03:31

################################################################################
                     [1m Learning iteration 201/2000 [0m                      

                       Computation: 50172 steps/s (collection: 1.850s, learning 0.109s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.2031
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 61.3640
                       Mean reward: 3.67
               Mean episode length: 224.31
    Episode_Reward/reaching_object: 1.0065
     Episode_Reward/lifting_object: -0.0674
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 1.96s
                      Time elapsed: 00:07:07
                               ETA: 01:03:28

################################################################################
                     [1m Learning iteration 202/2000 [0m                      

                       Computation: 51277 steps/s (collection: 1.825s, learning 0.093s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.2833
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 61.3982
                       Mean reward: 4.02
               Mean episode length: 223.46
    Episode_Reward/reaching_object: 1.0147
     Episode_Reward/lifting_object: -0.0699
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 1.92s
                      Time elapsed: 00:07:09
                               ETA: 01:03:24

################################################################################
                     [1m Learning iteration 203/2000 [0m                      

                       Computation: 49400 steps/s (collection: 1.890s, learning 0.100s)
             Mean action noise std: 1.76
          Mean value_function loss: 3.1599
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 61.4549
                       Mean reward: 4.65
               Mean episode length: 228.64
    Episode_Reward/reaching_object: 1.0382
     Episode_Reward/lifting_object: -0.1033
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 19.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 1.99s
                      Time elapsed: 00:07:11
                               ETA: 01:03:21

################################################################################
                     [1m Learning iteration 204/2000 [0m                      

                       Computation: 51014 steps/s (collection: 1.819s, learning 0.108s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.1974
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 61.4734
                       Mean reward: 5.74
               Mean episode length: 241.35
    Episode_Reward/reaching_object: 1.1000
     Episode_Reward/lifting_object: -0.0357
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 1.93s
                      Time elapsed: 00:07:13
                               ETA: 01:03:17

################################################################################
                     [1m Learning iteration 205/2000 [0m                      

                       Computation: 52010 steps/s (collection: 1.788s, learning 0.103s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.1302
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 61.5252
                       Mean reward: 4.49
               Mean episode length: 239.35
    Episode_Reward/reaching_object: 1.1213
     Episode_Reward/lifting_object: -0.1471
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 1.89s
                      Time elapsed: 00:07:15
                               ETA: 01:03:13

################################################################################
                     [1m Learning iteration 206/2000 [0m                      

                       Computation: 49334 steps/s (collection: 1.838s, learning 0.155s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.2574
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 61.5932
                       Mean reward: 4.52
               Mean episode length: 240.07
    Episode_Reward/reaching_object: 1.1118
     Episode_Reward/lifting_object: -0.1765
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 1.99s
                      Time elapsed: 00:07:17
                               ETA: 01:03:10

################################################################################
                     [1m Learning iteration 207/2000 [0m                      

                       Computation: 50924 steps/s (collection: 1.812s, learning 0.118s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.6428
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 61.6271
                       Mean reward: 4.03
               Mean episode length: 230.19
    Episode_Reward/reaching_object: 1.0864
     Episode_Reward/lifting_object: -0.0749
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 1.93s
                      Time elapsed: 00:07:19
                               ETA: 01:03:06

################################################################################
                     [1m Learning iteration 208/2000 [0m                      

                       Computation: 50358 steps/s (collection: 1.855s, learning 0.097s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.9230
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 61.6478
                       Mean reward: 3.52
               Mean episode length: 230.99
    Episode_Reward/reaching_object: 1.0831
     Episode_Reward/lifting_object: -0.1603
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 1.95s
                      Time elapsed: 00:07:21
                               ETA: 01:03:03

################################################################################
                     [1m Learning iteration 209/2000 [0m                      

                       Computation: 48980 steps/s (collection: 1.903s, learning 0.104s)
             Mean action noise std: 1.77
          Mean value_function loss: 1.1089
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 61.6802
                       Mean reward: 2.60
               Mean episode length: 231.90
    Episode_Reward/reaching_object: 1.0772
     Episode_Reward/lifting_object: -0.3478
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 2.01s
                      Time elapsed: 00:07:23
                               ETA: 01:03:00

################################################################################
                     [1m Learning iteration 210/2000 [0m                      

                       Computation: 51915 steps/s (collection: 1.793s, learning 0.100s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.1111
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 61.6981
                       Mean reward: 4.74
               Mean episode length: 230.47
    Episode_Reward/reaching_object: 1.0690
     Episode_Reward/lifting_object: -0.1335
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 1.89s
                      Time elapsed: 00:07:25
                               ETA: 01:02:56

################################################################################
                     [1m Learning iteration 211/2000 [0m                      

                       Computation: 50523 steps/s (collection: 1.833s, learning 0.113s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.0404
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 61.7448
                       Mean reward: 5.31
               Mean episode length: 234.31
    Episode_Reward/reaching_object: 1.1472
     Episode_Reward/lifting_object: -0.0319
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 1.95s
                      Time elapsed: 00:07:27
                               ETA: 01:02:52

################################################################################
                     [1m Learning iteration 212/2000 [0m                      

                       Computation: 51942 steps/s (collection: 1.799s, learning 0.093s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.0936
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 61.8019
                       Mean reward: 5.28
               Mean episode length: 237.41
    Episode_Reward/reaching_object: 1.0979
     Episode_Reward/lifting_object: -0.0422
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 1.89s
                      Time elapsed: 00:07:28
                               ETA: 01:02:48

################################################################################
                     [1m Learning iteration 213/2000 [0m                      

                       Computation: 51346 steps/s (collection: 1.821s, learning 0.093s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.6477
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 61.8396
                       Mean reward: 5.33
               Mean episode length: 236.87
    Episode_Reward/reaching_object: 1.1105
     Episode_Reward/lifting_object: -0.0173
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 19.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 1.91s
                      Time elapsed: 00:07:30
                               ETA: 01:02:44

################################################################################
                     [1m Learning iteration 214/2000 [0m                      

                       Computation: 48542 steps/s (collection: 1.934s, learning 0.091s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.9131
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 61.8518
                       Mean reward: 3.85
               Mean episode length: 227.96
    Episode_Reward/reaching_object: 1.0518
     Episode_Reward/lifting_object: -0.1235
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 2.03s
                      Time elapsed: 00:07:32
                               ETA: 01:02:42

################################################################################
                     [1m Learning iteration 215/2000 [0m                      

                       Computation: 51682 steps/s (collection: 1.807s, learning 0.095s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.3156
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 61.8669
                       Mean reward: 4.36
               Mean episode length: 239.40
    Episode_Reward/reaching_object: 1.0862
     Episode_Reward/lifting_object: -0.0804
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 1.90s
                      Time elapsed: 00:07:34
                               ETA: 01:02:38

################################################################################
                     [1m Learning iteration 216/2000 [0m                      

                       Computation: 51636 steps/s (collection: 1.812s, learning 0.092s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.5738
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 61.9056
                       Mean reward: 4.91
               Mean episode length: 232.61
    Episode_Reward/reaching_object: 1.1082
     Episode_Reward/lifting_object: -0.0140
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 1.90s
                      Time elapsed: 00:07:36
                               ETA: 01:02:34

################################################################################
                     [1m Learning iteration 217/2000 [0m                      

                       Computation: 51200 steps/s (collection: 1.804s, learning 0.116s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.0348
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 61.9353
                       Mean reward: 4.21
               Mean episode length: 234.11
    Episode_Reward/reaching_object: 1.1237
     Episode_Reward/lifting_object: -0.1162
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 1.92s
                      Time elapsed: 00:07:38
                               ETA: 01:02:30

################################################################################
                     [1m Learning iteration 218/2000 [0m                      

                       Computation: 50538 steps/s (collection: 1.835s, learning 0.110s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.0712
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 61.9895
                       Mean reward: 4.29
               Mean episode length: 227.11
    Episode_Reward/reaching_object: 1.0535
     Episode_Reward/lifting_object: -0.0615
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 1.95s
                      Time elapsed: 00:07:40
                               ETA: 01:02:27

################################################################################
                     [1m Learning iteration 219/2000 [0m                      

                       Computation: 51542 steps/s (collection: 1.797s, learning 0.110s)
             Mean action noise std: 1.80
          Mean value_function loss: 0.9932
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 62.0606
                       Mean reward: 5.10
               Mean episode length: 234.58
    Episode_Reward/reaching_object: 1.0948
     Episode_Reward/lifting_object: -0.0528
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 1.91s
                      Time elapsed: 00:07:42
                               ETA: 01:02:23

################################################################################
                     [1m Learning iteration 220/2000 [0m                      

                       Computation: 51416 steps/s (collection: 1.793s, learning 0.119s)
             Mean action noise std: 1.80
          Mean value_function loss: 0.2082
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.0805
                       Mean reward: 3.33
               Mean episode length: 220.14
    Episode_Reward/reaching_object: 1.0671
     Episode_Reward/lifting_object: -0.1410
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 1.91s
                      Time elapsed: 00:07:44
                               ETA: 01:02:20

################################################################################
                     [1m Learning iteration 221/2000 [0m                      

                       Computation: 51584 steps/s (collection: 1.811s, learning 0.095s)
             Mean action noise std: 1.80
          Mean value_function loss: 0.3711
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 62.1236
                       Mean reward: 4.92
               Mean episode length: 231.53
    Episode_Reward/reaching_object: 1.0991
     Episode_Reward/lifting_object: -0.0259
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 1.91s
                      Time elapsed: 00:07:46
                               ETA: 01:02:16

################################################################################
                     [1m Learning iteration 222/2000 [0m                      

                       Computation: 50792 steps/s (collection: 1.839s, learning 0.096s)
             Mean action noise std: 1.80
          Mean value_function loss: 0.3906
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.1744
                       Mean reward: 4.93
               Mean episode length: 241.68
    Episode_Reward/reaching_object: 1.1421
     Episode_Reward/lifting_object: -0.0579
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 1.94s
                      Time elapsed: 00:07:48
                               ETA: 01:02:13

################################################################################
                     [1m Learning iteration 223/2000 [0m                      

                       Computation: 51867 steps/s (collection: 1.802s, learning 0.093s)
             Mean action noise std: 1.81
          Mean value_function loss: 0.0755
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 62.2304
                       Mean reward: 5.44
               Mean episode length: 238.34
    Episode_Reward/reaching_object: 1.1364
     Episode_Reward/lifting_object: -0.0421
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 1.90s
                      Time elapsed: 00:07:50
                               ETA: 01:02:09

################################################################################
                     [1m Learning iteration 224/2000 [0m                      

                       Computation: 51759 steps/s (collection: 1.809s, learning 0.091s)
             Mean action noise std: 1.81
          Mean value_function loss: 1.0565
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 62.2815
                       Mean reward: 5.07
               Mean episode length: 238.89
    Episode_Reward/reaching_object: 1.1601
     Episode_Reward/lifting_object: -0.0629
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 18.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 1.90s
                      Time elapsed: 00:07:52
                               ETA: 01:02:05

################################################################################
                     [1m Learning iteration 225/2000 [0m                      

                       Computation: 51740 steps/s (collection: 1.801s, learning 0.099s)
             Mean action noise std: 1.81
          Mean value_function loss: 0.0546
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 62.2935
                       Mean reward: 5.82
               Mean episode length: 238.86
    Episode_Reward/reaching_object: 1.1673
     Episode_Reward/lifting_object: -0.0349
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 1.90s
                      Time elapsed: 00:07:53
                               ETA: 01:02:02

################################################################################
                     [1m Learning iteration 226/2000 [0m                      

                       Computation: 52363 steps/s (collection: 1.787s, learning 0.091s)
             Mean action noise std: 1.81
          Mean value_function loss: 0.2766
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 62.3381
                       Mean reward: 5.23
               Mean episode length: 231.46
    Episode_Reward/reaching_object: 1.1627
     Episode_Reward/lifting_object: -0.0563
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 1.88s
                      Time elapsed: 00:07:55
                               ETA: 01:01:58

################################################################################
                     [1m Learning iteration 227/2000 [0m                      

                       Computation: 51485 steps/s (collection: 1.813s, learning 0.096s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.2279
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 62.4067
                       Mean reward: 5.26
               Mean episode length: 227.43
    Episode_Reward/reaching_object: 1.1540
     Episode_Reward/lifting_object: -0.0545
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 1.91s
                      Time elapsed: 00:07:57
                               ETA: 01:01:54

################################################################################
                     [1m Learning iteration 228/2000 [0m                      

                       Computation: 50520 steps/s (collection: 1.857s, learning 0.089s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.7077
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 62.4234
                       Mean reward: 4.70
               Mean episode length: 229.22
    Episode_Reward/reaching_object: 1.1602
     Episode_Reward/lifting_object: -0.0572
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 1.95s
                      Time elapsed: 00:07:59
                               ETA: 01:01:51

################################################################################
                     [1m Learning iteration 229/2000 [0m                      

                       Computation: 52101 steps/s (collection: 1.795s, learning 0.092s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.9512
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 62.4363
                       Mean reward: 4.60
               Mean episode length: 234.02
    Episode_Reward/reaching_object: 1.1189
     Episode_Reward/lifting_object: -0.1227
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 1.89s
                      Time elapsed: 00:08:01
                               ETA: 01:01:47

################################################################################
                     [1m Learning iteration 230/2000 [0m                      

                       Computation: 51650 steps/s (collection: 1.802s, learning 0.101s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.1524
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 62.4836
                       Mean reward: 5.08
               Mean episode length: 240.28
    Episode_Reward/reaching_object: 1.1925
     Episode_Reward/lifting_object: -0.0358
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 1.90s
                      Time elapsed: 00:08:03
                               ETA: 01:01:44

################################################################################
                     [1m Learning iteration 231/2000 [0m                      

                       Computation: 52517 steps/s (collection: 1.778s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.5843
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 62.5580
                       Mean reward: 5.27
               Mean episode length: 238.52
    Episode_Reward/reaching_object: 1.1851
     Episode_Reward/lifting_object: -0.0646
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 1.87s
                      Time elapsed: 00:08:05
                               ETA: 01:01:40

################################################################################
                     [1m Learning iteration 232/2000 [0m                      

                       Computation: 52071 steps/s (collection: 1.798s, learning 0.090s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.2930
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 62.5990
                       Mean reward: 4.70
               Mean episode length: 239.37
    Episode_Reward/reaching_object: 1.2119
     Episode_Reward/lifting_object: -0.0799
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 1.89s
                      Time elapsed: 00:08:07
                               ETA: 01:01:36

################################################################################
                     [1m Learning iteration 233/2000 [0m                      

                       Computation: 51208 steps/s (collection: 1.822s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.3775
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 62.6319
                       Mean reward: 5.67
               Mean episode length: 237.49
    Episode_Reward/reaching_object: 1.2153
     Episode_Reward/lifting_object: -0.0695
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 1.92s
                      Time elapsed: 00:08:09
                               ETA: 01:01:33

################################################################################
                     [1m Learning iteration 234/2000 [0m                      

                       Computation: 51505 steps/s (collection: 1.806s, learning 0.103s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.2734
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 62.6698
                       Mean reward: 5.80
               Mean episode length: 242.63
    Episode_Reward/reaching_object: 1.2116
     Episode_Reward/lifting_object: -0.0653
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 18.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 1.91s
                      Time elapsed: 00:08:11
                               ETA: 01:01:29

################################################################################
                     [1m Learning iteration 235/2000 [0m                      

                       Computation: 51510 steps/s (collection: 1.793s, learning 0.115s)
             Mean action noise std: 1.84
          Mean value_function loss: 0.1372
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 62.7241
                       Mean reward: 5.66
               Mean episode length: 242.09
    Episode_Reward/reaching_object: 1.2370
     Episode_Reward/lifting_object: -0.0695
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 1.91s
                      Time elapsed: 00:08:12
                               ETA: 01:01:26

################################################################################
                     [1m Learning iteration 236/2000 [0m                      

                       Computation: 50848 steps/s (collection: 1.834s, learning 0.099s)
             Mean action noise std: 1.84
          Mean value_function loss: 1.0521
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 62.7610
                       Mean reward: 5.97
               Mean episode length: 240.62
    Episode_Reward/reaching_object: 1.1885
     Episode_Reward/lifting_object: -0.0390
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 1.93s
                      Time elapsed: 00:08:14
                               ETA: 01:01:23

################################################################################
                     [1m Learning iteration 237/2000 [0m                      

                       Computation: 51688 steps/s (collection: 1.806s, learning 0.095s)
             Mean action noise std: 1.84
          Mean value_function loss: 0.0383
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.8343
                       Mean reward: 5.76
               Mean episode length: 238.31
    Episode_Reward/reaching_object: 1.2068
     Episode_Reward/lifting_object: -0.1125
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 1.90s
                      Time elapsed: 00:08:16
                               ETA: 01:01:19

################################################################################
                     [1m Learning iteration 238/2000 [0m                      

                       Computation: 51694 steps/s (collection: 1.810s, learning 0.092s)
             Mean action noise std: 1.85
          Mean value_function loss: 0.1102
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 62.8936
                       Mean reward: 5.64
               Mean episode length: 229.17
    Episode_Reward/reaching_object: 1.1506
     Episode_Reward/lifting_object: -0.0540
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 1.90s
                      Time elapsed: 00:08:18
                               ETA: 01:01:16

################################################################################
                     [1m Learning iteration 239/2000 [0m                      

                       Computation: 51137 steps/s (collection: 1.830s, learning 0.093s)
             Mean action noise std: 1.85
          Mean value_function loss: 1.6459
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 62.9118
                       Mean reward: 5.41
               Mean episode length: 230.85
    Episode_Reward/reaching_object: 1.1924
     Episode_Reward/lifting_object: -0.0835
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 1.92s
                      Time elapsed: 00:08:20
                               ETA: 01:01:13

################################################################################
                     [1m Learning iteration 240/2000 [0m                      

                       Computation: 50971 steps/s (collection: 1.838s, learning 0.091s)
             Mean action noise std: 1.85
          Mean value_function loss: 0.6341
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 62.9267
                       Mean reward: 4.88
               Mean episode length: 240.06
    Episode_Reward/reaching_object: 1.1761
     Episode_Reward/lifting_object: -0.1633
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 1.93s
                      Time elapsed: 00:08:22
                               ETA: 01:01:09

################################################################################
                     [1m Learning iteration 241/2000 [0m                      

                       Computation: 51376 steps/s (collection: 1.823s, learning 0.091s)
             Mean action noise std: 1.85
          Mean value_function loss: 0.2184
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 62.9822
                       Mean reward: 4.85
               Mean episode length: 242.82
    Episode_Reward/reaching_object: 1.2206
     Episode_Reward/lifting_object: -0.0320
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 1.91s
                      Time elapsed: 00:08:24
                               ETA: 01:01:06

################################################################################
                     [1m Learning iteration 242/2000 [0m                      

                       Computation: 51573 steps/s (collection: 1.814s, learning 0.092s)
             Mean action noise std: 1.86
          Mean value_function loss: 0.3317
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 63.0486
                       Mean reward: 5.22
               Mean episode length: 241.83
    Episode_Reward/reaching_object: 1.2424
     Episode_Reward/lifting_object: -0.0670
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 1.91s
                      Time elapsed: 00:08:26
                               ETA: 01:01:03

################################################################################
                     [1m Learning iteration 243/2000 [0m                      

                       Computation: 51632 steps/s (collection: 1.799s, learning 0.105s)
             Mean action noise std: 1.86
          Mean value_function loss: 0.2888
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 63.0965
                       Mean reward: 5.68
               Mean episode length: 240.69
    Episode_Reward/reaching_object: 1.2080
     Episode_Reward/lifting_object: -0.0134
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 1.90s
                      Time elapsed: 00:08:28
                               ETA: 01:00:59

################################################################################
                     [1m Learning iteration 244/2000 [0m                      

                       Computation: 50960 steps/s (collection: 1.841s, learning 0.088s)
             Mean action noise std: 1.86
          Mean value_function loss: 0.3833
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 63.1594
                       Mean reward: 5.90
               Mean episode length: 233.93
    Episode_Reward/reaching_object: 1.1931
     Episode_Reward/lifting_object: -0.0601
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 1.93s
                      Time elapsed: 00:08:30
                               ETA: 01:00:56

################################################################################
                     [1m Learning iteration 245/2000 [0m                      

                       Computation: 51438 steps/s (collection: 1.820s, learning 0.091s)
             Mean action noise std: 1.86
          Mean value_function loss: 0.8342
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 63.1860
                       Mean reward: 6.04
               Mean episode length: 239.97
    Episode_Reward/reaching_object: 1.2296
     Episode_Reward/lifting_object: -0.0738
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 1.91s
                      Time elapsed: 00:08:32
                               ETA: 01:00:53

################################################################################
                     [1m Learning iteration 246/2000 [0m                      

                       Computation: 51482 steps/s (collection: 1.812s, learning 0.097s)
             Mean action noise std: 1.86
          Mean value_function loss: 0.2740
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 63.1974
                       Mean reward: 4.68
               Mean episode length: 241.56
    Episode_Reward/reaching_object: 1.2424
     Episode_Reward/lifting_object: -0.1566
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 1.91s
                      Time elapsed: 00:08:33
                               ETA: 01:00:49

################################################################################
                     [1m Learning iteration 247/2000 [0m                      

                       Computation: 52104 steps/s (collection: 1.794s, learning 0.093s)
             Mean action noise std: 1.87
          Mean value_function loss: 0.2635
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 63.2292
                       Mean reward: 5.98
               Mean episode length: 237.24
    Episode_Reward/reaching_object: 1.2738
     Episode_Reward/lifting_object: -0.0262
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 1.89s
                      Time elapsed: 00:08:35
                               ETA: 01:00:46

################################################################################
                     [1m Learning iteration 248/2000 [0m                      

                       Computation: 51750 steps/s (collection: 1.791s, learning 0.109s)
             Mean action noise std: 1.87
          Mean value_function loss: 0.5507
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 63.2628
                       Mean reward: 5.99
               Mean episode length: 246.29
    Episode_Reward/reaching_object: 1.2415
     Episode_Reward/lifting_object: -0.1591
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 1.90s
                      Time elapsed: 00:08:37
                               ETA: 01:00:43

################################################################################
                     [1m Learning iteration 249/2000 [0m                      

                       Computation: 51634 steps/s (collection: 1.787s, learning 0.117s)
             Mean action noise std: 1.87
          Mean value_function loss: 0.0438
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 63.2742
                       Mean reward: 5.83
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 1.2893
     Episode_Reward/lifting_object: -0.0244
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 1.90s
                      Time elapsed: 00:08:39
                               ETA: 01:00:39

################################################################################
                     [1m Learning iteration 250/2000 [0m                      

                       Computation: 52307 steps/s (collection: 1.771s, learning 0.109s)
             Mean action noise std: 1.87
          Mean value_function loss: 0.2042
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 63.3111
                       Mean reward: 5.90
               Mean episode length: 243.57
    Episode_Reward/reaching_object: 1.2710
     Episode_Reward/lifting_object: -0.0274
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 1.88s
                      Time elapsed: 00:08:41
                               ETA: 01:00:36

################################################################################
                     [1m Learning iteration 251/2000 [0m                      

                       Computation: 51181 steps/s (collection: 1.821s, learning 0.100s)
             Mean action noise std: 1.88
          Mean value_function loss: 0.0399
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 63.3572
                       Mean reward: 6.00
               Mean episode length: 239.33
    Episode_Reward/reaching_object: 1.3011
     Episode_Reward/lifting_object: -0.0786
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 1.92s
                      Time elapsed: 00:08:43
                               ETA: 01:00:33

################################################################################
                     [1m Learning iteration 252/2000 [0m                      

                       Computation: 51510 steps/s (collection: 1.812s, learning 0.096s)
             Mean action noise std: 1.88
          Mean value_function loss: 0.1200
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 63.4350
                       Mean reward: 5.74
               Mean episode length: 242.53
    Episode_Reward/reaching_object: 1.2771
     Episode_Reward/lifting_object: -0.0652
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 1.91s
                      Time elapsed: 00:08:45
                               ETA: 01:00:29

################################################################################
                     [1m Learning iteration 253/2000 [0m                      

                       Computation: 49178 steps/s (collection: 1.900s, learning 0.099s)
             Mean action noise std: 1.89
          Mean value_function loss: 0.2349
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 63.5138
                       Mean reward: 6.16
               Mean episode length: 237.46
    Episode_Reward/reaching_object: 1.2817
     Episode_Reward/lifting_object: -0.0978
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 2.00s
                      Time elapsed: 00:08:47
                               ETA: 01:00:27

################################################################################
                     [1m Learning iteration 254/2000 [0m                      

                       Computation: 49366 steps/s (collection: 1.893s, learning 0.099s)
             Mean action noise std: 1.89
          Mean value_function loss: 0.2859
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 63.5845
                       Mean reward: 5.25
               Mean episode length: 244.56
    Episode_Reward/reaching_object: 1.2669
     Episode_Reward/lifting_object: -0.0667
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 1.99s
                      Time elapsed: 00:08:49
                               ETA: 01:00:24

################################################################################
                     [1m Learning iteration 255/2000 [0m                      

                       Computation: 51297 steps/s (collection: 1.825s, learning 0.091s)
             Mean action noise std: 1.89
          Mean value_function loss: 0.2074
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 63.6471
                       Mean reward: 5.52
               Mean episode length: 243.95
    Episode_Reward/reaching_object: 1.2756
     Episode_Reward/lifting_object: -0.0576
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 1.92s
                      Time elapsed: 00:08:51
                               ETA: 01:00:21

################################################################################
                     [1m Learning iteration 256/2000 [0m                      

                       Computation: 48792 steps/s (collection: 1.922s, learning 0.093s)
             Mean action noise std: 1.89
          Mean value_function loss: 0.6600
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 63.6740
                       Mean reward: 4.99
               Mean episode length: 241.23
    Episode_Reward/reaching_object: 1.2259
     Episode_Reward/lifting_object: -0.0771
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 2.01s
                      Time elapsed: 00:08:53
                               ETA: 01:00:19

################################################################################
                     [1m Learning iteration 257/2000 [0m                      

                       Computation: 50770 steps/s (collection: 1.841s, learning 0.095s)
             Mean action noise std: 1.89
          Mean value_function loss: 0.0919
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 63.6852
                       Mean reward: 5.69
               Mean episode length: 244.64
    Episode_Reward/reaching_object: 1.2815
     Episode_Reward/lifting_object: -0.0406
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 1.94s
                      Time elapsed: 00:08:55
                               ETA: 01:00:16

################################################################################
                     [1m Learning iteration 258/2000 [0m                      

                       Computation: 51402 steps/s (collection: 1.814s, learning 0.098s)
             Mean action noise std: 1.90
          Mean value_function loss: 0.2000
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 63.7185
                       Mean reward: 6.10
               Mean episode length: 242.38
    Episode_Reward/reaching_object: 1.3049
     Episode_Reward/lifting_object: -0.0260
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 1.91s
                      Time elapsed: 00:08:57
                               ETA: 01:00:12

################################################################################
                     [1m Learning iteration 259/2000 [0m                      

                       Computation: 50457 steps/s (collection: 1.855s, learning 0.094s)
             Mean action noise std: 1.90
          Mean value_function loss: 0.4721
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 63.7672
                       Mean reward: 5.83
               Mean episode length: 244.95
    Episode_Reward/reaching_object: 1.3309
     Episode_Reward/lifting_object: -0.0432
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 1.95s
                      Time elapsed: 00:08:59
                               ETA: 01:00:09

################################################################################
                     [1m Learning iteration 260/2000 [0m                      

                       Computation: 49090 steps/s (collection: 1.910s, learning 0.092s)
             Mean action noise std: 1.90
          Mean value_function loss: 0.2078
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 63.8216
                       Mean reward: 5.28
               Mean episode length: 246.43
    Episode_Reward/reaching_object: 1.3042
     Episode_Reward/lifting_object: -0.0551
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 2.00s
                      Time elapsed: 00:09:01
                               ETA: 01:00:07

################################################################################
                     [1m Learning iteration 261/2000 [0m                      

                       Computation: 51121 steps/s (collection: 1.834s, learning 0.089s)
             Mean action noise std: 1.91
          Mean value_function loss: 0.4312
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 63.9013
                       Mean reward: 6.19
               Mean episode length: 241.21
    Episode_Reward/reaching_object: 1.3252
     Episode_Reward/lifting_object: -0.0340
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 1.92s
                      Time elapsed: 00:09:03
                               ETA: 01:00:04

################################################################################
                     [1m Learning iteration 262/2000 [0m                      

                       Computation: 50483 steps/s (collection: 1.853s, learning 0.095s)
             Mean action noise std: 1.91
          Mean value_function loss: 0.2981
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 63.9728
                       Mean reward: 6.19
               Mean episode length: 239.77
    Episode_Reward/reaching_object: 1.3571
     Episode_Reward/lifting_object: -0.0838
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 1.95s
                      Time elapsed: 00:09:04
                               ETA: 01:00:01

################################################################################
                     [1m Learning iteration 263/2000 [0m                      

                       Computation: 50854 steps/s (collection: 1.838s, learning 0.095s)
             Mean action noise std: 1.91
          Mean value_function loss: 0.9486
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 64.0095
                       Mean reward: 6.25
               Mean episode length: 239.04
    Episode_Reward/reaching_object: 1.3347
     Episode_Reward/lifting_object: -0.1240
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0334
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 1.93s
                      Time elapsed: 00:09:06
                               ETA: 00:59:58

################################################################################
                     [1m Learning iteration 264/2000 [0m                      

                       Computation: 50551 steps/s (collection: 1.841s, learning 0.104s)
             Mean action noise std: 1.92
          Mean value_function loss: 0.3785
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 64.0349
                       Mean reward: 5.59
               Mean episode length: 242.74
    Episode_Reward/reaching_object: 1.3068
     Episode_Reward/lifting_object: -0.0341
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 1.94s
                      Time elapsed: 00:09:08
                               ETA: 00:59:55

################################################################################
                     [1m Learning iteration 265/2000 [0m                      

                       Computation: 49586 steps/s (collection: 1.869s, learning 0.113s)
             Mean action noise std: 1.92
          Mean value_function loss: 0.3120
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 64.0977
                       Mean reward: 6.74
               Mean episode length: 246.88
    Episode_Reward/reaching_object: 1.3510
     Episode_Reward/lifting_object: -0.0199
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 1.98s
                      Time elapsed: 00:09:10
                               ETA: 00:59:52

################################################################################
                     [1m Learning iteration 266/2000 [0m                      

                       Computation: 51254 steps/s (collection: 1.803s, learning 0.115s)
             Mean action noise std: 1.93
          Mean value_function loss: 0.1427
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 64.1946
                       Mean reward: 5.80
               Mean episode length: 243.39
    Episode_Reward/reaching_object: 1.3393
     Episode_Reward/lifting_object: -0.0953
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 1.92s
                      Time elapsed: 00:09:12
                               ETA: 00:59:49

################################################################################
                     [1m Learning iteration 267/2000 [0m                      

                       Computation: 50887 steps/s (collection: 1.817s, learning 0.115s)
             Mean action noise std: 1.93
          Mean value_function loss: 0.4534
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 64.2353
                       Mean reward: 5.98
               Mean episode length: 238.09
    Episode_Reward/reaching_object: 1.3261
     Episode_Reward/lifting_object: -0.0962
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 1.93s
                      Time elapsed: 00:09:14
                               ETA: 00:59:46

################################################################################
                     [1m Learning iteration 268/2000 [0m                      

                       Computation: 50857 steps/s (collection: 1.836s, learning 0.097s)
             Mean action noise std: 1.93
          Mean value_function loss: 0.2405
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 64.2626
                       Mean reward: 5.91
               Mean episode length: 228.58
    Episode_Reward/reaching_object: 1.2744
     Episode_Reward/lifting_object: -0.0369
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 1.93s
                      Time elapsed: 00:09:16
                               ETA: 00:59:43

################################################################################
                     [1m Learning iteration 269/2000 [0m                      

                       Computation: 50451 steps/s (collection: 1.851s, learning 0.098s)
             Mean action noise std: 1.93
          Mean value_function loss: 0.6542
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 64.3182
                       Mean reward: 5.92
               Mean episode length: 232.33
    Episode_Reward/reaching_object: 1.2171
     Episode_Reward/lifting_object: -0.0119
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 1.95s
                      Time elapsed: 00:09:18
                               ETA: 00:59:41

################################################################################
                     [1m Learning iteration 270/2000 [0m                      

                       Computation: 48964 steps/s (collection: 1.916s, learning 0.091s)
             Mean action noise std: 1.93
          Mean value_function loss: 0.9507
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 64.3406
                       Mean reward: 4.69
               Mean episode length: 222.07
    Episode_Reward/reaching_object: 1.2535
     Episode_Reward/lifting_object: -0.0507
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 2.01s
                      Time elapsed: 00:09:20
                               ETA: 00:59:38

################################################################################
                     [1m Learning iteration 271/2000 [0m                      

                       Computation: 49821 steps/s (collection: 1.877s, learning 0.096s)
             Mean action noise std: 1.94
          Mean value_function loss: 0.1020
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 64.3511
                       Mean reward: 6.66
               Mean episode length: 239.82
    Episode_Reward/reaching_object: 1.3217
     Episode_Reward/lifting_object: -0.0109
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 1.97s
                      Time elapsed: 00:09:22
                               ETA: 00:59:35

################################################################################
                     [1m Learning iteration 272/2000 [0m                      

                       Computation: 49452 steps/s (collection: 1.891s, learning 0.097s)
             Mean action noise std: 1.94
          Mean value_function loss: 0.2042
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 64.3806
                       Mean reward: 6.50
               Mean episode length: 236.34
    Episode_Reward/reaching_object: 1.3434
     Episode_Reward/lifting_object: -0.0130
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 1.99s
                      Time elapsed: 00:09:24
                               ETA: 00:59:33

################################################################################
                     [1m Learning iteration 273/2000 [0m                      

                       Computation: 49552 steps/s (collection: 1.879s, learning 0.105s)
             Mean action noise std: 1.94
          Mean value_function loss: 0.6441
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 64.4243
                       Mean reward: 6.76
               Mean episode length: 243.76
    Episode_Reward/reaching_object: 1.3047
     Episode_Reward/lifting_object: -0.0505
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 1.98s
                      Time elapsed: 00:09:26
                               ETA: 00:59:30

################################################################################
                     [1m Learning iteration 274/2000 [0m                      

                       Computation: 50224 steps/s (collection: 1.860s, learning 0.098s)
             Mean action noise std: 1.94
          Mean value_function loss: 0.5311
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 64.4780
                       Mean reward: 6.35
               Mean episode length: 235.94
    Episode_Reward/reaching_object: 1.3261
     Episode_Reward/lifting_object: 0.0109
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 1.96s
                      Time elapsed: 00:09:28
                               ETA: 00:59:28

################################################################################
                     [1m Learning iteration 275/2000 [0m                      

                       Computation: 50832 steps/s (collection: 1.838s, learning 0.095s)
             Mean action noise std: 1.95
          Mean value_function loss: 0.4898
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 64.5247
                       Mean reward: 5.86
               Mean episode length: 233.31
    Episode_Reward/reaching_object: 1.3168
     Episode_Reward/lifting_object: -0.0264
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 1.93s
                      Time elapsed: 00:09:30
                               ETA: 00:59:25

################################################################################
                     [1m Learning iteration 276/2000 [0m                      

                       Computation: 50392 steps/s (collection: 1.863s, learning 0.088s)
             Mean action noise std: 1.95
          Mean value_function loss: 0.2083
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 64.5857
                       Mean reward: 5.91
               Mean episode length: 239.07
    Episode_Reward/reaching_object: 1.3819
     Episode_Reward/lifting_object: -0.0803
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0348
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 1.95s
                      Time elapsed: 00:09:32
                               ETA: 00:59:22

################################################################################
                     [1m Learning iteration 277/2000 [0m                      

                       Computation: 50542 steps/s (collection: 1.852s, learning 0.093s)
             Mean action noise std: 1.95
          Mean value_function loss: 0.4863
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 64.6324
                       Mean reward: 5.99
               Mean episode length: 230.62
    Episode_Reward/reaching_object: 1.2763
     Episode_Reward/lifting_object: -0.0743
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 1.94s
                      Time elapsed: 00:09:34
                               ETA: 00:59:19

################################################################################
                     [1m Learning iteration 278/2000 [0m                      

                       Computation: 49909 steps/s (collection: 1.881s, learning 0.089s)
             Mean action noise std: 1.96
          Mean value_function loss: 0.5960
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 64.6646
                       Mean reward: 5.77
               Mean episode length: 236.46
    Episode_Reward/reaching_object: 1.2760
     Episode_Reward/lifting_object: -0.1606
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 1.97s
                      Time elapsed: 00:09:36
                               ETA: 00:59:16

################################################################################
                     [1m Learning iteration 279/2000 [0m                      

                       Computation: 48642 steps/s (collection: 1.930s, learning 0.091s)
             Mean action noise std: 1.96
          Mean value_function loss: 1.2920
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 64.6773
                       Mean reward: 6.66
               Mean episode length: 235.51
    Episode_Reward/reaching_object: 1.2833
     Episode_Reward/lifting_object: -0.0849
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 2.02s
                      Time elapsed: 00:09:38
                               ETA: 00:59:14

################################################################################
                     [1m Learning iteration 280/2000 [0m                      

                       Computation: 48355 steps/s (collection: 1.924s, learning 0.109s)
             Mean action noise std: 1.96
          Mean value_function loss: 0.1024
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 64.6876
                       Mean reward: 5.59
               Mean episode length: 244.73
    Episode_Reward/reaching_object: 1.3017
     Episode_Reward/lifting_object: -0.0135
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 2.03s
                      Time elapsed: 00:09:40
                               ETA: 00:59:12

################################################################################
                     [1m Learning iteration 281/2000 [0m                      

                       Computation: 50466 steps/s (collection: 1.837s, learning 0.111s)
             Mean action noise std: 1.96
          Mean value_function loss: 0.0373
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 64.7263
                       Mean reward: 6.72
               Mean episode length: 239.41
    Episode_Reward/reaching_object: 1.3392
     Episode_Reward/lifting_object: 0.0041
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 1.95s
                      Time elapsed: 00:09:42
                               ETA: 00:59:09

################################################################################
                     [1m Learning iteration 282/2000 [0m                      

                       Computation: 49155 steps/s (collection: 1.877s, learning 0.123s)
             Mean action noise std: 1.96
          Mean value_function loss: 0.2793
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 64.7676
                       Mean reward: 6.84
               Mean episode length: 248.51
    Episode_Reward/reaching_object: 1.3831
     Episode_Reward/lifting_object: -0.0632
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 2.00s
                      Time elapsed: 00:09:44
                               ETA: 00:59:07

################################################################################
                     [1m Learning iteration 283/2000 [0m                      

                       Computation: 49495 steps/s (collection: 1.888s, learning 0.099s)
             Mean action noise std: 1.97
          Mean value_function loss: 0.3108
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 64.8139
                       Mean reward: 6.29
               Mean episode length: 242.69
    Episode_Reward/reaching_object: 1.3336
     Episode_Reward/lifting_object: -0.0924
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 1.99s
                      Time elapsed: 00:09:46
                               ETA: 00:59:04

################################################################################
                     [1m Learning iteration 284/2000 [0m                      

                       Computation: 51215 steps/s (collection: 1.823s, learning 0.097s)
             Mean action noise std: 1.97
          Mean value_function loss: 0.6023
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 64.8758
                       Mean reward: 6.71
               Mean episode length: 244.13
    Episode_Reward/reaching_object: 1.3676
     Episode_Reward/lifting_object: -0.0282
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 1.92s
                      Time elapsed: 00:09:48
                               ETA: 00:59:01

################################################################################
                     [1m Learning iteration 285/2000 [0m                      

                       Computation: 50927 steps/s (collection: 1.838s, learning 0.092s)
             Mean action noise std: 1.97
          Mean value_function loss: 0.2706
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 64.9251
                       Mean reward: 6.79
               Mean episode length: 246.38
    Episode_Reward/reaching_object: 1.3611
     Episode_Reward/lifting_object: -0.0274
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 1.93s
                      Time elapsed: 00:09:50
                               ETA: 00:58:58

################################################################################
                     [1m Learning iteration 286/2000 [0m                      

                       Computation: 49184 steps/s (collection: 1.887s, learning 0.111s)
             Mean action noise std: 1.98
          Mean value_function loss: 0.1941
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 65.0118
                       Mean reward: 6.61
               Mean episode length: 243.44
    Episode_Reward/reaching_object: 1.4089
     Episode_Reward/lifting_object: 0.0115
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 2.00s
                      Time elapsed: 00:09:52
                               ETA: 00:58:56

################################################################################
                     [1m Learning iteration 287/2000 [0m                      

                       Computation: 49557 steps/s (collection: 1.884s, learning 0.100s)
             Mean action noise std: 1.98
          Mean value_function loss: 0.8937
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 65.0727
                       Mean reward: 5.91
               Mean episode length: 235.73
    Episode_Reward/reaching_object: 1.3237
     Episode_Reward/lifting_object: -0.0805
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 1.98s
                      Time elapsed: 00:09:54
                               ETA: 00:58:53

################################################################################
                     [1m Learning iteration 288/2000 [0m                      

                       Computation: 50013 steps/s (collection: 1.852s, learning 0.114s)
             Mean action noise std: 1.98
          Mean value_function loss: 1.4736
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 65.0955
                       Mean reward: 5.77
               Mean episode length: 243.49
    Episode_Reward/reaching_object: 1.3340
     Episode_Reward/lifting_object: -0.1128
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 1.97s
                      Time elapsed: 00:09:56
                               ETA: 00:58:51

################################################################################
                     [1m Learning iteration 289/2000 [0m                      

                       Computation: 50267 steps/s (collection: 1.857s, learning 0.099s)
             Mean action noise std: 1.99
          Mean value_function loss: 0.5130
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 65.1216
                       Mean reward: 5.63
               Mean episode length: 245.92
    Episode_Reward/reaching_object: 1.3250
     Episode_Reward/lifting_object: -0.0901
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0348
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 1.96s
                      Time elapsed: 00:09:58
                               ETA: 00:58:48

################################################################################
                     [1m Learning iteration 290/2000 [0m                      

                       Computation: 50504 steps/s (collection: 1.855s, learning 0.092s)
             Mean action noise std: 1.99
          Mean value_function loss: 0.3091
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 65.1794
                       Mean reward: 5.92
               Mean episode length: 241.39
    Episode_Reward/reaching_object: 1.3532
     Episode_Reward/lifting_object: -0.0625
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 1.95s
                      Time elapsed: 00:09:59
                               ETA: 00:58:45

################################################################################
                     [1m Learning iteration 291/2000 [0m                      

                       Computation: 50839 steps/s (collection: 1.841s, learning 0.093s)
             Mean action noise std: 1.99
          Mean value_function loss: 0.3330
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 65.2404
                       Mean reward: 6.50
               Mean episode length: 240.09
    Episode_Reward/reaching_object: 1.3480
     Episode_Reward/lifting_object: -0.0442
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0350
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 1.93s
                      Time elapsed: 00:10:01
                               ETA: 00:58:42

################################################################################
                     [1m Learning iteration 292/2000 [0m                      

                       Computation: 50165 steps/s (collection: 1.866s, learning 0.094s)
             Mean action noise std: 2.00
          Mean value_function loss: 0.3554
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 65.3143
                       Mean reward: 6.70
               Mean episode length: 240.57
    Episode_Reward/reaching_object: 1.3337
     Episode_Reward/lifting_object: 0.0440
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 1.96s
                      Time elapsed: 00:10:03
                               ETA: 00:58:40

################################################################################
                     [1m Learning iteration 293/2000 [0m                      

                       Computation: 48041 steps/s (collection: 1.942s, learning 0.104s)
             Mean action noise std: 2.00
          Mean value_function loss: 0.2868
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 65.3805
                       Mean reward: 6.37
               Mean episode length: 238.61
    Episode_Reward/reaching_object: 1.3248
     Episode_Reward/lifting_object: 0.0021
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 2.05s
                      Time elapsed: 00:10:05
                               ETA: 00:58:37

################################################################################
                     [1m Learning iteration 294/2000 [0m                      

                       Computation: 49028 steps/s (collection: 1.913s, learning 0.092s)
             Mean action noise std: 2.01
          Mean value_function loss: 0.5067
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 65.4321
                       Mean reward: 6.90
               Mean episode length: 243.36
    Episode_Reward/reaching_object: 1.3608
     Episode_Reward/lifting_object: 0.0009
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 2.01s
                      Time elapsed: 00:10:07
                               ETA: 00:58:35

################################################################################
                     [1m Learning iteration 295/2000 [0m                      

                       Computation: 48894 steps/s (collection: 1.903s, learning 0.107s)
             Mean action noise std: 2.01
          Mean value_function loss: 1.4550
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 65.4720
                       Mean reward: 6.73
               Mean episode length: 245.74
    Episode_Reward/reaching_object: 1.3665
     Episode_Reward/lifting_object: -0.0526
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 2.01s
                      Time elapsed: 00:10:09
                               ETA: 00:58:33

################################################################################
                     [1m Learning iteration 296/2000 [0m                      

                       Computation: 49016 steps/s (collection: 1.889s, learning 0.117s)
             Mean action noise std: 2.01
          Mean value_function loss: 0.2475
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 65.4833
                       Mean reward: 6.85
               Mean episode length: 245.90
    Episode_Reward/reaching_object: 1.3831
     Episode_Reward/lifting_object: -0.0111
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 2.01s
                      Time elapsed: 00:10:11
                               ETA: 00:58:30

################################################################################
                     [1m Learning iteration 297/2000 [0m                      

                       Computation: 49708 steps/s (collection: 1.865s, learning 0.113s)
             Mean action noise std: 2.01
          Mean value_function loss: 0.2380
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 65.5201
                       Mean reward: 6.98
               Mean episode length: 244.98
    Episode_Reward/reaching_object: 1.3624
     Episode_Reward/lifting_object: 0.0058
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 1.98s
                      Time elapsed: 00:10:13
                               ETA: 00:58:28

################################################################################
                     [1m Learning iteration 298/2000 [0m                      

                       Computation: 49545 steps/s (collection: 1.877s, learning 0.108s)
             Mean action noise std: 2.02
          Mean value_function loss: 0.3790
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 65.5840
                       Mean reward: 6.54
               Mean episode length: 243.69
    Episode_Reward/reaching_object: 1.3667
     Episode_Reward/lifting_object: -0.0444
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 1.98s
                      Time elapsed: 00:10:15
                               ETA: 00:58:25

################################################################################
                     [1m Learning iteration 299/2000 [0m                      

                       Computation: 49595 steps/s (collection: 1.882s, learning 0.100s)
             Mean action noise std: 2.02
          Mean value_function loss: 0.3669
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 65.6538
                       Mean reward: 7.32
               Mean episode length: 245.74
    Episode_Reward/reaching_object: 1.3685
     Episode_Reward/lifting_object: 0.0285
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 1.98s
                      Time elapsed: 00:10:17
                               ETA: 00:58:23

################################################################################
                     [1m Learning iteration 300/2000 [0m                      

                       Computation: 49492 steps/s (collection: 1.890s, learning 0.097s)
             Mean action noise std: 2.02
          Mean value_function loss: 0.4925
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 65.6991
                       Mean reward: 6.44
               Mean episode length: 244.33
    Episode_Reward/reaching_object: 1.3635
     Episode_Reward/lifting_object: -0.0474
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 1.99s
                      Time elapsed: 00:10:19
                               ETA: 00:58:20

################################################################################
                     [1m Learning iteration 301/2000 [0m                      

                       Computation: 49517 steps/s (collection: 1.889s, learning 0.097s)
             Mean action noise std: 2.03
          Mean value_function loss: 0.4938
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 65.7496
                       Mean reward: 6.65
               Mean episode length: 245.50
    Episode_Reward/reaching_object: 1.3424
     Episode_Reward/lifting_object: 0.0573
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 1.99s
                      Time elapsed: 00:10:21
                               ETA: 00:58:18

################################################################################
                     [1m Learning iteration 302/2000 [0m                      

                       Computation: 49646 steps/s (collection: 1.876s, learning 0.104s)
             Mean action noise std: 2.03
          Mean value_function loss: 0.2795
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 65.8057
                       Mean reward: 6.15
               Mean episode length: 245.61
    Episode_Reward/reaching_object: 1.3527
     Episode_Reward/lifting_object: -0.0527
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 1.98s
                      Time elapsed: 00:10:23
                               ETA: 00:58:15

################################################################################
                     [1m Learning iteration 303/2000 [0m                      

                       Computation: 49261 steps/s (collection: 1.907s, learning 0.089s)
             Mean action noise std: 2.03
          Mean value_function loss: 1.4210
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 65.8601
                       Mean reward: 6.13
               Mean episode length: 238.38
    Episode_Reward/reaching_object: 1.3797
     Episode_Reward/lifting_object: -0.0728
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 2.00s
                      Time elapsed: 00:10:25
                               ETA: 00:58:13

################################################################################
                     [1m Learning iteration 304/2000 [0m                      

                       Computation: 50049 steps/s (collection: 1.869s, learning 0.095s)
             Mean action noise std: 2.03
          Mean value_function loss: 0.5954
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 65.8733
                       Mean reward: 5.88
               Mean episode length: 242.89
    Episode_Reward/reaching_object: 1.3855
     Episode_Reward/lifting_object: -0.0546
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 1.96s
                      Time elapsed: 00:10:27
                               ETA: 00:58:10

################################################################################
                     [1m Learning iteration 305/2000 [0m                      

                       Computation: 50180 steps/s (collection: 1.864s, learning 0.095s)
             Mean action noise std: 2.04
          Mean value_function loss: 0.3490
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 65.9065
                       Mean reward: 6.48
               Mean episode length: 237.91
    Episode_Reward/reaching_object: 1.3369
     Episode_Reward/lifting_object: -0.1279
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 1.96s
                      Time elapsed: 00:10:29
                               ETA: 00:58:08

################################################################################
                     [1m Learning iteration 306/2000 [0m                      

                       Computation: 50003 steps/s (collection: 1.860s, learning 0.106s)
             Mean action noise std: 2.04
          Mean value_function loss: 0.2268
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 65.9760
                       Mean reward: 6.48
               Mean episode length: 241.61
    Episode_Reward/reaching_object: 1.4341
     Episode_Reward/lifting_object: -0.0150
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 1.97s
                      Time elapsed: 00:10:31
                               ETA: 00:58:05

################################################################################
                     [1m Learning iteration 307/2000 [0m                      

                       Computation: 50307 steps/s (collection: 1.862s, learning 0.092s)
             Mean action noise std: 2.05
          Mean value_function loss: 0.3044
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 66.0635
                       Mean reward: 5.76
               Mean episode length: 241.45
    Episode_Reward/reaching_object: 1.3807
     Episode_Reward/lifting_object: -0.0362
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 1.95s
                      Time elapsed: 00:10:33
                               ETA: 00:58:03

################################################################################
                     [1m Learning iteration 308/2000 [0m                      

                       Computation: 49761 steps/s (collection: 1.882s, learning 0.093s)
             Mean action noise std: 2.05
          Mean value_function loss: 0.2406
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 66.1110
                       Mean reward: 6.04
               Mean episode length: 244.33
    Episode_Reward/reaching_object: 1.3788
     Episode_Reward/lifting_object: -0.0698
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 18.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 1.98s
                      Time elapsed: 00:10:35
                               ETA: 00:58:00

################################################################################
                     [1m Learning iteration 309/2000 [0m                      

                       Computation: 49502 steps/s (collection: 1.883s, learning 0.103s)
             Mean action noise std: 2.05
          Mean value_function loss: 0.2884
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 66.1742
                       Mean reward: 7.03
               Mean episode length: 243.17
    Episode_Reward/reaching_object: 1.4213
     Episode_Reward/lifting_object: -0.0779
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 1.99s
                      Time elapsed: 00:10:37
                               ETA: 00:57:58

################################################################################
                     [1m Learning iteration 310/2000 [0m                      

                       Computation: 49400 steps/s (collection: 1.900s, learning 0.090s)
             Mean action noise std: 2.06
          Mean value_function loss: 0.5414
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 66.2055
                       Mean reward: 6.34
               Mean episode length: 234.16
    Episode_Reward/reaching_object: 1.3696
     Episode_Reward/lifting_object: -0.0559
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 1.99s
                      Time elapsed: 00:10:39
                               ETA: 00:57:55

################################################################################
                     [1m Learning iteration 311/2000 [0m                      

                       Computation: 48897 steps/s (collection: 1.899s, learning 0.112s)
             Mean action noise std: 2.06
          Mean value_function loss: 0.4128
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 66.2476
                       Mean reward: 6.80
               Mean episode length: 245.58
    Episode_Reward/reaching_object: 1.3691
     Episode_Reward/lifting_object: 0.0190
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 2.01s
                      Time elapsed: 00:10:41
                               ETA: 00:57:53

################################################################################
                     [1m Learning iteration 312/2000 [0m                      

                       Computation: 48982 steps/s (collection: 1.912s, learning 0.095s)
             Mean action noise std: 2.06
          Mean value_function loss: 0.2558
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 66.3205
                       Mean reward: 6.75
               Mean episode length: 241.58
    Episode_Reward/reaching_object: 1.3740
     Episode_Reward/lifting_object: 0.0391
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 2.01s
                      Time elapsed: 00:10:43
                               ETA: 00:57:51

################################################################################
                     [1m Learning iteration 313/2000 [0m                      

                       Computation: 48006 steps/s (collection: 1.940s, learning 0.108s)
             Mean action noise std: 2.07
          Mean value_function loss: 0.5160
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 66.3713
                       Mean reward: 6.33
               Mean episode length: 227.96
    Episode_Reward/reaching_object: 1.3776
     Episode_Reward/lifting_object: -0.0115
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 2.05s
                      Time elapsed: 00:10:45
                               ETA: 00:57:48

################################################################################
                     [1m Learning iteration 314/2000 [0m                      

                       Computation: 48597 steps/s (collection: 1.926s, learning 0.097s)
             Mean action noise std: 2.07
          Mean value_function loss: 0.4520
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 66.4387
                       Mean reward: 7.24
               Mean episode length: 241.39
    Episode_Reward/reaching_object: 1.3859
     Episode_Reward/lifting_object: 0.0518
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 2.02s
                      Time elapsed: 00:10:47
                               ETA: 00:57:46

################################################################################
                     [1m Learning iteration 315/2000 [0m                      

                       Computation: 49122 steps/s (collection: 1.910s, learning 0.091s)
             Mean action noise std: 2.08
          Mean value_function loss: 0.3378
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 66.4929
                       Mean reward: 6.70
               Mean episode length: 233.13
    Episode_Reward/reaching_object: 1.3780
     Episode_Reward/lifting_object: 0.0549
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 2.00s
                      Time elapsed: 00:10:49
                               ETA: 00:57:44

################################################################################
                     [1m Learning iteration 316/2000 [0m                      

                       Computation: 48704 steps/s (collection: 1.925s, learning 0.094s)
             Mean action noise std: 2.08
          Mean value_function loss: 0.4328
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 66.5599
                       Mean reward: 6.82
               Mean episode length: 240.79
    Episode_Reward/reaching_object: 1.3845
     Episode_Reward/lifting_object: 0.0073
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 2.02s
                      Time elapsed: 00:10:51
                               ETA: 00:57:42

################################################################################
                     [1m Learning iteration 317/2000 [0m                      

                       Computation: 48159 steps/s (collection: 1.946s, learning 0.096s)
             Mean action noise std: 2.08
          Mean value_function loss: 0.7493
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 66.6148
                       Mean reward: 6.09
               Mean episode length: 240.57
    Episode_Reward/reaching_object: 1.3463
     Episode_Reward/lifting_object: 0.0355
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 2.04s
                      Time elapsed: 00:10:53
                               ETA: 00:57:40

################################################################################
                     [1m Learning iteration 318/2000 [0m                      

                       Computation: 47951 steps/s (collection: 1.940s, learning 0.111s)
             Mean action noise std: 2.09
          Mean value_function loss: 0.6584
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 66.6711
                       Mean reward: 6.09
               Mean episode length: 244.34
    Episode_Reward/reaching_object: 1.3720
     Episode_Reward/lifting_object: -0.0411
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 2.05s
                      Time elapsed: 00:10:55
                               ETA: 00:57:37

################################################################################
                     [1m Learning iteration 319/2000 [0m                      

                       Computation: 49015 steps/s (collection: 1.911s, learning 0.095s)
             Mean action noise std: 2.09
          Mean value_function loss: 0.3907
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 66.7431
                       Mean reward: 6.26
               Mean episode length: 236.90
    Episode_Reward/reaching_object: 1.3709
     Episode_Reward/lifting_object: -0.0239
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 2.01s
                      Time elapsed: 00:10:57
                               ETA: 00:57:35

################################################################################
                     [1m Learning iteration 320/2000 [0m                      

                       Computation: 48498 steps/s (collection: 1.935s, learning 0.092s)
             Mean action noise std: 2.10
          Mean value_function loss: 0.5312
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 66.8195
                       Mean reward: 5.56
               Mean episode length: 243.35
    Episode_Reward/reaching_object: 1.3862
     Episode_Reward/lifting_object: -0.0563
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 2.03s
                      Time elapsed: 00:10:59
                               ETA: 00:57:33

################################################################################
                     [1m Learning iteration 321/2000 [0m                      

                       Computation: 47619 steps/s (collection: 1.974s, learning 0.090s)
             Mean action noise std: 2.10
          Mean value_function loss: 1.0905
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 66.8741
                       Mean reward: 6.66
               Mean episode length: 242.03
    Episode_Reward/reaching_object: 1.4256
     Episode_Reward/lifting_object: -0.0187
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 2.06s
                      Time elapsed: 00:11:01
                               ETA: 00:57:31

################################################################################
                     [1m Learning iteration 322/2000 [0m                      

                       Computation: 47399 steps/s (collection: 1.974s, learning 0.100s)
             Mean action noise std: 2.11
          Mean value_function loss: 0.7474
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 66.9224
                       Mean reward: 6.41
               Mean episode length: 239.39
    Episode_Reward/reaching_object: 1.3706
     Episode_Reward/lifting_object: -0.0367
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 2.07s
                      Time elapsed: 00:11:03
                               ETA: 00:57:29

################################################################################
                     [1m Learning iteration 323/2000 [0m                      

                       Computation: 48065 steps/s (collection: 1.948s, learning 0.098s)
             Mean action noise std: 2.11
          Mean value_function loss: 0.5779
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 67.0022
                       Mean reward: 7.39
               Mean episode length: 245.38
    Episode_Reward/reaching_object: 1.4506
     Episode_Reward/lifting_object: 0.0565
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 2.05s
                      Time elapsed: 00:11:06
                               ETA: 00:57:27

################################################################################
                     [1m Learning iteration 324/2000 [0m                      

                       Computation: 46957 steps/s (collection: 1.993s, learning 0.101s)
             Mean action noise std: 2.12
          Mean value_function loss: 0.4378
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 67.0843
                       Mean reward: 5.94
               Mean episode length: 241.86
    Episode_Reward/reaching_object: 1.4039
     Episode_Reward/lifting_object: -0.0609
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 2.09s
                      Time elapsed: 00:11:08
                               ETA: 00:57:25

################################################################################
                     [1m Learning iteration 325/2000 [0m                      

                       Computation: 46200 steps/s (collection: 2.027s, learning 0.101s)
             Mean action noise std: 2.12
          Mean value_function loss: 0.4562
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 67.1355
                       Mean reward: 7.18
               Mean episode length: 236.06
    Episode_Reward/reaching_object: 1.3836
     Episode_Reward/lifting_object: 0.0735
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 2.13s
                      Time elapsed: 00:11:10
                               ETA: 00:57:23

################################################################################
                     [1m Learning iteration 326/2000 [0m                      

                       Computation: 47249 steps/s (collection: 1.988s, learning 0.093s)
             Mean action noise std: 2.13
          Mean value_function loss: 0.3567
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.2104
                       Mean reward: 6.75
               Mean episode length: 243.62
    Episode_Reward/reaching_object: 1.3936
     Episode_Reward/lifting_object: 0.0542
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 2.08s
                      Time elapsed: 00:11:12
                               ETA: 00:57:21

################################################################################
                     [1m Learning iteration 327/2000 [0m                      

                       Computation: 46946 steps/s (collection: 2.003s, learning 0.091s)
             Mean action noise std: 2.13
          Mean value_function loss: 1.3534
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 67.2799
                       Mean reward: 6.56
               Mean episode length: 239.45
    Episode_Reward/reaching_object: 1.4070
     Episode_Reward/lifting_object: -0.0864
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 2.09s
                      Time elapsed: 00:11:14
                               ETA: 00:57:19

################################################################################
                     [1m Learning iteration 328/2000 [0m                      

                       Computation: 46710 steps/s (collection: 2.006s, learning 0.098s)
             Mean action noise std: 2.13
          Mean value_function loss: 0.6842
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 67.3124
                       Mean reward: 7.16
               Mean episode length: 241.37
    Episode_Reward/reaching_object: 1.3354
     Episode_Reward/lifting_object: 0.0596
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 2.10s
                      Time elapsed: 00:11:16
                               ETA: 00:57:18

################################################################################
                     [1m Learning iteration 329/2000 [0m                      

                       Computation: 47041 steps/s (collection: 1.991s, learning 0.099s)
             Mean action noise std: 2.13
          Mean value_function loss: 0.5259
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 67.3509
                       Mean reward: 7.28
               Mean episode length: 239.40
    Episode_Reward/reaching_object: 1.4029
     Episode_Reward/lifting_object: -0.0298
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 2.09s
                      Time elapsed: 00:11:18
                               ETA: 00:57:16

################################################################################
                     [1m Learning iteration 330/2000 [0m                      

                       Computation: 47165 steps/s (collection: 1.988s, learning 0.096s)
             Mean action noise std: 2.14
          Mean value_function loss: 0.3024
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 67.4078
                       Mean reward: 5.94
               Mean episode length: 238.05
    Episode_Reward/reaching_object: 1.3499
     Episode_Reward/lifting_object: -0.0496
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 2.08s
                      Time elapsed: 00:11:20
                               ETA: 00:57:14

################################################################################
                     [1m Learning iteration 331/2000 [0m                      

                       Computation: 46167 steps/s (collection: 2.031s, learning 0.099s)
             Mean action noise std: 2.14
          Mean value_function loss: 0.7109
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 67.4472
                       Mean reward: 6.40
               Mean episode length: 239.29
    Episode_Reward/reaching_object: 1.4367
     Episode_Reward/lifting_object: -0.0898
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 2.13s
                      Time elapsed: 00:11:22
                               ETA: 00:57:12

################################################################################
                     [1m Learning iteration 332/2000 [0m                      

                       Computation: 46876 steps/s (collection: 1.982s, learning 0.115s)
             Mean action noise std: 2.14
          Mean value_function loss: 1.2072
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 67.4778
                       Mean reward: 6.90
               Mean episode length: 231.60
    Episode_Reward/reaching_object: 1.4028
     Episode_Reward/lifting_object: 0.0709
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 2.10s
                      Time elapsed: 00:11:24
                               ETA: 00:57:10

################################################################################
                     [1m Learning iteration 333/2000 [0m                      

                       Computation: 19108 steps/s (collection: 5.018s, learning 0.127s)
             Mean action noise std: 2.15
          Mean value_function loss: 0.2404
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 67.5340
                       Mean reward: 6.49
               Mean episode length: 244.13
    Episode_Reward/reaching_object: 1.3828
     Episode_Reward/lifting_object: -0.0745
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 5.14s
                      Time elapsed: 00:11:30
                               ETA: 00:57:24

################################################################################
                     [1m Learning iteration 334/2000 [0m                      

                       Computation: 14336 steps/s (collection: 6.742s, learning 0.115s)
             Mean action noise std: 2.15
          Mean value_function loss: 0.2674
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 67.6027
                       Mean reward: 8.55
               Mean episode length: 239.96
    Episode_Reward/reaching_object: 1.4202
     Episode_Reward/lifting_object: 0.0853
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 6.86s
                      Time elapsed: 00:11:36
                               ETA: 00:57:45

################################################################################
                     [1m Learning iteration 335/2000 [0m                      

                       Computation: 14235 steps/s (collection: 6.783s, learning 0.122s)
             Mean action noise std: 2.16
          Mean value_function loss: 0.8594
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.6699
                       Mean reward: 6.33
               Mean episode length: 233.53
    Episode_Reward/reaching_object: 1.3821
     Episode_Reward/lifting_object: 0.0161
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 6.91s
                      Time elapsed: 00:11:43
                               ETA: 00:58:07

################################################################################
                     [1m Learning iteration 336/2000 [0m                      

                       Computation: 13597 steps/s (collection: 7.039s, learning 0.190s)
             Mean action noise std: 2.16
          Mean value_function loss: 0.8467
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 67.7176
                       Mean reward: 6.51
               Mean episode length: 236.02
    Episode_Reward/reaching_object: 1.3701
     Episode_Reward/lifting_object: -0.0030
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 7.23s
                      Time elapsed: 00:11:51
                               ETA: 00:58:31

################################################################################
                     [1m Learning iteration 337/2000 [0m                      

                       Computation: 13957 steps/s (collection: 6.920s, learning 0.123s)
             Mean action noise std: 2.16
          Mean value_function loss: 0.5534
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 67.7699
                       Mean reward: 7.35
               Mean episode length: 238.67
    Episode_Reward/reaching_object: 1.4160
     Episode_Reward/lifting_object: 0.0578
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 7.04s
                      Time elapsed: 00:11:58
                               ETA: 00:58:53

################################################################################
                     [1m Learning iteration 338/2000 [0m                      

                       Computation: 14334 steps/s (collection: 6.730s, learning 0.128s)
             Mean action noise std: 2.17
          Mean value_function loss: 1.0851
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 67.8091
                       Mean reward: 6.16
               Mean episode length: 233.89
    Episode_Reward/reaching_object: 1.3673
     Episode_Reward/lifting_object: 0.0213
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 6.86s
                      Time elapsed: 00:12:04
                               ETA: 00:59:14

################################################################################
                     [1m Learning iteration 339/2000 [0m                      

                       Computation: 14313 steps/s (collection: 6.725s, learning 0.143s)
             Mean action noise std: 2.17
          Mean value_function loss: 0.6269
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 67.8596
                       Mean reward: 6.86
               Mean episode length: 231.94
    Episode_Reward/reaching_object: 1.3956
     Episode_Reward/lifting_object: 0.0139
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 6.87s
                      Time elapsed: 00:12:11
                               ETA: 00:59:35

################################################################################
                     [1m Learning iteration 340/2000 [0m                      

                       Computation: 14068 steps/s (collection: 6.834s, learning 0.154s)
             Mean action noise std: 2.17
          Mean value_function loss: 0.9870
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 67.9207
                       Mean reward: 6.38
               Mean episode length: 225.94
    Episode_Reward/reaching_object: 1.3565
     Episode_Reward/lifting_object: 0.0308
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 6.99s
                      Time elapsed: 00:12:18
                               ETA: 00:59:56

################################################################################
                     [1m Learning iteration 341/2000 [0m                      

                       Computation: 13814 steps/s (collection: 6.995s, learning 0.121s)
             Mean action noise std: 2.18
          Mean value_function loss: 0.4768
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 67.9584
                       Mean reward: 6.18
               Mean episode length: 230.51
    Episode_Reward/reaching_object: 1.3838
     Episode_Reward/lifting_object: -0.0075
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 7.12s
                      Time elapsed: 00:12:25
                               ETA: 01:00:18

################################################################################
                     [1m Learning iteration 342/2000 [0m                      

                       Computation: 48734 steps/s (collection: 1.904s, learning 0.113s)
             Mean action noise std: 2.18
          Mean value_function loss: 0.6236
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 67.9997
                       Mean reward: 6.42
               Mean episode length: 247.24
    Episode_Reward/reaching_object: 1.3277
     Episode_Reward/lifting_object: 0.0624
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 2.02s
                      Time elapsed: 00:12:27
                               ETA: 01:00:15

################################################################################
                     [1m Learning iteration 343/2000 [0m                      

                       Computation: 49881 steps/s (collection: 1.867s, learning 0.104s)
             Mean action noise std: 2.18
          Mean value_function loss: 0.4189
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 68.0523
                       Mean reward: 7.50
               Mean episode length: 237.68
    Episode_Reward/reaching_object: 1.3687
     Episode_Reward/lifting_object: 0.0843
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 1.97s
                      Time elapsed: 00:12:29
                               ETA: 01:00:12

################################################################################
                     [1m Learning iteration 344/2000 [0m                      

                       Computation: 48541 steps/s (collection: 1.930s, learning 0.095s)
             Mean action noise std: 2.19
          Mean value_function loss: 0.4937
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 68.1073
                       Mean reward: 6.19
               Mean episode length: 239.28
    Episode_Reward/reaching_object: 1.3153
     Episode_Reward/lifting_object: 0.0029
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 2.03s
                      Time elapsed: 00:12:31
                               ETA: 01:00:09

################################################################################
                     [1m Learning iteration 345/2000 [0m                      

                       Computation: 47905 steps/s (collection: 1.956s, learning 0.096s)
             Mean action noise std: 2.19
          Mean value_function loss: 0.1744
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 68.1716
                       Mean reward: 7.06
               Mean episode length: 242.60
    Episode_Reward/reaching_object: 1.4036
     Episode_Reward/lifting_object: 0.0156
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 2.05s
                      Time elapsed: 00:12:34
                               ETA: 01:00:06

################################################################################
                     [1m Learning iteration 346/2000 [0m                      

                       Computation: 47521 steps/s (collection: 1.978s, learning 0.091s)
             Mean action noise std: 2.20
          Mean value_function loss: 0.7162
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 68.2332
                       Mean reward: 6.19
               Mean episode length: 230.72
    Episode_Reward/reaching_object: 1.3392
     Episode_Reward/lifting_object: 0.1276
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 2.07s
                      Time elapsed: 00:12:36
                               ETA: 01:00:03

################################################################################
                     [1m Learning iteration 347/2000 [0m                      

                       Computation: 47347 steps/s (collection: 1.973s, learning 0.103s)
             Mean action noise std: 2.20
          Mean value_function loss: 0.5311
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 68.2709
                       Mean reward: 6.48
               Mean episode length: 241.18
    Episode_Reward/reaching_object: 1.3894
     Episode_Reward/lifting_object: 0.0237
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 2.08s
                      Time elapsed: 00:12:38
                               ETA: 01:00:01

################################################################################
                     [1m Learning iteration 348/2000 [0m                      

                       Computation: 49800 steps/s (collection: 1.867s, learning 0.107s)
             Mean action noise std: 2.20
          Mean value_function loss: 0.7756
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 68.3328
                       Mean reward: 6.73
               Mean episode length: 237.94
    Episode_Reward/reaching_object: 1.3633
     Episode_Reward/lifting_object: 0.0549
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 1.97s
                      Time elapsed: 00:12:40
                               ETA: 00:59:58

################################################################################
                     [1m Learning iteration 349/2000 [0m                      

                       Computation: 48903 steps/s (collection: 1.911s, learning 0.100s)
             Mean action noise std: 2.21
          Mean value_function loss: 0.4721
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 68.3789
                       Mean reward: 6.84
               Mean episode length: 242.46
    Episode_Reward/reaching_object: 1.3853
     Episode_Reward/lifting_object: 0.0534
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 2.01s
                      Time elapsed: 00:12:42
                               ETA: 00:59:55

################################################################################
                     [1m Learning iteration 350/2000 [0m                      

                       Computation: 48915 steps/s (collection: 1.909s, learning 0.101s)
             Mean action noise std: 2.21
          Mean value_function loss: 0.3851
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 68.4298
                       Mean reward: 6.88
               Mean episode length: 239.62
    Episode_Reward/reaching_object: 1.4250
     Episode_Reward/lifting_object: 0.0074
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 2.01s
                      Time elapsed: 00:12:44
                               ETA: 00:59:52

################################################################################
                     [1m Learning iteration 351/2000 [0m                      

                       Computation: 48650 steps/s (collection: 1.906s, learning 0.115s)
             Mean action noise std: 2.21
          Mean value_function loss: 0.6683
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 68.4884
                       Mean reward: 6.32
               Mean episode length: 234.47
    Episode_Reward/reaching_object: 1.3674
     Episode_Reward/lifting_object: 0.1370
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 2.02s
                      Time elapsed: 00:12:46
                               ETA: 00:59:49

################################################################################
                     [1m Learning iteration 352/2000 [0m                      

                       Computation: 49924 steps/s (collection: 1.870s, learning 0.099s)
             Mean action noise std: 2.22
          Mean value_function loss: 0.4539
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 68.5508
                       Mean reward: 6.85
               Mean episode length: 232.00
    Episode_Reward/reaching_object: 1.3785
     Episode_Reward/lifting_object: 0.0921
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 1.97s
                      Time elapsed: 00:12:48
                               ETA: 00:59:46

################################################################################
                     [1m Learning iteration 353/2000 [0m                      

                       Computation: 49422 steps/s (collection: 1.899s, learning 0.090s)
             Mean action noise std: 2.22
          Mean value_function loss: 0.3926
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 68.6115
                       Mean reward: 6.52
               Mean episode length: 241.41
    Episode_Reward/reaching_object: 1.3856
     Episode_Reward/lifting_object: 0.0864
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 1.99s
                      Time elapsed: 00:12:50
                               ETA: 00:59:43

################################################################################
                     [1m Learning iteration 354/2000 [0m                      

                       Computation: 50469 steps/s (collection: 1.859s, learning 0.089s)
             Mean action noise std: 2.23
          Mean value_function loss: 2.6958
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 68.6582
                       Mean reward: 6.97
               Mean episode length: 234.92
    Episode_Reward/reaching_object: 1.3081
     Episode_Reward/lifting_object: 0.0096
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 1.95s
                      Time elapsed: 00:12:52
                               ETA: 00:59:39

################################################################################
                     [1m Learning iteration 355/2000 [0m                      

                       Computation: 50000 steps/s (collection: 1.875s, learning 0.092s)
             Mean action noise std: 2.23
          Mean value_function loss: 0.5191
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 68.6692
                       Mean reward: 6.85
               Mean episode length: 237.22
    Episode_Reward/reaching_object: 1.3412
     Episode_Reward/lifting_object: 0.1047
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 1.97s
                      Time elapsed: 00:12:54
                               ETA: 00:59:36

################################################################################
                     [1m Learning iteration 356/2000 [0m                      

                       Computation: 50309 steps/s (collection: 1.849s, learning 0.105s)
             Mean action noise std: 2.23
          Mean value_function loss: 0.3919
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 68.7052
                       Mean reward: 6.80
               Mean episode length: 226.15
    Episode_Reward/reaching_object: 1.4032
     Episode_Reward/lifting_object: -0.0293
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 1.95s
                      Time elapsed: 00:12:55
                               ETA: 00:59:33

################################################################################
                     [1m Learning iteration 357/2000 [0m                      

                       Computation: 49129 steps/s (collection: 1.904s, learning 0.097s)
             Mean action noise std: 2.23
          Mean value_function loss: 0.3893
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 68.7632
                       Mean reward: 4.83
               Mean episode length: 235.46
    Episode_Reward/reaching_object: 1.3646
     Episode_Reward/lifting_object: -0.0050
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 2.00s
                      Time elapsed: 00:12:57
                               ETA: 00:59:30

################################################################################
                     [1m Learning iteration 358/2000 [0m                      

                       Computation: 49520 steps/s (collection: 1.875s, learning 0.110s)
             Mean action noise std: 2.24
          Mean value_function loss: 0.6353
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 68.8121
                       Mean reward: 7.41
               Mean episode length: 240.52
    Episode_Reward/reaching_object: 1.3968
     Episode_Reward/lifting_object: 0.0600
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 1.99s
                      Time elapsed: 00:12:59
                               ETA: 00:59:27

################################################################################
                     [1m Learning iteration 359/2000 [0m                      

                       Computation: 50143 steps/s (collection: 1.863s, learning 0.097s)
             Mean action noise std: 2.24
          Mean value_function loss: 0.3989
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 68.8661
                       Mean reward: 6.53
               Mean episode length: 238.65
    Episode_Reward/reaching_object: 1.4196
     Episode_Reward/lifting_object: 0.1183
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 1.96s
                      Time elapsed: 00:13:01
                               ETA: 00:59:24

################################################################################
                     [1m Learning iteration 360/2000 [0m                      

                       Computation: 50061 steps/s (collection: 1.868s, learning 0.096s)
             Mean action noise std: 2.25
          Mean value_function loss: 0.2900
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 68.9437
                       Mean reward: 7.14
               Mean episode length: 241.70
    Episode_Reward/reaching_object: 1.4561
     Episode_Reward/lifting_object: 0.1890
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 1.96s
                      Time elapsed: 00:13:03
                               ETA: 00:59:21

################################################################################
                     [1m Learning iteration 361/2000 [0m                      

                       Computation: 49483 steps/s (collection: 1.892s, learning 0.095s)
             Mean action noise std: 2.25
          Mean value_function loss: 0.3604
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 69.0166
                       Mean reward: 7.30
               Mean episode length: 243.00
    Episode_Reward/reaching_object: 1.4262
     Episode_Reward/lifting_object: 0.0947
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 1.99s
                      Time elapsed: 00:13:05
                               ETA: 00:59:18

################################################################################
                     [1m Learning iteration 362/2000 [0m                      

                       Computation: 49994 steps/s (collection: 1.859s, learning 0.108s)
             Mean action noise std: 2.26
          Mean value_function loss: 0.4143
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 69.0733
                       Mean reward: 8.15
               Mean episode length: 245.86
    Episode_Reward/reaching_object: 1.4280
     Episode_Reward/lifting_object: 0.2473
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 1.97s
                      Time elapsed: 00:13:07
                               ETA: 00:59:15

################################################################################
                     [1m Learning iteration 363/2000 [0m                      

                       Computation: 47146 steps/s (collection: 1.976s, learning 0.109s)
             Mean action noise std: 2.26
          Mean value_function loss: 0.5166
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 69.1398
                       Mean reward: 6.74
               Mean episode length: 240.26
    Episode_Reward/reaching_object: 1.3650
     Episode_Reward/lifting_object: 0.1028
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 2.09s
                      Time elapsed: 00:13:09
                               ETA: 00:59:12

################################################################################
                     [1m Learning iteration 364/2000 [0m                      

                       Computation: 49133 steps/s (collection: 1.900s, learning 0.101s)
             Mean action noise std: 2.27
          Mean value_function loss: 0.4216
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 69.1989
                       Mean reward: 7.53
               Mean episode length: 241.81
    Episode_Reward/reaching_object: 1.3946
     Episode_Reward/lifting_object: 0.0758
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 2.00s
                      Time elapsed: 00:13:11
                               ETA: 00:59:09

################################################################################
                     [1m Learning iteration 365/2000 [0m                      

                       Computation: 48387 steps/s (collection: 1.942s, learning 0.090s)
             Mean action noise std: 2.27
          Mean value_function loss: 0.3031
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 69.2634
                       Mean reward: 8.12
               Mean episode length: 241.81
    Episode_Reward/reaching_object: 1.3452
     Episode_Reward/lifting_object: 0.2016
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 2.03s
                      Time elapsed: 00:13:13
                               ETA: 00:59:06

################################################################################
                     [1m Learning iteration 366/2000 [0m                      

                       Computation: 49766 steps/s (collection: 1.879s, learning 0.096s)
             Mean action noise std: 2.27
          Mean value_function loss: 0.6770
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 69.3051
                       Mean reward: 6.36
               Mean episode length: 236.59
    Episode_Reward/reaching_object: 1.3477
     Episode_Reward/lifting_object: 0.1374
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 1.98s
                      Time elapsed: 00:13:15
                               ETA: 00:59:03

################################################################################
                     [1m Learning iteration 367/2000 [0m                      

                       Computation: 49272 steps/s (collection: 1.901s, learning 0.095s)
             Mean action noise std: 2.28
          Mean value_function loss: 1.0951
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 69.3378
                       Mean reward: 7.06
               Mean episode length: 242.11
    Episode_Reward/reaching_object: 1.3687
     Episode_Reward/lifting_object: 0.0723
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 2.00s
                      Time elapsed: 00:13:17
                               ETA: 00:59:00

################################################################################
                     [1m Learning iteration 368/2000 [0m                      

                       Computation: 49831 steps/s (collection: 1.885s, learning 0.088s)
             Mean action noise std: 2.28
          Mean value_function loss: 1.0098
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 69.3866
                       Mean reward: 6.68
               Mean episode length: 238.59
    Episode_Reward/reaching_object: 1.2978
     Episode_Reward/lifting_object: 0.0902
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 1.97s
                      Time elapsed: 00:13:19
                               ETA: 00:58:57

################################################################################
                     [1m Learning iteration 369/2000 [0m                      

                       Computation: 50102 steps/s (collection: 1.871s, learning 0.091s)
             Mean action noise std: 2.28
          Mean value_function loss: 1.6397
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 69.4137
                       Mean reward: 7.39
               Mean episode length: 240.83
    Episode_Reward/reaching_object: 1.3295
     Episode_Reward/lifting_object: 0.1983
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 1.96s
                      Time elapsed: 00:13:21
                               ETA: 00:58:54

################################################################################
                     [1m Learning iteration 370/2000 [0m                      

                       Computation: 50065 steps/s (collection: 1.867s, learning 0.096s)
             Mean action noise std: 2.28
          Mean value_function loss: 0.3695
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 69.4266
                       Mean reward: 6.92
               Mean episode length: 236.55
    Episode_Reward/reaching_object: 1.3699
     Episode_Reward/lifting_object: 0.1057
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 1.96s
                      Time elapsed: 00:13:23
                               ETA: 00:58:51

################################################################################
                     [1m Learning iteration 371/2000 [0m                      

                       Computation: 48807 steps/s (collection: 1.919s, learning 0.096s)
             Mean action noise std: 2.29
          Mean value_function loss: 0.5116
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 69.4627
                       Mean reward: 6.76
               Mean episode length: 245.50
    Episode_Reward/reaching_object: 1.3797
     Episode_Reward/lifting_object: -0.0360
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 2.01s
                      Time elapsed: 00:13:25
                               ETA: 00:58:48

################################################################################
                     [1m Learning iteration 372/2000 [0m                      

                       Computation: 49036 steps/s (collection: 1.895s, learning 0.110s)
             Mean action noise std: 2.29
          Mean value_function loss: 0.4839
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 69.5318
                       Mean reward: 7.95
               Mean episode length: 240.58
    Episode_Reward/reaching_object: 1.4122
     Episode_Reward/lifting_object: 0.1073
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 2.00s
                      Time elapsed: 00:13:27
                               ETA: 00:58:45

################################################################################
                     [1m Learning iteration 373/2000 [0m                      

                       Computation: 49443 steps/s (collection: 1.877s, learning 0.112s)
             Mean action noise std: 2.30
          Mean value_function loss: 0.7072
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 69.5964
                       Mean reward: 7.14
               Mean episode length: 236.12
    Episode_Reward/reaching_object: 1.3221
     Episode_Reward/lifting_object: 0.0794
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 1.99s
                      Time elapsed: 00:13:29
                               ETA: 00:58:43

################################################################################
                     [1m Learning iteration 374/2000 [0m                      

                       Computation: 50291 steps/s (collection: 1.861s, learning 0.094s)
             Mean action noise std: 2.30
          Mean value_function loss: 0.4704
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 69.6445
                       Mean reward: 6.76
               Mean episode length: 232.85
    Episode_Reward/reaching_object: 1.2812
     Episode_Reward/lifting_object: 0.1054
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 1.95s
                      Time elapsed: 00:13:31
                               ETA: 00:58:39

################################################################################
                     [1m Learning iteration 375/2000 [0m                      

                       Computation: 49499 steps/s (collection: 1.893s, learning 0.093s)
             Mean action noise std: 2.30
          Mean value_function loss: 1.0158
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 69.6955
                       Mean reward: 7.65
               Mean episode length: 236.32
    Episode_Reward/reaching_object: 1.3303
     Episode_Reward/lifting_object: 0.2215
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 1.99s
                      Time elapsed: 00:13:33
                               ETA: 00:58:37

################################################################################
                     [1m Learning iteration 376/2000 [0m                      

                       Computation: 48656 steps/s (collection: 1.921s, learning 0.100s)
             Mean action noise std: 2.31
          Mean value_function loss: 0.8064
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 69.7571
                       Mean reward: 8.06
               Mean episode length: 240.17
    Episode_Reward/reaching_object: 1.3705
     Episode_Reward/lifting_object: 0.2271
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 2.02s
                      Time elapsed: 00:13:35
                               ETA: 00:58:34

################################################################################
                     [1m Learning iteration 377/2000 [0m                      

                       Computation: 47380 steps/s (collection: 1.981s, learning 0.094s)
             Mean action noise std: 2.31
          Mean value_function loss: 0.3869
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 69.8087
                       Mean reward: 6.51
               Mean episode length: 239.36
    Episode_Reward/reaching_object: 1.2997
     Episode_Reward/lifting_object: 0.2081
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 2.07s
                      Time elapsed: 00:13:37
                               ETA: 00:58:31

################################################################################
                     [1m Learning iteration 378/2000 [0m                      

                       Computation: 48782 steps/s (collection: 1.908s, learning 0.107s)
             Mean action noise std: 2.32
          Mean value_function loss: 0.8394
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 69.8599
                       Mean reward: 7.53
               Mean episode length: 240.72
    Episode_Reward/reaching_object: 1.3764
     Episode_Reward/lifting_object: 0.1559
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 2.02s
                      Time elapsed: 00:13:39
                               ETA: 00:58:28

################################################################################
                     [1m Learning iteration 379/2000 [0m                      

                       Computation: 48952 steps/s (collection: 1.910s, learning 0.098s)
             Mean action noise std: 2.32
          Mean value_function loss: 0.5384
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 69.9281
                       Mean reward: 7.39
               Mean episode length: 245.75
    Episode_Reward/reaching_object: 1.3390
     Episode_Reward/lifting_object: 0.2213
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 2.01s
                      Time elapsed: 00:13:41
                               ETA: 00:58:26

################################################################################
                     [1m Learning iteration 380/2000 [0m                      

                       Computation: 49395 steps/s (collection: 1.896s, learning 0.095s)
             Mean action noise std: 2.33
          Mean value_function loss: 1.0657
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 69.9980
                       Mean reward: 7.89
               Mean episode length: 234.07
    Episode_Reward/reaching_object: 1.3175
     Episode_Reward/lifting_object: 0.2460
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 1.99s
                      Time elapsed: 00:13:43
                               ETA: 00:58:23

################################################################################
                     [1m Learning iteration 381/2000 [0m                      

                       Computation: 49672 steps/s (collection: 1.882s, learning 0.097s)
             Mean action noise std: 2.33
          Mean value_function loss: 0.4697
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 70.0363
                       Mean reward: 8.03
               Mean episode length: 241.50
    Episode_Reward/reaching_object: 1.3215
     Episode_Reward/lifting_object: 0.2492
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 1.98s
                      Time elapsed: 00:13:45
                               ETA: 00:58:20

################################################################################
                     [1m Learning iteration 382/2000 [0m                      

                       Computation: 49125 steps/s (collection: 1.905s, learning 0.096s)
             Mean action noise std: 2.33
          Mean value_function loss: 1.0884
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 70.0820
                       Mean reward: 6.71
               Mean episode length: 230.50
    Episode_Reward/reaching_object: 1.3328
     Episode_Reward/lifting_object: 0.1625
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 2.00s
                      Time elapsed: 00:13:47
                               ETA: 00:58:17

################################################################################
                     [1m Learning iteration 383/2000 [0m                      

                       Computation: 49946 steps/s (collection: 1.878s, learning 0.090s)
             Mean action noise std: 2.33
          Mean value_function loss: 1.4272
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 70.1203
                       Mean reward: 6.51
               Mean episode length: 229.72
    Episode_Reward/reaching_object: 1.2759
     Episode_Reward/lifting_object: 0.1486
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 1.97s
                      Time elapsed: 00:13:49
                               ETA: 00:58:14

################################################################################
                     [1m Learning iteration 384/2000 [0m                      

                       Computation: 48946 steps/s (collection: 1.921s, learning 0.087s)
             Mean action noise std: 2.34
          Mean value_function loss: 0.7639
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 70.1669
                       Mean reward: 7.23
               Mean episode length: 233.57
    Episode_Reward/reaching_object: 1.3158
     Episode_Reward/lifting_object: 0.1947
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 2.01s
                      Time elapsed: 00:13:51
                               ETA: 00:58:11

################################################################################
                     [1m Learning iteration 385/2000 [0m                      

                       Computation: 48829 steps/s (collection: 1.923s, learning 0.090s)
             Mean action noise std: 2.34
          Mean value_function loss: 0.8761
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 70.2228
                       Mean reward: 7.19
               Mean episode length: 237.80
    Episode_Reward/reaching_object: 1.3187
     Episode_Reward/lifting_object: 0.1351
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 2.01s
                      Time elapsed: 00:13:53
                               ETA: 00:58:08

################################################################################
                     [1m Learning iteration 386/2000 [0m                      

                       Computation: 50351 steps/s (collection: 1.859s, learning 0.093s)
             Mean action noise std: 2.35
          Mean value_function loss: 0.6893
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 70.2718
                       Mean reward: 7.01
               Mean episode length: 240.00
    Episode_Reward/reaching_object: 1.3024
     Episode_Reward/lifting_object: 0.2203
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 1.95s
                      Time elapsed: 00:13:55
                               ETA: 00:58:05

################################################################################
                     [1m Learning iteration 387/2000 [0m                      

                       Computation: 49566 steps/s (collection: 1.879s, learning 0.104s)
             Mean action noise std: 2.35
          Mean value_function loss: 1.1271
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 70.3311
                       Mean reward: 7.08
               Mean episode length: 237.56
    Episode_Reward/reaching_object: 1.3242
     Episode_Reward/lifting_object: 0.2405
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 1.98s
                      Time elapsed: 00:13:57
                               ETA: 00:58:02

################################################################################
                     [1m Learning iteration 388/2000 [0m                      

                       Computation: 49280 steps/s (collection: 1.888s, learning 0.107s)
             Mean action noise std: 2.35
          Mean value_function loss: 1.1522
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 70.3838
                       Mean reward: 6.59
               Mean episode length: 222.47
    Episode_Reward/reaching_object: 1.2660
     Episode_Reward/lifting_object: 0.2166
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 1.99s
                      Time elapsed: 00:13:59
                               ETA: 00:58:00

################################################################################
                     [1m Learning iteration 389/2000 [0m                      

                       Computation: 46176 steps/s (collection: 2.007s, learning 0.122s)
             Mean action noise std: 2.36
          Mean value_function loss: 1.0086
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 70.4181
                       Mean reward: 8.95
               Mean episode length: 225.16
    Episode_Reward/reaching_object: 1.2820
     Episode_Reward/lifting_object: 0.4751
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 2.13s
                      Time elapsed: 00:14:01
                               ETA: 00:57:57

################################################################################
                     [1m Learning iteration 390/2000 [0m                      

                       Computation: 49270 steps/s (collection: 1.906s, learning 0.090s)
             Mean action noise std: 2.36
          Mean value_function loss: 1.0715
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 70.4588
                       Mean reward: 8.63
               Mean episode length: 234.80
    Episode_Reward/reaching_object: 1.3089
     Episode_Reward/lifting_object: 0.4008
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 2.00s
                      Time elapsed: 00:14:03
                               ETA: 00:57:54

################################################################################
                     [1m Learning iteration 391/2000 [0m                      

                       Computation: 49689 steps/s (collection: 1.890s, learning 0.089s)
             Mean action noise std: 2.36
          Mean value_function loss: 1.3471
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 70.4939
                       Mean reward: 8.00
               Mean episode length: 238.37
    Episode_Reward/reaching_object: 1.3631
     Episode_Reward/lifting_object: 0.4142
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 1.98s
                      Time elapsed: 00:14:05
                               ETA: 00:57:52

################################################################################
                     [1m Learning iteration 392/2000 [0m                      

                       Computation: 47842 steps/s (collection: 1.956s, learning 0.099s)
             Mean action noise std: 2.37
          Mean value_function loss: 1.2323
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 70.5332
                       Mean reward: 10.73
               Mean episode length: 235.00
    Episode_Reward/reaching_object: 1.3577
     Episode_Reward/lifting_object: 0.3442
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 2.05s
                      Time elapsed: 00:14:07
                               ETA: 00:57:49

################################################################################
                     [1m Learning iteration 393/2000 [0m                      

                       Computation: 49433 steps/s (collection: 1.892s, learning 0.097s)
             Mean action noise std: 2.37
          Mean value_function loss: 0.8438
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 70.5829
                       Mean reward: 7.62
               Mean episode length: 234.39
    Episode_Reward/reaching_object: 1.3258
     Episode_Reward/lifting_object: 0.4166
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 1.99s
                      Time elapsed: 00:14:09
                               ETA: 00:57:46

################################################################################
                     [1m Learning iteration 394/2000 [0m                      

                       Computation: 48213 steps/s (collection: 1.950s, learning 0.089s)
             Mean action noise std: 2.37
          Mean value_function loss: 1.2914
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 70.6184
                       Mean reward: 7.61
               Mean episode length: 228.44
    Episode_Reward/reaching_object: 1.2927
     Episode_Reward/lifting_object: 0.3217
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 2.04s
                      Time elapsed: 00:14:11
                               ETA: 00:57:44

################################################################################
                     [1m Learning iteration 395/2000 [0m                      

                       Computation: 49505 steps/s (collection: 1.880s, learning 0.106s)
             Mean action noise std: 2.38
          Mean value_function loss: 1.3524
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 70.6627
                       Mean reward: 7.85
               Mean episode length: 229.76
    Episode_Reward/reaching_object: 1.2743
     Episode_Reward/lifting_object: 0.4073
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 1.99s
                      Time elapsed: 00:14:13
                               ETA: 00:57:41

################################################################################
                     [1m Learning iteration 396/2000 [0m                      

                       Computation: 50057 steps/s (collection: 1.877s, learning 0.087s)
             Mean action noise std: 2.38
          Mean value_function loss: 1.6233
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 70.7158
                       Mean reward: 8.78
               Mean episode length: 233.01
    Episode_Reward/reaching_object: 1.2310
     Episode_Reward/lifting_object: 0.5459
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 1.96s
                      Time elapsed: 00:14:15
                               ETA: 00:57:38

################################################################################
                     [1m Learning iteration 397/2000 [0m                      

                       Computation: 50426 steps/s (collection: 1.858s, learning 0.092s)
             Mean action noise std: 2.38
          Mean value_function loss: 1.5040
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 70.7635
                       Mean reward: 6.80
               Mean episode length: 219.62
    Episode_Reward/reaching_object: 1.1517
     Episode_Reward/lifting_object: 0.4807
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 1.95s
                      Time elapsed: 00:14:17
                               ETA: 00:57:35

################################################################################
                     [1m Learning iteration 398/2000 [0m                      

                       Computation: 49762 steps/s (collection: 1.887s, learning 0.089s)
             Mean action noise std: 2.39
          Mean value_function loss: 1.9718
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 70.8100
                       Mean reward: 8.49
               Mean episode length: 222.70
    Episode_Reward/reaching_object: 1.1478
     Episode_Reward/lifting_object: 0.5489
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 1.98s
                      Time elapsed: 00:14:19
                               ETA: 00:57:32

################################################################################
                     [1m Learning iteration 399/2000 [0m                      

                       Computation: 49629 steps/s (collection: 1.894s, learning 0.087s)
             Mean action noise std: 2.39
          Mean value_function loss: 1.5221
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 70.8619
                       Mean reward: 7.38
               Mean episode length: 224.87
    Episode_Reward/reaching_object: 1.1541
     Episode_Reward/lifting_object: 0.4565
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 1.98s
                      Time elapsed: 00:14:21
                               ETA: 00:57:29

################################################################################
                     [1m Learning iteration 400/2000 [0m                      

                       Computation: 49775 steps/s (collection: 1.876s, learning 0.099s)
             Mean action noise std: 2.40
          Mean value_function loss: 1.6587
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 70.9265
                       Mean reward: 9.48
               Mean episode length: 225.98
    Episode_Reward/reaching_object: 1.1748
     Episode_Reward/lifting_object: 0.6959
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 1.97s
                      Time elapsed: 00:14:23
                               ETA: 00:57:26

################################################################################
                     [1m Learning iteration 401/2000 [0m                      

                       Computation: 48121 steps/s (collection: 1.935s, learning 0.108s)
             Mean action noise std: 2.40
          Mean value_function loss: 1.7059
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 70.9712
                       Mean reward: 6.91
               Mean episode length: 208.38
    Episode_Reward/reaching_object: 1.1409
     Episode_Reward/lifting_object: 0.6609
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 2.04s
                      Time elapsed: 00:14:25
                               ETA: 00:57:24

################################################################################
                     [1m Learning iteration 402/2000 [0m                      

                       Computation: 47323 steps/s (collection: 1.965s, learning 0.112s)
             Mean action noise std: 2.40
          Mean value_function loss: 2.0059
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 71.0067
                       Mean reward: 7.83
               Mean episode length: 225.61
    Episode_Reward/reaching_object: 1.1367
     Episode_Reward/lifting_object: 0.4933
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 2.08s
                      Time elapsed: 00:14:27
                               ETA: 00:57:21

################################################################################
                     [1m Learning iteration 403/2000 [0m                      

                       Computation: 49445 steps/s (collection: 1.879s, learning 0.109s)
             Mean action noise std: 2.41
          Mean value_function loss: 1.7135
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 71.0425
                       Mean reward: 6.96
               Mean episode length: 207.04
    Episode_Reward/reaching_object: 1.1042
     Episode_Reward/lifting_object: 0.5218
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 1.99s
                      Time elapsed: 00:14:29
                               ETA: 00:57:18

################################################################################
                     [1m Learning iteration 404/2000 [0m                      

                       Computation: 48961 steps/s (collection: 1.901s, learning 0.107s)
             Mean action noise std: 2.41
          Mean value_function loss: 2.2484
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 71.0744
                       Mean reward: 8.77
               Mean episode length: 226.16
    Episode_Reward/reaching_object: 1.1199
     Episode_Reward/lifting_object: 0.7215
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 2.01s
                      Time elapsed: 00:14:31
                               ETA: 00:57:16

################################################################################
                     [1m Learning iteration 405/2000 [0m                      

                       Computation: 50212 steps/s (collection: 1.870s, learning 0.088s)
             Mean action noise std: 2.41
          Mean value_function loss: 1.9134
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 71.1024
                       Mean reward: 9.02
               Mean episode length: 208.47
    Episode_Reward/reaching_object: 1.1320
     Episode_Reward/lifting_object: 0.6305
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 1.96s
                      Time elapsed: 00:14:33
                               ETA: 00:57:13

################################################################################
                     [1m Learning iteration 406/2000 [0m                      

                       Computation: 49780 steps/s (collection: 1.887s, learning 0.088s)
             Mean action noise std: 2.41
          Mean value_function loss: 2.4096
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 71.1183
                       Mean reward: 10.19
               Mean episode length: 214.62
    Episode_Reward/reaching_object: 1.1266
     Episode_Reward/lifting_object: 0.7911
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 1.97s
                      Time elapsed: 00:14:35
                               ETA: 00:57:10

################################################################################
                     [1m Learning iteration 407/2000 [0m                      

                       Computation: 48785 steps/s (collection: 1.926s, learning 0.089s)
             Mean action noise std: 2.41
          Mean value_function loss: 2.1992
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 71.1410
                       Mean reward: 8.26
               Mean episode length: 213.53
    Episode_Reward/reaching_object: 1.0789
     Episode_Reward/lifting_object: 0.7611
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 2.02s
                      Time elapsed: 00:14:37
                               ETA: 00:57:07

################################################################################
                     [1m Learning iteration 408/2000 [0m                      

                       Computation: 50629 steps/s (collection: 1.854s, learning 0.088s)
             Mean action noise std: 2.42
          Mean value_function loss: 1.7206
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 71.1793
                       Mean reward: 9.08
               Mean episode length: 219.35
    Episode_Reward/reaching_object: 1.0712
     Episode_Reward/lifting_object: 0.7049
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 1.94s
                      Time elapsed: 00:14:39
                               ETA: 00:57:04

################################################################################
                     [1m Learning iteration 409/2000 [0m                      

                       Computation: 49939 steps/s (collection: 1.865s, learning 0.104s)
             Mean action noise std: 2.42
          Mean value_function loss: 2.1903
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 71.2199
                       Mean reward: 8.85
               Mean episode length: 219.26
    Episode_Reward/reaching_object: 1.1155
     Episode_Reward/lifting_object: 0.7717
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 1.97s
                      Time elapsed: 00:14:41
                               ETA: 00:57:01

################################################################################
                     [1m Learning iteration 410/2000 [0m                      

                       Computation: 49838 steps/s (collection: 1.867s, learning 0.106s)
             Mean action noise std: 2.42
          Mean value_function loss: 2.9678
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 71.2522
                       Mean reward: 8.76
               Mean episode length: 213.31
    Episode_Reward/reaching_object: 1.0684
     Episode_Reward/lifting_object: 0.6454
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 1.97s
                      Time elapsed: 00:14:43
                               ETA: 00:56:58

################################################################################
                     [1m Learning iteration 411/2000 [0m                      

                       Computation: 49960 steps/s (collection: 1.875s, learning 0.092s)
             Mean action noise std: 2.43
          Mean value_function loss: 3.3275
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 71.2978
                       Mean reward: 10.12
               Mean episode length: 217.26
    Episode_Reward/reaching_object: 1.0420
     Episode_Reward/lifting_object: 0.9058
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 1.97s
                      Time elapsed: 00:14:45
                               ETA: 00:56:56

################################################################################
                     [1m Learning iteration 412/2000 [0m                      

                       Computation: 49806 steps/s (collection: 1.878s, learning 0.096s)
             Mean action noise std: 2.43
          Mean value_function loss: 2.5106
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 71.3545
                       Mean reward: 9.13
               Mean episode length: 215.90
    Episode_Reward/reaching_object: 1.0085
     Episode_Reward/lifting_object: 0.9111
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 1.97s
                      Time elapsed: 00:14:47
                               ETA: 00:56:53

################################################################################
                     [1m Learning iteration 413/2000 [0m                      

                       Computation: 49376 steps/s (collection: 1.905s, learning 0.086s)
             Mean action noise std: 2.43
          Mean value_function loss: 5.1185
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 71.3865
                       Mean reward: 10.30
               Mean episode length: 205.88
    Episode_Reward/reaching_object: 0.9692
     Episode_Reward/lifting_object: 0.6619
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 1.99s
                      Time elapsed: 00:14:49
                               ETA: 00:56:50

################################################################################
                     [1m Learning iteration 414/2000 [0m                      

                       Computation: 49931 steps/s (collection: 1.881s, learning 0.088s)
             Mean action noise std: 2.43
          Mean value_function loss: 2.7949
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 71.3989
                       Mean reward: 6.92
               Mean episode length: 198.17
    Episode_Reward/reaching_object: 0.9593
     Episode_Reward/lifting_object: 0.9067
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 1.97s
                      Time elapsed: 00:14:51
                               ETA: 00:56:47

################################################################################
                     [1m Learning iteration 415/2000 [0m                      

                       Computation: 49828 steps/s (collection: 1.881s, learning 0.092s)
             Mean action noise std: 2.43
          Mean value_function loss: 2.4817
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 71.4056
                       Mean reward: 7.88
               Mean episode length: 208.26
    Episode_Reward/reaching_object: 0.9894
     Episode_Reward/lifting_object: 0.7561
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 1.97s
                      Time elapsed: 00:14:53
                               ETA: 00:56:44

################################################################################
                     [1m Learning iteration 416/2000 [0m                      

                       Computation: 50605 steps/s (collection: 1.846s, learning 0.097s)
             Mean action noise std: 2.43
          Mean value_function loss: 1.7862
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 71.4111
                       Mean reward: 8.23
               Mean episode length: 208.43
    Episode_Reward/reaching_object: 1.0122
     Episode_Reward/lifting_object: 0.8348
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 1.94s
                      Time elapsed: 00:14:55
                               ETA: 00:56:41

################################################################################
                     [1m Learning iteration 417/2000 [0m                      

                       Computation: 49507 steps/s (collection: 1.878s, learning 0.108s)
             Mean action noise std: 2.44
          Mean value_function loss: 2.3454
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 71.4309
                       Mean reward: 7.86
               Mean episode length: 209.92
    Episode_Reward/reaching_object: 0.9590
     Episode_Reward/lifting_object: 0.7824
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 1.99s
                      Time elapsed: 00:14:57
                               ETA: 00:56:39

################################################################################
                     [1m Learning iteration 418/2000 [0m                      

                       Computation: 49535 steps/s (collection: 1.883s, learning 0.102s)
             Mean action noise std: 2.44
          Mean value_function loss: 2.2325
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 71.4643
                       Mean reward: 8.28
               Mean episode length: 195.07
    Episode_Reward/reaching_object: 0.9777
     Episode_Reward/lifting_object: 0.8729
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 1.98s
                      Time elapsed: 00:14:59
                               ETA: 00:56:36

################################################################################
                     [1m Learning iteration 419/2000 [0m                      

                       Computation: 49383 steps/s (collection: 1.872s, learning 0.119s)
             Mean action noise std: 2.44
          Mean value_function loss: 2.4291
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 71.5121
                       Mean reward: 7.32
               Mean episode length: 204.12
    Episode_Reward/reaching_object: 0.9621
     Episode_Reward/lifting_object: 0.7648
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 1.99s
                      Time elapsed: 00:15:01
                               ETA: 00:56:33

################################################################################
                     [1m Learning iteration 420/2000 [0m                      

                       Computation: 50269 steps/s (collection: 1.853s, learning 0.103s)
             Mean action noise std: 2.45
          Mean value_function loss: 3.0535
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 71.5573
                       Mean reward: 10.61
               Mean episode length: 205.24
    Episode_Reward/reaching_object: 0.9095
     Episode_Reward/lifting_object: 0.9437
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 1.96s
                      Time elapsed: 00:15:03
                               ETA: 00:56:30

################################################################################
                     [1m Learning iteration 421/2000 [0m                      

                       Computation: 50121 steps/s (collection: 1.873s, learning 0.088s)
             Mean action noise std: 2.45
          Mean value_function loss: 3.1073
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 71.5948
                       Mean reward: 7.67
               Mean episode length: 196.94
    Episode_Reward/reaching_object: 0.9335
     Episode_Reward/lifting_object: 0.8695
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 1.96s
                      Time elapsed: 00:15:05
                               ETA: 00:56:27

################################################################################
                     [1m Learning iteration 422/2000 [0m                      

                       Computation: 50116 steps/s (collection: 1.870s, learning 0.092s)
             Mean action noise std: 2.45
          Mean value_function loss: 3.9527
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 71.6206
                       Mean reward: 8.57
               Mean episode length: 199.14
    Episode_Reward/reaching_object: 0.9563
     Episode_Reward/lifting_object: 0.9039
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 1.96s
                      Time elapsed: 00:15:07
                               ETA: 00:56:25

################################################################################
                     [1m Learning iteration 423/2000 [0m                      

                       Computation: 50925 steps/s (collection: 1.835s, learning 0.095s)
             Mean action noise std: 2.45
          Mean value_function loss: 3.2355
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 71.6382
                       Mean reward: 10.74
               Mean episode length: 200.55
    Episode_Reward/reaching_object: 0.9704
     Episode_Reward/lifting_object: 1.0859
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 1.93s
                      Time elapsed: 00:15:09
                               ETA: 00:56:22

################################################################################
                     [1m Learning iteration 424/2000 [0m                      

                       Computation: 49851 steps/s (collection: 1.870s, learning 0.102s)
             Mean action noise std: 2.45
          Mean value_function loss: 2.6005
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 71.6559
                       Mean reward: 9.06
               Mean episode length: 196.60
    Episode_Reward/reaching_object: 0.9537
     Episode_Reward/lifting_object: 0.9659
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 1.97s
                      Time elapsed: 00:15:11
                               ETA: 00:56:19

################################################################################
                     [1m Learning iteration 425/2000 [0m                      

                       Computation: 48568 steps/s (collection: 1.928s, learning 0.096s)
             Mean action noise std: 2.46
          Mean value_function loss: 3.3848
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 71.6844
                       Mean reward: 8.77
               Mean episode length: 200.95
    Episode_Reward/reaching_object: 0.9677
     Episode_Reward/lifting_object: 1.0486
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 2.02s
                      Time elapsed: 00:15:13
                               ETA: 00:56:16

################################################################################
                     [1m Learning iteration 426/2000 [0m                      

                       Computation: 50181 steps/s (collection: 1.869s, learning 0.090s)
             Mean action noise std: 2.46
          Mean value_function loss: 3.1220
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 71.7197
                       Mean reward: 8.09
               Mean episode length: 191.97
    Episode_Reward/reaching_object: 0.9295
     Episode_Reward/lifting_object: 0.8190
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 1.96s
                      Time elapsed: 00:15:15
                               ETA: 00:56:13

################################################################################
                     [1m Learning iteration 427/2000 [0m                      

                       Computation: 49242 steps/s (collection: 1.904s, learning 0.093s)
             Mean action noise std: 2.46
          Mean value_function loss: 2.7188
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 71.7599
                       Mean reward: 9.35
               Mean episode length: 189.63
    Episode_Reward/reaching_object: 0.9549
     Episode_Reward/lifting_object: 0.9302
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 2.00s
                      Time elapsed: 00:15:17
                               ETA: 00:56:11

################################################################################
                     [1m Learning iteration 428/2000 [0m                      

                       Computation: 49816 steps/s (collection: 1.880s, learning 0.093s)
             Mean action noise std: 2.47
          Mean value_function loss: 4.2014
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 71.8034
                       Mean reward: 9.70
               Mean episode length: 210.44
    Episode_Reward/reaching_object: 0.9788
     Episode_Reward/lifting_object: 1.0121
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 1.97s
                      Time elapsed: 00:15:19
                               ETA: 00:56:08

################################################################################
                     [1m Learning iteration 429/2000 [0m                      

                       Computation: 47689 steps/s (collection: 1.969s, learning 0.093s)
             Mean action noise std: 2.47
          Mean value_function loss: 3.4935
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 71.8284
                       Mean reward: 9.75
               Mean episode length: 208.39
    Episode_Reward/reaching_object: 0.9736
     Episode_Reward/lifting_object: 1.2410
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 2.06s
                      Time elapsed: 00:15:21
                               ETA: 00:56:06

################################################################################
                     [1m Learning iteration 430/2000 [0m                      

                       Computation: 49214 steps/s (collection: 1.910s, learning 0.088s)
             Mean action noise std: 2.47
          Mean value_function loss: 3.5425
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 71.8629
                       Mean reward: 11.96
               Mean episode length: 196.48
    Episode_Reward/reaching_object: 0.9446
     Episode_Reward/lifting_object: 1.4341
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 2.00s
                      Time elapsed: 00:15:23
                               ETA: 00:56:03

################################################################################
                     [1m Learning iteration 431/2000 [0m                      

                       Computation: 48120 steps/s (collection: 1.949s, learning 0.094s)
             Mean action noise std: 2.47
          Mean value_function loss: 3.9365
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 71.8971
                       Mean reward: 9.97
               Mean episode length: 206.51
    Episode_Reward/reaching_object: 1.0228
     Episode_Reward/lifting_object: 1.3700
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 2.04s
                      Time elapsed: 00:15:25
                               ETA: 00:56:00

################################################################################
                     [1m Learning iteration 432/2000 [0m                      

                       Computation: 46775 steps/s (collection: 1.996s, learning 0.106s)
             Mean action noise std: 2.48
          Mean value_function loss: 4.2524
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 71.9349
                       Mean reward: 12.38
               Mean episode length: 213.24
    Episode_Reward/reaching_object: 1.0088
     Episode_Reward/lifting_object: 1.3458
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 2.10s
                      Time elapsed: 00:15:27
                               ETA: 00:55:58

################################################################################
                     [1m Learning iteration 433/2000 [0m                      

                       Computation: 49746 steps/s (collection: 1.874s, learning 0.102s)
             Mean action noise std: 2.48
          Mean value_function loss: 4.6761
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 71.9833
                       Mean reward: 10.07
               Mean episode length: 197.98
    Episode_Reward/reaching_object: 0.9600
     Episode_Reward/lifting_object: 1.2729
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 1.98s
                      Time elapsed: 00:15:29
                               ETA: 00:55:55

################################################################################
                     [1m Learning iteration 434/2000 [0m                      

                       Computation: 49123 steps/s (collection: 1.905s, learning 0.097s)
             Mean action noise std: 2.48
          Mean value_function loss: 5.6834
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 72.0073
                       Mean reward: 10.94
               Mean episode length: 196.96
    Episode_Reward/reaching_object: 0.9431
     Episode_Reward/lifting_object: 1.3078
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 2.00s
                      Time elapsed: 00:15:31
                               ETA: 00:55:53

################################################################################
                     [1m Learning iteration 435/2000 [0m                      

                       Computation: 49040 steps/s (collection: 1.908s, learning 0.096s)
             Mean action noise std: 2.49
          Mean value_function loss: 5.3019
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 72.0425
                       Mean reward: 13.19
               Mean episode length: 188.10
    Episode_Reward/reaching_object: 0.9436
     Episode_Reward/lifting_object: 1.5589
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 2.00s
                      Time elapsed: 00:15:33
                               ETA: 00:55:50

################################################################################
                     [1m Learning iteration 436/2000 [0m                      

                       Computation: 48978 steps/s (collection: 1.903s, learning 0.104s)
             Mean action noise std: 2.49
          Mean value_function loss: 5.1693
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 72.0764
                       Mean reward: 13.19
               Mean episode length: 200.16
    Episode_Reward/reaching_object: 0.9478
     Episode_Reward/lifting_object: 1.4364
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 2.01s
                      Time elapsed: 00:15:35
                               ETA: 00:55:47

################################################################################
                     [1m Learning iteration 437/2000 [0m                      

                       Computation: 49138 steps/s (collection: 1.893s, learning 0.108s)
             Mean action noise std: 2.49
          Mean value_function loss: 5.3559
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 72.0934
                       Mean reward: 11.78
               Mean episode length: 187.58
    Episode_Reward/reaching_object: 0.9658
     Episode_Reward/lifting_object: 1.5550
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 2.00s
                      Time elapsed: 00:15:37
                               ETA: 00:55:45

################################################################################
                     [1m Learning iteration 438/2000 [0m                      

                       Computation: 50176 steps/s (collection: 1.872s, learning 0.088s)
             Mean action noise std: 2.49
          Mean value_function loss: 5.4372
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 72.1219
                       Mean reward: 13.26
               Mean episode length: 194.14
    Episode_Reward/reaching_object: 0.9699
     Episode_Reward/lifting_object: 1.7387
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 1.96s
                      Time elapsed: 00:15:39
                               ETA: 00:55:42

################################################################################
                     [1m Learning iteration 439/2000 [0m                      

                       Computation: 48346 steps/s (collection: 1.932s, learning 0.101s)
             Mean action noise std: 2.49
          Mean value_function loss: 5.8981
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 72.1560
                       Mean reward: 13.52
               Mean episode length: 184.87
    Episode_Reward/reaching_object: 0.9045
     Episode_Reward/lifting_object: 1.5888
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.0417
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 2.03s
                      Time elapsed: 00:15:41
                               ETA: 00:55:40

################################################################################
                     [1m Learning iteration 440/2000 [0m                      

                       Computation: 49419 steps/s (collection: 1.892s, learning 0.098s)
             Mean action noise std: 2.49
          Mean value_function loss: 5.1955
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 72.1670
                       Mean reward: 11.34
               Mean episode length: 196.66
    Episode_Reward/reaching_object: 0.9675
     Episode_Reward/lifting_object: 1.4613
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 1.99s
                      Time elapsed: 00:15:43
                               ETA: 00:55:37

################################################################################
                     [1m Learning iteration 441/2000 [0m                      

                       Computation: 47970 steps/s (collection: 1.949s, learning 0.101s)
             Mean action noise std: 2.49
          Mean value_function loss: 4.3978
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 72.1695
                       Mean reward: 11.42
               Mean episode length: 197.00
    Episode_Reward/reaching_object: 0.9967
     Episode_Reward/lifting_object: 1.7352
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 2.05s
                      Time elapsed: 00:15:45
                               ETA: 00:55:34

################################################################################
                     [1m Learning iteration 442/2000 [0m                      

                       Computation: 47489 steps/s (collection: 1.966s, learning 0.104s)
             Mean action noise std: 2.49
          Mean value_function loss: 5.3703
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 72.1745
                       Mean reward: 13.41
               Mean episode length: 204.30
    Episode_Reward/reaching_object: 0.9964
     Episode_Reward/lifting_object: 1.6016
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 2.07s
                      Time elapsed: 00:15:47
                               ETA: 00:55:32

################################################################################
                     [1m Learning iteration 443/2000 [0m                      

                       Computation: 48754 steps/s (collection: 1.929s, learning 0.088s)
             Mean action noise std: 2.50
          Mean value_function loss: 4.8621
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 72.1799
                       Mean reward: 15.74
               Mean episode length: 207.17
    Episode_Reward/reaching_object: 1.0084
     Episode_Reward/lifting_object: 1.9682
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 2.02s
                      Time elapsed: 00:15:49
                               ETA: 00:55:29

################################################################################
                     [1m Learning iteration 444/2000 [0m                      

                       Computation: 50510 steps/s (collection: 1.855s, learning 0.092s)
             Mean action noise std: 2.50
          Mean value_function loss: 4.7487
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 72.1998
                       Mean reward: 15.93
               Mean episode length: 217.64
    Episode_Reward/reaching_object: 1.0189
     Episode_Reward/lifting_object: 1.8610
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 1.95s
                      Time elapsed: 00:15:51
                               ETA: 00:55:27

################################################################################
                     [1m Learning iteration 445/2000 [0m                      

                       Computation: 50522 steps/s (collection: 1.859s, learning 0.087s)
             Mean action noise std: 2.50
          Mean value_function loss: 4.8211
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 72.2297
                       Mean reward: 16.91
               Mean episode length: 212.14
    Episode_Reward/reaching_object: 1.0164
     Episode_Reward/lifting_object: 1.6986
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 1.95s
                      Time elapsed: 00:15:53
                               ETA: 00:55:24

################################################################################
                     [1m Learning iteration 446/2000 [0m                      

                       Computation: 50741 steps/s (collection: 1.851s, learning 0.087s)
             Mean action noise std: 2.50
          Mean value_function loss: 5.5400
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 72.2639
                       Mean reward: 14.54
               Mean episode length: 199.63
    Episode_Reward/reaching_object: 0.9698
     Episode_Reward/lifting_object: 1.7080
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 1.94s
                      Time elapsed: 00:15:55
                               ETA: 00:55:21

################################################################################
                     [1m Learning iteration 447/2000 [0m                      

                       Computation: 49283 steps/s (collection: 1.887s, learning 0.108s)
             Mean action noise std: 2.51
          Mean value_function loss: 7.3246
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 72.3012
                       Mean reward: 13.85
               Mean episode length: 208.34
    Episode_Reward/reaching_object: 1.0186
     Episode_Reward/lifting_object: 1.8760
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 1.99s
                      Time elapsed: 00:15:57
                               ETA: 00:55:18

################################################################################
                     [1m Learning iteration 448/2000 [0m                      

                       Computation: 48786 steps/s (collection: 1.906s, learning 0.109s)
             Mean action noise std: 2.51
          Mean value_function loss: 6.7322
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 72.3408
                       Mean reward: 14.95
               Mean episode length: 199.92
    Episode_Reward/reaching_object: 0.9984
     Episode_Reward/lifting_object: 1.8269
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 2.01s
                      Time elapsed: 00:15:59
                               ETA: 00:55:16

################################################################################
                     [1m Learning iteration 449/2000 [0m                      

                       Computation: 49313 steps/s (collection: 1.891s, learning 0.102s)
             Mean action noise std: 2.51
          Mean value_function loss: 8.3979
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 72.3652
                       Mean reward: 14.72
               Mean episode length: 204.46
    Episode_Reward/reaching_object: 1.0189
     Episode_Reward/lifting_object: 1.8115
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 1.99s
                      Time elapsed: 00:16:01
                               ETA: 00:55:13

################################################################################
                     [1m Learning iteration 450/2000 [0m                      

                       Computation: 49434 steps/s (collection: 1.899s, learning 0.090s)
             Mean action noise std: 2.51
          Mean value_function loss: 6.1300
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 72.3922
                       Mean reward: 14.20
               Mean episode length: 213.74
    Episode_Reward/reaching_object: 1.0220
     Episode_Reward/lifting_object: 2.0073
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 1.99s
                      Time elapsed: 00:16:03
                               ETA: 00:55:11

################################################################################
                     [1m Learning iteration 451/2000 [0m                      

                       Computation: 49892 steps/s (collection: 1.874s, learning 0.097s)
             Mean action noise std: 2.52
          Mean value_function loss: 6.5035
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 72.4188
                       Mean reward: 14.19
               Mean episode length: 208.54
    Episode_Reward/reaching_object: 1.0092
     Episode_Reward/lifting_object: 2.1219
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 1.97s
                      Time elapsed: 00:16:05
                               ETA: 00:55:08

################################################################################
                     [1m Learning iteration 452/2000 [0m                      

                       Computation: 49098 steps/s (collection: 1.906s, learning 0.096s)
             Mean action noise std: 2.52
          Mean value_function loss: 8.3900
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 72.4330
                       Mean reward: 17.73
               Mean episode length: 195.09
    Episode_Reward/reaching_object: 1.0262
     Episode_Reward/lifting_object: 2.2789
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 2.00s
                      Time elapsed: 00:16:07
                               ETA: 00:55:05

################################################################################
                     [1m Learning iteration 453/2000 [0m                      

                       Computation: 49577 steps/s (collection: 1.880s, learning 0.103s)
             Mean action noise std: 2.52
          Mean value_function loss: 7.7545
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 72.4498
                       Mean reward: 16.66
               Mean episode length: 207.26
    Episode_Reward/reaching_object: 1.0184
     Episode_Reward/lifting_object: 2.1150
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 1.98s
                      Time elapsed: 00:16:09
                               ETA: 00:55:03

################################################################################
                     [1m Learning iteration 454/2000 [0m                      

                       Computation: 49323 steps/s (collection: 1.895s, learning 0.098s)
             Mean action noise std: 2.52
          Mean value_function loss: 7.2559
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 72.4806
                       Mean reward: 15.78
               Mean episode length: 217.68
    Episode_Reward/reaching_object: 1.0354
     Episode_Reward/lifting_object: 2.0935
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 1.99s
                      Time elapsed: 00:16:11
                               ETA: 00:55:00

################################################################################
                     [1m Learning iteration 455/2000 [0m                      

                       Computation: 49906 steps/s (collection: 1.880s, learning 0.090s)
             Mean action noise std: 2.52
          Mean value_function loss: 8.7648
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 72.5138
                       Mean reward: 16.86
               Mean episode length: 209.50
    Episode_Reward/reaching_object: 1.0497
     Episode_Reward/lifting_object: 2.2836
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 1.97s
                      Time elapsed: 00:16:13
                               ETA: 00:54:57

################################################################################
                     [1m Learning iteration 456/2000 [0m                      

                       Computation: 50092 steps/s (collection: 1.875s, learning 0.088s)
             Mean action noise std: 2.53
          Mean value_function loss: 8.6876
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 72.5437
                       Mean reward: 15.36
               Mean episode length: 220.18
    Episode_Reward/reaching_object: 1.0759
     Episode_Reward/lifting_object: 2.4026
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 1.96s
                      Time elapsed: 00:16:15
                               ETA: 00:54:55

################################################################################
                     [1m Learning iteration 457/2000 [0m                      

                       Computation: 49175 steps/s (collection: 1.904s, learning 0.095s)
             Mean action noise std: 2.53
          Mean value_function loss: 9.2898
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 72.5631
                       Mean reward: 15.11
               Mean episode length: 208.26
    Episode_Reward/reaching_object: 1.0543
     Episode_Reward/lifting_object: 2.2908
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 2.00s
                      Time elapsed: 00:16:17
                               ETA: 00:54:52

################################################################################
                     [1m Learning iteration 458/2000 [0m                      

                       Computation: 49456 steps/s (collection: 1.895s, learning 0.093s)
             Mean action noise std: 2.53
          Mean value_function loss: 12.9451
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 72.5904
                       Mean reward: 15.00
               Mean episode length: 208.21
    Episode_Reward/reaching_object: 1.0063
     Episode_Reward/lifting_object: 2.3577
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 1.99s
                      Time elapsed: 00:16:19
                               ETA: 00:54:49

################################################################################
                     [1m Learning iteration 459/2000 [0m                      

                       Computation: 49368 steps/s (collection: 1.898s, learning 0.094s)
             Mean action noise std: 2.53
          Mean value_function loss: 16.5315
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 72.6224
                       Mean reward: 20.21
               Mean episode length: 212.46
    Episode_Reward/reaching_object: 1.0673
     Episode_Reward/lifting_object: 2.7994
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 1.99s
                      Time elapsed: 00:16:21
                               ETA: 00:54:47

################################################################################
                     [1m Learning iteration 460/2000 [0m                      

                       Computation: 49219 steps/s (collection: 1.900s, learning 0.097s)
             Mean action noise std: 2.54
          Mean value_function loss: 15.2162
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.6558
                       Mean reward: 16.51
               Mean episode length: 198.86
    Episode_Reward/reaching_object: 1.0088
     Episode_Reward/lifting_object: 2.7407
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 2.00s
                      Time elapsed: 00:16:23
                               ETA: 00:54:44

################################################################################
                     [1m Learning iteration 461/2000 [0m                      

                       Computation: 48115 steps/s (collection: 1.953s, learning 0.090s)
             Mean action noise std: 2.54
          Mean value_function loss: 11.7114
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 72.6978
                       Mean reward: 19.43
               Mean episode length: 206.01
    Episode_Reward/reaching_object: 1.0528
     Episode_Reward/lifting_object: 2.5167
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 2.04s
                      Time elapsed: 00:16:25
                               ETA: 00:54:42

################################################################################
                     [1m Learning iteration 462/2000 [0m                      

                       Computation: 46548 steps/s (collection: 2.016s, learning 0.096s)
             Mean action noise std: 2.54
          Mean value_function loss: 9.3656
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 72.7214
                       Mean reward: 18.48
               Mean episode length: 205.58
    Episode_Reward/reaching_object: 0.9947
     Episode_Reward/lifting_object: 2.4929
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 2.11s
                      Time elapsed: 00:16:27
                               ETA: 00:54:39

################################################################################
                     [1m Learning iteration 463/2000 [0m                      

                       Computation: 48096 steps/s (collection: 1.937s, learning 0.107s)
             Mean action noise std: 2.54
          Mean value_function loss: 9.6382
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 72.7301
                       Mean reward: 16.51
               Mean episode length: 199.34
    Episode_Reward/reaching_object: 0.9461
     Episode_Reward/lifting_object: 2.4778
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 2.04s
                      Time elapsed: 00:16:29
                               ETA: 00:54:37

################################################################################
                     [1m Learning iteration 464/2000 [0m                      

                       Computation: 48525 steps/s (collection: 1.929s, learning 0.097s)
             Mean action noise std: 2.54
          Mean value_function loss: 8.4177
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 72.7483
                       Mean reward: 24.66
               Mean episode length: 208.50
    Episode_Reward/reaching_object: 0.9939
     Episode_Reward/lifting_object: 2.9388
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 2.03s
                      Time elapsed: 00:16:31
                               ETA: 00:54:35

################################################################################
                     [1m Learning iteration 465/2000 [0m                      

                       Computation: 48213 steps/s (collection: 1.951s, learning 0.088s)
             Mean action noise std: 2.55
          Mean value_function loss: 11.5503
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 72.7784
                       Mean reward: 17.71
               Mean episode length: 193.12
    Episode_Reward/reaching_object: 0.9371
     Episode_Reward/lifting_object: 2.6292
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 2.04s
                      Time elapsed: 00:16:33
                               ETA: 00:54:32

################################################################################
                     [1m Learning iteration 466/2000 [0m                      

                       Computation: 48344 steps/s (collection: 1.926s, learning 0.107s)
             Mean action noise std: 2.55
          Mean value_function loss: 11.9193
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 72.8024
                       Mean reward: 21.27
               Mean episode length: 199.53
    Episode_Reward/reaching_object: 0.9903
     Episode_Reward/lifting_object: 3.2046
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 2.03s
                      Time elapsed: 00:16:35
                               ETA: 00:54:30

################################################################################
                     [1m Learning iteration 467/2000 [0m                      

                       Computation: 45796 steps/s (collection: 2.030s, learning 0.117s)
             Mean action noise std: 2.55
          Mean value_function loss: 10.2760
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 72.8190
                       Mean reward: 19.59
               Mean episode length: 202.98
    Episode_Reward/reaching_object: 1.0074
     Episode_Reward/lifting_object: 2.6522
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 2.15s
                      Time elapsed: 00:16:37
                               ETA: 00:54:28

################################################################################
                     [1m Learning iteration 468/2000 [0m                      

                       Computation: 45379 steps/s (collection: 1.980s, learning 0.187s)
             Mean action noise std: 2.55
          Mean value_function loss: 13.8190
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 72.8418
                       Mean reward: 23.92
               Mean episode length: 212.77
    Episode_Reward/reaching_object: 1.0149
     Episode_Reward/lifting_object: 3.3974
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 2.17s
                      Time elapsed: 00:16:39
                               ETA: 00:54:26

################################################################################
                     [1m Learning iteration 469/2000 [0m                      

                       Computation: 45852 steps/s (collection: 2.037s, learning 0.107s)
             Mean action noise std: 2.55
          Mean value_function loss: 13.2088
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 72.8725
                       Mean reward: 20.62
               Mean episode length: 199.86
    Episode_Reward/reaching_object: 0.9968
     Episode_Reward/lifting_object: 2.7743
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 2.14s
                      Time elapsed: 00:16:42
                               ETA: 00:54:23

################################################################################
                     [1m Learning iteration 470/2000 [0m                      

                       Computation: 44389 steps/s (collection: 2.119s, learning 0.096s)
             Mean action noise std: 2.55
          Mean value_function loss: 12.6788
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 72.8938
                       Mean reward: 19.76
               Mean episode length: 210.07
    Episode_Reward/reaching_object: 1.0086
     Episode_Reward/lifting_object: 3.2166
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 2.21s
                      Time elapsed: 00:16:44
                               ETA: 00:54:22

################################################################################
                     [1m Learning iteration 471/2000 [0m                      

                       Computation: 48345 steps/s (collection: 1.944s, learning 0.090s)
             Mean action noise std: 2.56
          Mean value_function loss: 15.3214
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 72.9053
                       Mean reward: 19.66
               Mean episode length: 205.32
    Episode_Reward/reaching_object: 1.0090
     Episode_Reward/lifting_object: 3.0584
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 2.03s
                      Time elapsed: 00:16:46
                               ETA: 00:54:19

################################################################################
                     [1m Learning iteration 472/2000 [0m                      

                       Computation: 47642 steps/s (collection: 1.966s, learning 0.098s)
             Mean action noise std: 2.56
          Mean value_function loss: 10.7596
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 72.9140
                       Mean reward: 26.52
               Mean episode length: 205.54
    Episode_Reward/reaching_object: 1.0058
     Episode_Reward/lifting_object: 3.8251
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 2.06s
                      Time elapsed: 00:16:48
                               ETA: 00:54:17

################################################################################
                     [1m Learning iteration 473/2000 [0m                      

                       Computation: 48964 steps/s (collection: 1.918s, learning 0.090s)
             Mean action noise std: 2.56
          Mean value_function loss: 12.3786
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 72.9276
                       Mean reward: 21.07
               Mean episode length: 200.67
    Episode_Reward/reaching_object: 1.0215
     Episode_Reward/lifting_object: 3.4946
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 2.01s
                      Time elapsed: 00:16:50
                               ETA: 00:54:14

################################################################################
                     [1m Learning iteration 474/2000 [0m                      

                       Computation: 49009 steps/s (collection: 1.913s, learning 0.093s)
             Mean action noise std: 2.56
          Mean value_function loss: 11.7454
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 72.9506
                       Mean reward: 24.49
               Mean episode length: 189.11
    Episode_Reward/reaching_object: 0.9881
     Episode_Reward/lifting_object: 3.1645
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 2.01s
                      Time elapsed: 00:16:52
                               ETA: 00:54:12

################################################################################
                     [1m Learning iteration 475/2000 [0m                      

                       Computation: 48371 steps/s (collection: 1.937s, learning 0.096s)
             Mean action noise std: 2.56
          Mean value_function loss: 15.3744
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 72.9718
                       Mean reward: 22.13
               Mean episode length: 193.18
    Episode_Reward/reaching_object: 0.9504
     Episode_Reward/lifting_object: 3.1535
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 2.03s
                      Time elapsed: 00:16:54
                               ETA: 00:54:09

################################################################################
                     [1m Learning iteration 476/2000 [0m                      

                       Computation: 46132 steps/s (collection: 2.039s, learning 0.092s)
             Mean action noise std: 2.56
          Mean value_function loss: 14.8912
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 72.9947
                       Mean reward: 25.07
               Mean episode length: 208.88
    Episode_Reward/reaching_object: 0.9468
     Episode_Reward/lifting_object: 3.1245
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 2.13s
                      Time elapsed: 00:16:56
                               ETA: 00:54:07

################################################################################
                     [1m Learning iteration 477/2000 [0m                      

                       Computation: 43854 steps/s (collection: 2.042s, learning 0.199s)
             Mean action noise std: 2.56
          Mean value_function loss: 15.5279
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 73.0073
                       Mean reward: 19.74
               Mean episode length: 202.28
    Episode_Reward/reaching_object: 1.0085
     Episode_Reward/lifting_object: 3.5286
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 2.24s
                      Time elapsed: 00:16:58
                               ETA: 00:54:05

################################################################################
                     [1m Learning iteration 478/2000 [0m                      

                       Computation: 46062 steps/s (collection: 2.030s, learning 0.105s)
             Mean action noise std: 2.57
          Mean value_function loss: 13.0311
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 73.0225
                       Mean reward: 22.90
               Mean episode length: 202.28
    Episode_Reward/reaching_object: 0.9913
     Episode_Reward/lifting_object: 3.5341
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 2.13s
                      Time elapsed: 00:17:00
                               ETA: 00:54:03

################################################################################
                     [1m Learning iteration 479/2000 [0m                      

                       Computation: 45212 steps/s (collection: 1.964s, learning 0.210s)
             Mean action noise std: 2.57
          Mean value_function loss: 17.4255
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 73.0500
                       Mean reward: 25.49
               Mean episode length: 193.71
    Episode_Reward/reaching_object: 0.9881
     Episode_Reward/lifting_object: 3.4325
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 2.17s
                      Time elapsed: 00:17:03
                               ETA: 00:54:01

################################################################################
                     [1m Learning iteration 480/2000 [0m                      

                       Computation: 45860 steps/s (collection: 2.039s, learning 0.105s)
             Mean action noise std: 2.57
          Mean value_function loss: 16.9882
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.0805
                       Mean reward: 24.89
               Mean episode length: 197.80
    Episode_Reward/reaching_object: 1.0032
     Episode_Reward/lifting_object: 3.7583
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 2.14s
                      Time elapsed: 00:17:05
                               ETA: 00:53:59

################################################################################
                     [1m Learning iteration 481/2000 [0m                      

                       Computation: 48065 steps/s (collection: 1.952s, learning 0.094s)
             Mean action noise std: 2.57
          Mean value_function loss: 17.9015
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 73.1024
                       Mean reward: 20.03
               Mean episode length: 193.02
    Episode_Reward/reaching_object: 1.0003
     Episode_Reward/lifting_object: 3.8382
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 2.05s
                      Time elapsed: 00:17:07
                               ETA: 00:53:57

################################################################################
                     [1m Learning iteration 482/2000 [0m                      

                       Computation: 44223 steps/s (collection: 2.075s, learning 0.148s)
             Mean action noise std: 2.57
          Mean value_function loss: 14.9176
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 73.1188
                       Mean reward: 24.65
               Mean episode length: 211.79
    Episode_Reward/reaching_object: 1.0446
     Episode_Reward/lifting_object: 4.0223
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 2.22s
                      Time elapsed: 00:17:09
                               ETA: 00:53:55

################################################################################
                     [1m Learning iteration 483/2000 [0m                      

                       Computation: 47313 steps/s (collection: 1.961s, learning 0.117s)
             Mean action noise std: 2.58
          Mean value_function loss: 11.6680
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 73.1347
                       Mean reward: 22.26
               Mean episode length: 206.75
    Episode_Reward/reaching_object: 1.0496
     Episode_Reward/lifting_object: 4.1111
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 2.08s
                      Time elapsed: 00:17:11
                               ETA: 00:53:53

################################################################################
                     [1m Learning iteration 484/2000 [0m                      

                       Computation: 43831 steps/s (collection: 2.062s, learning 0.180s)
             Mean action noise std: 2.58
          Mean value_function loss: 16.8888
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 73.1517
                       Mean reward: 29.05
               Mean episode length: 224.13
    Episode_Reward/reaching_object: 1.0773
     Episode_Reward/lifting_object: 4.6138
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 2.24s
                      Time elapsed: 00:17:13
                               ETA: 00:53:51

################################################################################
                     [1m Learning iteration 485/2000 [0m                      

                       Computation: 43700 steps/s (collection: 2.084s, learning 0.165s)
             Mean action noise std: 2.58
          Mean value_function loss: 15.3398
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 73.1703
                       Mean reward: 22.83
               Mean episode length: 213.09
    Episode_Reward/reaching_object: 1.0634
     Episode_Reward/lifting_object: 4.2613
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 2.25s
                      Time elapsed: 00:17:16
                               ETA: 00:53:49

################################################################################
                     [1m Learning iteration 486/2000 [0m                      

                       Computation: 47224 steps/s (collection: 1.942s, learning 0.140s)
             Mean action noise std: 2.58
          Mean value_function loss: 14.7259
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 73.1950
                       Mean reward: 29.15
               Mean episode length: 214.43
    Episode_Reward/reaching_object: 1.1112
     Episode_Reward/lifting_object: 4.9656
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 2.08s
                      Time elapsed: 00:17:18
                               ETA: 00:53:47

################################################################################
                     [1m Learning iteration 487/2000 [0m                      

                       Computation: 47524 steps/s (collection: 1.968s, learning 0.101s)
             Mean action noise std: 2.58
          Mean value_function loss: 23.9373
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.2190
                       Mean reward: 29.61
               Mean episode length: 209.47
    Episode_Reward/reaching_object: 1.0625
     Episode_Reward/lifting_object: 4.8943
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 2.07s
                      Time elapsed: 00:17:20
                               ETA: 00:53:44

################################################################################
                     [1m Learning iteration 488/2000 [0m                      

                       Computation: 46393 steps/s (collection: 1.994s, learning 0.125s)
             Mean action noise std: 2.58
          Mean value_function loss: 18.4075
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 73.2420
                       Mean reward: 31.93
               Mean episode length: 221.31
    Episode_Reward/reaching_object: 1.0871
     Episode_Reward/lifting_object: 4.5182
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 2.12s
                      Time elapsed: 00:17:22
                               ETA: 00:53:42

################################################################################
                     [1m Learning iteration 489/2000 [0m                      

                       Computation: 45564 steps/s (collection: 2.021s, learning 0.136s)
             Mean action noise std: 2.59
          Mean value_function loss: 19.8967
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 73.2533
                       Mean reward: 27.35
               Mean episode length: 198.58
    Episode_Reward/reaching_object: 1.0539
     Episode_Reward/lifting_object: 4.8697
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 2.16s
                      Time elapsed: 00:17:24
                               ETA: 00:53:40

################################################################################
                     [1m Learning iteration 490/2000 [0m                      

                       Computation: 40812 steps/s (collection: 2.307s, learning 0.102s)
             Mean action noise std: 2.59
          Mean value_function loss: 18.4772
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.2681
                       Mean reward: 35.51
               Mean episode length: 207.62
    Episode_Reward/reaching_object: 1.0462
     Episode_Reward/lifting_object: 5.1148
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 2.41s
                      Time elapsed: 00:17:26
                               ETA: 00:53:39

################################################################################
                     [1m Learning iteration 491/2000 [0m                      

                       Computation: 41301 steps/s (collection: 2.179s, learning 0.202s)
             Mean action noise std: 2.59
          Mean value_function loss: 20.6193
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 73.2816
                       Mean reward: 27.89
               Mean episode length: 197.15
    Episode_Reward/reaching_object: 1.0364
     Episode_Reward/lifting_object: 4.8704
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 2.38s
                      Time elapsed: 00:17:29
                               ETA: 00:53:38

################################################################################
                     [1m Learning iteration 492/2000 [0m                      

                       Computation: 45917 steps/s (collection: 2.039s, learning 0.102s)
             Mean action noise std: 2.59
          Mean value_function loss: 18.4181
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 73.2883
                       Mean reward: 36.45
               Mean episode length: 191.80
    Episode_Reward/reaching_object: 1.0205
     Episode_Reward/lifting_object: 5.0769
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 2.14s
                      Time elapsed: 00:17:31
                               ETA: 00:53:35

################################################################################
                     [1m Learning iteration 493/2000 [0m                      

                       Computation: 45679 steps/s (collection: 2.046s, learning 0.106s)
             Mean action noise std: 2.59
          Mean value_function loss: 22.8304
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 73.2924
                       Mean reward: 24.01
               Mean episode length: 177.02
    Episode_Reward/reaching_object: 0.9840
     Episode_Reward/lifting_object: 5.0590
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 2.15s
                      Time elapsed: 00:17:33
                               ETA: 00:53:33

################################################################################
                     [1m Learning iteration 494/2000 [0m                      

                       Computation: 46973 steps/s (collection: 1.999s, learning 0.094s)
             Mean action noise std: 2.59
          Mean value_function loss: 25.3306
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 73.2939
                       Mean reward: 27.05
               Mean episode length: 174.70
    Episode_Reward/reaching_object: 1.0065
     Episode_Reward/lifting_object: 5.6500
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 2.09s
                      Time elapsed: 00:17:35
                               ETA: 00:53:31

################################################################################
                     [1m Learning iteration 495/2000 [0m                      

                       Computation: 45387 steps/s (collection: 2.020s, learning 0.146s)
             Mean action noise std: 2.59
          Mean value_function loss: 22.5502
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 73.2946
                       Mean reward: 29.92
               Mean episode length: 193.29
    Episode_Reward/reaching_object: 0.9703
     Episode_Reward/lifting_object: 4.8684
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 2.17s
                      Time elapsed: 00:17:37
                               ETA: 00:53:29

################################################################################
                     [1m Learning iteration 496/2000 [0m                      

                       Computation: 46046 steps/s (collection: 2.004s, learning 0.131s)
             Mean action noise std: 2.59
          Mean value_function loss: 19.7251
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 73.2950
                       Mean reward: 32.54
               Mean episode length: 190.49
    Episode_Reward/reaching_object: 1.0036
     Episode_Reward/lifting_object: 5.5997
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 2.13s
                      Time elapsed: 00:17:39
                               ETA: 00:53:27

################################################################################
                     [1m Learning iteration 497/2000 [0m                      

                       Computation: 46998 steps/s (collection: 1.995s, learning 0.097s)
             Mean action noise std: 2.59
          Mean value_function loss: 25.0457
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 73.2956
                       Mean reward: 32.53
               Mean episode length: 187.98
    Episode_Reward/reaching_object: 1.0004
     Episode_Reward/lifting_object: 5.5648
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 2.09s
                      Time elapsed: 00:17:42
                               ETA: 00:53:25

################################################################################
                     [1m Learning iteration 498/2000 [0m                      

                       Computation: 45458 steps/s (collection: 2.040s, learning 0.122s)
             Mean action noise std: 2.59
          Mean value_function loss: 19.8163
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 73.2962
                       Mean reward: 34.86
               Mean episode length: 188.66
    Episode_Reward/reaching_object: 1.0566
     Episode_Reward/lifting_object: 6.3357
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 2.16s
                      Time elapsed: 00:17:44
                               ETA: 00:53:23

################################################################################
                     [1m Learning iteration 499/2000 [0m                      

                       Computation: 45350 steps/s (collection: 2.077s, learning 0.091s)
             Mean action noise std: 2.59
          Mean value_function loss: 19.8355
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 73.3012
                       Mean reward: 36.10
               Mean episode length: 215.74
    Episode_Reward/reaching_object: 1.0779
     Episode_Reward/lifting_object: 5.9516
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 2.17s
                      Time elapsed: 00:17:46
                               ETA: 00:53:21

################################################################################
                     [1m Learning iteration 500/2000 [0m                      

                       Computation: 46664 steps/s (collection: 2.014s, learning 0.093s)
             Mean action noise std: 2.59
          Mean value_function loss: 21.5146
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 73.3113
                       Mean reward: 35.81
               Mean episode length: 205.67
    Episode_Reward/reaching_object: 1.0904
     Episode_Reward/lifting_object: 6.0366
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 2.11s
                      Time elapsed: 00:17:48
                               ETA: 00:53:18

################################################################################
                     [1m Learning iteration 501/2000 [0m                      

                       Computation: 46640 steps/s (collection: 2.008s, learning 0.100s)
             Mean action noise std: 2.59
          Mean value_function loss: 25.0780
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 73.3262
                       Mean reward: 41.54
               Mean episode length: 212.71
    Episode_Reward/reaching_object: 1.1120
     Episode_Reward/lifting_object: 6.4737
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 2.11s
                      Time elapsed: 00:17:50
                               ETA: 00:53:16

################################################################################
                     [1m Learning iteration 502/2000 [0m                      

                       Computation: 47515 steps/s (collection: 1.965s, learning 0.104s)
             Mean action noise std: 2.59
          Mean value_function loss: 23.3201
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 73.3446
                       Mean reward: 41.08
               Mean episode length: 208.03
    Episode_Reward/reaching_object: 1.0981
     Episode_Reward/lifting_object: 6.9162
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 2.07s
                      Time elapsed: 00:17:52
                               ETA: 00:53:14

################################################################################
                     [1m Learning iteration 503/2000 [0m                      

                       Computation: 44651 steps/s (collection: 2.043s, learning 0.159s)
             Mean action noise std: 2.59
          Mean value_function loss: 22.8780
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 73.3598
                       Mean reward: 39.72
               Mean episode length: 199.28
    Episode_Reward/reaching_object: 1.1117
     Episode_Reward/lifting_object: 6.4460
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 2.20s
                      Time elapsed: 00:17:54
                               ETA: 00:53:12

################################################################################
                     [1m Learning iteration 504/2000 [0m                      

                       Computation: 46059 steps/s (collection: 2.038s, learning 0.096s)
             Mean action noise std: 2.59
          Mean value_function loss: 19.6196
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 73.3652
                       Mean reward: 34.54
               Mean episode length: 211.88
    Episode_Reward/reaching_object: 1.0986
     Episode_Reward/lifting_object: 6.1835
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 2.13s
                      Time elapsed: 00:17:56
                               ETA: 00:53:10

################################################################################
                     [1m Learning iteration 505/2000 [0m                      

                       Computation: 47330 steps/s (collection: 1.976s, learning 0.101s)
             Mean action noise std: 2.60
          Mean value_function loss: 24.0638
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 73.3743
                       Mean reward: 38.21
               Mean episode length: 217.09
    Episode_Reward/reaching_object: 1.1247
     Episode_Reward/lifting_object: 7.0547
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 2.08s
                      Time elapsed: 00:17:59
                               ETA: 00:53:08

################################################################################
                     [1m Learning iteration 506/2000 [0m                      

                       Computation: 45956 steps/s (collection: 2.030s, learning 0.109s)
             Mean action noise std: 2.60
          Mean value_function loss: 22.0142
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 73.3855
                       Mean reward: 46.65
               Mean episode length: 218.71
    Episode_Reward/reaching_object: 1.0826
     Episode_Reward/lifting_object: 6.8208
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 2.14s
                      Time elapsed: 00:18:01
                               ETA: 00:53:05

################################################################################
                     [1m Learning iteration 507/2000 [0m                      

                       Computation: 46298 steps/s (collection: 2.008s, learning 0.115s)
             Mean action noise std: 2.60
          Mean value_function loss: 24.4035
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 73.3923
                       Mean reward: 41.57
               Mean episode length: 212.50
    Episode_Reward/reaching_object: 1.0727
     Episode_Reward/lifting_object: 6.8548
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 2.12s
                      Time elapsed: 00:18:03
                               ETA: 00:53:03

################################################################################
                     [1m Learning iteration 508/2000 [0m                      

                       Computation: 46325 steps/s (collection: 2.015s, learning 0.107s)
             Mean action noise std: 2.60
          Mean value_function loss: 28.2071
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 73.3979
                       Mean reward: 33.21
               Mean episode length: 207.00
    Episode_Reward/reaching_object: 1.0606
     Episode_Reward/lifting_object: 6.1012
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 2.12s
                      Time elapsed: 00:18:05
                               ETA: 00:53:01

################################################################################
                     [1m Learning iteration 509/2000 [0m                      

                       Computation: 46734 steps/s (collection: 2.007s, learning 0.097s)
             Mean action noise std: 2.60
          Mean value_function loss: 24.0616
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 73.4033
                       Mean reward: 36.34
               Mean episode length: 193.02
    Episode_Reward/reaching_object: 1.0612
     Episode_Reward/lifting_object: 6.8673
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 2.10s
                      Time elapsed: 00:18:07
                               ETA: 00:52:59

################################################################################
                     [1m Learning iteration 510/2000 [0m                      

                       Computation: 46268 steps/s (collection: 2.029s, learning 0.096s)
             Mean action noise std: 2.60
          Mean value_function loss: 27.4017
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 73.4142
                       Mean reward: 34.90
               Mean episode length: 187.98
    Episode_Reward/reaching_object: 1.0095
     Episode_Reward/lifting_object: 6.6323
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 2.12s
                      Time elapsed: 00:18:09
                               ETA: 00:52:57

################################################################################
                     [1m Learning iteration 511/2000 [0m                      

                       Computation: 47494 steps/s (collection: 1.968s, learning 0.102s)
             Mean action noise std: 2.60
          Mean value_function loss: 33.7268
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 73.4283
                       Mean reward: 48.82
               Mean episode length: 191.69
    Episode_Reward/reaching_object: 1.0790
     Episode_Reward/lifting_object: 8.0158
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 2.07s
                      Time elapsed: 00:18:11
                               ETA: 00:52:54

################################################################################
                     [1m Learning iteration 512/2000 [0m                      

                       Computation: 46860 steps/s (collection: 1.983s, learning 0.115s)
             Mean action noise std: 2.60
          Mean value_function loss: 27.6605
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 73.4449
                       Mean reward: 41.15
               Mean episode length: 193.53
    Episode_Reward/reaching_object: 1.0621
     Episode_Reward/lifting_object: 6.9922
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 2.10s
                      Time elapsed: 00:18:13
                               ETA: 00:52:52

################################################################################
                     [1m Learning iteration 513/2000 [0m                      

                       Computation: 46306 steps/s (collection: 2.014s, learning 0.109s)
             Mean action noise std: 2.60
          Mean value_function loss: 28.1531
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 73.4569
                       Mean reward: 46.27
               Mean episode length: 215.85
    Episode_Reward/reaching_object: 1.0740
     Episode_Reward/lifting_object: 7.1827
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 2.12s
                      Time elapsed: 00:18:15
                               ETA: 00:52:50

################################################################################
                     [1m Learning iteration 514/2000 [0m                      

                       Computation: 47195 steps/s (collection: 1.982s, learning 0.101s)
             Mean action noise std: 2.60
          Mean value_function loss: 31.0092
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 73.4650
                       Mean reward: 46.77
               Mean episode length: 203.47
    Episode_Reward/reaching_object: 1.0974
     Episode_Reward/lifting_object: 8.0079
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0707
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 2.08s
                      Time elapsed: 00:18:18
                               ETA: 00:52:48

################################################################################
                     [1m Learning iteration 515/2000 [0m                      

                       Computation: 46863 steps/s (collection: 1.977s, learning 0.121s)
             Mean action noise std: 2.60
          Mean value_function loss: 28.1709
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 73.4745
                       Mean reward: 35.37
               Mean episode length: 180.91
    Episode_Reward/reaching_object: 1.0672
     Episode_Reward/lifting_object: 7.1108
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0701
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 2.10s
                      Time elapsed: 00:18:20
                               ETA: 00:52:46

################################################################################
                     [1m Learning iteration 516/2000 [0m                      

                       Computation: 45746 steps/s (collection: 2.003s, learning 0.146s)
             Mean action noise std: 2.61
          Mean value_function loss: 33.5351
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 73.4852
                       Mean reward: 49.29
               Mean episode length: 203.72
    Episode_Reward/reaching_object: 1.0853
     Episode_Reward/lifting_object: 7.9643
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0709
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 2.15s
                      Time elapsed: 00:18:22
                               ETA: 00:52:43

################################################################################
                     [1m Learning iteration 517/2000 [0m                      

                       Computation: 45868 steps/s (collection: 2.034s, learning 0.109s)
             Mean action noise std: 2.61
          Mean value_function loss: 35.7862
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 73.4900
                       Mean reward: 43.09
               Mean episode length: 211.46
    Episode_Reward/reaching_object: 1.0913
     Episode_Reward/lifting_object: 7.9936
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 2.14s
                      Time elapsed: 00:18:24
                               ETA: 00:52:41

################################################################################
                     [1m Learning iteration 518/2000 [0m                      

                       Computation: 46175 steps/s (collection: 1.998s, learning 0.131s)
             Mean action noise std: 2.61
          Mean value_function loss: 33.9291
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 73.4941
                       Mean reward: 46.61
               Mean episode length: 198.78
    Episode_Reward/reaching_object: 1.0762
     Episode_Reward/lifting_object: 8.1803
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0705
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 2.13s
                      Time elapsed: 00:18:26
                               ETA: 00:52:39

################################################################################
                     [1m Learning iteration 519/2000 [0m                      

                       Computation: 45168 steps/s (collection: 2.058s, learning 0.119s)
             Mean action noise std: 2.61
          Mean value_function loss: 34.9035
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 73.5052
                       Mean reward: 41.06
               Mean episode length: 197.21
    Episode_Reward/reaching_object: 1.0611
     Episode_Reward/lifting_object: 7.8336
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0707
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 2.18s
                      Time elapsed: 00:18:28
                               ETA: 00:52:37

################################################################################
                     [1m Learning iteration 520/2000 [0m                      

                       Computation: 45730 steps/s (collection: 2.043s, learning 0.107s)
             Mean action noise std: 2.61
          Mean value_function loss: 32.0744
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 73.5151
                       Mean reward: 49.62
               Mean episode length: 206.13
    Episode_Reward/reaching_object: 1.0683
     Episode_Reward/lifting_object: 7.5119
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 2.15s
                      Time elapsed: 00:18:30
                               ETA: 00:52:35

################################################################################
                     [1m Learning iteration 521/2000 [0m                      

                       Computation: 46265 steps/s (collection: 2.023s, learning 0.102s)
             Mean action noise std: 2.61
          Mean value_function loss: 31.4736
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 73.5189
                       Mean reward: 44.27
               Mean episode length: 190.83
    Episode_Reward/reaching_object: 1.0530
     Episode_Reward/lifting_object: 8.1863
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 2.12s
                      Time elapsed: 00:18:33
                               ETA: 00:52:33

################################################################################
                     [1m Learning iteration 522/2000 [0m                      

                       Computation: 45722 steps/s (collection: 2.023s, learning 0.127s)
             Mean action noise std: 2.61
          Mean value_function loss: 33.3195
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 73.5218
                       Mean reward: 50.11
               Mean episode length: 197.33
    Episode_Reward/reaching_object: 1.0709
     Episode_Reward/lifting_object: 8.9567
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 2.15s
                      Time elapsed: 00:18:35
                               ETA: 00:52:31

################################################################################
                     [1m Learning iteration 523/2000 [0m                      

                       Computation: 44576 steps/s (collection: 2.046s, learning 0.160s)
             Mean action noise std: 2.61
          Mean value_function loss: 28.0442
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 73.5242
                       Mean reward: 57.71
               Mean episode length: 205.27
    Episode_Reward/reaching_object: 1.0997
     Episode_Reward/lifting_object: 9.1001
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0723
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 2.21s
                      Time elapsed: 00:18:37
                               ETA: 00:52:29

################################################################################
                     [1m Learning iteration 524/2000 [0m                      

                       Computation: 44464 steps/s (collection: 2.076s, learning 0.135s)
             Mean action noise std: 2.61
          Mean value_function loss: 33.8713
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 73.5261
                       Mean reward: 48.69
               Mean episode length: 190.88
    Episode_Reward/reaching_object: 1.1173
     Episode_Reward/lifting_object: 9.0755
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 2.21s
                      Time elapsed: 00:18:39
                               ETA: 00:52:27

################################################################################
                     [1m Learning iteration 525/2000 [0m                      

                       Computation: 45724 steps/s (collection: 2.055s, learning 0.095s)
             Mean action noise std: 2.61
          Mean value_function loss: 41.9113
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 73.5279
                       Mean reward: 56.19
               Mean episode length: 194.87
    Episode_Reward/reaching_object: 1.0732
     Episode_Reward/lifting_object: 9.2480
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 2.15s
                      Time elapsed: 00:18:41
                               ETA: 00:52:25

################################################################################
                     [1m Learning iteration 526/2000 [0m                      

                       Computation: 44899 steps/s (collection: 2.021s, learning 0.169s)
             Mean action noise std: 2.61
          Mean value_function loss: 45.0844
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 73.5352
                       Mean reward: 51.28
               Mean episode length: 199.12
    Episode_Reward/reaching_object: 1.0631
     Episode_Reward/lifting_object: 8.8076
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 2.19s
                      Time elapsed: 00:18:43
                               ETA: 00:52:23

################################################################################
                     [1m Learning iteration 527/2000 [0m                      

                       Computation: 45053 steps/s (collection: 2.075s, learning 0.107s)
             Mean action noise std: 2.61
          Mean value_function loss: 46.4246
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.5451
                       Mean reward: 49.04
               Mean episode length: 179.02
    Episode_Reward/reaching_object: 1.0477
     Episode_Reward/lifting_object: 8.6586
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 2.18s
                      Time elapsed: 00:18:46
                               ETA: 00:52:21

################################################################################
                     [1m Learning iteration 528/2000 [0m                      

                       Computation: 43774 steps/s (collection: 2.058s, learning 0.188s)
             Mean action noise std: 2.61
          Mean value_function loss: 37.9624
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 73.5533
                       Mean reward: 50.21
               Mean episode length: 193.54
    Episode_Reward/reaching_object: 1.0455
     Episode_Reward/lifting_object: 9.1933
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0714
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 2.25s
                      Time elapsed: 00:18:48
                               ETA: 00:52:19

################################################################################
                     [1m Learning iteration 529/2000 [0m                      

                       Computation: 45645 steps/s (collection: 2.060s, learning 0.094s)
             Mean action noise std: 2.61
          Mean value_function loss: 32.2847
               Mean surrogate loss: 0.0110
                 Mean entropy loss: 73.5573
                       Mean reward: 52.65
               Mean episode length: 196.28
    Episode_Reward/reaching_object: 1.0630
     Episode_Reward/lifting_object: 8.6133
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0730
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 2.15s
                      Time elapsed: 00:18:50
                               ETA: 00:52:17

################################################################################
                     [1m Learning iteration 530/2000 [0m                      

                       Computation: 45115 steps/s (collection: 2.085s, learning 0.094s)
             Mean action noise std: 2.61
          Mean value_function loss: 34.6721
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 73.5579
                       Mean reward: 45.23
               Mean episode length: 184.08
    Episode_Reward/reaching_object: 1.0121
     Episode_Reward/lifting_object: 8.5506
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0710
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 2.18s
                      Time elapsed: 00:18:52
                               ETA: 00:52:15

################################################################################
                     [1m Learning iteration 531/2000 [0m                      

                       Computation: 44193 steps/s (collection: 2.122s, learning 0.102s)
             Mean action noise std: 2.61
          Mean value_function loss: 38.0505
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 73.5581
                       Mean reward: 49.07
               Mean episode length: 186.77
    Episode_Reward/reaching_object: 1.0465
     Episode_Reward/lifting_object: 9.5818
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0732
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 2.22s
                      Time elapsed: 00:18:54
                               ETA: 00:52:13

################################################################################
                     [1m Learning iteration 532/2000 [0m                      

                       Computation: 45429 steps/s (collection: 2.050s, learning 0.114s)
             Mean action noise std: 2.61
          Mean value_function loss: 39.5603
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 73.5588
                       Mean reward: 48.75
               Mean episode length: 194.02
    Episode_Reward/reaching_object: 1.0244
     Episode_Reward/lifting_object: 8.7683
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0726
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 2.16s
                      Time elapsed: 00:18:57
                               ETA: 00:52:11

################################################################################
                     [1m Learning iteration 533/2000 [0m                      

                       Computation: 43596 steps/s (collection: 2.114s, learning 0.141s)
             Mean action noise std: 2.61
          Mean value_function loss: 48.3098
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 73.5604
                       Mean reward: 43.54
               Mean episode length: 181.56
    Episode_Reward/reaching_object: 1.0240
     Episode_Reward/lifting_object: 8.7305
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0732
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 2.25s
                      Time elapsed: 00:18:59
                               ETA: 00:52:09

################################################################################
                     [1m Learning iteration 534/2000 [0m                      

                       Computation: 44466 steps/s (collection: 2.087s, learning 0.124s)
             Mean action noise std: 2.61
          Mean value_function loss: 47.0397
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.5661
                       Mean reward: 44.74
               Mean episode length: 168.40
    Episode_Reward/reaching_object: 0.9740
     Episode_Reward/lifting_object: 8.7577
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 2.21s
                      Time elapsed: 00:19:01
                               ETA: 00:52:07

################################################################################
                     [1m Learning iteration 535/2000 [0m                      

                       Computation: 44510 steps/s (collection: 2.045s, learning 0.164s)
             Mean action noise std: 2.61
          Mean value_function loss: 48.2097
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 73.5748
                       Mean reward: 50.09
               Mean episode length: 183.26
    Episode_Reward/reaching_object: 1.0224
     Episode_Reward/lifting_object: 9.1633
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 15.0833
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 2.21s
                      Time elapsed: 00:19:03
                               ETA: 00:52:06

################################################################################
                     [1m Learning iteration 536/2000 [0m                      

                       Computation: 44814 steps/s (collection: 2.094s, learning 0.100s)
             Mean action noise std: 2.61
          Mean value_function loss: 43.7991
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 73.5823
                       Mean reward: 50.78
               Mean episode length: 179.27
    Episode_Reward/reaching_object: 0.9873
     Episode_Reward/lifting_object: 8.5747
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 2.19s
                      Time elapsed: 00:19:05
                               ETA: 00:52:04

################################################################################
                     [1m Learning iteration 537/2000 [0m                      

                       Computation: 45120 steps/s (collection: 2.080s, learning 0.099s)
             Mean action noise std: 2.61
          Mean value_function loss: 42.5807
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 73.5895
                       Mean reward: 47.44
               Mean episode length: 176.78
    Episode_Reward/reaching_object: 0.9789
     Episode_Reward/lifting_object: 8.6090
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0707
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 13.9583
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 2.18s
                      Time elapsed: 00:19:08
                               ETA: 00:52:02

################################################################################
                     [1m Learning iteration 538/2000 [0m                      

                       Computation: 45979 steps/s (collection: 2.039s, learning 0.099s)
             Mean action noise std: 2.61
          Mean value_function loss: 41.4601
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 73.5919
                       Mean reward: 55.95
               Mean episode length: 183.37
    Episode_Reward/reaching_object: 1.0451
     Episode_Reward/lifting_object: 9.3906
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0743
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 2.14s
                      Time elapsed: 00:19:10
                               ETA: 00:51:59

################################################################################
                     [1m Learning iteration 539/2000 [0m                      

                       Computation: 46269 steps/s (collection: 2.016s, learning 0.109s)
             Mean action noise std: 2.62
          Mean value_function loss: 42.3871
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 73.5937
                       Mean reward: 50.57
               Mean episode length: 179.50
    Episode_Reward/reaching_object: 0.9987
     Episode_Reward/lifting_object: 9.5774
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.9167
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 2.12s
                      Time elapsed: 00:19:12
                               ETA: 00:51:57

################################################################################
                     [1m Learning iteration 540/2000 [0m                      

                       Computation: 45688 steps/s (collection: 2.052s, learning 0.100s)
             Mean action noise std: 2.62
          Mean value_function loss: 31.9499
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 73.5964
                       Mean reward: 51.07
               Mean episode length: 179.63
    Episode_Reward/reaching_object: 1.0110
     Episode_Reward/lifting_object: 9.8662
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 14.3333
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 2.15s
                      Time elapsed: 00:19:14
                               ETA: 00:51:55

################################################################################
                     [1m Learning iteration 541/2000 [0m                      

                       Computation: 46018 steps/s (collection: 2.037s, learning 0.100s)
             Mean action noise std: 2.62
          Mean value_function loss: 35.5537
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 73.6025
                       Mean reward: 50.18
               Mean episode length: 172.63
    Episode_Reward/reaching_object: 0.9850
     Episode_Reward/lifting_object: 10.0004
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 2.14s
                      Time elapsed: 00:19:16
                               ETA: 00:51:53

################################################################################
                     [1m Learning iteration 542/2000 [0m                      

                       Computation: 44796 steps/s (collection: 2.065s, learning 0.129s)
             Mean action noise std: 2.62
          Mean value_function loss: 40.7072
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 73.6108
                       Mean reward: 64.50
               Mean episode length: 195.04
    Episode_Reward/reaching_object: 1.0254
     Episode_Reward/lifting_object: 10.1959
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 2.19s
                      Time elapsed: 00:19:18
                               ETA: 00:51:51

################################################################################
                     [1m Learning iteration 543/2000 [0m                      

                       Computation: 40471 steps/s (collection: 2.333s, learning 0.096s)
             Mean action noise std: 2.62
          Mean value_function loss: 46.9711
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 73.6210
                       Mean reward: 55.04
               Mean episode length: 162.31
    Episode_Reward/reaching_object: 1.0144
     Episode_Reward/lifting_object: 10.5527
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0702
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 2.43s
                      Time elapsed: 00:19:21
                               ETA: 00:51:50

################################################################################
                     [1m Learning iteration 544/2000 [0m                      

                       Computation: 45468 steps/s (collection: 2.067s, learning 0.095s)
             Mean action noise std: 2.62
          Mean value_function loss: 54.6364
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 73.6277
                       Mean reward: 54.41
               Mean episode length: 175.36
    Episode_Reward/reaching_object: 1.0330
     Episode_Reward/lifting_object: 10.7121
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0725
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 2.16s
                      Time elapsed: 00:19:23
                               ETA: 00:51:48

################################################################################
                     [1m Learning iteration 545/2000 [0m                      

                       Computation: 46299 steps/s (collection: 2.027s, learning 0.097s)
             Mean action noise std: 2.62
          Mean value_function loss: 50.4612
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 73.6376
                       Mean reward: 54.53
               Mean episode length: 178.14
    Episode_Reward/reaching_object: 1.0177
     Episode_Reward/lifting_object: 10.3755
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0710
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 14.5000
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 2.12s
                      Time elapsed: 00:19:25
                               ETA: 00:51:46

################################################################################
                     [1m Learning iteration 546/2000 [0m                      

                       Computation: 44768 steps/s (collection: 2.093s, learning 0.103s)
             Mean action noise std: 2.62
          Mean value_function loss: 54.7375
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 73.6516
                       Mean reward: 46.47
               Mean episode length: 181.20
    Episode_Reward/reaching_object: 1.0095
     Episode_Reward/lifting_object: 10.1183
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 2.20s
                      Time elapsed: 00:19:27
                               ETA: 00:51:44

################################################################################
                     [1m Learning iteration 547/2000 [0m                      

                       Computation: 45360 steps/s (collection: 2.054s, learning 0.114s)
             Mean action noise std: 2.62
          Mean value_function loss: 48.3554
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 73.6572
                       Mean reward: 60.37
               Mean episode length: 176.47
    Episode_Reward/reaching_object: 1.0327
     Episode_Reward/lifting_object: 11.1048
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 14.9583
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 2.17s
                      Time elapsed: 00:19:29
                               ETA: 00:51:42

################################################################################
                     [1m Learning iteration 548/2000 [0m                      

                       Computation: 45523 steps/s (collection: 2.038s, learning 0.121s)
             Mean action noise std: 2.62
          Mean value_function loss: 65.6676
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 73.6585
                       Mean reward: 61.85
               Mean episode length: 175.39
    Episode_Reward/reaching_object: 1.0190
     Episode_Reward/lifting_object: 10.8516
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 2.16s
                      Time elapsed: 00:19:32
                               ETA: 00:51:39

################################################################################
                     [1m Learning iteration 549/2000 [0m                      

                       Computation: 46270 steps/s (collection: 2.010s, learning 0.115s)
             Mean action noise std: 2.62
          Mean value_function loss: 56.3984
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 73.6596
                       Mean reward: 55.95
               Mean episode length: 183.61
    Episode_Reward/reaching_object: 1.0421
     Episode_Reward/lifting_object: 10.5270
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0731
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 2.12s
                      Time elapsed: 00:19:34
                               ETA: 00:51:37

################################################################################
                     [1m Learning iteration 550/2000 [0m                      

                       Computation: 45971 steps/s (collection: 2.035s, learning 0.104s)
             Mean action noise std: 2.62
          Mean value_function loss: 46.1840
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 73.6601
                       Mean reward: 68.04
               Mean episode length: 180.69
    Episode_Reward/reaching_object: 1.0194
     Episode_Reward/lifting_object: 11.0867
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 2.14s
                      Time elapsed: 00:19:36
                               ETA: 00:51:35

################################################################################
                     [1m Learning iteration 551/2000 [0m                      

                       Computation: 44774 steps/s (collection: 2.096s, learning 0.100s)
             Mean action noise std: 2.62
          Mean value_function loss: 56.6652
               Mean surrogate loss: 0.0109
                 Mean entropy loss: 73.6603
                       Mean reward: 66.23
               Mean episode length: 179.39
    Episode_Reward/reaching_object: 1.0319
     Episode_Reward/lifting_object: 12.0751
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 15.1667
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 2.20s
                      Time elapsed: 00:19:38
                               ETA: 00:51:33

################################################################################
                     [1m Learning iteration 552/2000 [0m                      

                       Computation: 45437 steps/s (collection: 2.059s, learning 0.105s)
             Mean action noise std: 2.62
          Mean value_function loss: 48.5147
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 73.6605
                       Mean reward: 67.09
               Mean episode length: 178.97
    Episode_Reward/reaching_object: 1.0446
     Episode_Reward/lifting_object: 11.6310
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0729
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 16.5000
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 2.16s
                      Time elapsed: 00:19:40
                               ETA: 00:51:31

################################################################################
                     [1m Learning iteration 553/2000 [0m                      

                       Computation: 46501 steps/s (collection: 2.020s, learning 0.094s)
             Mean action noise std: 2.62
          Mean value_function loss: 61.3116
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 73.6637
                       Mean reward: 54.62
               Mean episode length: 183.03
    Episode_Reward/reaching_object: 1.0284
     Episode_Reward/lifting_object: 11.1447
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0721
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 16.0417
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 2.11s
                      Time elapsed: 00:19:42
                               ETA: 00:51:29

################################################################################
                     [1m Learning iteration 554/2000 [0m                      

                       Computation: 45428 steps/s (collection: 2.067s, learning 0.097s)
             Mean action noise std: 2.62
          Mean value_function loss: 58.2364
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.6722
                       Mean reward: 58.74
               Mean episode length: 175.61
    Episode_Reward/reaching_object: 1.0451
     Episode_Reward/lifting_object: 11.7614
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0722
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 15.0833
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 2.16s
                      Time elapsed: 00:19:44
                               ETA: 00:51:27

################################################################################
                     [1m Learning iteration 555/2000 [0m                      

                       Computation: 46128 steps/s (collection: 2.033s, learning 0.098s)
             Mean action noise std: 2.62
          Mean value_function loss: 66.4359
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 73.6820
                       Mean reward: 54.30
               Mean episode length: 169.29
    Episode_Reward/reaching_object: 1.0354
     Episode_Reward/lifting_object: 11.4936
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 2.13s
                      Time elapsed: 00:19:47
                               ETA: 00:51:25

################################################################################
                     [1m Learning iteration 556/2000 [0m                      

                       Computation: 46490 steps/s (collection: 2.023s, learning 0.092s)
             Mean action noise std: 2.62
          Mean value_function loss: 85.4730
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 73.6890
                       Mean reward: 65.07
               Mean episode length: 180.79
    Episode_Reward/reaching_object: 1.0389
     Episode_Reward/lifting_object: 12.0465
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0707
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 16.2917
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 2.11s
                      Time elapsed: 00:19:49
                               ETA: 00:51:23

################################################################################
                     [1m Learning iteration 557/2000 [0m                      

                       Computation: 46280 steps/s (collection: 2.026s, learning 0.098s)
             Mean action noise std: 2.62
          Mean value_function loss: 67.4793
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 73.6956
                       Mean reward: 64.87
               Mean episode length: 185.18
    Episode_Reward/reaching_object: 1.0328
     Episode_Reward/lifting_object: 10.8436
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0714
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 2.12s
                      Time elapsed: 00:19:51
                               ETA: 00:51:20

################################################################################
                     [1m Learning iteration 558/2000 [0m                      

                       Computation: 46399 steps/s (collection: 2.022s, learning 0.097s)
             Mean action noise std: 2.63
          Mean value_function loss: 69.4757
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 73.7025
                       Mean reward: 68.45
               Mean episode length: 169.64
    Episode_Reward/reaching_object: 1.0215
     Episode_Reward/lifting_object: 11.4372
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 16.6250
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 2.12s
                      Time elapsed: 00:19:53
                               ETA: 00:51:18

################################################################################
                     [1m Learning iteration 559/2000 [0m                      

                       Computation: 46104 steps/s (collection: 2.036s, learning 0.096s)
             Mean action noise std: 2.63
          Mean value_function loss: 51.6136
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 73.7078
                       Mean reward: 72.99
               Mean episode length: 182.04
    Episode_Reward/reaching_object: 1.0490
     Episode_Reward/lifting_object: 12.1947
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0718
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 15.1667
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 2.13s
                      Time elapsed: 00:19:55
                               ETA: 00:51:16

################################################################################
                     [1m Learning iteration 560/2000 [0m                      

                       Computation: 45099 steps/s (collection: 2.080s, learning 0.100s)
             Mean action noise std: 2.63
          Mean value_function loss: 54.0262
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 73.7121
                       Mean reward: 68.54
               Mean episode length: 167.33
    Episode_Reward/reaching_object: 1.0279
     Episode_Reward/lifting_object: 12.5374
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0704
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 16.4583
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 2.18s
                      Time elapsed: 00:19:57
                               ETA: 00:51:14

################################################################################
                     [1m Learning iteration 561/2000 [0m                      

                       Computation: 46029 steps/s (collection: 2.041s, learning 0.095s)
             Mean action noise std: 2.63
          Mean value_function loss: 66.6558
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 73.7203
                       Mean reward: 61.94
               Mean episode length: 177.02
    Episode_Reward/reaching_object: 1.0595
     Episode_Reward/lifting_object: 12.5160
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0721
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 16.2917
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 2.14s
                      Time elapsed: 00:19:59
                               ETA: 00:51:12

################################################################################
                     [1m Learning iteration 562/2000 [0m                      

                       Computation: 45168 steps/s (collection: 2.058s, learning 0.119s)
             Mean action noise std: 2.63
          Mean value_function loss: 66.2910
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 73.7321
                       Mean reward: 80.94
               Mean episode length: 179.97
    Episode_Reward/reaching_object: 1.0818
     Episode_Reward/lifting_object: 13.4972
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0726
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 16.6667
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 2.18s
                      Time elapsed: 00:20:02
                               ETA: 00:51:10

################################################################################
                     [1m Learning iteration 563/2000 [0m                      

                       Computation: 44717 steps/s (collection: 2.085s, learning 0.113s)
             Mean action noise std: 2.63
          Mean value_function loss: 69.7717
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 73.7407
                       Mean reward: 70.31
               Mean episode length: 160.08
    Episode_Reward/reaching_object: 1.0093
     Episode_Reward/lifting_object: 12.7905
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 18.3333
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 2.20s
                      Time elapsed: 00:20:04
                               ETA: 00:51:08

################################################################################
                     [1m Learning iteration 564/2000 [0m                      

                       Computation: 45505 steps/s (collection: 2.060s, learning 0.101s)
             Mean action noise std: 2.63
          Mean value_function loss: 74.0767
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 73.7477
                       Mean reward: 63.06
               Mean episode length: 165.40
    Episode_Reward/reaching_object: 1.0068
     Episode_Reward/lifting_object: 12.1978
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 20.5417
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 2.16s
                      Time elapsed: 00:20:06
                               ETA: 00:51:06

################################################################################
                     [1m Learning iteration 565/2000 [0m                      

                       Computation: 44867 steps/s (collection: 2.092s, learning 0.099s)
             Mean action noise std: 2.63
          Mean value_function loss: 61.6349
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 73.7526
                       Mean reward: 73.55
               Mean episode length: 170.83
    Episode_Reward/reaching_object: 1.0350
     Episode_Reward/lifting_object: 13.5424
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0709
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 17.9167
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 2.19s
                      Time elapsed: 00:20:08
                               ETA: 00:51:04

################################################################################
                     [1m Learning iteration 566/2000 [0m                      

                       Computation: 46298 steps/s (collection: 2.017s, learning 0.107s)
             Mean action noise std: 2.63
          Mean value_function loss: 65.4194
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 73.7565
                       Mean reward: 78.88
               Mean episode length: 183.86
    Episode_Reward/reaching_object: 0.9797
     Episode_Reward/lifting_object: 12.3494
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 5.5000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 16.8750
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 2.12s
                      Time elapsed: 00:20:10
                               ETA: 00:51:02

################################################################################
                     [1m Learning iteration 567/2000 [0m                      

                       Computation: 44311 steps/s (collection: 2.091s, learning 0.127s)
             Mean action noise std: 2.63
          Mean value_function loss: 57.8996
               Mean surrogate loss: 0.0103
                 Mean entropy loss: 73.7604
                       Mean reward: 72.97
               Mean episode length: 166.59
    Episode_Reward/reaching_object: 1.0079
     Episode_Reward/lifting_object: 13.6557
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 18.8750
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 2.22s
                      Time elapsed: 00:20:12
                               ETA: 00:51:00

################################################################################
                     [1m Learning iteration 568/2000 [0m                      

                       Computation: 45380 steps/s (collection: 2.066s, learning 0.100s)
             Mean action noise std: 2.63
          Mean value_function loss: 67.2952
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 73.7617
                       Mean reward: 72.40
               Mean episode length: 166.17
    Episode_Reward/reaching_object: 1.0035
     Episode_Reward/lifting_object: 13.3508
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 17.9583
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 2.17s
                      Time elapsed: 00:20:15
                               ETA: 00:50:58

################################################################################
                     [1m Learning iteration 569/2000 [0m                      

                       Computation: 45479 steps/s (collection: 2.070s, learning 0.092s)
             Mean action noise std: 2.63
          Mean value_function loss: 62.1162
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 73.7652
                       Mean reward: 71.97
               Mean episode length: 163.76
    Episode_Reward/reaching_object: 1.0195
     Episode_Reward/lifting_object: 13.3067
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 18.6250
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 2.16s
                      Time elapsed: 00:20:17
                               ETA: 00:50:56

################################################################################
                     [1m Learning iteration 570/2000 [0m                      

                       Computation: 44859 steps/s (collection: 2.099s, learning 0.093s)
             Mean action noise std: 2.63
          Mean value_function loss: 72.4824
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 73.7713
                       Mean reward: 81.27
               Mean episode length: 172.79
    Episode_Reward/reaching_object: 0.9984
     Episode_Reward/lifting_object: 13.2652
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0705
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 18.2083
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 2.19s
                      Time elapsed: 00:20:19
                               ETA: 00:50:54

################################################################################
                     [1m Learning iteration 571/2000 [0m                      

                       Computation: 46280 steps/s (collection: 2.030s, learning 0.094s)
             Mean action noise std: 2.63
          Mean value_function loss: 66.0214
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 73.7766
                       Mean reward: 64.91
               Mean episode length: 162.30
    Episode_Reward/reaching_object: 0.9789
     Episode_Reward/lifting_object: 13.3000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0694
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 18.3333
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 2.12s
                      Time elapsed: 00:20:21
                               ETA: 00:50:51

################################################################################
                     [1m Learning iteration 572/2000 [0m                      

                       Computation: 45938 steps/s (collection: 2.041s, learning 0.099s)
             Mean action noise std: 2.63
          Mean value_function loss: 87.1272
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 73.7802
                       Mean reward: 79.23
               Mean episode length: 168.87
    Episode_Reward/reaching_object: 1.0008
     Episode_Reward/lifting_object: 14.3958
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0709
      Episode_Termination/time_out: 5.3750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 18.8750
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 2.14s
                      Time elapsed: 00:20:23
                               ETA: 00:50:49

################################################################################
                     [1m Learning iteration 573/2000 [0m                      

                       Computation: 45463 steps/s (collection: 2.070s, learning 0.092s)
             Mean action noise std: 2.63
          Mean value_function loss: 82.2231
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 73.7852
                       Mean reward: 71.38
               Mean episode length: 165.27
    Episode_Reward/reaching_object: 0.9952
     Episode_Reward/lifting_object: 13.2445
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 19.2083
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 2.16s
                      Time elapsed: 00:20:25
                               ETA: 00:50:47

################################################################################
                     [1m Learning iteration 574/2000 [0m                      

                       Computation: 45898 steps/s (collection: 2.049s, learning 0.093s)
             Mean action noise std: 2.63
          Mean value_function loss: 64.1370
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 73.7905
                       Mean reward: 68.57
               Mean episode length: 169.29
    Episode_Reward/reaching_object: 0.9990
     Episode_Reward/lifting_object: 13.5142
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 18.5833
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 2.14s
                      Time elapsed: 00:20:28
                               ETA: 00:50:45

################################################################################
                     [1m Learning iteration 575/2000 [0m                      

                       Computation: 45642 steps/s (collection: 2.057s, learning 0.097s)
             Mean action noise std: 2.63
          Mean value_function loss: 68.3108
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 73.7919
                       Mean reward: 66.34
               Mean episode length: 157.88
    Episode_Reward/reaching_object: 1.0074
     Episode_Reward/lifting_object: 13.5828
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0721
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 18.3333
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 2.15s
                      Time elapsed: 00:20:30
                               ETA: 00:50:43

################################################################################
                     [1m Learning iteration 576/2000 [0m                      

                       Computation: 44714 steps/s (collection: 2.088s, learning 0.110s)
             Mean action noise std: 2.63
          Mean value_function loss: 69.9647
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 73.7945
                       Mean reward: 79.49
               Mean episode length: 167.50
    Episode_Reward/reaching_object: 1.0261
     Episode_Reward/lifting_object: 14.6530
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 17.1250
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 2.20s
                      Time elapsed: 00:20:32
                               ETA: 00:50:41

################################################################################
                     [1m Learning iteration 577/2000 [0m                      

                       Computation: 44331 steps/s (collection: 2.101s, learning 0.116s)
             Mean action noise std: 2.63
          Mean value_function loss: 86.9662
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.8002
                       Mean reward: 71.54
               Mean episode length: 164.92
    Episode_Reward/reaching_object: 1.0407
     Episode_Reward/lifting_object: 14.7215
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0741
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 17.7917
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 2.22s
                      Time elapsed: 00:20:34
                               ETA: 00:50:39

################################################################################
                     [1m Learning iteration 578/2000 [0m                      

                       Computation: 46056 steps/s (collection: 2.038s, learning 0.097s)
             Mean action noise std: 2.64
          Mean value_function loss: 77.9561
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 73.8069
                       Mean reward: 86.50
               Mean episode length: 165.17
    Episode_Reward/reaching_object: 1.0175
     Episode_Reward/lifting_object: 14.5478
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0722
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 18.8333
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 2.13s
                      Time elapsed: 00:20:36
                               ETA: 00:50:37

################################################################################
                     [1m Learning iteration 579/2000 [0m                      

                       Computation: 44145 steps/s (collection: 2.133s, learning 0.094s)
             Mean action noise std: 2.64
          Mean value_function loss: 69.2410
               Mean surrogate loss: 0.0121
                 Mean entropy loss: 73.8109
                       Mean reward: 88.61
               Mean episode length: 168.97
    Episode_Reward/reaching_object: 1.0206
     Episode_Reward/lifting_object: 15.3620
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0721
      Episode_Termination/time_out: 5.5000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 19.9167
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 2.23s
                      Time elapsed: 00:20:39
                               ETA: 00:50:35

################################################################################
                     [1m Learning iteration 580/2000 [0m                      

                       Computation: 45447 steps/s (collection: 2.067s, learning 0.097s)
             Mean action noise std: 2.64
          Mean value_function loss: 73.2677
               Mean surrogate loss: 0.0158
                 Mean entropy loss: 73.8115
                       Mean reward: 68.49
               Mean episode length: 165.14
    Episode_Reward/reaching_object: 1.0217
     Episode_Reward/lifting_object: 14.6765
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0720
      Episode_Termination/time_out: 5.3750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 18.4583
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 2.16s
                      Time elapsed: 00:20:41
                               ETA: 00:50:33

################################################################################
                     [1m Learning iteration 581/2000 [0m                      

                       Computation: 45715 steps/s (collection: 2.058s, learning 0.093s)
             Mean action noise std: 2.64
          Mean value_function loss: 77.4446
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 73.8116
                       Mean reward: 84.15
               Mean episode length: 163.97
    Episode_Reward/reaching_object: 1.0114
     Episode_Reward/lifting_object: 15.5878
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 20.0417
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 2.15s
                      Time elapsed: 00:20:43
                               ETA: 00:50:31

################################################################################
                     [1m Learning iteration 582/2000 [0m                      

                       Computation: 46208 steps/s (collection: 2.031s, learning 0.097s)
             Mean action noise std: 2.64
          Mean value_function loss: 69.0050
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 73.8127
                       Mean reward: 79.76
               Mean episode length: 153.45
    Episode_Reward/reaching_object: 0.9919
     Episode_Reward/lifting_object: 15.4046
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0708
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 20.2083
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 2.13s
                      Time elapsed: 00:20:45
                               ETA: 00:50:29

################################################################################
                     [1m Learning iteration 583/2000 [0m                      

                       Computation: 45738 steps/s (collection: 2.059s, learning 0.091s)
             Mean action noise std: 2.64
          Mean value_function loss: 80.1699
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 73.8156
                       Mean reward: 81.30
               Mean episode length: 162.35
    Episode_Reward/reaching_object: 1.0185
     Episode_Reward/lifting_object: 16.2320
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0731
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 18.6667
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 2.15s
                      Time elapsed: 00:20:47
                               ETA: 00:50:27

################################################################################
                     [1m Learning iteration 584/2000 [0m                      

                       Computation: 45926 steps/s (collection: 2.044s, learning 0.096s)
             Mean action noise std: 2.64
          Mean value_function loss: 89.1027
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 73.8219
                       Mean reward: 76.50
               Mean episode length: 158.22
    Episode_Reward/reaching_object: 1.0066
     Episode_Reward/lifting_object: 14.8837
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0718
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 19.7500
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 2.14s
                      Time elapsed: 00:20:49
                               ETA: 00:50:25

################################################################################
                     [1m Learning iteration 585/2000 [0m                      

                       Computation: 46216 steps/s (collection: 2.026s, learning 0.102s)
             Mean action noise std: 2.64
          Mean value_function loss: 78.7508
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 73.8262
                       Mean reward: 75.99
               Mean episode length: 168.96
    Episode_Reward/reaching_object: 1.0280
     Episode_Reward/lifting_object: 15.0275
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0741
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 17.5000
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 2.13s
                      Time elapsed: 00:20:51
                               ETA: 00:50:22

################################################################################
                     [1m Learning iteration 586/2000 [0m                      

                       Computation: 45724 steps/s (collection: 2.040s, learning 0.110s)
             Mean action noise std: 2.64
          Mean value_function loss: 71.4218
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 73.8288
                       Mean reward: 78.65
               Mean episode length: 163.16
    Episode_Reward/reaching_object: 1.0070
     Episode_Reward/lifting_object: 14.9954
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0725
      Episode_Termination/time_out: 5.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 17.4167
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 2.15s
                      Time elapsed: 00:20:54
                               ETA: 00:50:20

################################################################################
                     [1m Learning iteration 587/2000 [0m                      

                       Computation: 45863 steps/s (collection: 2.045s, learning 0.099s)
             Mean action noise std: 2.64
          Mean value_function loss: 110.4838
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 73.8345
                       Mean reward: 88.33
               Mean episode length: 171.96
    Episode_Reward/reaching_object: 1.0356
     Episode_Reward/lifting_object: 16.0592
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0742
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 18.2917
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 2.14s
                      Time elapsed: 00:20:56
                               ETA: 00:50:18

################################################################################
                     [1m Learning iteration 588/2000 [0m                      

                       Computation: 46218 steps/s (collection: 2.033s, learning 0.094s)
             Mean action noise std: 2.64
          Mean value_function loss: 90.9102
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 73.8425
                       Mean reward: 80.91
               Mean episode length: 167.36
    Episode_Reward/reaching_object: 0.9927
     Episode_Reward/lifting_object: 14.9304
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 5.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 19.8333
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 2.13s
                      Time elapsed: 00:20:58
                               ETA: 00:50:16

################################################################################
                     [1m Learning iteration 589/2000 [0m                      

                       Computation: 45735 steps/s (collection: 2.055s, learning 0.094s)
             Mean action noise std: 2.64
          Mean value_function loss: 95.3974
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 73.8460
                       Mean reward: 81.77
               Mean episode length: 167.05
    Episode_Reward/reaching_object: 1.0304
     Episode_Reward/lifting_object: 15.8398
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0743
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 18.2083
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 2.15s
                      Time elapsed: 00:21:00
                               ETA: 00:50:14

################################################################################
                     [1m Learning iteration 590/2000 [0m                      

                       Computation: 45271 steps/s (collection: 2.069s, learning 0.103s)
             Mean action noise std: 2.64
          Mean value_function loss: 88.5723
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 73.8496
                       Mean reward: 80.80
               Mean episode length: 165.44
    Episode_Reward/reaching_object: 1.0353
     Episode_Reward/lifting_object: 15.7588
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0743
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 19.1250
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 2.17s
                      Time elapsed: 00:21:02
                               ETA: 00:50:12

################################################################################
                     [1m Learning iteration 591/2000 [0m                      

                       Computation: 45294 steps/s (collection: 2.049s, learning 0.122s)
             Mean action noise std: 2.64
          Mean value_function loss: 79.9658
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.8513
                       Mean reward: 99.85
               Mean episode length: 180.51
    Episode_Reward/reaching_object: 1.0452
     Episode_Reward/lifting_object: 16.0882
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 18.2500
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 2.17s
                      Time elapsed: 00:21:04
                               ETA: 00:50:10

################################################################################
                     [1m Learning iteration 592/2000 [0m                      

                       Computation: 45552 steps/s (collection: 2.058s, learning 0.100s)
             Mean action noise std: 2.64
          Mean value_function loss: 94.8556
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 73.8548
                       Mean reward: 82.62
               Mean episode length: 172.01
    Episode_Reward/reaching_object: 1.0661
     Episode_Reward/lifting_object: 17.3274
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0755
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 19.2083
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 2.16s
                      Time elapsed: 00:21:06
                               ETA: 00:50:08

################################################################################
                     [1m Learning iteration 593/2000 [0m                      

                       Computation: 45548 steps/s (collection: 2.056s, learning 0.102s)
             Mean action noise std: 2.64
          Mean value_function loss: 81.0839
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 73.8592
                       Mean reward: 88.23
               Mean episode length: 170.33
    Episode_Reward/reaching_object: 1.0530
     Episode_Reward/lifting_object: 17.3515
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0749
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 18.2917
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 2.16s
                      Time elapsed: 00:21:09
                               ETA: 00:50:06

################################################################################
                     [1m Learning iteration 594/2000 [0m                      

                       Computation: 46005 steps/s (collection: 2.035s, learning 0.102s)
             Mean action noise std: 2.64
          Mean value_function loss: 81.7669
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 73.8616
                       Mean reward: 92.02
               Mean episode length: 162.35
    Episode_Reward/reaching_object: 1.0155
     Episode_Reward/lifting_object: 16.6944
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0708
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 20.1250
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 2.14s
                      Time elapsed: 00:21:11
                               ETA: 00:50:03

################################################################################
                     [1m Learning iteration 595/2000 [0m                      

                       Computation: 45087 steps/s (collection: 2.067s, learning 0.113s)
             Mean action noise std: 2.64
          Mean value_function loss: 87.3279
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 73.8641
                       Mean reward: 77.58
               Mean episode length: 156.58
    Episode_Reward/reaching_object: 1.0252
     Episode_Reward/lifting_object: 16.4834
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 20.7083
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 2.18s
                      Time elapsed: 00:21:13
                               ETA: 00:50:01

################################################################################
                     [1m Learning iteration 596/2000 [0m                      

                       Computation: 45672 steps/s (collection: 2.039s, learning 0.113s)
             Mean action noise std: 2.64
          Mean value_function loss: 90.4033
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 73.8663
                       Mean reward: 89.26
               Mean episode length: 153.60
    Episode_Reward/reaching_object: 1.0373
     Episode_Reward/lifting_object: 17.7792
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 21.5417
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 2.15s
                      Time elapsed: 00:21:15
                               ETA: 00:49:59

################################################################################
                     [1m Learning iteration 597/2000 [0m                      

                       Computation: 45147 steps/s (collection: 2.078s, learning 0.099s)
             Mean action noise std: 2.64
          Mean value_function loss: 85.4069
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 73.8694
                       Mean reward: 106.17
               Mean episode length: 162.37
    Episode_Reward/reaching_object: 1.0391
     Episode_Reward/lifting_object: 18.8744
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 21.8750
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 2.18s
                      Time elapsed: 00:21:17
                               ETA: 00:49:57

################################################################################
                     [1m Learning iteration 598/2000 [0m                      

                       Computation: 45289 steps/s (collection: 2.070s, learning 0.101s)
             Mean action noise std: 2.64
          Mean value_function loss: 74.7679
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 73.8704
                       Mean reward: 106.81
               Mean episode length: 165.01
    Episode_Reward/reaching_object: 1.0254
     Episode_Reward/lifting_object: 18.1891
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 20.5833
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 2.17s
                      Time elapsed: 00:21:19
                               ETA: 00:49:55

################################################################################
                     [1m Learning iteration 599/2000 [0m                      

                       Computation: 45681 steps/s (collection: 2.052s, learning 0.100s)
             Mean action noise std: 2.64
          Mean value_function loss: 91.6962
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 73.8709
                       Mean reward: 94.69
               Mean episode length: 152.68
    Episode_Reward/reaching_object: 1.0051
     Episode_Reward/lifting_object: 17.6369
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 21.9167
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 2.15s
                      Time elapsed: 00:21:22
                               ETA: 00:49:53

################################################################################
                     [1m Learning iteration 600/2000 [0m                      

                       Computation: 44841 steps/s (collection: 2.083s, learning 0.109s)
             Mean action noise std: 2.64
          Mean value_function loss: 92.3711
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 73.8722
                       Mean reward: 102.59
               Mean episode length: 150.64
    Episode_Reward/reaching_object: 1.0382
     Episode_Reward/lifting_object: 18.7003
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0709
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 22.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 2.19s
                      Time elapsed: 00:21:24
                               ETA: 00:49:51

################################################################################
                     [1m Learning iteration 601/2000 [0m                      

                       Computation: 44594 steps/s (collection: 2.101s, learning 0.104s)
             Mean action noise std: 2.64
          Mean value_function loss: 85.7220
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 73.8729
                       Mean reward: 95.53
               Mean episode length: 153.50
    Episode_Reward/reaching_object: 0.9750
     Episode_Reward/lifting_object: 17.3489
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 21.7083
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 2.20s
                      Time elapsed: 00:21:26
                               ETA: 00:49:49

################################################################################
                     [1m Learning iteration 602/2000 [0m                      

                       Computation: 44995 steps/s (collection: 2.074s, learning 0.111s)
             Mean action noise std: 2.64
          Mean value_function loss: 93.9380
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 73.8738
                       Mean reward: 113.17
               Mean episode length: 160.95
    Episode_Reward/reaching_object: 1.0031
     Episode_Reward/lifting_object: 18.5708
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 22.5417
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 2.18s
                      Time elapsed: 00:21:28
                               ETA: 00:49:47

################################################################################
                     [1m Learning iteration 603/2000 [0m                      

                       Computation: 45405 steps/s (collection: 2.069s, learning 0.097s)
             Mean action noise std: 2.64
          Mean value_function loss: 77.9712
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 73.8750
                       Mean reward: 91.25
               Mean episode length: 154.46
    Episode_Reward/reaching_object: 0.9865
     Episode_Reward/lifting_object: 18.0039
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 21.7083
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 2.17s
                      Time elapsed: 00:21:30
                               ETA: 00:49:45

################################################################################
                     [1m Learning iteration 604/2000 [0m                      

                       Computation: 45711 steps/s (collection: 2.044s, learning 0.106s)
             Mean action noise std: 2.64
          Mean value_function loss: 90.5451
               Mean surrogate loss: 0.0104
                 Mean entropy loss: 73.8760
                       Mean reward: 92.39
               Mean episode length: 145.29
    Episode_Reward/reaching_object: 0.9818
     Episode_Reward/lifting_object: 17.9846
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 4.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 21.5833
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 2.15s
                      Time elapsed: 00:21:32
                               ETA: 00:49:43

################################################################################
                     [1m Learning iteration 605/2000 [0m                      

                       Computation: 46106 steps/s (collection: 2.030s, learning 0.103s)
             Mean action noise std: 2.64
          Mean value_function loss: 90.5244
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 73.8762
                       Mean reward: 100.51
               Mean episode length: 156.82
    Episode_Reward/reaching_object: 1.0237
     Episode_Reward/lifting_object: 19.6648
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 20.7083
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 2.13s
                      Time elapsed: 00:21:35
                               ETA: 00:49:41

################################################################################
                     [1m Learning iteration 606/2000 [0m                      

                       Computation: 46232 steps/s (collection: 2.024s, learning 0.102s)
             Mean action noise std: 2.64
          Mean value_function loss: 90.2161
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 73.8770
                       Mean reward: 90.78
               Mean episode length: 145.14
    Episode_Reward/reaching_object: 0.9761
     Episode_Reward/lifting_object: 18.1862
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0694
      Episode_Termination/time_out: 4.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 23.7500
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 2.13s
                      Time elapsed: 00:21:37
                               ETA: 00:49:39

################################################################################
                     [1m Learning iteration 607/2000 [0m                      

                       Computation: 45425 steps/s (collection: 2.061s, learning 0.103s)
             Mean action noise std: 2.64
          Mean value_function loss: 116.8977
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 73.8793
                       Mean reward: 96.61
               Mean episode length: 149.85
    Episode_Reward/reaching_object: 1.0138
     Episode_Reward/lifting_object: 19.4920
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0723
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 21.5417
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 2.16s
                      Time elapsed: 00:21:39
                               ETA: 00:49:37

################################################################################
                     [1m Learning iteration 608/2000 [0m                      

                       Computation: 45694 steps/s (collection: 2.051s, learning 0.101s)
             Mean action noise std: 2.64
          Mean value_function loss: 118.9408
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 73.8829
                       Mean reward: 92.11
               Mean episode length: 153.25
    Episode_Reward/reaching_object: 1.0059
     Episode_Reward/lifting_object: 18.1016
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 23.4167
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 2.15s
                      Time elapsed: 00:21:41
                               ETA: 00:49:34

################################################################################
                     [1m Learning iteration 609/2000 [0m                      

                       Computation: 46558 steps/s (collection: 2.023s, learning 0.088s)
             Mean action noise std: 2.64
          Mean value_function loss: 111.0513
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 73.8862
                       Mean reward: 96.13
               Mean episode length: 154.65
    Episode_Reward/reaching_object: 1.0202
     Episode_Reward/lifting_object: 19.6534
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0718
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 21.9167
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 2.11s
                      Time elapsed: 00:21:43
                               ETA: 00:49:32

################################################################################
                     [1m Learning iteration 610/2000 [0m                      

                       Computation: 46532 steps/s (collection: 2.020s, learning 0.093s)
             Mean action noise std: 2.64
          Mean value_function loss: 97.2901
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 73.8871
                       Mean reward: 92.45
               Mean episode length: 147.37
    Episode_Reward/reaching_object: 1.0307
     Episode_Reward/lifting_object: 19.9204
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 22.2917
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 2.11s
                      Time elapsed: 00:21:45
                               ETA: 00:49:30

################################################################################
                     [1m Learning iteration 611/2000 [0m                      

                       Computation: 46216 steps/s (collection: 2.033s, learning 0.095s)
             Mean action noise std: 2.64
          Mean value_function loss: 102.2570
               Mean surrogate loss: 0.0102
                 Mean entropy loss: 73.8874
                       Mean reward: 87.60
               Mean episode length: 138.64
    Episode_Reward/reaching_object: 0.9729
     Episode_Reward/lifting_object: 19.2926
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 23.2500
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 2.13s
                      Time elapsed: 00:21:47
                               ETA: 00:49:28

################################################################################
                     [1m Learning iteration 612/2000 [0m                      

                       Computation: 46199 steps/s (collection: 2.025s, learning 0.103s)
             Mean action noise std: 2.64
          Mean value_function loss: 114.6208
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 73.8875
                       Mean reward: 107.67
               Mean episode length: 143.55
    Episode_Reward/reaching_object: 1.0062
     Episode_Reward/lifting_object: 19.2824
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 22.0417
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 2.13s
                      Time elapsed: 00:21:50
                               ETA: 00:49:26

################################################################################
                     [1m Learning iteration 613/2000 [0m                      

                       Computation: 46483 steps/s (collection: 2.007s, learning 0.108s)
             Mean action noise std: 2.64
          Mean value_function loss: 113.2936
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 73.8883
                       Mean reward: 98.73
               Mean episode length: 143.31
    Episode_Reward/reaching_object: 0.9588
     Episode_Reward/lifting_object: 18.4087
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 24.8750
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 2.11s
                      Time elapsed: 00:21:52
                               ETA: 00:49:24

################################################################################
                     [1m Learning iteration 614/2000 [0m                      

                       Computation: 46448 steps/s (collection: 2.025s, learning 0.091s)
             Mean action noise std: 2.64
          Mean value_function loss: 114.5889
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 73.8903
                       Mean reward: 106.43
               Mean episode length: 148.36
    Episode_Reward/reaching_object: 1.0021
     Episode_Reward/lifting_object: 20.7057
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0686
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 23.9167
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 2.12s
                      Time elapsed: 00:21:54
                               ETA: 00:49:21

################################################################################
                     [1m Learning iteration 615/2000 [0m                      

                       Computation: 46269 steps/s (collection: 2.027s, learning 0.098s)
             Mean action noise std: 2.64
          Mean value_function loss: 171.9481
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 73.8940
                       Mean reward: 91.46
               Mean episode length: 138.71
    Episode_Reward/reaching_object: 0.9837
     Episode_Reward/lifting_object: 19.0042
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 23.8750
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 2.12s
                      Time elapsed: 00:21:56
                               ETA: 00:49:19

################################################################################
                     [1m Learning iteration 616/2000 [0m                      

                       Computation: 46053 steps/s (collection: 2.039s, learning 0.096s)
             Mean action noise std: 2.64
          Mean value_function loss: 137.6797
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 73.9014
                       Mean reward: 103.70
               Mean episode length: 147.94
    Episode_Reward/reaching_object: 0.9716
     Episode_Reward/lifting_object: 18.9079
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 3.8750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 22.2917
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 2.13s
                      Time elapsed: 00:21:58
                               ETA: 00:49:17

################################################################################
                     [1m Learning iteration 617/2000 [0m                      

                       Computation: 46240 steps/s (collection: 2.035s, learning 0.091s)
             Mean action noise std: 2.64
          Mean value_function loss: 133.4090
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 73.9069
                       Mean reward: 107.17
               Mean episode length: 150.46
    Episode_Reward/reaching_object: 1.0069
     Episode_Reward/lifting_object: 19.6450
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0695
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 22.8333
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 2.13s
                      Time elapsed: 00:22:00
                               ETA: 00:49:15

################################################################################
                     [1m Learning iteration 618/2000 [0m                      

                       Computation: 46498 steps/s (collection: 1.996s, learning 0.118s)
             Mean action noise std: 2.64
          Mean value_function loss: 117.6165
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 73.9097
                       Mean reward: 108.90
               Mean episode length: 147.06
    Episode_Reward/reaching_object: 1.0165
     Episode_Reward/lifting_object: 20.9245
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0704
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 21.4583
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 2.11s
                      Time elapsed: 00:22:02
                               ETA: 00:49:13

################################################################################
                     [1m Learning iteration 619/2000 [0m                      

                       Computation: 46304 steps/s (collection: 2.010s, learning 0.112s)
             Mean action noise std: 2.65
          Mean value_function loss: 118.3279
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 73.9102
                       Mean reward: 109.97
               Mean episode length: 157.44
    Episode_Reward/reaching_object: 1.0400
     Episode_Reward/lifting_object: 20.8810
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 22.8750
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 2.12s
                      Time elapsed: 00:22:04
                               ETA: 00:49:11

################################################################################
                     [1m Learning iteration 620/2000 [0m                      

                       Computation: 46410 steps/s (collection: 2.011s, learning 0.107s)
             Mean action noise std: 2.65
          Mean value_function loss: 106.9490
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 73.9108
                       Mean reward: 117.56
               Mean episode length: 157.45
    Episode_Reward/reaching_object: 1.0224
     Episode_Reward/lifting_object: 21.0625
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0707
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 21.3333
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 2.12s
                      Time elapsed: 00:22:06
                               ETA: 00:49:08

################################################################################
                     [1m Learning iteration 621/2000 [0m                      

                       Computation: 47222 steps/s (collection: 1.991s, learning 0.091s)
             Mean action noise std: 2.65
          Mean value_function loss: 122.9478
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 73.9111
                       Mean reward: 106.92
               Mean episode length: 152.98
    Episode_Reward/reaching_object: 1.0158
     Episode_Reward/lifting_object: 21.0051
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 4.1250
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 22.1250
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 2.08s
                      Time elapsed: 00:22:09
                               ETA: 00:49:06

################################################################################
                     [1m Learning iteration 622/2000 [0m                      

                       Computation: 47429 steps/s (collection: 1.980s, learning 0.093s)
             Mean action noise std: 2.65
          Mean value_function loss: 122.0244
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 73.9123
                       Mean reward: 121.65
               Mean episode length: 157.29
    Episode_Reward/reaching_object: 1.0162
     Episode_Reward/lifting_object: 21.7352
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0710
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 23.7917
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 2.07s
                      Time elapsed: 00:22:11
                               ETA: 00:49:04

################################################################################
                     [1m Learning iteration 623/2000 [0m                      

                       Computation: 47329 steps/s (collection: 1.979s, learning 0.098s)
             Mean action noise std: 2.65
          Mean value_function loss: 120.9102
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 73.9161
                       Mean reward: 120.68
               Mean episode length: 159.88
    Episode_Reward/reaching_object: 1.0617
     Episode_Reward/lifting_object: 22.5418
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0725
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 23.7083
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 2.08s
                      Time elapsed: 00:22:13
                               ETA: 00:49:02

################################################################################
                     [1m Learning iteration 624/2000 [0m                      

                       Computation: 47283 steps/s (collection: 1.983s, learning 0.096s)
             Mean action noise std: 2.65
          Mean value_function loss: 130.6203
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 73.9198
                       Mean reward: 131.66
               Mean episode length: 169.97
    Episode_Reward/reaching_object: 1.0726
     Episode_Reward/lifting_object: 22.4608
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 22.3333
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 2.08s
                      Time elapsed: 00:22:15
                               ETA: 00:48:59

################################################################################
                     [1m Learning iteration 625/2000 [0m                      

                       Computation: 47170 steps/s (collection: 1.993s, learning 0.092s)
             Mean action noise std: 2.65
          Mean value_function loss: 102.3178
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 73.9214
                       Mean reward: 111.21
               Mean episode length: 148.16
    Episode_Reward/reaching_object: 1.0190
     Episode_Reward/lifting_object: 21.6514
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 22.1250
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 2.08s
                      Time elapsed: 00:22:17
                               ETA: 00:48:57

################################################################################
                     [1m Learning iteration 626/2000 [0m                      

                       Computation: 47066 steps/s (collection: 1.988s, learning 0.101s)
             Mean action noise std: 2.65
          Mean value_function loss: 127.3307
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 73.9236
                       Mean reward: 103.47
               Mean episode length: 141.06
    Episode_Reward/reaching_object: 1.0127
     Episode_Reward/lifting_object: 21.9022
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 24.1667
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 2.09s
                      Time elapsed: 00:22:19
                               ETA: 00:48:55

################################################################################
                     [1m Learning iteration 627/2000 [0m                      

                       Computation: 47368 steps/s (collection: 1.984s, learning 0.092s)
             Mean action noise std: 2.65
          Mean value_function loss: 145.7197
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 73.9265
                       Mean reward: 88.08
               Mean episode length: 148.73
    Episode_Reward/reaching_object: 1.0243
     Episode_Reward/lifting_object: 21.5481
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 24.0833
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 2.08s
                      Time elapsed: 00:22:21
                               ETA: 00:48:53

################################################################################
                     [1m Learning iteration 628/2000 [0m                      

                       Computation: 47224 steps/s (collection: 1.982s, learning 0.100s)
             Mean action noise std: 2.65
          Mean value_function loss: 169.7441
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 73.9338
                       Mean reward: 113.26
               Mean episode length: 147.29
    Episode_Reward/reaching_object: 0.9762
     Episode_Reward/lifting_object: 20.9676
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 24.2917
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 2.08s
                      Time elapsed: 00:22:23
                               ETA: 00:48:50

################################################################################
                     [1m Learning iteration 629/2000 [0m                      

                       Computation: 46323 steps/s (collection: 2.021s, learning 0.102s)
             Mean action noise std: 2.65
          Mean value_function loss: 125.9724
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 73.9387
                       Mean reward: 103.26
               Mean episode length: 145.04
    Episode_Reward/reaching_object: 1.0121
     Episode_Reward/lifting_object: 21.8775
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0686
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 25.6250
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 2.12s
                      Time elapsed: 00:22:25
                               ETA: 00:48:48

################################################################################
                     [1m Learning iteration 630/2000 [0m                      

                       Computation: 46945 steps/s (collection: 1.996s, learning 0.098s)
             Mean action noise std: 2.65
          Mean value_function loss: 129.7243
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 73.9427
                       Mean reward: 116.63
               Mean episode length: 143.17
    Episode_Reward/reaching_object: 0.9895
     Episode_Reward/lifting_object: 21.6377
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 4.2917
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 26.5417
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 2.09s
                      Time elapsed: 00:22:27
                               ETA: 00:48:46

################################################################################
                     [1m Learning iteration 631/2000 [0m                      

                       Computation: 46029 steps/s (collection: 2.038s, learning 0.098s)
             Mean action noise std: 2.65
          Mean value_function loss: 140.4084
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 73.9478
                       Mean reward: 116.53
               Mean episode length: 143.32
    Episode_Reward/reaching_object: 0.9968
     Episode_Reward/lifting_object: 22.0306
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 22.9167
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 2.14s
                      Time elapsed: 00:22:29
                               ETA: 00:48:44

################################################################################
                     [1m Learning iteration 632/2000 [0m                      

                       Computation: 45971 steps/s (collection: 2.035s, learning 0.103s)
             Mean action noise std: 2.65
          Mean value_function loss: 141.7557
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 73.9513
                       Mean reward: 123.96
               Mean episode length: 152.80
    Episode_Reward/reaching_object: 0.9815
     Episode_Reward/lifting_object: 21.0590
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 23.7500
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 2.14s
                      Time elapsed: 00:22:32
                               ETA: 00:48:42

################################################################################
                     [1m Learning iteration 633/2000 [0m                      

                       Computation: 45993 steps/s (collection: 2.038s, learning 0.099s)
             Mean action noise std: 2.65
          Mean value_function loss: 139.0041
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 73.9541
                       Mean reward: 113.03
               Mean episode length: 147.74
    Episode_Reward/reaching_object: 0.9918
     Episode_Reward/lifting_object: 22.1448
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 24.4167
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 2.14s
                      Time elapsed: 00:22:34
                               ETA: 00:48:39

################################################################################
                     [1m Learning iteration 634/2000 [0m                      

                       Computation: 45305 steps/s (collection: 2.068s, learning 0.102s)
             Mean action noise std: 2.65
          Mean value_function loss: 127.4413
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 73.9573
                       Mean reward: 113.05
               Mean episode length: 146.30
    Episode_Reward/reaching_object: 1.0216
     Episode_Reward/lifting_object: 24.0501
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 23.0000
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 2.17s
                      Time elapsed: 00:22:36
                               ETA: 00:48:37

################################################################################
                     [1m Learning iteration 635/2000 [0m                      

                       Computation: 46570 steps/s (collection: 2.018s, learning 0.093s)
             Mean action noise std: 2.65
          Mean value_function loss: 179.9952
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 73.9593
                       Mean reward: 122.28
               Mean episode length: 138.95
    Episode_Reward/reaching_object: 0.9817
     Episode_Reward/lifting_object: 22.4157
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 27.8333
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 2.11s
                      Time elapsed: 00:22:38
                               ETA: 00:48:35

################################################################################
                     [1m Learning iteration 636/2000 [0m                      

                       Computation: 45218 steps/s (collection: 2.073s, learning 0.101s)
             Mean action noise std: 2.65
          Mean value_function loss: 170.6766
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.9635
                       Mean reward: 128.62
               Mean episode length: 157.85
    Episode_Reward/reaching_object: 0.9540
     Episode_Reward/lifting_object: 21.9834
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 26.4167
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 2.17s
                      Time elapsed: 00:22:40
                               ETA: 00:48:33

################################################################################
                     [1m Learning iteration 637/2000 [0m                      

                       Computation: 45746 steps/s (collection: 2.056s, learning 0.093s)
             Mean action noise std: 2.65
          Mean value_function loss: 152.9575
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 73.9679
                       Mean reward: 98.04
               Mean episode length: 136.30
    Episode_Reward/reaching_object: 0.9859
     Episode_Reward/lifting_object: 23.0517
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 25.8333
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 2.15s
                      Time elapsed: 00:22:42
                               ETA: 00:48:31

################################################################################
                     [1m Learning iteration 638/2000 [0m                      

                       Computation: 43596 steps/s (collection: 2.147s, learning 0.108s)
             Mean action noise std: 2.65
          Mean value_function loss: 141.5396
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 73.9700
                       Mean reward: 113.85
               Mean episode length: 142.64
    Episode_Reward/reaching_object: 0.9632
     Episode_Reward/lifting_object: 22.3300
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 2.9583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 26.0833
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 2.25s
                      Time elapsed: 00:22:45
                               ETA: 00:48:29

################################################################################
                     [1m Learning iteration 639/2000 [0m                      

                       Computation: 44748 steps/s (collection: 2.097s, learning 0.099s)
             Mean action noise std: 2.65
          Mean value_function loss: 131.0422
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 73.9724
                       Mean reward: 113.17
               Mean episode length: 136.09
    Episode_Reward/reaching_object: 0.9765
     Episode_Reward/lifting_object: 22.3709
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 26.8333
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 2.20s
                      Time elapsed: 00:22:47
                               ETA: 00:48:27

################################################################################
                     [1m Learning iteration 640/2000 [0m                      

                       Computation: 42205 steps/s (collection: 2.193s, learning 0.136s)
             Mean action noise std: 2.65
          Mean value_function loss: 127.0557
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 73.9738
                       Mean reward: 127.19
               Mean episode length: 135.66
    Episode_Reward/reaching_object: 0.9779
     Episode_Reward/lifting_object: 23.5889
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 27.9167
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 2.33s
                      Time elapsed: 00:22:49
                               ETA: 00:48:25

################################################################################
                     [1m Learning iteration 641/2000 [0m                      

                       Computation: 39319 steps/s (collection: 2.371s, learning 0.129s)
             Mean action noise std: 2.65
          Mean value_function loss: 118.0602
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 73.9750
                       Mean reward: 117.55
               Mean episode length: 132.81
    Episode_Reward/reaching_object: 0.9696
     Episode_Reward/lifting_object: 23.8127
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 26.0417
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 2.50s
                      Time elapsed: 00:22:52
                               ETA: 00:48:24

################################################################################
                     [1m Learning iteration 642/2000 [0m                      

                       Computation: 41233 steps/s (collection: 2.251s, learning 0.133s)
             Mean action noise std: 2.65
          Mean value_function loss: 116.4745
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 73.9761
                       Mean reward: 131.17
               Mean episode length: 142.60
    Episode_Reward/reaching_object: 1.0009
     Episode_Reward/lifting_object: 25.0277
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 24.6667
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 2.38s
                      Time elapsed: 00:22:54
                               ETA: 00:48:22

################################################################################
                     [1m Learning iteration 643/2000 [0m                      

                       Computation: 42313 steps/s (collection: 2.217s, learning 0.107s)
             Mean action noise std: 2.65
          Mean value_function loss: 126.1966
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 73.9770
                       Mean reward: 122.83
               Mean episode length: 143.35
    Episode_Reward/reaching_object: 0.9497
     Episode_Reward/lifting_object: 23.7566
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 27.6667
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 2.32s
                      Time elapsed: 00:22:56
                               ETA: 00:48:21

################################################################################
                     [1m Learning iteration 644/2000 [0m                      

                       Computation: 43387 steps/s (collection: 2.161s, learning 0.105s)
             Mean action noise std: 2.65
          Mean value_function loss: 128.4955
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 73.9779
                       Mean reward: 125.50
               Mean episode length: 134.28
    Episode_Reward/reaching_object: 0.9769
     Episode_Reward/lifting_object: 24.3447
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 26.0417
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 2.27s
                      Time elapsed: 00:22:59
                               ETA: 00:48:19

################################################################################
                     [1m Learning iteration 645/2000 [0m                      

                       Computation: 42177 steps/s (collection: 2.209s, learning 0.122s)
             Mean action noise std: 2.65
          Mean value_function loss: 134.7693
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 73.9795
                       Mean reward: 112.45
               Mean episode length: 130.69
    Episode_Reward/reaching_object: 0.9655
     Episode_Reward/lifting_object: 24.3169
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 26.3750
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 2.33s
                      Time elapsed: 00:23:01
                               ETA: 00:48:17

################################################################################
                     [1m Learning iteration 646/2000 [0m                      

                       Computation: 42294 steps/s (collection: 2.200s, learning 0.124s)
             Mean action noise std: 2.65
          Mean value_function loss: 147.5859
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 73.9834
                       Mean reward: 141.50
               Mean episode length: 148.43
    Episode_Reward/reaching_object: 0.9798
     Episode_Reward/lifting_object: 24.3558
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 25.0417
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 2.32s
                      Time elapsed: 00:23:03
                               ETA: 00:48:15

################################################################################
                     [1m Learning iteration 647/2000 [0m                      

                       Computation: 30473 steps/s (collection: 2.911s, learning 0.315s)
             Mean action noise std: 2.65
          Mean value_function loss: 153.4418
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 73.9861
                       Mean reward: 125.97
               Mean episode length: 134.73
    Episode_Reward/reaching_object: 0.9625
     Episode_Reward/lifting_object: 23.8452
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 25.7083
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 3.23s
                      Time elapsed: 00:23:06
                               ETA: 00:48:15

################################################################################
                     [1m Learning iteration 648/2000 [0m                      

                       Computation: 34736 steps/s (collection: 2.691s, learning 0.139s)
             Mean action noise std: 2.65
          Mean value_function loss: 146.3920
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 73.9881
                       Mean reward: 147.30
               Mean episode length: 153.72
    Episode_Reward/reaching_object: 0.9850
     Episode_Reward/lifting_object: 24.4225
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 26.9583
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 2.83s
                      Time elapsed: 00:23:09
                               ETA: 00:48:15

################################################################################
                     [1m Learning iteration 649/2000 [0m                      

                       Computation: 44494 steps/s (collection: 2.117s, learning 0.092s)
             Mean action noise std: 2.65
          Mean value_function loss: 142.9385
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 73.9912
                       Mean reward: 123.59
               Mean episode length: 130.19
    Episode_Reward/reaching_object: 0.9508
     Episode_Reward/lifting_object: 24.2205
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 24.7083
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 2.21s
                      Time elapsed: 00:23:12
                               ETA: 00:48:13

################################################################################
                     [1m Learning iteration 650/2000 [0m                      

                       Computation: 37407 steps/s (collection: 2.502s, learning 0.126s)
             Mean action noise std: 2.65
          Mean value_function loss: 143.7384
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 73.9959
                       Mean reward: 130.63
               Mean episode length: 147.99
    Episode_Reward/reaching_object: 0.9832
     Episode_Reward/lifting_object: 25.3533
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 25.8333
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 2.63s
                      Time elapsed: 00:23:14
                               ETA: 00:48:12

################################################################################
                     [1m Learning iteration 651/2000 [0m                      

                       Computation: 38356 steps/s (collection: 2.441s, learning 0.122s)
             Mean action noise std: 2.65
          Mean value_function loss: 142.1690
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 73.9995
                       Mean reward: 145.06
               Mean episode length: 146.72
    Episode_Reward/reaching_object: 1.0081
     Episode_Reward/lifting_object: 26.8805
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 25.0833
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 2.56s
                      Time elapsed: 00:23:17
                               ETA: 00:48:10

################################################################################
                     [1m Learning iteration 652/2000 [0m                      

                       Computation: 42966 steps/s (collection: 2.181s, learning 0.107s)
             Mean action noise std: 2.65
          Mean value_function loss: 146.7010
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 74.0005
                       Mean reward: 141.14
               Mean episode length: 147.23
    Episode_Reward/reaching_object: 0.9982
     Episode_Reward/lifting_object: 26.4625
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 26.5417
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 2.29s
                      Time elapsed: 00:23:19
                               ETA: 00:48:09

################################################################################
                     [1m Learning iteration 653/2000 [0m                      

                       Computation: 43586 steps/s (collection: 2.154s, learning 0.102s)
             Mean action noise std: 2.65
          Mean value_function loss: 133.7626
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 74.0022
                       Mean reward: 133.15
               Mean episode length: 143.52
    Episode_Reward/reaching_object: 0.9837
     Episode_Reward/lifting_object: 26.5787
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 23.5833
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 2.26s
                      Time elapsed: 00:23:21
                               ETA: 00:48:07

################################################################################
                     [1m Learning iteration 654/2000 [0m                      

                       Computation: 43718 steps/s (collection: 2.155s, learning 0.094s)
             Mean action noise std: 2.65
          Mean value_function loss: 147.3426
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 74.0045
                       Mean reward: 132.15
               Mean episode length: 142.52
    Episode_Reward/reaching_object: 1.0017
     Episode_Reward/lifting_object: 26.8985
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0694
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 26.5000
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 2.25s
                      Time elapsed: 00:23:24
                               ETA: 00:48:05

################################################################################
                     [1m Learning iteration 655/2000 [0m                      

                       Computation: 44482 steps/s (collection: 2.108s, learning 0.102s)
             Mean action noise std: 2.65
          Mean value_function loss: 136.0504
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 74.0060
                       Mean reward: 137.28
               Mean episode length: 143.84
    Episode_Reward/reaching_object: 1.0081
     Episode_Reward/lifting_object: 26.9103
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 23.7083
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 2.21s
                      Time elapsed: 00:23:26
                               ETA: 00:48:03

################################################################################
                     [1m Learning iteration 656/2000 [0m                      

                       Computation: 45096 steps/s (collection: 2.068s, learning 0.112s)
             Mean action noise std: 2.66
          Mean value_function loss: 156.6180
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 74.0081
                       Mean reward: 140.86
               Mean episode length: 146.53
    Episode_Reward/reaching_object: 0.9812
     Episode_Reward/lifting_object: 26.6738
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0692
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 25.8750
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 2.18s
                      Time elapsed: 00:23:28
                               ETA: 00:48:01

################################################################################
                     [1m Learning iteration 657/2000 [0m                      

                       Computation: 44870 steps/s (collection: 2.096s, learning 0.095s)
             Mean action noise std: 2.66
          Mean value_function loss: 146.1598
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 74.0095
                       Mean reward: 130.22
               Mean episode length: 143.87
    Episode_Reward/reaching_object: 0.9884
     Episode_Reward/lifting_object: 26.4328
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 26.4167
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 2.19s
                      Time elapsed: 00:23:30
                               ETA: 00:47:59

################################################################################
                     [1m Learning iteration 658/2000 [0m                      

                       Computation: 44723 steps/s (collection: 2.104s, learning 0.095s)
             Mean action noise std: 2.66
          Mean value_function loss: 142.2324
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 74.0103
                       Mean reward: 132.66
               Mean episode length: 142.09
    Episode_Reward/reaching_object: 0.9990
     Episode_Reward/lifting_object: 26.8711
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 25.2917
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 2.20s
                      Time elapsed: 00:23:32
                               ETA: 00:47:57

################################################################################
                     [1m Learning iteration 659/2000 [0m                      

                       Computation: 45512 steps/s (collection: 2.050s, learning 0.110s)
             Mean action noise std: 2.66
          Mean value_function loss: 144.8900
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 74.0109
                       Mean reward: 161.24
               Mean episode length: 157.60
    Episode_Reward/reaching_object: 0.9850
     Episode_Reward/lifting_object: 26.8961
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 24.5417
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 2.16s
                      Time elapsed: 00:23:34
                               ETA: 00:47:54

################################################################################
                     [1m Learning iteration 660/2000 [0m                      

                       Computation: 45459 steps/s (collection: 2.047s, learning 0.116s)
             Mean action noise std: 2.66
          Mean value_function loss: 151.1909
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 74.0125
                       Mean reward: 142.22
               Mean episode length: 139.43
    Episode_Reward/reaching_object: 1.0138
     Episode_Reward/lifting_object: 28.5080
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0721
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 23.5417
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 2.16s
                      Time elapsed: 00:23:37
                               ETA: 00:47:52

################################################################################
                     [1m Learning iteration 661/2000 [0m                      

                       Computation: 43937 steps/s (collection: 2.136s, learning 0.101s)
             Mean action noise std: 2.66
          Mean value_function loss: 162.6593
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 74.0145
                       Mean reward: 148.26
               Mean episode length: 142.49
    Episode_Reward/reaching_object: 0.9938
     Episode_Reward/lifting_object: 27.7844
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 26.1667
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 2.24s
                      Time elapsed: 00:23:39
                               ETA: 00:47:50

################################################################################
                     [1m Learning iteration 662/2000 [0m                      

                       Computation: 44597 steps/s (collection: 2.099s, learning 0.105s)
             Mean action noise std: 2.66
          Mean value_function loss: 147.7366
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 74.0163
                       Mean reward: 161.24
               Mean episode length: 151.22
    Episode_Reward/reaching_object: 0.9964
     Episode_Reward/lifting_object: 28.1817
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0695
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 27.3750
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 2.20s
                      Time elapsed: 00:23:41
                               ETA: 00:47:48

################################################################################
                     [1m Learning iteration 663/2000 [0m                      

                       Computation: 45471 steps/s (collection: 2.069s, learning 0.093s)
             Mean action noise std: 2.66
          Mean value_function loss: 151.1972
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 74.0188
                       Mean reward: 143.70
               Mean episode length: 144.92
    Episode_Reward/reaching_object: 1.0222
     Episode_Reward/lifting_object: 29.0508
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0710
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 24.5833
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 2.16s
                      Time elapsed: 00:23:43
                               ETA: 00:47:46

################################################################################
                     [1m Learning iteration 664/2000 [0m                      

                       Computation: 44385 steps/s (collection: 2.109s, learning 0.105s)
             Mean action noise std: 2.66
          Mean value_function loss: 169.8523
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 74.0211
                       Mean reward: 148.84
               Mean episode length: 141.56
    Episode_Reward/reaching_object: 1.0045
     Episode_Reward/lifting_object: 28.2357
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0692
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 26.1250
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 2.21s
                      Time elapsed: 00:23:45
                               ETA: 00:47:44

################################################################################
                     [1m Learning iteration 665/2000 [0m                      

                       Computation: 44466 steps/s (collection: 2.117s, learning 0.094s)
             Mean action noise std: 2.66
          Mean value_function loss: 155.3424
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 74.0235
                       Mean reward: 131.64
               Mean episode length: 129.22
    Episode_Reward/reaching_object: 0.9712
     Episode_Reward/lifting_object: 27.4330
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 24.4167
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 2.21s
                      Time elapsed: 00:23:48
                               ETA: 00:47:42

################################################################################
                     [1m Learning iteration 666/2000 [0m                      

                       Computation: 25789 steps/s (collection: 3.704s, learning 0.108s)
             Mean action noise std: 2.66
          Mean value_function loss: 142.2603
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 74.0268
                       Mean reward: 143.78
               Mean episode length: 139.15
    Episode_Reward/reaching_object: 1.0093
     Episode_Reward/lifting_object: 28.6912
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 26.4583
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 3.81s
                      Time elapsed: 00:23:51
                               ETA: 00:47:43

################################################################################
                     [1m Learning iteration 667/2000 [0m                      

                       Computation: 13872 steps/s (collection: 6.964s, learning 0.122s)
             Mean action noise std: 2.66
          Mean value_function loss: 158.7825
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 74.0290
                       Mean reward: 145.86
               Mean episode length: 140.23
    Episode_Reward/reaching_object: 1.0199
     Episode_Reward/lifting_object: 29.6363
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 25.3750
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 7.09s
                      Time elapsed: 00:23:59
                               ETA: 00:47:51

################################################################################
                     [1m Learning iteration 668/2000 [0m                      

                       Computation: 14267 steps/s (collection: 6.780s, learning 0.110s)
             Mean action noise std: 2.66
          Mean value_function loss: 159.1557
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 74.0299
                       Mean reward: 158.38
               Mean episode length: 147.29
    Episode_Reward/reaching_object: 1.0086
     Episode_Reward/lifting_object: 29.2160
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 25.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 6.89s
                      Time elapsed: 00:24:05
                               ETA: 00:47:58

################################################################################
                     [1m Learning iteration 669/2000 [0m                      

                       Computation: 14559 steps/s (collection: 6.610s, learning 0.142s)
             Mean action noise std: 2.66
          Mean value_function loss: 152.1154
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 74.0307
                       Mean reward: 146.18
               Mean episode length: 143.10
    Episode_Reward/reaching_object: 1.0067
     Episode_Reward/lifting_object: 29.2588
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 24.7500
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 6.75s
                      Time elapsed: 00:24:12
                               ETA: 00:48:05

################################################################################
                     [1m Learning iteration 670/2000 [0m                      

                       Computation: 13904 steps/s (collection: 6.941s, learning 0.129s)
             Mean action noise std: 2.66
          Mean value_function loss: 156.4534
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 74.0325
                       Mean reward: 154.08
               Mean episode length: 141.56
    Episode_Reward/reaching_object: 0.9923
     Episode_Reward/lifting_object: 29.1732
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 25.6667
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 7.07s
                      Time elapsed: 00:24:19
                               ETA: 00:48:13

################################################################################
                     [1m Learning iteration 671/2000 [0m                      

                       Computation: 14042 steps/s (collection: 6.857s, learning 0.143s)
             Mean action noise std: 2.66
          Mean value_function loss: 160.0080
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 74.0340
                       Mean reward: 154.24
               Mean episode length: 141.06
    Episode_Reward/reaching_object: 1.0115
     Episode_Reward/lifting_object: 30.5055
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 26.3750
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 7.00s
                      Time elapsed: 00:24:26
                               ETA: 00:48:20

################################################################################
                     [1m Learning iteration 672/2000 [0m                      

                       Computation: 14191 steps/s (collection: 6.808s, learning 0.119s)
             Mean action noise std: 2.66
          Mean value_function loss: 182.1877
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 74.0351
                       Mean reward: 158.79
               Mean episode length: 143.00
    Episode_Reward/reaching_object: 1.0102
     Episode_Reward/lifting_object: 30.9677
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 26.2083
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 6.93s
                      Time elapsed: 00:24:33
                               ETA: 00:48:27

################################################################################
                     [1m Learning iteration 673/2000 [0m                      

                       Computation: 14407 steps/s (collection: 6.709s, learning 0.114s)
             Mean action noise std: 2.66
          Mean value_function loss: 179.1966
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 74.0355
                       Mean reward: 163.42
               Mean episode length: 145.80
    Episode_Reward/reaching_object: 1.0351
     Episode_Reward/lifting_object: 30.9674
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0702
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 24.6667
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 6.82s
                      Time elapsed: 00:24:40
                               ETA: 00:48:34

################################################################################
                     [1m Learning iteration 674/2000 [0m                      

                       Computation: 14017 steps/s (collection: 6.891s, learning 0.122s)
             Mean action noise std: 2.66
          Mean value_function loss: 177.3247
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 74.0370
                       Mean reward: 156.36
               Mean episode length: 144.62
    Episode_Reward/reaching_object: 1.0008
     Episode_Reward/lifting_object: 30.2716
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 26.2500
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 7.01s
                      Time elapsed: 00:24:47
                               ETA: 00:48:42

################################################################################
                     [1m Learning iteration 675/2000 [0m                      

                       Computation: 23916 steps/s (collection: 4.000s, learning 0.111s)
             Mean action noise std: 2.66
          Mean value_function loss: 166.5168
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 74.0385
                       Mean reward: 145.91
               Mean episode length: 131.38
    Episode_Reward/reaching_object: 0.9852
     Episode_Reward/lifting_object: 29.9715
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 27.1667
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 4.11s
                      Time elapsed: 00:24:51
                               ETA: 00:48:43

################################################################################
                     [1m Learning iteration 676/2000 [0m                      

                       Computation: 46219 steps/s (collection: 2.033s, learning 0.094s)
             Mean action noise std: 2.66
          Mean value_function loss: 159.4760
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 74.0387
                       Mean reward: 186.13
               Mean episode length: 158.96
    Episode_Reward/reaching_object: 1.0172
     Episode_Reward/lifting_object: 31.1531
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0694
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 25.9167
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 2.13s
                      Time elapsed: 00:24:53
                               ETA: 00:48:41

################################################################################
                     [1m Learning iteration 677/2000 [0m                      

                       Computation: 46917 steps/s (collection: 2.007s, learning 0.088s)
             Mean action noise std: 2.66
          Mean value_function loss: 157.7195
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 74.0391
                       Mean reward: 156.16
               Mean episode length: 135.06
    Episode_Reward/reaching_object: 0.9890
     Episode_Reward/lifting_object: 30.6663
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 24.3750
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 2.10s
                      Time elapsed: 00:24:55
                               ETA: 00:48:38

################################################################################
                     [1m Learning iteration 678/2000 [0m                      

                       Computation: 45178 steps/s (collection: 2.079s, learning 0.097s)
             Mean action noise std: 2.66
          Mean value_function loss: 170.2472
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 74.0391
                       Mean reward: 156.49
               Mean episode length: 131.36
    Episode_Reward/reaching_object: 0.9904
     Episode_Reward/lifting_object: 31.4547
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 26.2500
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 2.18s
                      Time elapsed: 00:24:58
                               ETA: 00:48:36

################################################################################
                     [1m Learning iteration 679/2000 [0m                      

                       Computation: 44685 steps/s (collection: 2.108s, learning 0.092s)
             Mean action noise std: 2.66
          Mean value_function loss: 158.5789
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 74.0392
                       Mean reward: 177.36
               Mean episode length: 147.02
    Episode_Reward/reaching_object: 0.9918
     Episode_Reward/lifting_object: 31.6439
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0686
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 26.6667
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 2.20s
                      Time elapsed: 00:25:00
                               ETA: 00:48:34

################################################################################
                     [1m Learning iteration 680/2000 [0m                      

                       Computation: 46265 steps/s (collection: 2.028s, learning 0.097s)
             Mean action noise std: 2.66
          Mean value_function loss: 164.1682
               Mean surrogate loss: 0.0105
                 Mean entropy loss: 74.0392
                       Mean reward: 179.92
               Mean episode length: 150.42
    Episode_Reward/reaching_object: 0.9996
     Episode_Reward/lifting_object: 32.0928
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 25.2917
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 2.12s
                      Time elapsed: 00:25:02
                               ETA: 00:48:32

################################################################################
                     [1m Learning iteration 681/2000 [0m                      

                       Computation: 46054 steps/s (collection: 2.031s, learning 0.104s)
             Mean action noise std: 2.66
          Mean value_function loss: 181.8176
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 74.0393
                       Mean reward: 168.57
               Mean episode length: 143.78
    Episode_Reward/reaching_object: 1.0167
     Episode_Reward/lifting_object: 32.6482
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0701
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 25.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 2.13s
                      Time elapsed: 00:25:04
                               ETA: 00:48:29

################################################################################
                     [1m Learning iteration 682/2000 [0m                      

                       Computation: 45656 steps/s (collection: 2.060s, learning 0.094s)
             Mean action noise std: 2.66
          Mean value_function loss: 167.6321
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 74.0393
                       Mean reward: 174.34
               Mean episode length: 148.62
    Episode_Reward/reaching_object: 1.0116
     Episode_Reward/lifting_object: 32.8724
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 26.2083
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 2.15s
                      Time elapsed: 00:25:06
                               ETA: 00:48:27

################################################################################
                     [1m Learning iteration 683/2000 [0m                      

                       Computation: 46274 steps/s (collection: 2.035s, learning 0.090s)
             Mean action noise std: 2.66
          Mean value_function loss: 168.3527
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 74.0394
                       Mean reward: 153.32
               Mean episode length: 134.50
    Episode_Reward/reaching_object: 0.9815
     Episode_Reward/lifting_object: 31.4382
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 26.5417
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 2.12s
                      Time elapsed: 00:25:08
                               ETA: 00:48:25

################################################################################
                     [1m Learning iteration 684/2000 [0m                      

                       Computation: 46252 steps/s (collection: 2.029s, learning 0.096s)
             Mean action noise std: 2.66
          Mean value_function loss: 175.6975
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 74.0394
                       Mean reward: 157.94
               Mean episode length: 133.41
    Episode_Reward/reaching_object: 0.9990
     Episode_Reward/lifting_object: 32.8054
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 27.5000
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 2.13s
                      Time elapsed: 00:25:10
                               ETA: 00:48:22

################################################################################
                     [1m Learning iteration 685/2000 [0m                      

                       Computation: 45899 steps/s (collection: 2.043s, learning 0.099s)
             Mean action noise std: 2.66
          Mean value_function loss: 200.3226
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 74.0395
                       Mean reward: 162.11
               Mean episode length: 136.87
    Episode_Reward/reaching_object: 0.9847
     Episode_Reward/lifting_object: 32.3083
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 27.7083
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 2.14s
                      Time elapsed: 00:25:13
                               ETA: 00:48:20

################################################################################
                     [1m Learning iteration 686/2000 [0m                      

                       Computation: 46291 steps/s (collection: 2.032s, learning 0.092s)
             Mean action noise std: 2.66
          Mean value_function loss: 179.3583
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 74.0396
                       Mean reward: 174.47
               Mean episode length: 134.80
    Episode_Reward/reaching_object: 0.9754
     Episode_Reward/lifting_object: 32.1974
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 25.7917
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 2.12s
                      Time elapsed: 00:25:15
                               ETA: 00:48:17

################################################################################
                     [1m Learning iteration 687/2000 [0m                      

                       Computation: 46770 steps/s (collection: 2.014s, learning 0.088s)
             Mean action noise std: 2.66
          Mean value_function loss: 183.7352
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 74.0398
                       Mean reward: 158.80
               Mean episode length: 129.09
    Episode_Reward/reaching_object: 0.9745
     Episode_Reward/lifting_object: 32.3343
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 28.2500
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 2.10s
                      Time elapsed: 00:25:17
                               ETA: 00:48:15

################################################################################
                     [1m Learning iteration 688/2000 [0m                      

                       Computation: 45602 steps/s (collection: 2.044s, learning 0.111s)
             Mean action noise std: 2.66
          Mean value_function loss: 167.3516
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 74.0408
                       Mean reward: 162.34
               Mean episode length: 128.18
    Episode_Reward/reaching_object: 0.9783
     Episode_Reward/lifting_object: 32.5428
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 28.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 2.16s
                      Time elapsed: 00:25:19
                               ETA: 00:48:13

################################################################################
                     [1m Learning iteration 689/2000 [0m                      

                       Computation: 45757 steps/s (collection: 2.041s, learning 0.107s)
             Mean action noise std: 2.66
          Mean value_function loss: 175.5959
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 74.0414
                       Mean reward: 173.02
               Mean episode length: 136.33
    Episode_Reward/reaching_object: 0.9865
     Episode_Reward/lifting_object: 33.0785
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 25.8333
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 2.15s
                      Time elapsed: 00:25:21
                               ETA: 00:48:10

################################################################################
                     [1m Learning iteration 690/2000 [0m                      

                       Computation: 45110 steps/s (collection: 2.084s, learning 0.096s)
             Mean action noise std: 2.66
          Mean value_function loss: 192.3409
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 74.0420
                       Mean reward: 146.34
               Mean episode length: 125.22
    Episode_Reward/reaching_object: 1.0177
     Episode_Reward/lifting_object: 33.9857
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 28.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 2.18s
                      Time elapsed: 00:25:23
                               ETA: 00:48:08

################################################################################
                     [1m Learning iteration 691/2000 [0m                      

                       Computation: 45523 steps/s (collection: 2.065s, learning 0.094s)
             Mean action noise std: 2.66
          Mean value_function loss: 180.3297
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.0433
                       Mean reward: 171.19
               Mean episode length: 135.75
    Episode_Reward/reaching_object: 0.9468
     Episode_Reward/lifting_object: 31.5432
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 27.9583
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 2.16s
                      Time elapsed: 00:25:25
                               ETA: 00:48:06

################################################################################
                     [1m Learning iteration 692/2000 [0m                      

                       Computation: 45813 steps/s (collection: 2.045s, learning 0.100s)
             Mean action noise std: 2.66
          Mean value_function loss: 181.6398
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 74.0450
                       Mean reward: 161.88
               Mean episode length: 133.74
    Episode_Reward/reaching_object: 0.9431
     Episode_Reward/lifting_object: 30.6354
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 28.0417
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 2.15s
                      Time elapsed: 00:25:28
                               ETA: 00:48:04

################################################################################
                     [1m Learning iteration 693/2000 [0m                      

                       Computation: 43107 steps/s (collection: 2.171s, learning 0.109s)
             Mean action noise std: 2.66
          Mean value_function loss: 206.1166
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 74.0477
                       Mean reward: 186.07
               Mean episode length: 146.50
    Episode_Reward/reaching_object: 0.9838
     Episode_Reward/lifting_object: 32.6862
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 27.2917
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 2.28s
                      Time elapsed: 00:25:30
                               ETA: 00:48:02

################################################################################
                     [1m Learning iteration 694/2000 [0m                      

                       Computation: 45561 steps/s (collection: 2.061s, learning 0.097s)
             Mean action noise std: 2.66
          Mean value_function loss: 207.8843
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 74.0500
                       Mean reward: 158.61
               Mean episode length: 133.96
    Episode_Reward/reaching_object: 0.9684
     Episode_Reward/lifting_object: 31.6126
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 26.4167
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 2.16s
                      Time elapsed: 00:25:32
                               ETA: 00:47:59

################################################################################
                     [1m Learning iteration 695/2000 [0m                      

                       Computation: 46753 steps/s (collection: 2.009s, learning 0.094s)
             Mean action noise std: 2.66
          Mean value_function loss: 238.0389
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 74.0525
                       Mean reward: 169.37
               Mean episode length: 132.71
    Episode_Reward/reaching_object: 0.9347
     Episode_Reward/lifting_object: 31.6877
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 29.9167
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 2.10s
                      Time elapsed: 00:25:34
                               ETA: 00:47:57

################################################################################
                     [1m Learning iteration 696/2000 [0m                      

                       Computation: 46382 steps/s (collection: 2.026s, learning 0.093s)
             Mean action noise std: 2.66
          Mean value_function loss: 192.7666
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 74.0533
                       Mean reward: 179.63
               Mean episode length: 138.36
    Episode_Reward/reaching_object: 0.9732
     Episode_Reward/lifting_object: 32.8368
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 28.0417
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 2.12s
                      Time elapsed: 00:25:36
                               ETA: 00:47:54

################################################################################
                     [1m Learning iteration 697/2000 [0m                      

                       Computation: 46339 steps/s (collection: 2.033s, learning 0.089s)
             Mean action noise std: 2.66
          Mean value_function loss: 201.5022
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 74.0538
                       Mean reward: 162.98
               Mean episode length: 133.68
    Episode_Reward/reaching_object: 0.9596
     Episode_Reward/lifting_object: 32.4750
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 28.7500
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 2.12s
                      Time elapsed: 00:25:38
                               ETA: 00:47:52

################################################################################
                     [1m Learning iteration 698/2000 [0m                      

                       Computation: 46605 steps/s (collection: 2.017s, learning 0.093s)
             Mean action noise std: 2.66
          Mean value_function loss: 198.3793
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.0547
                       Mean reward: 160.53
               Mean episode length: 125.85
    Episode_Reward/reaching_object: 0.9343
     Episode_Reward/lifting_object: 30.9859
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 27.5833
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 2.11s
                      Time elapsed: 00:25:40
                               ETA: 00:47:50

################################################################################
                     [1m Learning iteration 699/2000 [0m                      

                       Computation: 44361 steps/s (collection: 2.123s, learning 0.093s)
             Mean action noise std: 2.66
          Mean value_function loss: 228.2723
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 74.0573
                       Mean reward: 164.90
               Mean episode length: 131.25
    Episode_Reward/reaching_object: 0.9784
     Episode_Reward/lifting_object: 33.6214
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 28.3750
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 2.22s
                      Time elapsed: 00:25:43
                               ETA: 00:47:48

################################################################################
                     [1m Learning iteration 700/2000 [0m                      

                       Computation: 44940 steps/s (collection: 2.095s, learning 0.092s)
             Mean action noise std: 2.66
          Mean value_function loss: 204.9160
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 74.0601
                       Mean reward: 167.91
               Mean episode length: 134.21
    Episode_Reward/reaching_object: 0.9730
     Episode_Reward/lifting_object: 33.3560
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 26.5417
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 2.19s
                      Time elapsed: 00:25:45
                               ETA: 00:47:45

################################################################################
                     [1m Learning iteration 701/2000 [0m                      

                       Computation: 44995 steps/s (collection: 2.087s, learning 0.098s)
             Mean action noise std: 2.66
          Mean value_function loss: 211.2494
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 74.0617
                       Mean reward: 179.65
               Mean episode length: 136.75
    Episode_Reward/reaching_object: 0.9711
     Episode_Reward/lifting_object: 33.5647
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 28.4167
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 2.18s
                      Time elapsed: 00:25:47
                               ETA: 00:47:43

################################################################################
                     [1m Learning iteration 702/2000 [0m                      

                       Computation: 43898 steps/s (collection: 2.137s, learning 0.103s)
             Mean action noise std: 2.66
          Mean value_function loss: 187.5939
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 74.0629
                       Mean reward: 161.72
               Mean episode length: 132.79
    Episode_Reward/reaching_object: 0.9692
     Episode_Reward/lifting_object: 33.3990
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 27.0417
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 2.24s
                      Time elapsed: 00:25:49
                               ETA: 00:47:41

################################################################################
                     [1m Learning iteration 703/2000 [0m                      

                       Computation: 45349 steps/s (collection: 2.052s, learning 0.116s)
             Mean action noise std: 2.66
          Mean value_function loss: 219.3186
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 74.0638
                       Mean reward: 156.38
               Mean episode length: 119.40
    Episode_Reward/reaching_object: 0.9839
     Episode_Reward/lifting_object: 34.8342
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 26.2500
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 2.17s
                      Time elapsed: 00:25:51
                               ETA: 00:47:39

################################################################################
                     [1m Learning iteration 704/2000 [0m                      

                       Computation: 45831 steps/s (collection: 2.054s, learning 0.091s)
             Mean action noise std: 2.66
          Mean value_function loss: 200.6062
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 74.0652
                       Mean reward: 185.14
               Mean episode length: 135.59
    Episode_Reward/reaching_object: 0.9956
     Episode_Reward/lifting_object: 35.0121
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 25.1250
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 2.14s
                      Time elapsed: 00:25:54
                               ETA: 00:47:36

################################################################################
                     [1m Learning iteration 705/2000 [0m                      

                       Computation: 45442 steps/s (collection: 2.053s, learning 0.110s)
             Mean action noise std: 2.66
          Mean value_function loss: 199.5226
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 74.0661
                       Mean reward: 192.92
               Mean episode length: 145.03
    Episode_Reward/reaching_object: 1.0065
     Episode_Reward/lifting_object: 35.7683
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 24.8333
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 2.16s
                      Time elapsed: 00:25:56
                               ETA: 00:47:34

################################################################################
                     [1m Learning iteration 706/2000 [0m                      

                       Computation: 45757 steps/s (collection: 2.041s, learning 0.108s)
             Mean action noise std: 2.66
          Mean value_function loss: 217.4852
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 74.0667
                       Mean reward: 195.24
               Mean episode length: 143.21
    Episode_Reward/reaching_object: 1.0408
     Episode_Reward/lifting_object: 37.0189
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 24.0417
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 2.15s
                      Time elapsed: 00:25:58
                               ETA: 00:47:32

################################################################################
                     [1m Learning iteration 707/2000 [0m                      

                       Computation: 45694 steps/s (collection: 2.054s, learning 0.097s)
             Mean action noise std: 2.66
          Mean value_function loss: 207.4667
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 74.0674
                       Mean reward: 178.98
               Mean episode length: 139.43
    Episode_Reward/reaching_object: 1.0166
     Episode_Reward/lifting_object: 36.0871
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 26.2083
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 2.15s
                      Time elapsed: 00:26:00
                               ETA: 00:47:29

################################################################################
                     [1m Learning iteration 708/2000 [0m                      

                       Computation: 46168 steps/s (collection: 2.030s, learning 0.099s)
             Mean action noise std: 2.66
          Mean value_function loss: 202.4075
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 74.0693
                       Mean reward: 196.26
               Mean episode length: 142.17
    Episode_Reward/reaching_object: 1.0577
     Episode_Reward/lifting_object: 37.7934
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 24.8750
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 2.13s
                      Time elapsed: 00:26:02
                               ETA: 00:47:27

################################################################################
                     [1m Learning iteration 709/2000 [0m                      

                       Computation: 45839 steps/s (collection: 2.050s, learning 0.095s)
             Mean action noise std: 2.66
          Mean value_function loss: 217.0404
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 74.0706
                       Mean reward: 209.33
               Mean episode length: 146.11
    Episode_Reward/reaching_object: 1.0684
     Episode_Reward/lifting_object: 38.8313
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 26.7917
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 2.14s
                      Time elapsed: 00:26:04
                               ETA: 00:47:25

################################################################################
                     [1m Learning iteration 710/2000 [0m                      

                       Computation: 45782 steps/s (collection: 2.052s, learning 0.096s)
             Mean action noise std: 2.66
          Mean value_function loss: 219.6374
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 74.0709
                       Mean reward: 192.35
               Mean episode length: 142.39
    Episode_Reward/reaching_object: 1.0578
     Episode_Reward/lifting_object: 38.5299
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 24.1667
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 2.15s
                      Time elapsed: 00:26:06
                               ETA: 00:47:22

################################################################################
                     [1m Learning iteration 711/2000 [0m                      

                       Computation: 45883 steps/s (collection: 2.048s, learning 0.094s)
             Mean action noise std: 2.66
          Mean value_function loss: 189.6521
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 74.0711
                       Mean reward: 195.77
               Mean episode length: 142.04
    Episode_Reward/reaching_object: 1.0644
     Episode_Reward/lifting_object: 39.0446
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0692
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 25.0833
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 2.14s
                      Time elapsed: 00:26:09
                               ETA: 00:47:20

################################################################################
                     [1m Learning iteration 712/2000 [0m                      

                       Computation: 45313 steps/s (collection: 2.074s, learning 0.095s)
             Mean action noise std: 2.66
          Mean value_function loss: 197.8836
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 74.0711
                       Mean reward: 201.35
               Mean episode length: 145.31
    Episode_Reward/reaching_object: 1.0660
     Episode_Reward/lifting_object: 38.9894
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 24.1667
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 2.17s
                      Time elapsed: 00:26:11
                               ETA: 00:47:18

################################################################################
                     [1m Learning iteration 713/2000 [0m                      

                       Computation: 45942 steps/s (collection: 2.050s, learning 0.090s)
             Mean action noise std: 2.66
          Mean value_function loss: 205.7695
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 74.0715
                       Mean reward: 197.61
               Mean episode length: 143.39
    Episode_Reward/reaching_object: 1.0721
     Episode_Reward/lifting_object: 39.8972
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0692
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 23.4583
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 2.14s
                      Time elapsed: 00:26:13
                               ETA: 00:47:16

################################################################################
                     [1m Learning iteration 714/2000 [0m                      

                       Computation: 45600 steps/s (collection: 2.058s, learning 0.098s)
             Mean action noise std: 2.66
          Mean value_function loss: 204.6911
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 74.0717
                       Mean reward: 213.70
               Mean episode length: 148.50
    Episode_Reward/reaching_object: 1.0850
     Episode_Reward/lifting_object: 40.8112
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 23.8333
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 2.16s
                      Time elapsed: 00:26:15
                               ETA: 00:47:13

################################################################################
                     [1m Learning iteration 715/2000 [0m                      

                       Computation: 46108 steps/s (collection: 2.039s, learning 0.093s)
             Mean action noise std: 2.66
          Mean value_function loss: 206.9629
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 74.0719
                       Mean reward: 227.49
               Mean episode length: 154.80
    Episode_Reward/reaching_object: 1.0977
     Episode_Reward/lifting_object: 40.6062
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 23.4167
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 2.13s
                      Time elapsed: 00:26:17
                               ETA: 00:47:11

################################################################################
                     [1m Learning iteration 716/2000 [0m                      

                       Computation: 45486 steps/s (collection: 2.053s, learning 0.109s)
             Mean action noise std: 2.66
          Mean value_function loss: 200.3475
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 74.0720
                       Mean reward: 201.96
               Mean episode length: 145.41
    Episode_Reward/reaching_object: 1.0479
     Episode_Reward/lifting_object: 39.0910
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 25.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 2.16s
                      Time elapsed: 00:26:19
                               ETA: 00:47:09

################################################################################
                     [1m Learning iteration 717/2000 [0m                      

                       Computation: 44500 steps/s (collection: 2.088s, learning 0.121s)
             Mean action noise std: 2.66
          Mean value_function loss: 210.1345
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 74.0726
                       Mean reward: 189.44
               Mean episode length: 137.67
    Episode_Reward/reaching_object: 1.0726
     Episode_Reward/lifting_object: 38.8644
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 23.7917
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 2.21s
                      Time elapsed: 00:26:22
                               ETA: 00:47:07

################################################################################
                     [1m Learning iteration 718/2000 [0m                      

                       Computation: 45832 steps/s (collection: 2.055s, learning 0.090s)
             Mean action noise std: 2.66
          Mean value_function loss: 219.7282
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 74.0730
                       Mean reward: 221.07
               Mean episode length: 150.80
    Episode_Reward/reaching_object: 1.0941
     Episode_Reward/lifting_object: 41.4618
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0714
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 22.8333
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 2.14s
                      Time elapsed: 00:26:24
                               ETA: 00:47:04

################################################################################
                     [1m Learning iteration 719/2000 [0m                      

                       Computation: 45577 steps/s (collection: 2.059s, learning 0.098s)
             Mean action noise std: 2.66
          Mean value_function loss: 218.6963
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 74.0730
                       Mean reward: 197.00
               Mean episode length: 144.35
    Episode_Reward/reaching_object: 1.0687
     Episode_Reward/lifting_object: 40.4244
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 25.4583
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 2.16s
                      Time elapsed: 00:26:26
                               ETA: 00:47:02

################################################################################
                     [1m Learning iteration 720/2000 [0m                      

                       Computation: 45762 steps/s (collection: 2.056s, learning 0.092s)
             Mean action noise std: 2.66
          Mean value_function loss: 226.8814
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 74.0728
                       Mean reward: 221.19
               Mean episode length: 150.46
    Episode_Reward/reaching_object: 1.1210
     Episode_Reward/lifting_object: 43.7274
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0726
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 24.9583
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 2.15s
                      Time elapsed: 00:26:28
                               ETA: 00:47:00

################################################################################
                     [1m Learning iteration 721/2000 [0m                      

                       Computation: 45818 steps/s (collection: 2.046s, learning 0.099s)
             Mean action noise std: 2.66
          Mean value_function loss: 217.5830
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 74.0728
                       Mean reward: 219.07
               Mean episode length: 149.05
    Episode_Reward/reaching_object: 1.1014
     Episode_Reward/lifting_object: 41.8955
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0722
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 22.7083
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 2.15s
                      Time elapsed: 00:26:30
                               ETA: 00:46:57

################################################################################
                     [1m Learning iteration 722/2000 [0m                      

                       Computation: 45510 steps/s (collection: 2.065s, learning 0.095s)
             Mean action noise std: 2.66
          Mean value_function loss: 222.6287
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 74.0729
                       Mean reward: 208.18
               Mean episode length: 142.72
    Episode_Reward/reaching_object: 1.0667
     Episode_Reward/lifting_object: 41.1707
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0695
      Episode_Termination/time_out: 4.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 24.1250
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 2.16s
                      Time elapsed: 00:26:32
                               ETA: 00:46:55

################################################################################
                     [1m Learning iteration 723/2000 [0m                      

                       Computation: 45514 steps/s (collection: 2.072s, learning 0.088s)
             Mean action noise std: 2.66
          Mean value_function loss: 244.8745
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 74.0731
                       Mean reward: 228.97
               Mean episode length: 147.69
    Episode_Reward/reaching_object: 1.0919
     Episode_Reward/lifting_object: 43.6355
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 24.5833
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 2.16s
                      Time elapsed: 00:26:34
                               ETA: 00:46:53

################################################################################
                     [1m Learning iteration 724/2000 [0m                      

                       Computation: 45835 steps/s (collection: 2.054s, learning 0.091s)
             Mean action noise std: 2.66
          Mean value_function loss: 251.6475
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 74.0733
                       Mean reward: 205.83
               Mean episode length: 138.93
    Episode_Reward/reaching_object: 1.0870
     Episode_Reward/lifting_object: 42.6104
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 24.8333
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 2.14s
                      Time elapsed: 00:26:37
                               ETA: 00:46:50

################################################################################
                     [1m Learning iteration 725/2000 [0m                      

                       Computation: 42755 steps/s (collection: 2.202s, learning 0.097s)
             Mean action noise std: 2.66
          Mean value_function loss: 229.8615
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 74.0734
                       Mean reward: 198.40
               Mean episode length: 134.85
    Episode_Reward/reaching_object: 1.0564
     Episode_Reward/lifting_object: 41.7423
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 24.5417
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 2.30s
                      Time elapsed: 00:26:39
                               ETA: 00:46:48

################################################################################
                     [1m Learning iteration 726/2000 [0m                      

                       Computation: 44512 steps/s (collection: 2.113s, learning 0.095s)
             Mean action noise std: 2.66
          Mean value_function loss: 235.2947
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 74.0734
                       Mean reward: 225.22
               Mean episode length: 146.43
    Episode_Reward/reaching_object: 1.0717
     Episode_Reward/lifting_object: 42.7902
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 3.8750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 24.0417
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 2.21s
                      Time elapsed: 00:26:41
                               ETA: 00:46:46

################################################################################
                     [1m Learning iteration 727/2000 [0m                      

                       Computation: 45591 steps/s (collection: 2.061s, learning 0.096s)
             Mean action noise std: 2.66
          Mean value_function loss: 230.1543
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 74.0738
                       Mean reward: 253.49
               Mean episode length: 157.77
    Episode_Reward/reaching_object: 1.0866
     Episode_Reward/lifting_object: 42.8993
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0704
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 26.3333
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 2.16s
                      Time elapsed: 00:26:43
                               ETA: 00:46:44

################################################################################
                     [1m Learning iteration 728/2000 [0m                      

                       Computation: 45785 steps/s (collection: 2.055s, learning 0.092s)
             Mean action noise std: 2.66
          Mean value_function loss: 274.3233
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 74.0747
                       Mean reward: 203.42
               Mean episode length: 141.25
    Episode_Reward/reaching_object: 1.0447
     Episode_Reward/lifting_object: 41.2680
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 26.9167
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 2.15s
                      Time elapsed: 00:26:45
                               ETA: 00:46:42

################################################################################
                     [1m Learning iteration 729/2000 [0m                      

                       Computation: 46128 steps/s (collection: 2.037s, learning 0.094s)
             Mean action noise std: 2.66
          Mean value_function loss: 254.8518
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 74.0750
                       Mean reward: 230.00
               Mean episode length: 146.58
    Episode_Reward/reaching_object: 1.0277
     Episode_Reward/lifting_object: 40.7196
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 27.8750
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 2.13s
                      Time elapsed: 00:26:48
                               ETA: 00:46:39

################################################################################
                     [1m Learning iteration 730/2000 [0m                      

                       Computation: 45416 steps/s (collection: 2.052s, learning 0.113s)
             Mean action noise std: 2.66
          Mean value_function loss: 245.2859
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.0755
                       Mean reward: 210.73
               Mean episode length: 140.69
    Episode_Reward/reaching_object: 1.0569
     Episode_Reward/lifting_object: 43.1582
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 27.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 2.16s
                      Time elapsed: 00:26:50
                               ETA: 00:46:37

################################################################################
                     [1m Learning iteration 731/2000 [0m                      

                       Computation: 45695 steps/s (collection: 2.043s, learning 0.109s)
             Mean action noise std: 2.66
          Mean value_function loss: 272.6080
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 74.0761
                       Mean reward: 237.96
               Mean episode length: 146.19
    Episode_Reward/reaching_object: 1.0164
     Episode_Reward/lifting_object: 40.6860
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.6250
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 2.15s
                      Time elapsed: 00:26:52
                               ETA: 00:46:35

################################################################################
                     [1m Learning iteration 732/2000 [0m                      

                       Computation: 45555 steps/s (collection: 2.064s, learning 0.094s)
             Mean action noise std: 2.66
          Mean value_function loss: 273.3018
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 74.0779
                       Mean reward: 221.13
               Mean episode length: 142.45
    Episode_Reward/reaching_object: 1.0306
     Episode_Reward/lifting_object: 42.6944
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 26.4583
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 2.16s
                      Time elapsed: 00:26:54
                               ETA: 00:46:32

################################################################################
                     [1m Learning iteration 733/2000 [0m                      

                       Computation: 45172 steps/s (collection: 2.081s, learning 0.095s)
             Mean action noise std: 2.66
          Mean value_function loss: 256.0849
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 74.0797
                       Mean reward: 228.58
               Mean episode length: 142.47
    Episode_Reward/reaching_object: 1.0477
     Episode_Reward/lifting_object: 43.0809
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 23.7083
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 2.18s
                      Time elapsed: 00:26:56
                               ETA: 00:46:30

################################################################################
                     [1m Learning iteration 734/2000 [0m                      

                       Computation: 45219 steps/s (collection: 2.072s, learning 0.102s)
             Mean action noise std: 2.66
          Mean value_function loss: 256.9275
               Mean surrogate loss: 0.0127
                 Mean entropy loss: 74.0804
                       Mean reward: 225.55
               Mean episode length: 140.40
    Episode_Reward/reaching_object: 1.0409
     Episode_Reward/lifting_object: 43.7938
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 26.3333
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 2.17s
                      Time elapsed: 00:26:58
                               ETA: 00:46:28

################################################################################
                     [1m Learning iteration 735/2000 [0m                      

                       Computation: 45167 steps/s (collection: 2.068s, learning 0.108s)
             Mean action noise std: 2.66
          Mean value_function loss: 268.7127
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 74.0805
                       Mean reward: 216.02
               Mean episode length: 139.49
    Episode_Reward/reaching_object: 1.0407
     Episode_Reward/lifting_object: 43.0818
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 24.6250
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 2.18s
                      Time elapsed: 00:27:01
                               ETA: 00:46:26

################################################################################
                     [1m Learning iteration 736/2000 [0m                      

                       Computation: 45244 steps/s (collection: 2.079s, learning 0.094s)
             Mean action noise std: 2.66
          Mean value_function loss: 252.5678
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 74.0807
                       Mean reward: 218.92
               Mean episode length: 138.03
    Episode_Reward/reaching_object: 1.0462
     Episode_Reward/lifting_object: 45.1612
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.2083
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 2.17s
                      Time elapsed: 00:27:03
                               ETA: 00:46:23

################################################################################
                     [1m Learning iteration 737/2000 [0m                      

                       Computation: 45793 steps/s (collection: 2.059s, learning 0.088s)
             Mean action noise std: 2.66
          Mean value_function loss: 272.7379
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 74.0808
                       Mean reward: 210.07
               Mean episode length: 135.40
    Episode_Reward/reaching_object: 1.0462
     Episode_Reward/lifting_object: 43.5940
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 26.6667
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 2.15s
                      Time elapsed: 00:27:05
                               ETA: 00:46:21

################################################################################
                     [1m Learning iteration 738/2000 [0m                      

                       Computation: 45937 steps/s (collection: 2.047s, learning 0.093s)
             Mean action noise std: 2.66
          Mean value_function loss: 272.1164
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 74.0802
                       Mean reward: 237.27
               Mean episode length: 142.44
    Episode_Reward/reaching_object: 1.0348
     Episode_Reward/lifting_object: 44.4623
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 25.6667
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 2.14s
                      Time elapsed: 00:27:07
                               ETA: 00:46:19

################################################################################
                     [1m Learning iteration 739/2000 [0m                      

                       Computation: 45127 steps/s (collection: 2.082s, learning 0.096s)
             Mean action noise std: 2.66
          Mean value_function loss: 268.5289
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 74.0779
                       Mean reward: 224.25
               Mean episode length: 138.16
    Episode_Reward/reaching_object: 1.0931
     Episode_Reward/lifting_object: 47.4108
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 25.5000
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 2.18s
                      Time elapsed: 00:27:09
                               ETA: 00:46:17

################################################################################
                     [1m Learning iteration 740/2000 [0m                      

                       Computation: 45260 steps/s (collection: 2.072s, learning 0.100s)
             Mean action noise std: 2.66
          Mean value_function loss: 277.9558
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 74.0777
                       Mean reward: 230.21
               Mean episode length: 139.66
    Episode_Reward/reaching_object: 1.0546
     Episode_Reward/lifting_object: 46.1214
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 24.5417
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 2.17s
                      Time elapsed: 00:27:11
                               ETA: 00:46:14

################################################################################
                     [1m Learning iteration 741/2000 [0m                      

                       Computation: 46061 steps/s (collection: 2.044s, learning 0.090s)
             Mean action noise std: 2.66
          Mean value_function loss: 275.6147
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 74.0780
                       Mean reward: 242.28
               Mean episode length: 139.85
    Episode_Reward/reaching_object: 1.0534
     Episode_Reward/lifting_object: 46.3291
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 26.1250
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 2.13s
                      Time elapsed: 00:27:14
                               ETA: 00:46:12

################################################################################
                     [1m Learning iteration 742/2000 [0m                      

                       Computation: 45371 steps/s (collection: 2.077s, learning 0.090s)
             Mean action noise std: 2.66
          Mean value_function loss: 331.8138
               Mean surrogate loss: 0.0147
                 Mean entropy loss: 74.0782
                       Mean reward: 246.23
               Mean episode length: 142.69
    Episode_Reward/reaching_object: 1.0212
     Episode_Reward/lifting_object: 45.1622
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 25.5000
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 2.17s
                      Time elapsed: 00:27:16
                               ETA: 00:46:10

################################################################################
                     [1m Learning iteration 743/2000 [0m                      

                       Computation: 46026 steps/s (collection: 2.039s, learning 0.097s)
             Mean action noise std: 2.66
          Mean value_function loss: 318.8476
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 74.0782
                       Mean reward: 222.21
               Mean episode length: 128.65
    Episode_Reward/reaching_object: 1.0203
     Episode_Reward/lifting_object: 45.2858
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 27.2500
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 2.14s
                      Time elapsed: 00:27:18
                               ETA: 00:46:07

################################################################################
                     [1m Learning iteration 744/2000 [0m                      

                       Computation: 45565 steps/s (collection: 2.068s, learning 0.089s)
             Mean action noise std: 2.66
          Mean value_function loss: 283.6800
               Mean surrogate loss: 0.0130
                 Mean entropy loss: 74.0783
                       Mean reward: 233.45
               Mean episode length: 141.15
    Episode_Reward/reaching_object: 1.0713
     Episode_Reward/lifting_object: 47.6031
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 26.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 2.16s
                      Time elapsed: 00:27:20
                               ETA: 00:46:05

################################################################################
                     [1m Learning iteration 745/2000 [0m                      

                       Computation: 44659 steps/s (collection: 2.085s, learning 0.116s)
             Mean action noise std: 2.66
          Mean value_function loss: 296.8189
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 74.0784
                       Mean reward: 247.28
               Mean episode length: 143.19
    Episode_Reward/reaching_object: 1.0580
     Episode_Reward/lifting_object: 48.4337
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 23.7500
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 2.20s
                      Time elapsed: 00:27:22
                               ETA: 00:46:03

################################################################################
                     [1m Learning iteration 746/2000 [0m                      

                       Computation: 43941 steps/s (collection: 2.145s, learning 0.093s)
             Mean action noise std: 2.66
          Mean value_function loss: 279.4036
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 74.0785
                       Mean reward: 268.83
               Mean episode length: 151.14
    Episode_Reward/reaching_object: 1.0820
     Episode_Reward/lifting_object: 49.8390
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.3750
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 2.24s
                      Time elapsed: 00:27:24
                               ETA: 00:46:01

################################################################################
                     [1m Learning iteration 747/2000 [0m                      

                       Computation: 44759 steps/s (collection: 2.108s, learning 0.089s)
             Mean action noise std: 2.66
          Mean value_function loss: 273.1303
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 74.0786
                       Mean reward: 247.30
               Mean episode length: 139.89
    Episode_Reward/reaching_object: 1.0836
     Episode_Reward/lifting_object: 49.1190
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.8333
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 2.20s
                      Time elapsed: 00:27:27
                               ETA: 00:45:59

################################################################################
                     [1m Learning iteration 748/2000 [0m                      

                       Computation: 43431 steps/s (collection: 2.169s, learning 0.094s)
             Mean action noise std: 2.66
          Mean value_function loss: 279.7538
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 74.0785
                       Mean reward: 251.13
               Mean episode length: 143.75
    Episode_Reward/reaching_object: 1.0654
     Episode_Reward/lifting_object: 48.7291
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 24.1667
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 2.26s
                      Time elapsed: 00:27:29
                               ETA: 00:45:57

################################################################################
                     [1m Learning iteration 749/2000 [0m                      

                       Computation: 45232 steps/s (collection: 2.076s, learning 0.097s)
             Mean action noise std: 2.66
          Mean value_function loss: 291.5928
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 74.0786
                       Mean reward: 224.79
               Mean episode length: 128.74
    Episode_Reward/reaching_object: 1.0598
     Episode_Reward/lifting_object: 48.5922
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 25.0417
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 2.17s
                      Time elapsed: 00:27:31
                               ETA: 00:45:54

################################################################################
                     [1m Learning iteration 750/2000 [0m                      

                       Computation: 45138 steps/s (collection: 2.075s, learning 0.103s)
             Mean action noise std: 2.66
          Mean value_function loss: 298.5018
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 74.0788
                       Mean reward: 261.70
               Mean episode length: 146.52
    Episode_Reward/reaching_object: 1.1218
     Episode_Reward/lifting_object: 52.2919
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 25.8750
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 2.18s
                      Time elapsed: 00:27:33
                               ETA: 00:45:52

################################################################################
                     [1m Learning iteration 751/2000 [0m                      

                       Computation: 44251 steps/s (collection: 2.111s, learning 0.110s)
             Mean action noise std: 2.66
          Mean value_function loss: 301.2551
               Mean surrogate loss: 0.0096
                 Mean entropy loss: 74.0789
                       Mean reward: 248.50
               Mean episode length: 139.95
    Episode_Reward/reaching_object: 1.0830
     Episode_Reward/lifting_object: 49.0228
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 24.7083
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 2.22s
                      Time elapsed: 00:27:35
                               ETA: 00:45:50

################################################################################
                     [1m Learning iteration 752/2000 [0m                      

                       Computation: 45238 steps/s (collection: 2.085s, learning 0.088s)
             Mean action noise std: 2.66
          Mean value_function loss: 291.7853
               Mean surrogate loss: 0.0113
                 Mean entropy loss: 74.0789
                       Mean reward: 257.54
               Mean episode length: 143.50
    Episode_Reward/reaching_object: 1.0785
     Episode_Reward/lifting_object: 50.0126
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 24.5833
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 2.17s
                      Time elapsed: 00:27:38
                               ETA: 00:45:48

################################################################################
                     [1m Learning iteration 753/2000 [0m                      

                       Computation: 45602 steps/s (collection: 2.064s, learning 0.092s)
             Mean action noise std: 2.66
          Mean value_function loss: 316.0916
               Mean surrogate loss: 0.0110
                 Mean entropy loss: 74.0789
                       Mean reward: 233.13
               Mean episode length: 131.84
    Episode_Reward/reaching_object: 1.0435
     Episode_Reward/lifting_object: 49.3679
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 25.2083
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 2.16s
                      Time elapsed: 00:27:40
                               ETA: 00:45:45

################################################################################
                     [1m Learning iteration 754/2000 [0m                      

                       Computation: 45694 steps/s (collection: 2.063s, learning 0.089s)
             Mean action noise std: 2.66
          Mean value_function loss: 332.1557
               Mean surrogate loss: 0.0112
                 Mean entropy loss: 74.0789
                       Mean reward: 254.11
               Mean episode length: 139.57
    Episode_Reward/reaching_object: 1.0646
     Episode_Reward/lifting_object: 49.5875
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 26.4583
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 2.15s
                      Time elapsed: 00:27:42
                               ETA: 00:45:43

################################################################################
                     [1m Learning iteration 755/2000 [0m                      

                       Computation: 45007 steps/s (collection: 2.083s, learning 0.101s)
             Mean action noise std: 2.66
          Mean value_function loss: 320.2994
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 74.0789
                       Mean reward: 260.93
               Mean episode length: 141.60
    Episode_Reward/reaching_object: 1.0718
     Episode_Reward/lifting_object: 51.3512
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 4.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 27.6250
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 2.18s
                      Time elapsed: 00:27:44
                               ETA: 00:45:41

################################################################################
                     [1m Learning iteration 756/2000 [0m                      

                       Computation: 45041 steps/s (collection: 2.086s, learning 0.097s)
             Mean action noise std: 2.66
          Mean value_function loss: 310.8443
               Mean surrogate loss: 0.0115
                 Mean entropy loss: 74.0789
                       Mean reward: 265.19
               Mean episode length: 140.26
    Episode_Reward/reaching_object: 1.0455
     Episode_Reward/lifting_object: 50.2821
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 24.7083
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 2.18s
                      Time elapsed: 00:27:46
                               ETA: 00:45:39

################################################################################
                     [1m Learning iteration 757/2000 [0m                      

                       Computation: 44544 steps/s (collection: 2.077s, learning 0.130s)
             Mean action noise std: 2.66
          Mean value_function loss: 327.2980
               Mean surrogate loss: 0.0107
                 Mean entropy loss: 74.0789
                       Mean reward: 265.92
               Mean episode length: 143.60
    Episode_Reward/reaching_object: 1.0882
     Episode_Reward/lifting_object: 51.9419
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 3.7083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 24.2500
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 2.21s
                      Time elapsed: 00:27:48
                               ETA: 00:45:36

################################################################################
                     [1m Learning iteration 758/2000 [0m                      

                       Computation: 45593 steps/s (collection: 2.065s, learning 0.092s)
             Mean action noise std: 2.66
          Mean value_function loss: 320.9731
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 74.0789
                       Mean reward: 222.45
               Mean episode length: 123.33
    Episode_Reward/reaching_object: 1.0188
     Episode_Reward/lifting_object: 48.8372
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 25.3750
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 2.16s
                      Time elapsed: 00:27:51
                               ETA: 00:45:34

################################################################################
                     [1m Learning iteration 759/2000 [0m                      

                       Computation: 45317 steps/s (collection: 2.078s, learning 0.091s)
             Mean action noise std: 2.66
          Mean value_function loss: 330.7444
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 74.0789
                       Mean reward: 279.85
               Mean episode length: 148.10
    Episode_Reward/reaching_object: 1.0421
     Episode_Reward/lifting_object: 50.1083
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.2083
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 2.17s
                      Time elapsed: 00:27:53
                               ETA: 00:45:32

################################################################################
                     [1m Learning iteration 760/2000 [0m                      

                       Computation: 41670 steps/s (collection: 2.230s, learning 0.129s)
             Mean action noise std: 2.66
          Mean value_function loss: 331.2978
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 74.0789
                       Mean reward: 290.26
               Mean episode length: 148.26
    Episode_Reward/reaching_object: 1.0814
     Episode_Reward/lifting_object: 52.8107
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 25.7917
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 2.36s
                      Time elapsed: 00:27:55
                               ETA: 00:45:30

################################################################################
                     [1m Learning iteration 761/2000 [0m                      

                       Computation: 40845 steps/s (collection: 2.305s, learning 0.102s)
             Mean action noise std: 2.66
          Mean value_function loss: 311.5930
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 74.0789
                       Mean reward: 260.48
               Mean episode length: 132.69
    Episode_Reward/reaching_object: 1.0322
     Episode_Reward/lifting_object: 50.1191
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 25.7083
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 2.41s
                      Time elapsed: 00:27:58
                               ETA: 00:45:28

################################################################################
                     [1m Learning iteration 762/2000 [0m                      

                       Computation: 44389 steps/s (collection: 2.110s, learning 0.105s)
             Mean action noise std: 2.66
          Mean value_function loss: 315.3563
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 74.0789
                       Mean reward: 272.88
               Mean episode length: 140.28
    Episode_Reward/reaching_object: 1.0676
     Episode_Reward/lifting_object: 53.3332
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 25.3333
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 2.21s
                      Time elapsed: 00:28:00
                               ETA: 00:45:26

################################################################################
                     [1m Learning iteration 763/2000 [0m                      

                       Computation: 40713 steps/s (collection: 2.287s, learning 0.127s)
             Mean action noise std: 2.66
          Mean value_function loss: 327.3094
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 74.0789
                       Mean reward: 277.92
               Mean episode length: 146.13
    Episode_Reward/reaching_object: 1.0409
     Episode_Reward/lifting_object: 50.6629
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 25.6667
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 2.41s
                      Time elapsed: 00:28:02
                               ETA: 00:45:24

################################################################################
                     [1m Learning iteration 764/2000 [0m                      

                       Computation: 44120 steps/s (collection: 2.114s, learning 0.115s)
             Mean action noise std: 2.66
          Mean value_function loss: 323.7716
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 74.0791
                       Mean reward: 264.24
               Mean episode length: 135.82
    Episode_Reward/reaching_object: 1.0522
     Episode_Reward/lifting_object: 52.1189
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 24.6667
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 2.23s
                      Time elapsed: 00:28:04
                               ETA: 00:45:22

################################################################################
                     [1m Learning iteration 765/2000 [0m                      

                       Computation: 44039 steps/s (collection: 2.143s, learning 0.090s)
             Mean action noise std: 2.66
          Mean value_function loss: 313.9402
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 74.0786
                       Mean reward: 232.77
               Mean episode length: 131.09
    Episode_Reward/reaching_object: 1.0527
     Episode_Reward/lifting_object: 52.0553
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 24.0833
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 2.23s
                      Time elapsed: 00:28:07
                               ETA: 00:45:20

################################################################################
                     [1m Learning iteration 766/2000 [0m                      

                       Computation: 41800 steps/s (collection: 2.255s, learning 0.097s)
             Mean action noise std: 2.66
          Mean value_function loss: 338.6083
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.0782
                       Mean reward: 281.91
               Mean episode length: 142.14
    Episode_Reward/reaching_object: 1.0315
     Episode_Reward/lifting_object: 51.1573
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 26.3333
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 2.35s
                      Time elapsed: 00:28:09
                               ETA: 00:45:18

################################################################################
                     [1m Learning iteration 767/2000 [0m                      

                       Computation: 45204 steps/s (collection: 2.070s, learning 0.105s)
             Mean action noise std: 2.66
          Mean value_function loss: 347.3741
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.0769
                       Mean reward: 230.18
               Mean episode length: 131.55
    Episode_Reward/reaching_object: 1.0838
     Episode_Reward/lifting_object: 52.5478
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 24.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 2.17s
                      Time elapsed: 00:28:11
                               ETA: 00:45:15

################################################################################
                     [1m Learning iteration 768/2000 [0m                      

                       Computation: 43009 steps/s (collection: 2.152s, learning 0.134s)
             Mean action noise std: 2.67
          Mean value_function loss: 392.8046
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 74.0772
                       Mean reward: 276.83
               Mean episode length: 140.30
    Episode_Reward/reaching_object: 1.0837
     Episode_Reward/lifting_object: 54.4921
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 3.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 26.0833
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 2.29s
                      Time elapsed: 00:28:13
                               ETA: 00:45:13

################################################################################
                     [1m Learning iteration 769/2000 [0m                      

                       Computation: 43727 steps/s (collection: 2.125s, learning 0.124s)
             Mean action noise std: 2.67
          Mean value_function loss: 363.5428
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 74.0774
                       Mean reward: 294.03
               Mean episode length: 145.43
    Episode_Reward/reaching_object: 1.0865
     Episode_Reward/lifting_object: 53.8794
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 23.5417
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 2.25s
                      Time elapsed: 00:28:16
                               ETA: 00:45:11

################################################################################
                     [1m Learning iteration 770/2000 [0m                      

                       Computation: 42942 steps/s (collection: 2.175s, learning 0.114s)
             Mean action noise std: 2.67
          Mean value_function loss: 371.5070
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 74.0774
                       Mean reward: 297.78
               Mean episode length: 146.17
    Episode_Reward/reaching_object: 1.0580
     Episode_Reward/lifting_object: 53.5403
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 24.1250
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 2.29s
                      Time elapsed: 00:28:18
                               ETA: 00:45:09

################################################################################
                     [1m Learning iteration 771/2000 [0m                      

                       Computation: 44958 steps/s (collection: 2.091s, learning 0.096s)
             Mean action noise std: 2.67
          Mean value_function loss: 408.2236
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 74.0770
                       Mean reward: 311.77
               Mean episode length: 151.13
    Episode_Reward/reaching_object: 1.1223
     Episode_Reward/lifting_object: 57.3175
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 23.1250
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 2.19s
                      Time elapsed: 00:28:20
                               ETA: 00:45:07

################################################################################
                     [1m Learning iteration 772/2000 [0m                      

                       Computation: 44427 steps/s (collection: 2.095s, learning 0.118s)
             Mean action noise std: 2.67
          Mean value_function loss: 427.2509
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 74.0770
                       Mean reward: 303.98
               Mean episode length: 152.82
    Episode_Reward/reaching_object: 1.1189
     Episode_Reward/lifting_object: 58.1855
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 23.8333
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 2.21s
                      Time elapsed: 00:28:22
                               ETA: 00:45:05

################################################################################
                     [1m Learning iteration 773/2000 [0m                      

                       Computation: 44546 steps/s (collection: 2.105s, learning 0.102s)
             Mean action noise std: 2.67
          Mean value_function loss: 345.3540
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 74.0766
                       Mean reward: 290.16
               Mean episode length: 145.36
    Episode_Reward/reaching_object: 1.1751
     Episode_Reward/lifting_object: 61.2534
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.8750
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 2.21s
                      Time elapsed: 00:28:25
                               ETA: 00:45:03

################################################################################
                     [1m Learning iteration 774/2000 [0m                      

                       Computation: 45065 steps/s (collection: 2.089s, learning 0.092s)
             Mean action noise std: 2.67
          Mean value_function loss: 354.3631
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 74.0755
                       Mean reward: 291.29
               Mean episode length: 146.29
    Episode_Reward/reaching_object: 1.1427
     Episode_Reward/lifting_object: 59.9957
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 21.4583
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 2.18s
                      Time elapsed: 00:28:27
                               ETA: 00:45:00

################################################################################
                     [1m Learning iteration 775/2000 [0m                      

                       Computation: 44078 steps/s (collection: 2.141s, learning 0.089s)
             Mean action noise std: 2.67
          Mean value_function loss: 354.1956
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 74.0751
                       Mean reward: 279.84
               Mean episode length: 140.12
    Episode_Reward/reaching_object: 1.0591
     Episode_Reward/lifting_object: 55.5525
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 22.1667
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 2.23s
                      Time elapsed: 00:28:29
                               ETA: 00:44:58

################################################################################
                     [1m Learning iteration 776/2000 [0m                      

                       Computation: 44549 steps/s (collection: 2.102s, learning 0.105s)
             Mean action noise std: 2.67
          Mean value_function loss: 381.1370
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 74.0740
                       Mean reward: 298.23
               Mean episode length: 147.42
    Episode_Reward/reaching_object: 1.1256
     Episode_Reward/lifting_object: 58.6940
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 25.2083
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 2.21s
                      Time elapsed: 00:28:31
                               ETA: 00:44:56

################################################################################
                     [1m Learning iteration 777/2000 [0m                      

                       Computation: 44047 steps/s (collection: 2.080s, learning 0.152s)
             Mean action noise std: 2.67
          Mean value_function loss: 406.5420
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 74.0735
                       Mean reward: 324.30
               Mean episode length: 151.29
    Episode_Reward/reaching_object: 1.1093
     Episode_Reward/lifting_object: 59.5277
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 24.0417
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 2.23s
                      Time elapsed: 00:28:33
                               ETA: 00:44:54

################################################################################
                     [1m Learning iteration 778/2000 [0m                      

                       Computation: 43555 steps/s (collection: 2.167s, learning 0.090s)
             Mean action noise std: 2.67
          Mean value_function loss: 410.7568
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 74.0732
                       Mean reward: 320.25
               Mean episode length: 151.18
    Episode_Reward/reaching_object: 1.1605
     Episode_Reward/lifting_object: 61.5747
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 23.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 2.26s
                      Time elapsed: 00:28:36
                               ETA: 00:44:52

################################################################################
                     [1m Learning iteration 779/2000 [0m                      

                       Computation: 41530 steps/s (collection: 2.255s, learning 0.112s)
             Mean action noise std: 2.67
          Mean value_function loss: 398.7234
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 74.0722
                       Mean reward: 329.47
               Mean episode length: 148.49
    Episode_Reward/reaching_object: 1.1348
     Episode_Reward/lifting_object: 61.3769
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 23.8750
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 2.37s
                      Time elapsed: 00:28:38
                               ETA: 00:44:50

################################################################################
                     [1m Learning iteration 780/2000 [0m                      

                       Computation: 43759 steps/s (collection: 2.147s, learning 0.100s)
             Mean action noise std: 2.67
          Mean value_function loss: 386.0138
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 74.0713
                       Mean reward: 282.75
               Mean episode length: 130.22
    Episode_Reward/reaching_object: 1.1040
     Episode_Reward/lifting_object: 58.9909
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 24.0417
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 2.25s
                      Time elapsed: 00:28:40
                               ETA: 00:44:48

################################################################################
                     [1m Learning iteration 781/2000 [0m                      

                       Computation: 42195 steps/s (collection: 2.214s, learning 0.116s)
             Mean action noise std: 2.67
          Mean value_function loss: 384.7035
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 74.0713
                       Mean reward: 294.58
               Mean episode length: 138.85
    Episode_Reward/reaching_object: 1.1370
     Episode_Reward/lifting_object: 62.1347
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.8333
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 2.33s
                      Time elapsed: 00:28:43
                               ETA: 00:44:46

################################################################################
                     [1m Learning iteration 782/2000 [0m                      

                       Computation: 43655 steps/s (collection: 2.162s, learning 0.090s)
             Mean action noise std: 2.67
          Mean value_function loss: 401.2528
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 74.0709
                       Mean reward: 311.67
               Mean episode length: 137.77
    Episode_Reward/reaching_object: 1.1183
     Episode_Reward/lifting_object: 60.1938
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 24.7917
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 2.25s
                      Time elapsed: 00:28:45
                               ETA: 00:44:44

################################################################################
                     [1m Learning iteration 783/2000 [0m                      

                       Computation: 43376 steps/s (collection: 2.160s, learning 0.107s)
             Mean action noise std: 2.67
          Mean value_function loss: 412.6573
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 74.0702
                       Mean reward: 283.76
               Mean episode length: 130.08
    Episode_Reward/reaching_object: 1.1234
     Episode_Reward/lifting_object: 61.0769
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 23.7500
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 2.27s
                      Time elapsed: 00:28:47
                               ETA: 00:44:41

################################################################################
                     [1m Learning iteration 784/2000 [0m                      

                       Computation: 44274 steps/s (collection: 2.125s, learning 0.095s)
             Mean action noise std: 2.67
          Mean value_function loss: 395.7657
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 74.0689
                       Mean reward: 311.04
               Mean episode length: 144.04
    Episode_Reward/reaching_object: 1.0713
     Episode_Reward/lifting_object: 59.7555
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 24.5000
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 2.22s
                      Time elapsed: 00:28:49
                               ETA: 00:44:39

################################################################################
                     [1m Learning iteration 785/2000 [0m                      

                       Computation: 42848 steps/s (collection: 2.144s, learning 0.150s)
             Mean action noise std: 2.67
          Mean value_function loss: 388.9582
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 74.0683
                       Mean reward: 332.71
               Mean episode length: 151.23
    Episode_Reward/reaching_object: 1.2104
     Episode_Reward/lifting_object: 68.7334
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.1667
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 2.29s
                      Time elapsed: 00:28:52
                               ETA: 00:44:37

################################################################################
                     [1m Learning iteration 786/2000 [0m                      

                       Computation: 44105 steps/s (collection: 2.132s, learning 0.097s)
             Mean action noise std: 2.67
          Mean value_function loss: 396.8578
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 74.0677
                       Mean reward: 398.23
               Mean episode length: 165.61
    Episode_Reward/reaching_object: 1.3081
     Episode_Reward/lifting_object: 76.7223
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.4583
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 2.23s
                      Time elapsed: 00:28:54
                               ETA: 00:44:35

################################################################################
                     [1m Learning iteration 787/2000 [0m                      

                       Computation: 41200 steps/s (collection: 2.203s, learning 0.183s)
             Mean action noise std: 2.67
          Mean value_function loss: 392.8930
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 74.0673
                       Mean reward: 396.75
               Mean episode length: 159.79
    Episode_Reward/reaching_object: 1.2920
     Episode_Reward/lifting_object: 76.5513
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 16.8333
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 2.39s
                      Time elapsed: 00:28:56
                               ETA: 00:44:33

################################################################################
                     [1m Learning iteration 788/2000 [0m                      

                       Computation: 40209 steps/s (collection: 2.329s, learning 0.116s)
             Mean action noise std: 2.67
          Mean value_function loss: 405.8360
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 74.0662
                       Mean reward: 387.31
               Mean episode length: 158.40
    Episode_Reward/reaching_object: 1.3260
     Episode_Reward/lifting_object: 80.4055
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 18.8333
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 2.44s
                      Time elapsed: 00:28:59
                               ETA: 00:44:31

################################################################################
                     [1m Learning iteration 789/2000 [0m                      

                       Computation: 42671 steps/s (collection: 2.188s, learning 0.116s)
             Mean action noise std: 2.67
          Mean value_function loss: 408.2102
               Mean surrogate loss: 0.0142
                 Mean entropy loss: 74.0653
                       Mean reward: 406.00
               Mean episode length: 161.90
    Episode_Reward/reaching_object: 1.3117
     Episode_Reward/lifting_object: 79.7746
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.1250
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 2.30s
                      Time elapsed: 00:29:01
                               ETA: 00:44:29

################################################################################
                     [1m Learning iteration 790/2000 [0m                      

                       Computation: 40499 steps/s (collection: 2.294s, learning 0.133s)
             Mean action noise std: 2.67
          Mean value_function loss: 407.3114
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 74.0653
                       Mean reward: 422.62
               Mean episode length: 163.65
    Episode_Reward/reaching_object: 1.3013
     Episode_Reward/lifting_object: 79.7312
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.3333
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 2.43s
                      Time elapsed: 00:29:04
                               ETA: 00:44:27

################################################################################
                     [1m Learning iteration 791/2000 [0m                      

                       Computation: 43083 steps/s (collection: 2.164s, learning 0.118s)
             Mean action noise std: 2.67
          Mean value_function loss: 404.6877
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 74.0653
                       Mean reward: 441.58
               Mean episode length: 172.24
    Episode_Reward/reaching_object: 1.3011
     Episode_Reward/lifting_object: 80.3405
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.8333
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 2.28s
                      Time elapsed: 00:29:06
                               ETA: 00:44:25

################################################################################
                     [1m Learning iteration 792/2000 [0m                      

                       Computation: 43232 steps/s (collection: 2.163s, learning 0.111s)
             Mean action noise std: 2.67
          Mean value_function loss: 380.0386
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 74.0655
                       Mean reward: 452.87
               Mean episode length: 171.68
    Episode_Reward/reaching_object: 1.3492
     Episode_Reward/lifting_object: 85.4938
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.6667
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 2.27s
                      Time elapsed: 00:29:08
                               ETA: 00:44:23

################################################################################
                     [1m Learning iteration 793/2000 [0m                      

                       Computation: 44542 steps/s (collection: 2.103s, learning 0.104s)
             Mean action noise std: 2.67
          Mean value_function loss: 414.4716
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 74.0659
                       Mean reward: 389.44
               Mean episode length: 165.35
    Episode_Reward/reaching_object: 1.3098
     Episode_Reward/lifting_object: 81.5173
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 18.0417
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 2.21s
                      Time elapsed: 00:29:10
                               ETA: 00:44:21

################################################################################
                     [1m Learning iteration 794/2000 [0m                      

                       Computation: 43758 steps/s (collection: 2.139s, learning 0.107s)
             Mean action noise std: 2.67
          Mean value_function loss: 411.9118
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 74.0661
                       Mean reward: 415.50
               Mean episode length: 163.14
    Episode_Reward/reaching_object: 1.3016
     Episode_Reward/lifting_object: 81.4415
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.5417
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 2.25s
                      Time elapsed: 00:29:13
                               ETA: 00:44:19

################################################################################
                     [1m Learning iteration 795/2000 [0m                      

                       Computation: 43718 steps/s (collection: 2.121s, learning 0.128s)
             Mean action noise std: 2.67
          Mean value_function loss: 406.5496
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.0653
                       Mean reward: 424.51
               Mean episode length: 166.77
    Episode_Reward/reaching_object: 1.3144
     Episode_Reward/lifting_object: 83.6249
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.3333
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 2.25s
                      Time elapsed: 00:29:15
                               ETA: 00:44:17

################################################################################
                     [1m Learning iteration 796/2000 [0m                      

                       Computation: 43633 steps/s (collection: 2.167s, learning 0.086s)
             Mean action noise std: 2.67
          Mean value_function loss: 416.1043
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 74.0642
                       Mean reward: 414.46
               Mean episode length: 165.29
    Episode_Reward/reaching_object: 1.2483
     Episode_Reward/lifting_object: 78.2604
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.0417
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 2.25s
                      Time elapsed: 00:29:17
                               ETA: 00:44:15

################################################################################
                     [1m Learning iteration 797/2000 [0m                      

                       Computation: 44314 steps/s (collection: 2.110s, learning 0.109s)
             Mean action noise std: 2.67
          Mean value_function loss: 408.1811
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 74.0639
                       Mean reward: 353.18
               Mean episode length: 149.56
    Episode_Reward/reaching_object: 1.2776
     Episode_Reward/lifting_object: 78.5006
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 18.1250
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 2.22s
                      Time elapsed: 00:29:19
                               ETA: 00:44:12

################################################################################
                     [1m Learning iteration 798/2000 [0m                      

                       Computation: 42611 steps/s (collection: 2.193s, learning 0.114s)
             Mean action noise std: 2.67
          Mean value_function loss: 401.5116
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 74.0635
                       Mean reward: 418.52
               Mean episode length: 172.63
    Episode_Reward/reaching_object: 1.3283
     Episode_Reward/lifting_object: 81.7419
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.7500
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 2.31s
                      Time elapsed: 00:29:22
                               ETA: 00:44:10

################################################################################
                     [1m Learning iteration 799/2000 [0m                      

                       Computation: 43737 steps/s (collection: 2.159s, learning 0.089s)
             Mean action noise std: 2.67
          Mean value_function loss: 404.8595
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 74.0623
                       Mean reward: 425.75
               Mean episode length: 171.05
    Episode_Reward/reaching_object: 1.3382
     Episode_Reward/lifting_object: 83.9447
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.5417
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 2.25s
                      Time elapsed: 00:29:24
                               ETA: 00:44:08

################################################################################
                     [1m Learning iteration 800/2000 [0m                      

                       Computation: 44230 steps/s (collection: 2.136s, learning 0.087s)
             Mean action noise std: 2.67
          Mean value_function loss: 424.2261
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 74.0620
                       Mean reward: 421.62
               Mean episode length: 166.04
    Episode_Reward/reaching_object: 1.3161
     Episode_Reward/lifting_object: 82.3263
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.3333
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 2.22s
                      Time elapsed: 00:29:26
                               ETA: 00:44:06

################################################################################
                     [1m Learning iteration 801/2000 [0m                      

                       Computation: 43974 steps/s (collection: 2.143s, learning 0.093s)
             Mean action noise std: 2.67
          Mean value_function loss: 424.1086
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 74.0618
                       Mean reward: 374.98
               Mean episode length: 155.86
    Episode_Reward/reaching_object: 1.3488
     Episode_Reward/lifting_object: 85.1967
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 16.7500
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 2.24s
                      Time elapsed: 00:29:28
                               ETA: 00:44:04

################################################################################
                     [1m Learning iteration 802/2000 [0m                      

                       Computation: 44067 steps/s (collection: 2.138s, learning 0.093s)
             Mean action noise std: 2.67
          Mean value_function loss: 408.1564
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 74.0617
                       Mean reward: 454.52
               Mean episode length: 169.45
    Episode_Reward/reaching_object: 1.3684
     Episode_Reward/lifting_object: 85.5883
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 2.23s
                      Time elapsed: 00:29:30
                               ETA: 00:44:02

################################################################################
                     [1m Learning iteration 803/2000 [0m                      

                       Computation: 44119 steps/s (collection: 2.107s, learning 0.121s)
             Mean action noise std: 2.67
          Mean value_function loss: 403.5927
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 74.0617
                       Mean reward: 426.48
               Mean episode length: 156.98
    Episode_Reward/reaching_object: 1.3735
     Episode_Reward/lifting_object: 89.8864
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.7917
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 2.23s
                      Time elapsed: 00:29:33
                               ETA: 00:43:59

################################################################################
                     [1m Learning iteration 804/2000 [0m                      

                       Computation: 43476 steps/s (collection: 2.159s, learning 0.103s)
             Mean action noise std: 2.67
          Mean value_function loss: 392.0637
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 74.0617
                       Mean reward: 503.01
               Mean episode length: 174.40
    Episode_Reward/reaching_object: 1.4198
     Episode_Reward/lifting_object: 95.2068
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.0417
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 2.26s
                      Time elapsed: 00:29:35
                               ETA: 00:43:57

################################################################################
                     [1m Learning iteration 805/2000 [0m                      

                       Computation: 43484 steps/s (collection: 2.159s, learning 0.102s)
             Mean action noise std: 2.67
          Mean value_function loss: 405.8274
               Mean surrogate loss: 0.0167
                 Mean entropy loss: 74.0615
                       Mean reward: 517.92
               Mean episode length: 179.32
    Episode_Reward/reaching_object: 1.4212
     Episode_Reward/lifting_object: 96.2805
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 2.26s
                      Time elapsed: 00:29:37
                               ETA: 00:43:55

################################################################################
                     [1m Learning iteration 806/2000 [0m                      

                       Computation: 44704 steps/s (collection: 2.102s, learning 0.097s)
             Mean action noise std: 2.67
          Mean value_function loss: 425.5534
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 74.0614
                       Mean reward: 464.65
               Mean episode length: 164.13
    Episode_Reward/reaching_object: 1.4067
     Episode_Reward/lifting_object: 96.7025
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 2.20s
                      Time elapsed: 00:29:39
                               ETA: 00:43:53

################################################################################
                     [1m Learning iteration 807/2000 [0m                      

                       Computation: 44052 steps/s (collection: 2.128s, learning 0.103s)
             Mean action noise std: 2.67
          Mean value_function loss: 417.1763
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 74.0608
                       Mean reward: 520.89
               Mean episode length: 176.59
    Episode_Reward/reaching_object: 1.4062
     Episode_Reward/lifting_object: 97.1357
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 2.23s
                      Time elapsed: 00:29:42
                               ETA: 00:43:51

################################################################################
                     [1m Learning iteration 808/2000 [0m                      

                       Computation: 42429 steps/s (collection: 2.138s, learning 0.179s)
             Mean action noise std: 2.67
          Mean value_function loss: 414.9137
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 74.0597
                       Mean reward: 536.38
               Mean episode length: 182.35
    Episode_Reward/reaching_object: 1.4400
     Episode_Reward/lifting_object: 101.7762
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 2.32s
                      Time elapsed: 00:29:44
                               ETA: 00:43:49

################################################################################
                     [1m Learning iteration 809/2000 [0m                      

                       Computation: 44401 steps/s (collection: 2.117s, learning 0.097s)
             Mean action noise std: 2.67
          Mean value_function loss: 437.4998
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 74.0605
                       Mean reward: 469.97
               Mean episode length: 167.78
    Episode_Reward/reaching_object: 1.3681
     Episode_Reward/lifting_object: 97.3240
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.0833
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 2.21s
                      Time elapsed: 00:29:46
                               ETA: 00:43:47

################################################################################
                     [1m Learning iteration 810/2000 [0m                      

                       Computation: 43700 steps/s (collection: 2.148s, learning 0.102s)
             Mean action noise std: 2.67
          Mean value_function loss: 414.9265
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 74.0613
                       Mean reward: 507.10
               Mean episode length: 167.80
    Episode_Reward/reaching_object: 1.3533
     Episode_Reward/lifting_object: 97.6138
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 2.25s
                      Time elapsed: 00:29:48
                               ETA: 00:43:44

################################################################################
                     [1m Learning iteration 811/2000 [0m                      

                       Computation: 42674 steps/s (collection: 2.141s, learning 0.162s)
             Mean action noise std: 2.67
          Mean value_function loss: 394.2217
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 74.0618
                       Mean reward: 550.24
               Mean episode length: 177.44
    Episode_Reward/reaching_object: 1.5252
     Episode_Reward/lifting_object: 113.0937
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 2.30s
                      Time elapsed: 00:29:51
                               ETA: 00:43:42

################################################################################
                     [1m Learning iteration 812/2000 [0m                      

                       Computation: 39152 steps/s (collection: 2.415s, learning 0.096s)
             Mean action noise std: 2.67
          Mean value_function loss: 394.1633
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 74.0618
                       Mean reward: 524.35
               Mean episode length: 173.46
    Episode_Reward/reaching_object: 1.4226
     Episode_Reward/lifting_object: 103.6874
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 2.51s
                      Time elapsed: 00:29:53
                               ETA: 00:43:41

################################################################################
                     [1m Learning iteration 813/2000 [0m                      

                       Computation: 42821 steps/s (collection: 2.197s, learning 0.099s)
             Mean action noise std: 2.67
          Mean value_function loss: 377.1722
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 74.0633
                       Mean reward: 645.76
               Mean episode length: 202.55
    Episode_Reward/reaching_object: 1.5523
     Episode_Reward/lifting_object: 116.5685
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 2.30s
                      Time elapsed: 00:29:56
                               ETA: 00:43:39

################################################################################
                     [1m Learning iteration 814/2000 [0m                      

                       Computation: 38775 steps/s (collection: 2.409s, learning 0.127s)
             Mean action noise std: 2.67
          Mean value_function loss: 394.2392
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 74.0656
                       Mean reward: 573.83
               Mean episode length: 182.26
    Episode_Reward/reaching_object: 1.4730
     Episode_Reward/lifting_object: 109.6071
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 2.54s
                      Time elapsed: 00:29:58
                               ETA: 00:43:37

################################################################################
                     [1m Learning iteration 815/2000 [0m                      

                       Computation: 41419 steps/s (collection: 2.259s, learning 0.114s)
             Mean action noise std: 2.67
          Mean value_function loss: 365.0541
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 74.0666
                       Mean reward: 588.61
               Mean episode length: 190.45
    Episode_Reward/reaching_object: 1.5140
     Episode_Reward/lifting_object: 111.4263
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 2.37s
                      Time elapsed: 00:30:00
                               ETA: 00:43:35

################################################################################
                     [1m Learning iteration 816/2000 [0m                      

                       Computation: 41811 steps/s (collection: 2.257s, learning 0.094s)
             Mean action noise std: 2.67
          Mean value_function loss: 381.0885
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 74.0663
                       Mean reward: 539.13
               Mean episode length: 178.03
    Episode_Reward/reaching_object: 1.4989
     Episode_Reward/lifting_object: 110.9427
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 2.35s
                      Time elapsed: 00:30:03
                               ETA: 00:43:33

################################################################################
                     [1m Learning iteration 817/2000 [0m                      

                       Computation: 42301 steps/s (collection: 2.208s, learning 0.116s)
             Mean action noise std: 2.67
          Mean value_function loss: 389.9375
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 74.0657
                       Mean reward: 608.41
               Mean episode length: 192.16
    Episode_Reward/reaching_object: 1.5369
     Episode_Reward/lifting_object: 113.9125
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 2.32s
                      Time elapsed: 00:30:05
                               ETA: 00:43:31

################################################################################
                     [1m Learning iteration 818/2000 [0m                      

                       Computation: 44285 steps/s (collection: 2.115s, learning 0.105s)
             Mean action noise std: 2.67
          Mean value_function loss: 387.1733
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 74.0656
                       Mean reward: 525.63
               Mean episode length: 173.90
    Episode_Reward/reaching_object: 1.5698
     Episode_Reward/lifting_object: 118.8391
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.1667
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 2.22s
                      Time elapsed: 00:30:07
                               ETA: 00:43:29

################################################################################
                     [1m Learning iteration 819/2000 [0m                      

                       Computation: 43944 steps/s (collection: 2.140s, learning 0.097s)
             Mean action noise std: 2.67
          Mean value_function loss: 401.9784
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 74.0656
                       Mean reward: 617.61
               Mean episode length: 193.36
    Episode_Reward/reaching_object: 1.5879
     Episode_Reward/lifting_object: 118.9258
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 2.24s
                      Time elapsed: 00:30:10
                               ETA: 00:43:26

################################################################################
                     [1m Learning iteration 820/2000 [0m                      

                       Computation: 39963 steps/s (collection: 2.319s, learning 0.141s)
             Mean action noise std: 2.67
          Mean value_function loss: 400.3506
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 74.0657
                       Mean reward: 614.00
               Mean episode length: 191.77
    Episode_Reward/reaching_object: 1.5572
     Episode_Reward/lifting_object: 115.5758
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 2.46s
                      Time elapsed: 00:30:12
                               ETA: 00:43:25

################################################################################
                     [1m Learning iteration 821/2000 [0m                      

                       Computation: 41765 steps/s (collection: 2.242s, learning 0.112s)
             Mean action noise std: 2.67
          Mean value_function loss: 432.2932
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 74.0658
                       Mean reward: 607.39
               Mean episode length: 187.65
    Episode_Reward/reaching_object: 1.5184
     Episode_Reward/lifting_object: 112.4991
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 2.35s
                      Time elapsed: 00:30:14
                               ETA: 00:43:23

################################################################################
                     [1m Learning iteration 822/2000 [0m                      

                       Computation: 43636 steps/s (collection: 2.141s, learning 0.112s)
             Mean action noise std: 2.67
          Mean value_function loss: 415.0605
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 74.0659
                       Mean reward: 566.40
               Mean episode length: 179.41
    Episode_Reward/reaching_object: 1.4931
     Episode_Reward/lifting_object: 109.2968
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 2.25s
                      Time elapsed: 00:30:17
                               ETA: 00:43:20

################################################################################
                     [1m Learning iteration 823/2000 [0m                      

                       Computation: 43457 steps/s (collection: 2.147s, learning 0.115s)
             Mean action noise std: 2.67
          Mean value_function loss: 408.6037
               Mean surrogate loss: 0.0191
                 Mean entropy loss: 74.0663
                       Mean reward: 568.40
               Mean episode length: 178.80
    Episode_Reward/reaching_object: 1.4564
     Episode_Reward/lifting_object: 106.7087
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 2.26s
                      Time elapsed: 00:30:19
                               ETA: 00:43:18

################################################################################
                     [1m Learning iteration 824/2000 [0m                      

                       Computation: 41548 steps/s (collection: 2.256s, learning 0.110s)
             Mean action noise std: 2.67
          Mean value_function loss: 386.2578
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 74.0663
                       Mean reward: 578.59
               Mean episode length: 182.84
    Episode_Reward/reaching_object: 1.5134
     Episode_Reward/lifting_object: 113.4904
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 2.37s
                      Time elapsed: 00:30:21
                               ETA: 00:43:16

################################################################################
                     [1m Learning iteration 825/2000 [0m                      

                       Computation: 37501 steps/s (collection: 2.454s, learning 0.168s)
             Mean action noise std: 2.67
          Mean value_function loss: 382.8723
               Mean surrogate loss: 0.0145
                 Mean entropy loss: 74.0659
                       Mean reward: 595.48
               Mean episode length: 185.59
    Episode_Reward/reaching_object: 1.5037
     Episode_Reward/lifting_object: 110.9317
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 2.62s
                      Time elapsed: 00:30:24
                               ETA: 00:43:15

################################################################################
                     [1m Learning iteration 826/2000 [0m                      

                       Computation: 37865 steps/s (collection: 2.463s, learning 0.134s)
             Mean action noise std: 2.67
          Mean value_function loss: 388.6706
               Mean surrogate loss: 0.0130
                 Mean entropy loss: 74.0659
                       Mean reward: 612.35
               Mean episode length: 187.91
    Episode_Reward/reaching_object: 1.5259
     Episode_Reward/lifting_object: 114.2021
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 2.60s
                      Time elapsed: 00:30:26
                               ETA: 00:43:13

################################################################################
                     [1m Learning iteration 827/2000 [0m                      

                       Computation: 40489 steps/s (collection: 2.332s, learning 0.096s)
             Mean action noise std: 2.67
          Mean value_function loss: 387.8876
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 74.0659
                       Mean reward: 577.66
               Mean episode length: 182.79
    Episode_Reward/reaching_object: 1.5636
     Episode_Reward/lifting_object: 115.6313
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 2.43s
                      Time elapsed: 00:30:29
                               ETA: 00:43:11

################################################################################
                     [1m Learning iteration 828/2000 [0m                      

                       Computation: 40448 steps/s (collection: 2.318s, learning 0.112s)
             Mean action noise std: 2.67
          Mean value_function loss: 381.9359
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 74.0660
                       Mean reward: 596.20
               Mean episode length: 189.63
    Episode_Reward/reaching_object: 1.5940
     Episode_Reward/lifting_object: 118.3971
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 2.43s
                      Time elapsed: 00:30:31
                               ETA: 00:43:09

################################################################################
                     [1m Learning iteration 829/2000 [0m                      

                       Computation: 42790 steps/s (collection: 2.197s, learning 0.100s)
             Mean action noise std: 2.67
          Mean value_function loss: 388.1008
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 74.0662
                       Mean reward: 643.40
               Mean episode length: 199.73
    Episode_Reward/reaching_object: 1.6116
     Episode_Reward/lifting_object: 120.8747
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 2.30s
                      Time elapsed: 00:30:34
                               ETA: 00:43:07

################################################################################
                     [1m Learning iteration 830/2000 [0m                      

                       Computation: 40594 steps/s (collection: 2.273s, learning 0.149s)
             Mean action noise std: 2.67
          Mean value_function loss: 371.0424
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 74.0670
                       Mean reward: 640.96
               Mean episode length: 200.53
    Episode_Reward/reaching_object: 1.6491
     Episode_Reward/lifting_object: 124.4693
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0692
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 2.42s
                      Time elapsed: 00:30:36
                               ETA: 00:43:05

################################################################################
                     [1m Learning iteration 831/2000 [0m                      

                       Computation: 42238 steps/s (collection: 2.219s, learning 0.108s)
             Mean action noise std: 2.67
          Mean value_function loss: 368.0769
               Mean surrogate loss: 0.0095
                 Mean entropy loss: 74.0684
                       Mean reward: 730.29
               Mean episode length: 218.81
    Episode_Reward/reaching_object: 1.7244
     Episode_Reward/lifting_object: 131.2083
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 2.33s
                      Time elapsed: 00:30:38
                               ETA: 00:43:03

################################################################################
                     [1m Learning iteration 832/2000 [0m                      

                       Computation: 44387 steps/s (collection: 2.123s, learning 0.091s)
             Mean action noise std: 2.67
          Mean value_function loss: 381.6526
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 74.0688
                       Mean reward: 633.62
               Mean episode length: 193.03
    Episode_Reward/reaching_object: 1.6060
     Episode_Reward/lifting_object: 122.2287
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 2.21s
                      Time elapsed: 00:30:41
                               ETA: 00:43:01

################################################################################
                     [1m Learning iteration 833/2000 [0m                      

                       Computation: 43306 steps/s (collection: 2.177s, learning 0.093s)
             Mean action noise std: 2.67
          Mean value_function loss: 381.9082
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 74.0704
                       Mean reward: 672.71
               Mean episode length: 207.70
    Episode_Reward/reaching_object: 1.6013
     Episode_Reward/lifting_object: 122.7584
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 2.27s
                      Time elapsed: 00:30:43
                               ETA: 00:42:59

################################################################################
                     [1m Learning iteration 834/2000 [0m                      

                       Computation: 42874 steps/s (collection: 2.179s, learning 0.114s)
             Mean action noise std: 2.67
          Mean value_function loss: 383.3414
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 74.0728
                       Mean reward: 592.07
               Mean episode length: 185.83
    Episode_Reward/reaching_object: 1.5467
     Episode_Reward/lifting_object: 117.7364
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 2.29s
                      Time elapsed: 00:30:45
                               ETA: 00:42:57

################################################################################
                     [1m Learning iteration 835/2000 [0m                      

                       Computation: 42211 steps/s (collection: 2.209s, learning 0.120s)
             Mean action noise std: 2.67
          Mean value_function loss: 399.1780
               Mean surrogate loss: 0.0117
                 Mean entropy loss: 74.0739
                       Mean reward: 703.68
               Mean episode length: 205.20
    Episode_Reward/reaching_object: 1.5759
     Episode_Reward/lifting_object: 121.6465
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 2.33s
                      Time elapsed: 00:30:48
                               ETA: 00:42:55

################################################################################
                     [1m Learning iteration 836/2000 [0m                      

                       Computation: 43057 steps/s (collection: 2.170s, learning 0.114s)
             Mean action noise std: 2.67
          Mean value_function loss: 397.5516
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 74.0740
                       Mean reward: 592.14
               Mean episode length: 182.18
    Episode_Reward/reaching_object: 1.5218
     Episode_Reward/lifting_object: 117.6212
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 2.28s
                      Time elapsed: 00:30:50
                               ETA: 00:42:53

################################################################################
                     [1m Learning iteration 837/2000 [0m                      

                       Computation: 41504 steps/s (collection: 2.248s, learning 0.121s)
             Mean action noise std: 2.67
          Mean value_function loss: 389.6920
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 74.0747
                       Mean reward: 590.69
               Mean episode length: 180.44
    Episode_Reward/reaching_object: 1.5252
     Episode_Reward/lifting_object: 117.9191
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 2.37s
                      Time elapsed: 00:30:52
                               ETA: 00:42:51

################################################################################
                     [1m Learning iteration 838/2000 [0m                      

                       Computation: 43304 steps/s (collection: 2.180s, learning 0.091s)
             Mean action noise std: 2.67
          Mean value_function loss: 372.9689
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 74.0763
                       Mean reward: 652.60
               Mean episode length: 198.50
    Episode_Reward/reaching_object: 1.6204
     Episode_Reward/lifting_object: 127.1165
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 2.27s
                      Time elapsed: 00:30:54
                               ETA: 00:42:49

################################################################################
                     [1m Learning iteration 839/2000 [0m                      

                       Computation: 40439 steps/s (collection: 2.274s, learning 0.157s)
             Mean action noise std: 2.67
          Mean value_function loss: 367.2745
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 74.0773
                       Mean reward: 622.94
               Mean episode length: 189.44
    Episode_Reward/reaching_object: 1.5930
     Episode_Reward/lifting_object: 125.7694
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 2.43s
                      Time elapsed: 00:30:57
                               ETA: 00:42:47

################################################################################
                     [1m Learning iteration 840/2000 [0m                      

                       Computation: 41839 steps/s (collection: 2.170s, learning 0.180s)
             Mean action noise std: 2.67
          Mean value_function loss: 365.7339
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 74.0795
                       Mean reward: 675.38
               Mean episode length: 203.99
    Episode_Reward/reaching_object: 1.5893
     Episode_Reward/lifting_object: 124.2100
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 2.35s
                      Time elapsed: 00:30:59
                               ETA: 00:42:45

################################################################################
                     [1m Learning iteration 841/2000 [0m                      

                       Computation: 39943 steps/s (collection: 2.313s, learning 0.149s)
             Mean action noise std: 2.67
          Mean value_function loss: 373.3562
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 74.0816
                       Mean reward: 649.77
               Mean episode length: 195.63
    Episode_Reward/reaching_object: 1.6283
     Episode_Reward/lifting_object: 129.3663
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 2.46s
                      Time elapsed: 00:31:02
                               ETA: 00:42:43

################################################################################
                     [1m Learning iteration 842/2000 [0m                      

                       Computation: 42792 steps/s (collection: 2.191s, learning 0.106s)
             Mean action noise std: 2.67
          Mean value_function loss: 362.2793
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.0827
                       Mean reward: 645.24
               Mean episode length: 194.15
    Episode_Reward/reaching_object: 1.6369
     Episode_Reward/lifting_object: 130.9423
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 2.30s
                      Time elapsed: 00:31:04
                               ETA: 00:42:41

################################################################################
                     [1m Learning iteration 843/2000 [0m                      

                       Computation: 42021 steps/s (collection: 2.220s, learning 0.120s)
             Mean action noise std: 2.67
          Mean value_function loss: 346.3023
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 74.0846
                       Mean reward: 669.02
               Mean episode length: 194.58
    Episode_Reward/reaching_object: 1.6763
     Episode_Reward/lifting_object: 134.5225
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 2.34s
                      Time elapsed: 00:31:06
                               ETA: 00:42:39

################################################################################
                     [1m Learning iteration 844/2000 [0m                      

                       Computation: 42789 steps/s (collection: 2.198s, learning 0.100s)
             Mean action noise std: 2.67
          Mean value_function loss: 359.5687
               Mean surrogate loss: 0.0168
                 Mean entropy loss: 74.0853
                       Mean reward: 652.05
               Mean episode length: 194.04
    Episode_Reward/reaching_object: 1.6546
     Episode_Reward/lifting_object: 132.9096
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 2.30s
                      Time elapsed: 00:31:09
                               ETA: 00:42:37

################################################################################
                     [1m Learning iteration 845/2000 [0m                      

                       Computation: 42465 steps/s (collection: 2.200s, learning 0.115s)
             Mean action noise std: 2.67
          Mean value_function loss: 369.7132
               Mean surrogate loss: 0.0116
                 Mean entropy loss: 74.0855
                       Mean reward: 675.66
               Mean episode length: 199.62
    Episode_Reward/reaching_object: 1.6032
     Episode_Reward/lifting_object: 126.8579
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 2.31s
                      Time elapsed: 00:31:11
                               ETA: 00:42:34

################################################################################
                     [1m Learning iteration 846/2000 [0m                      

                       Computation: 43167 steps/s (collection: 2.170s, learning 0.108s)
             Mean action noise std: 2.67
          Mean value_function loss: 356.6508
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 74.0855
                       Mean reward: 620.89
               Mean episode length: 185.34
    Episode_Reward/reaching_object: 1.5995
     Episode_Reward/lifting_object: 126.1181
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 2.28s
                      Time elapsed: 00:31:13
                               ETA: 00:42:32

################################################################################
                     [1m Learning iteration 847/2000 [0m                      

                       Computation: 42781 steps/s (collection: 2.204s, learning 0.094s)
             Mean action noise std: 2.67
          Mean value_function loss: 371.7110
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 74.0855
                       Mean reward: 646.29
               Mean episode length: 194.67
    Episode_Reward/reaching_object: 1.6045
     Episode_Reward/lifting_object: 126.5810
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 2.30s
                      Time elapsed: 00:31:15
                               ETA: 00:42:30

################################################################################
                     [1m Learning iteration 848/2000 [0m                      

                       Computation: 43169 steps/s (collection: 2.176s, learning 0.102s)
             Mean action noise std: 2.67
          Mean value_function loss: 360.7568
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 74.0856
                       Mean reward: 754.78
               Mean episode length: 216.35
    Episode_Reward/reaching_object: 1.6244
     Episode_Reward/lifting_object: 128.3618
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 2.28s
                      Time elapsed: 00:31:18
                               ETA: 00:42:28

################################################################################
                     [1m Learning iteration 849/2000 [0m                      

                       Computation: 41313 steps/s (collection: 2.246s, learning 0.134s)
             Mean action noise std: 2.67
          Mean value_function loss: 357.0114
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 74.0857
                       Mean reward: 622.75
               Mean episode length: 185.81
    Episode_Reward/reaching_object: 1.5912
     Episode_Reward/lifting_object: 124.6448
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 2.38s
                      Time elapsed: 00:31:20
                               ETA: 00:42:26

################################################################################
                     [1m Learning iteration 850/2000 [0m                      

                       Computation: 42830 steps/s (collection: 2.185s, learning 0.110s)
             Mean action noise std: 2.67
          Mean value_function loss: 374.6322
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 74.0858
                       Mean reward: 643.01
               Mean episode length: 188.11
    Episode_Reward/reaching_object: 1.5930
     Episode_Reward/lifting_object: 126.5421
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 2.30s
                      Time elapsed: 00:31:22
                               ETA: 00:42:24

################################################################################
                     [1m Learning iteration 851/2000 [0m                      

                       Computation: 42268 steps/s (collection: 2.212s, learning 0.114s)
             Mean action noise std: 2.67
          Mean value_function loss: 358.6246
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 74.0861
                       Mean reward: 592.34
               Mean episode length: 180.05
    Episode_Reward/reaching_object: 1.6039
     Episode_Reward/lifting_object: 126.2678
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 2.33s
                      Time elapsed: 00:31:25
                               ETA: 00:42:22

################################################################################
                     [1m Learning iteration 852/2000 [0m                      

                       Computation: 41939 steps/s (collection: 2.204s, learning 0.140s)
             Mean action noise std: 2.67
          Mean value_function loss: 22313.0547
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 74.0869
                       Mean reward: 658.30
               Mean episode length: 193.76
    Episode_Reward/reaching_object: 1.5741
     Episode_Reward/lifting_object: 124.4994
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 2.34s
                      Time elapsed: 00:31:27
                               ETA: 00:42:20

################################################################################
                     [1m Learning iteration 853/2000 [0m                      

                       Computation: 42604 steps/s (collection: 2.151s, learning 0.156s)
             Mean action noise std: 2.67
          Mean value_function loss: 15938761.0446
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 74.0872
                       Mean reward: 604.61
               Mean episode length: 186.09
    Episode_Reward/reaching_object: 1.5617
     Episode_Reward/lifting_object: 122.0575
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.5117
          Episode_Reward/joint_vel: -296.2052
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 2.31s
                      Time elapsed: 00:31:29
                               ETA: 00:42:18

################################################################################
                     [1m Learning iteration 854/2000 [0m                      

                       Computation: 42569 steps/s (collection: 2.217s, learning 0.093s)
             Mean action noise std: 2.67
          Mean value_function loss: 416.1186
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 74.0876
                       Mean reward: 561.61
               Mean episode length: 174.60
    Episode_Reward/reaching_object: 1.5344
     Episode_Reward/lifting_object: 120.7712
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 2.31s
                      Time elapsed: 00:31:32
                               ETA: 00:42:16

################################################################################
                     [1m Learning iteration 855/2000 [0m                      

                       Computation: 43795 steps/s (collection: 2.135s, learning 0.110s)
             Mean action noise std: 2.67
          Mean value_function loss: 451.2332
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 74.0882
                       Mean reward: 573.75
               Mean episode length: 175.98
    Episode_Reward/reaching_object: 1.4349
     Episode_Reward/lifting_object: 111.1130
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.1667
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 2.24s
                      Time elapsed: 00:31:34
                               ETA: 00:42:14

################################################################################
                     [1m Learning iteration 856/2000 [0m                      

                       Computation: 41341 steps/s (collection: 2.247s, learning 0.131s)
             Mean action noise std: 2.67
          Mean value_function loss: 424.7721
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 74.0887
                       Mean reward: 578.49
               Mean episode length: 177.66
    Episode_Reward/reaching_object: 1.4880
     Episode_Reward/lifting_object: 115.8867
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 2.38s
                      Time elapsed: 00:31:36
                               ETA: 00:42:12

################################################################################
                     [1m Learning iteration 857/2000 [0m                      

                       Computation: 43352 steps/s (collection: 2.164s, learning 0.104s)
             Mean action noise std: 2.67
          Mean value_function loss: 451.6416
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.0881
                       Mean reward: 505.84
               Mean episode length: 161.31
    Episode_Reward/reaching_object: 1.3826
     Episode_Reward/lifting_object: 105.2374
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 2.27s
                      Time elapsed: 00:31:39
                               ETA: 00:42:09

################################################################################
                     [1m Learning iteration 858/2000 [0m                      

                       Computation: 43767 steps/s (collection: 2.139s, learning 0.107s)
             Mean action noise std: 2.67
          Mean value_function loss: 475.3365
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 74.0871
                       Mean reward: 555.81
               Mean episode length: 174.42
    Episode_Reward/reaching_object: 1.3865
     Episode_Reward/lifting_object: 104.3630
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 16.5000
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 2.25s
                      Time elapsed: 00:31:41
                               ETA: 00:42:07

################################################################################
                     [1m Learning iteration 859/2000 [0m                      

                       Computation: 44049 steps/s (collection: 2.125s, learning 0.106s)
             Mean action noise std: 2.67
          Mean value_function loss: 516.8494
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 74.0879
                       Mean reward: 540.05
               Mean episode length: 171.36
    Episode_Reward/reaching_object: 1.4147
     Episode_Reward/lifting_object: 105.3234
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.6667
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 2.23s
                      Time elapsed: 00:31:43
                               ETA: 00:42:05

################################################################################
                     [1m Learning iteration 860/2000 [0m                      

                       Computation: 41191 steps/s (collection: 2.238s, learning 0.148s)
             Mean action noise std: 2.67
          Mean value_function loss: 462.1418
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 74.0892
                       Mean reward: 554.08
               Mean episode length: 174.61
    Episode_Reward/reaching_object: 1.3906
     Episode_Reward/lifting_object: 103.3773
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 2.39s
                      Time elapsed: 00:31:45
                               ETA: 00:42:03

################################################################################
                     [1m Learning iteration 861/2000 [0m                      

                       Computation: 41487 steps/s (collection: 2.268s, learning 0.102s)
             Mean action noise std: 2.67
          Mean value_function loss: 436.5667
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 74.0901
                       Mean reward: 475.91
               Mean episode length: 157.79
    Episode_Reward/reaching_object: 1.3930
     Episode_Reward/lifting_object: 102.1567
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.0417
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 2.37s
                      Time elapsed: 00:31:48
                               ETA: 00:42:01

################################################################################
                     [1m Learning iteration 862/2000 [0m                      

                       Computation: 43536 steps/s (collection: 2.152s, learning 0.106s)
             Mean action noise std: 2.67
          Mean value_function loss: 407.7987
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 74.0902
                       Mean reward: 511.45
               Mean episode length: 166.08
    Episode_Reward/reaching_object: 1.4332
     Episode_Reward/lifting_object: 107.3185
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.7500
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 2.26s
                      Time elapsed: 00:31:50
                               ETA: 00:41:59

################################################################################
                     [1m Learning iteration 863/2000 [0m                      

                       Computation: 42686 steps/s (collection: 2.186s, learning 0.117s)
             Mean action noise std: 2.67
          Mean value_function loss: 373.2243
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 74.0915
                       Mean reward: 548.96
               Mean episode length: 169.93
    Episode_Reward/reaching_object: 1.4697
     Episode_Reward/lifting_object: 111.3872
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 2.30s
                      Time elapsed: 00:31:52
                               ETA: 00:41:57

################################################################################
                     [1m Learning iteration 864/2000 [0m                      

                       Computation: 42168 steps/s (collection: 2.216s, learning 0.116s)
             Mean action noise std: 2.67
          Mean value_function loss: 384.5407
               Mean surrogate loss: 0.0193
                 Mean entropy loss: 74.0932
                       Mean reward: 573.98
               Mean episode length: 183.17
    Episode_Reward/reaching_object: 1.5279
     Episode_Reward/lifting_object: 114.4709
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 2.33s
                      Time elapsed: 00:31:55
                               ETA: 00:41:55

################################################################################
                     [1m Learning iteration 865/2000 [0m                      

                       Computation: 41610 steps/s (collection: 2.257s, learning 0.106s)
             Mean action noise std: 2.67
          Mean value_function loss: 382.8850
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 74.0936
                       Mean reward: 541.02
               Mean episode length: 170.42
    Episode_Reward/reaching_object: 1.4467
     Episode_Reward/lifting_object: 107.9112
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 2.36s
                      Time elapsed: 00:31:57
                               ETA: 00:41:53

################################################################################
                     [1m Learning iteration 866/2000 [0m                      

                       Computation: 44164 steps/s (collection: 2.113s, learning 0.113s)
             Mean action noise std: 2.67
          Mean value_function loss: 385.7761
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 74.0937
                       Mean reward: 613.05
               Mean episode length: 188.56
    Episode_Reward/reaching_object: 1.5847
     Episode_Reward/lifting_object: 121.2576
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.9583
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 2.23s
                      Time elapsed: 00:31:59
                               ETA: 00:41:51

################################################################################
                     [1m Learning iteration 867/2000 [0m                      

                       Computation: 43697 steps/s (collection: 2.146s, learning 0.104s)
             Mean action noise std: 2.67
          Mean value_function loss: 386.2302
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 74.0942
                       Mean reward: 657.81
               Mean episode length: 193.93
    Episode_Reward/reaching_object: 1.5838
     Episode_Reward/lifting_object: 121.5211
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 2.25s
                      Time elapsed: 00:32:02
                               ETA: 00:41:48

################################################################################
                     [1m Learning iteration 868/2000 [0m                      

                       Computation: 43740 steps/s (collection: 2.153s, learning 0.094s)
             Mean action noise std: 2.67
          Mean value_function loss: 348.5017
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 74.0948
                       Mean reward: 679.90
               Mean episode length: 202.72
    Episode_Reward/reaching_object: 1.6765
     Episode_Reward/lifting_object: 130.3247
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 2.25s
                      Time elapsed: 00:32:04
                               ETA: 00:41:46

################################################################################
                     [1m Learning iteration 869/2000 [0m                      

                       Computation: 43441 steps/s (collection: 2.158s, learning 0.105s)
             Mean action noise std: 2.67
          Mean value_function loss: 347.6975
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 74.0957
                       Mean reward: 699.72
               Mean episode length: 203.49
    Episode_Reward/reaching_object: 1.7329
     Episode_Reward/lifting_object: 137.7835
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 2.26s
                      Time elapsed: 00:32:06
                               ETA: 00:41:44

################################################################################
                     [1m Learning iteration 870/2000 [0m                      

                       Computation: 44075 steps/s (collection: 2.117s, learning 0.114s)
             Mean action noise std: 2.67
          Mean value_function loss: 357.7311
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 74.0980
                       Mean reward: 728.60
               Mean episode length: 210.69
    Episode_Reward/reaching_object: 1.6926
     Episode_Reward/lifting_object: 134.7385
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 2.23s
                      Time elapsed: 00:32:08
                               ETA: 00:41:42

################################################################################
                     [1m Learning iteration 871/2000 [0m                      

                       Computation: 44153 steps/s (collection: 2.135s, learning 0.091s)
             Mean action noise std: 2.67
          Mean value_function loss: 375.9732
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 74.0987
                       Mean reward: 629.10
               Mean episode length: 188.64
    Episode_Reward/reaching_object: 1.6563
     Episode_Reward/lifting_object: 131.1374
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 2.23s
                      Time elapsed: 00:32:11
                               ETA: 00:41:40

################################################################################
                     [1m Learning iteration 872/2000 [0m                      

                       Computation: 44070 steps/s (collection: 2.134s, learning 0.097s)
             Mean action noise std: 2.67
          Mean value_function loss: 363.8615
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 74.0991
                       Mean reward: 633.66
               Mean episode length: 185.81
    Episode_Reward/reaching_object: 1.6515
     Episode_Reward/lifting_object: 131.6504
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 2.23s
                      Time elapsed: 00:32:13
                               ETA: 00:41:37

################################################################################
                     [1m Learning iteration 873/2000 [0m                      

                       Computation: 43761 steps/s (collection: 2.152s, learning 0.095s)
             Mean action noise std: 2.67
          Mean value_function loss: 373.1028
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 74.1004
                       Mean reward: 627.39
               Mean episode length: 186.30
    Episode_Reward/reaching_object: 1.5706
     Episode_Reward/lifting_object: 123.3203
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 2.25s
                      Time elapsed: 00:32:15
                               ETA: 00:41:35

################################################################################
                     [1m Learning iteration 874/2000 [0m                      

                       Computation: 44242 steps/s (collection: 2.125s, learning 0.097s)
             Mean action noise std: 2.67
          Mean value_function loss: 387.2681
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 74.1018
                       Mean reward: 644.70
               Mean episode length: 189.72
    Episode_Reward/reaching_object: 1.5913
     Episode_Reward/lifting_object: 125.6203
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 2.22s
                      Time elapsed: 00:32:17
                               ETA: 00:41:33

################################################################################
                     [1m Learning iteration 875/2000 [0m                      

                       Computation: 43835 steps/s (collection: 2.144s, learning 0.099s)
             Mean action noise std: 2.67
          Mean value_function loss: 402.9503
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 74.1025
                       Mean reward: 651.51
               Mean episode length: 191.59
    Episode_Reward/reaching_object: 1.5805
     Episode_Reward/lifting_object: 124.5313
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 2.24s
                      Time elapsed: 00:32:19
                               ETA: 00:41:31

################################################################################
                     [1m Learning iteration 876/2000 [0m                      

                       Computation: 44110 steps/s (collection: 2.137s, learning 0.092s)
             Mean action noise std: 2.67
          Mean value_function loss: 419.2424
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 74.1029
                       Mean reward: 661.99
               Mean episode length: 193.84
    Episode_Reward/reaching_object: 1.6376
     Episode_Reward/lifting_object: 130.1231
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 2.23s
                      Time elapsed: 00:32:22
                               ETA: 00:41:29

################################################################################
                     [1m Learning iteration 877/2000 [0m                      

                       Computation: 43236 steps/s (collection: 2.162s, learning 0.112s)
             Mean action noise std: 2.67
          Mean value_function loss: 373.0022
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 74.1035
                       Mean reward: 656.90
               Mean episode length: 193.42
    Episode_Reward/reaching_object: 1.6126
     Episode_Reward/lifting_object: 127.9808
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 2.27s
                      Time elapsed: 00:32:24
                               ETA: 00:41:27

################################################################################
                     [1m Learning iteration 878/2000 [0m                      

                       Computation: 43651 steps/s (collection: 2.140s, learning 0.112s)
             Mean action noise std: 2.67
          Mean value_function loss: 367.3407
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 74.1050
                       Mean reward: 705.11
               Mean episode length: 205.44
    Episode_Reward/reaching_object: 1.6577
     Episode_Reward/lifting_object: 131.9963
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 2.25s
                      Time elapsed: 00:32:26
                               ETA: 00:41:24

################################################################################
                     [1m Learning iteration 879/2000 [0m                      

                       Computation: 44158 steps/s (collection: 2.126s, learning 0.100s)
             Mean action noise std: 2.67
          Mean value_function loss: 356.0400
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 74.1062
                       Mean reward: 640.74
               Mean episode length: 188.06
    Episode_Reward/reaching_object: 1.6541
     Episode_Reward/lifting_object: 132.1189
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 2.23s
                      Time elapsed: 00:32:28
                               ETA: 00:41:22

################################################################################
                     [1m Learning iteration 880/2000 [0m                      

                       Computation: 44379 steps/s (collection: 2.116s, learning 0.099s)
             Mean action noise std: 2.67
          Mean value_function loss: 314.6702
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 74.1080
                       Mean reward: 701.44
               Mean episode length: 204.33
    Episode_Reward/reaching_object: 1.7178
     Episode_Reward/lifting_object: 137.2689
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 2.22s
                      Time elapsed: 00:32:31
                               ETA: 00:41:20

################################################################################
                     [1m Learning iteration 881/2000 [0m                      

                       Computation: 44404 steps/s (collection: 2.121s, learning 0.093s)
             Mean action noise std: 2.67
          Mean value_function loss: 327.3570
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 74.1084
                       Mean reward: 651.48
               Mean episode length: 192.17
    Episode_Reward/reaching_object: 1.6251
     Episode_Reward/lifting_object: 128.8392
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 2.21s
                      Time elapsed: 00:32:33
                               ETA: 00:41:18

################################################################################
                     [1m Learning iteration 882/2000 [0m                      

                       Computation: 42147 steps/s (collection: 2.206s, learning 0.126s)
             Mean action noise std: 2.67
          Mean value_function loss: 319.0976
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 74.1086
                       Mean reward: 722.40
               Mean episode length: 208.61
    Episode_Reward/reaching_object: 1.7109
     Episode_Reward/lifting_object: 136.5666
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 2.33s
                      Time elapsed: 00:32:35
                               ETA: 00:41:16

################################################################################
                     [1m Learning iteration 883/2000 [0m                      

                       Computation: 42238 steps/s (collection: 2.229s, learning 0.098s)
             Mean action noise std: 2.67
          Mean value_function loss: 308.0693
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 74.1092
                       Mean reward: 712.19
               Mean episode length: 204.31
    Episode_Reward/reaching_object: 1.7117
     Episode_Reward/lifting_object: 137.4885
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 2.33s
                      Time elapsed: 00:32:38
                               ETA: 00:41:14

################################################################################
                     [1m Learning iteration 884/2000 [0m                      

                       Computation: 42923 steps/s (collection: 2.174s, learning 0.116s)
             Mean action noise std: 2.67
          Mean value_function loss: 341.6126
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 74.1116
                       Mean reward: 714.33
               Mean episode length: 205.19
    Episode_Reward/reaching_object: 1.6471
     Episode_Reward/lifting_object: 131.6306
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 2.29s
                      Time elapsed: 00:32:40
                               ETA: 00:41:12

################################################################################
                     [1m Learning iteration 885/2000 [0m                      

                       Computation: 43002 steps/s (collection: 2.180s, learning 0.106s)
             Mean action noise std: 2.67
          Mean value_function loss: 322.4533
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 74.1131
                       Mean reward: 743.17
               Mean episode length: 214.12
    Episode_Reward/reaching_object: 1.7151
     Episode_Reward/lifting_object: 136.3023
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 2.29s
                      Time elapsed: 00:32:42
                               ETA: 00:41:09

################################################################################
                     [1m Learning iteration 886/2000 [0m                      

                       Computation: 44041 steps/s (collection: 2.131s, learning 0.101s)
             Mean action noise std: 2.67
          Mean value_function loss: 328.4634
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 74.1134
                       Mean reward: 734.25
               Mean episode length: 210.77
    Episode_Reward/reaching_object: 1.7411
     Episode_Reward/lifting_object: 139.7754
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 2.23s
                      Time elapsed: 00:32:44
                               ETA: 00:41:07

################################################################################
                     [1m Learning iteration 887/2000 [0m                      

                       Computation: 43614 steps/s (collection: 2.158s, learning 0.095s)
             Mean action noise std: 2.67
          Mean value_function loss: 326.7333
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 74.1138
                       Mean reward: 737.82
               Mean episode length: 210.58
    Episode_Reward/reaching_object: 1.8152
     Episode_Reward/lifting_object: 147.3784
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 2.25s
                      Time elapsed: 00:32:47
                               ETA: 00:41:05

################################################################################
                     [1m Learning iteration 888/2000 [0m                      

                       Computation: 44252 steps/s (collection: 2.120s, learning 0.101s)
             Mean action noise std: 2.67
          Mean value_function loss: 319.3733
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 74.1142
                       Mean reward: 679.03
               Mean episode length: 194.06
    Episode_Reward/reaching_object: 1.7319
     Episode_Reward/lifting_object: 140.3413
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 2.22s
                      Time elapsed: 00:32:49
                               ETA: 00:41:03

################################################################################
                     [1m Learning iteration 889/2000 [0m                      

                       Computation: 43978 steps/s (collection: 2.132s, learning 0.104s)
             Mean action noise std: 2.67
          Mean value_function loss: 291.5714
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 74.1148
                       Mean reward: 752.89
               Mean episode length: 214.18
    Episode_Reward/reaching_object: 1.7691
     Episode_Reward/lifting_object: 144.1610
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 2.24s
                      Time elapsed: 00:32:51
                               ETA: 00:41:01

################################################################################
                     [1m Learning iteration 890/2000 [0m                      

                       Computation: 43309 steps/s (collection: 2.157s, learning 0.113s)
             Mean action noise std: 2.67
          Mean value_function loss: 289.7194
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 74.1153
                       Mean reward: 727.53
               Mean episode length: 209.71
    Episode_Reward/reaching_object: 1.7521
     Episode_Reward/lifting_object: 141.5569
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 2.27s
                      Time elapsed: 00:32:53
                               ETA: 00:40:59

################################################################################
                     [1m Learning iteration 891/2000 [0m                      

                       Computation: 43006 steps/s (collection: 2.166s, learning 0.120s)
             Mean action noise std: 2.67
          Mean value_function loss: 278.2336
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 74.1156
                       Mean reward: 746.10
               Mean episode length: 212.21
    Episode_Reward/reaching_object: 1.8077
     Episode_Reward/lifting_object: 146.4151
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 2.29s
                      Time elapsed: 00:32:56
                               ETA: 00:40:56

################################################################################
                     [1m Learning iteration 892/2000 [0m                      

                       Computation: 43451 steps/s (collection: 2.149s, learning 0.113s)
             Mean action noise std: 2.67
          Mean value_function loss: 291.0843
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 74.1157
                       Mean reward: 729.52
               Mean episode length: 213.80
    Episode_Reward/reaching_object: 1.7602
     Episode_Reward/lifting_object: 142.2126
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 2.26s
                      Time elapsed: 00:32:58
                               ETA: 00:40:54

################################################################################
                     [1m Learning iteration 893/2000 [0m                      

                       Computation: 43758 steps/s (collection: 2.138s, learning 0.109s)
             Mean action noise std: 2.67
          Mean value_function loss: 286.1304
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 74.1160
                       Mean reward: 763.61
               Mean episode length: 220.36
    Episode_Reward/reaching_object: 1.8075
     Episode_Reward/lifting_object: 146.7455
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 2.25s
                      Time elapsed: 00:33:00
                               ETA: 00:40:52

################################################################################
                     [1m Learning iteration 894/2000 [0m                      

                       Computation: 43444 steps/s (collection: 2.160s, learning 0.103s)
             Mean action noise std: 2.67
          Mean value_function loss: 283.5986
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 74.1167
                       Mean reward: 653.42
               Mean episode length: 190.61
    Episode_Reward/reaching_object: 1.7114
     Episode_Reward/lifting_object: 138.5843
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 2.26s
                      Time elapsed: 00:33:02
                               ETA: 00:40:50

################################################################################
                     [1m Learning iteration 895/2000 [0m                      

                       Computation: 44140 steps/s (collection: 2.124s, learning 0.104s)
             Mean action noise std: 2.67
          Mean value_function loss: 280.5872
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 74.1170
                       Mean reward: 725.72
               Mean episode length: 211.77
    Episode_Reward/reaching_object: 1.8236
     Episode_Reward/lifting_object: 148.6949
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 2.23s
                      Time elapsed: 00:33:05
                               ETA: 00:40:48

################################################################################
                     [1m Learning iteration 896/2000 [0m                      

                       Computation: 44209 steps/s (collection: 2.128s, learning 0.095s)
             Mean action noise std: 2.67
          Mean value_function loss: 284.0514
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 74.1171
                       Mean reward: 760.71
               Mean episode length: 217.32
    Episode_Reward/reaching_object: 1.8667
     Episode_Reward/lifting_object: 153.1735
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 2.22s
                      Time elapsed: 00:33:07
                               ETA: 00:40:45

################################################################################
                     [1m Learning iteration 897/2000 [0m                      

                       Computation: 44182 steps/s (collection: 2.124s, learning 0.101s)
             Mean action noise std: 2.67
          Mean value_function loss: 342.5560
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 74.1177
                       Mean reward: 748.18
               Mean episode length: 210.17
    Episode_Reward/reaching_object: 1.8084
     Episode_Reward/lifting_object: 148.0131
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 2.22s
                      Time elapsed: 00:33:09
                               ETA: 00:40:43

################################################################################
                     [1m Learning iteration 898/2000 [0m                      

                       Computation: 42854 steps/s (collection: 2.189s, learning 0.105s)
             Mean action noise std: 2.67
          Mean value_function loss: 322.3019
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 74.1202
                       Mean reward: 745.57
               Mean episode length: 213.75
    Episode_Reward/reaching_object: 1.7728
     Episode_Reward/lifting_object: 143.5975
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 2.29s
                      Time elapsed: 00:33:11
                               ETA: 00:40:41

################################################################################
                     [1m Learning iteration 899/2000 [0m                      

                       Computation: 43209 steps/s (collection: 2.168s, learning 0.107s)
             Mean action noise std: 2.67
          Mean value_function loss: 314.8539
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 74.1233
                       Mean reward: 680.57
               Mean episode length: 199.62
    Episode_Reward/reaching_object: 1.7288
     Episode_Reward/lifting_object: 140.8450
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 2.28s
                      Time elapsed: 00:33:14
                               ETA: 00:40:39

################################################################################
                     [1m Learning iteration 900/2000 [0m                      

                       Computation: 43763 steps/s (collection: 2.150s, learning 0.096s)
             Mean action noise std: 2.67
          Mean value_function loss: 314.8288
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 74.1261
                       Mean reward: 651.07
               Mean episode length: 190.86
    Episode_Reward/reaching_object: 1.7081
     Episode_Reward/lifting_object: 137.8083
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 2.25s
                      Time elapsed: 00:33:16
                               ETA: 00:40:37

################################################################################
                     [1m Learning iteration 901/2000 [0m                      

                       Computation: 43105 steps/s (collection: 2.189s, learning 0.092s)
             Mean action noise std: 2.67
          Mean value_function loss: 320.3549
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 74.1289
                       Mean reward: 718.55
               Mean episode length: 205.92
    Episode_Reward/reaching_object: 1.7054
     Episode_Reward/lifting_object: 138.3263
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 2.28s
                      Time elapsed: 00:33:18
                               ETA: 00:40:35

################################################################################
                     [1m Learning iteration 902/2000 [0m                      

                       Computation: 43901 steps/s (collection: 2.142s, learning 0.098s)
             Mean action noise std: 2.67
          Mean value_function loss: 291.9375
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 74.1331
                       Mean reward: 785.21
               Mean episode length: 221.82
    Episode_Reward/reaching_object: 1.8089
     Episode_Reward/lifting_object: 147.7947
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 2.24s
                      Time elapsed: 00:33:20
                               ETA: 00:40:33

################################################################################
                     [1m Learning iteration 903/2000 [0m                      

                       Computation: 43946 steps/s (collection: 2.119s, learning 0.118s)
             Mean action noise std: 2.67
          Mean value_function loss: 282.2847
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 74.1359
                       Mean reward: 797.65
               Mean episode length: 221.57
    Episode_Reward/reaching_object: 1.7835
     Episode_Reward/lifting_object: 144.4043
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 2.24s
                      Time elapsed: 00:33:23
                               ETA: 00:40:30

################################################################################
                     [1m Learning iteration 904/2000 [0m                      

                       Computation: 42971 steps/s (collection: 2.173s, learning 0.115s)
             Mean action noise std: 2.67
          Mean value_function loss: 298.6378
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 74.1368
                       Mean reward: 721.62
               Mean episode length: 208.82
    Episode_Reward/reaching_object: 1.7597
     Episode_Reward/lifting_object: 143.6027
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 2.29s
                      Time elapsed: 00:33:25
                               ETA: 00:40:28

################################################################################
                     [1m Learning iteration 905/2000 [0m                      

                       Computation: 42407 steps/s (collection: 2.190s, learning 0.128s)
             Mean action noise std: 2.68
          Mean value_function loss: 282.8523
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 74.1382
                       Mean reward: 752.87
               Mean episode length: 214.70
    Episode_Reward/reaching_object: 1.8517
     Episode_Reward/lifting_object: 150.6559
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 2.32s
                      Time elapsed: 00:33:27
                               ETA: 00:40:26

################################################################################
                     [1m Learning iteration 906/2000 [0m                      

                       Computation: 43730 steps/s (collection: 2.152s, learning 0.096s)
             Mean action noise std: 2.68
          Mean value_function loss: 295.6443
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 74.1411
                       Mean reward: 762.35
               Mean episode length: 216.37
    Episode_Reward/reaching_object: 1.8281
     Episode_Reward/lifting_object: 149.3105
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0694
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 2.25s
                      Time elapsed: 00:33:30
                               ETA: 00:40:24

################################################################################
                     [1m Learning iteration 907/2000 [0m                      

                       Computation: 44155 steps/s (collection: 2.125s, learning 0.101s)
             Mean action noise std: 2.68
          Mean value_function loss: 294.7022
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 74.1431
                       Mean reward: 754.82
               Mean episode length: 214.81
    Episode_Reward/reaching_object: 1.8090
     Episode_Reward/lifting_object: 146.7441
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0692
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 2.23s
                      Time elapsed: 00:33:32
                               ETA: 00:40:22

################################################################################
                     [1m Learning iteration 908/2000 [0m                      

                       Computation: 43902 steps/s (collection: 2.136s, learning 0.103s)
             Mean action noise std: 2.68
          Mean value_function loss: 303.2498
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.1465
                       Mean reward: 790.59
               Mean episode length: 218.80
    Episode_Reward/reaching_object: 1.8718
     Episode_Reward/lifting_object: 154.3251
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0694
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 2.24s
                      Time elapsed: 00:33:34
                               ETA: 00:40:20

################################################################################
                     [1m Learning iteration 909/2000 [0m                      

                       Computation: 43854 steps/s (collection: 2.132s, learning 0.110s)
             Mean action noise std: 2.68
          Mean value_function loss: 303.1922
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 74.1511
                       Mean reward: 785.40
               Mean episode length: 219.38
    Episode_Reward/reaching_object: 1.8396
     Episode_Reward/lifting_object: 150.7532
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 2.24s
                      Time elapsed: 00:33:36
                               ETA: 00:40:17

################################################################################
                     [1m Learning iteration 910/2000 [0m                      

                       Computation: 43856 steps/s (collection: 2.142s, learning 0.099s)
             Mean action noise std: 2.68
          Mean value_function loss: 309.1050
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 74.1522
                       Mean reward: 728.30
               Mean episode length: 206.56
    Episode_Reward/reaching_object: 1.7838
     Episode_Reward/lifting_object: 145.6119
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 2.24s
                      Time elapsed: 00:33:38
                               ETA: 00:40:15

################################################################################
                     [1m Learning iteration 911/2000 [0m                      

                       Computation: 43861 steps/s (collection: 2.139s, learning 0.103s)
             Mean action noise std: 2.68
          Mean value_function loss: 302.0199
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 74.1528
                       Mean reward: 757.03
               Mean episode length: 212.87
    Episode_Reward/reaching_object: 1.7157
     Episode_Reward/lifting_object: 138.6988
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 2.24s
                      Time elapsed: 00:33:41
                               ETA: 00:40:13

################################################################################
                     [1m Learning iteration 912/2000 [0m                      

                       Computation: 43391 steps/s (collection: 2.167s, learning 0.099s)
             Mean action noise std: 2.68
          Mean value_function loss: 295.8925
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 74.1551
                       Mean reward: 759.11
               Mean episode length: 211.85
    Episode_Reward/reaching_object: 1.8116
     Episode_Reward/lifting_object: 149.1661
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 2.27s
                      Time elapsed: 00:33:43
                               ETA: 00:40:11

################################################################################
                     [1m Learning iteration 913/2000 [0m                      

                       Computation: 43450 steps/s (collection: 2.161s, learning 0.101s)
             Mean action noise std: 2.68
          Mean value_function loss: 310.1273
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 74.1590
                       Mean reward: 748.98
               Mean episode length: 212.34
    Episode_Reward/reaching_object: 1.7558
     Episode_Reward/lifting_object: 142.9856
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 2.26s
                      Time elapsed: 00:33:45
                               ETA: 00:40:09

################################################################################
                     [1m Learning iteration 914/2000 [0m                      

                       Computation: 43923 steps/s (collection: 2.136s, learning 0.102s)
             Mean action noise std: 2.68
          Mean value_function loss: 311.8444
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 74.1640
                       Mean reward: 787.28
               Mean episode length: 220.37
    Episode_Reward/reaching_object: 1.7766
     Episode_Reward/lifting_object: 144.0554
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 2.24s
                      Time elapsed: 00:33:47
                               ETA: 00:40:06

################################################################################
                     [1m Learning iteration 915/2000 [0m                      

                       Computation: 43257 steps/s (collection: 2.167s, learning 0.106s)
             Mean action noise std: 2.68
          Mean value_function loss: 308.3796
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 74.1665
                       Mean reward: 757.58
               Mean episode length: 211.64
    Episode_Reward/reaching_object: 1.8312
     Episode_Reward/lifting_object: 149.9266
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 2.27s
                      Time elapsed: 00:33:50
                               ETA: 00:40:04

################################################################################
                     [1m Learning iteration 916/2000 [0m                      

                       Computation: 43694 steps/s (collection: 2.154s, learning 0.096s)
             Mean action noise std: 2.68
          Mean value_function loss: 298.7475
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 74.1704
                       Mean reward: 764.09
               Mean episode length: 215.27
    Episode_Reward/reaching_object: 1.8266
     Episode_Reward/lifting_object: 150.1394
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 2.25s
                      Time elapsed: 00:33:52
                               ETA: 00:40:02

################################################################################
                     [1m Learning iteration 917/2000 [0m                      

                       Computation: 44180 steps/s (collection: 2.129s, learning 0.096s)
             Mean action noise std: 2.68
          Mean value_function loss: 300.3931
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 74.1770
                       Mean reward: 784.65
               Mean episode length: 220.36
    Episode_Reward/reaching_object: 1.8114
     Episode_Reward/lifting_object: 148.9808
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 2.23s
                      Time elapsed: 00:33:54
                               ETA: 00:40:00

################################################################################
                     [1m Learning iteration 918/2000 [0m                      

                       Computation: 43042 steps/s (collection: 2.181s, learning 0.103s)
             Mean action noise std: 2.68
          Mean value_function loss: 272.3888
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 74.1810
                       Mean reward: 806.97
               Mean episode length: 223.10
    Episode_Reward/reaching_object: 1.8115
     Episode_Reward/lifting_object: 149.5684
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 2.28s
                      Time elapsed: 00:33:56
                               ETA: 00:39:58

################################################################################
                     [1m Learning iteration 919/2000 [0m                      

                       Computation: 44034 steps/s (collection: 2.118s, learning 0.114s)
             Mean action noise std: 2.68
          Mean value_function loss: 284.8358
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 74.1832
                       Mean reward: 806.57
               Mean episode length: 224.97
    Episode_Reward/reaching_object: 1.8596
     Episode_Reward/lifting_object: 151.5708
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0705
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 2.23s
                      Time elapsed: 00:33:59
                               ETA: 00:39:56

################################################################################
                     [1m Learning iteration 920/2000 [0m                      

                       Computation: 43622 steps/s (collection: 2.143s, learning 0.111s)
             Mean action noise std: 2.68
          Mean value_function loss: 275.1066
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 74.1837
                       Mean reward: 774.07
               Mean episode length: 221.87
    Episode_Reward/reaching_object: 1.9122
     Episode_Reward/lifting_object: 157.1086
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0710
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 2.25s
                      Time elapsed: 00:34:01
                               ETA: 00:39:53

################################################################################
                     [1m Learning iteration 921/2000 [0m                      

                       Computation: 44418 steps/s (collection: 2.113s, learning 0.101s)
             Mean action noise std: 2.68
          Mean value_function loss: 313.6964
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 74.1840
                       Mean reward: 745.54
               Mean episode length: 208.95
    Episode_Reward/reaching_object: 1.8375
     Episode_Reward/lifting_object: 151.8644
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 2.21s
                      Time elapsed: 00:34:03
                               ETA: 00:39:51

################################################################################
                     [1m Learning iteration 922/2000 [0m                      

                       Computation: 43944 steps/s (collection: 2.141s, learning 0.096s)
             Mean action noise std: 2.68
          Mean value_function loss: 268.2891
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 74.1846
                       Mean reward: 740.03
               Mean episode length: 208.14
    Episode_Reward/reaching_object: 1.8091
     Episode_Reward/lifting_object: 147.6176
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 2.24s
                      Time elapsed: 00:34:05
                               ETA: 00:39:49

################################################################################
                     [1m Learning iteration 923/2000 [0m                      

                       Computation: 43901 steps/s (collection: 2.136s, learning 0.103s)
             Mean action noise std: 2.68
          Mean value_function loss: 306.6912
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 74.1880
                       Mean reward: 785.94
               Mean episode length: 221.46
    Episode_Reward/reaching_object: 1.8206
     Episode_Reward/lifting_object: 148.9656
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 2.24s
                      Time elapsed: 00:34:08
                               ETA: 00:39:47

################################################################################
                     [1m Learning iteration 924/2000 [0m                      

                       Computation: 44222 steps/s (collection: 2.112s, learning 0.111s)
             Mean action noise std: 2.68
          Mean value_function loss: 308.6977
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.1974
                       Mean reward: 719.70
               Mean episode length: 205.12
    Episode_Reward/reaching_object: 1.7571
     Episode_Reward/lifting_object: 143.4639
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 2.22s
                      Time elapsed: 00:34:10
                               ETA: 00:39:45

################################################################################
                     [1m Learning iteration 925/2000 [0m                      

                       Computation: 43533 steps/s (collection: 2.164s, learning 0.094s)
             Mean action noise std: 2.68
          Mean value_function loss: 297.4860
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 74.2085
                       Mean reward: 728.92
               Mean episode length: 205.98
    Episode_Reward/reaching_object: 1.8141
     Episode_Reward/lifting_object: 149.0734
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 2.26s
                      Time elapsed: 00:34:12
                               ETA: 00:39:42

################################################################################
                     [1m Learning iteration 926/2000 [0m                      

                       Computation: 42078 steps/s (collection: 2.232s, learning 0.105s)
             Mean action noise std: 2.68
          Mean value_function loss: 292.8778
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 74.2152
                       Mean reward: 769.71
               Mean episode length: 215.88
    Episode_Reward/reaching_object: 1.8602
     Episode_Reward/lifting_object: 153.2141
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 2.34s
                      Time elapsed: 00:34:14
                               ETA: 00:39:40

################################################################################
                     [1m Learning iteration 927/2000 [0m                      

                       Computation: 43096 steps/s (collection: 2.184s, learning 0.097s)
             Mean action noise std: 2.68
          Mean value_function loss: 271.4562
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 74.2210
                       Mean reward: 758.73
               Mean episode length: 215.24
    Episode_Reward/reaching_object: 1.8194
     Episode_Reward/lifting_object: 149.0896
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0686
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 2.28s
                      Time elapsed: 00:34:17
                               ETA: 00:39:38

################################################################################
                     [1m Learning iteration 928/2000 [0m                      

                       Computation: 44116 steps/s (collection: 2.131s, learning 0.098s)
             Mean action noise std: 2.68
          Mean value_function loss: 268.3249
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 74.2292
                       Mean reward: 814.41
               Mean episode length: 227.10
    Episode_Reward/reaching_object: 1.8960
     Episode_Reward/lifting_object: 155.4884
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0718
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 2.23s
                      Time elapsed: 00:34:19
                               ETA: 00:39:36

################################################################################
                     [1m Learning iteration 929/2000 [0m                      

                       Computation: 42986 steps/s (collection: 2.190s, learning 0.097s)
             Mean action noise std: 2.68
          Mean value_function loss: 268.6012
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 74.2347
                       Mean reward: 777.87
               Mean episode length: 219.30
    Episode_Reward/reaching_object: 1.8708
     Episode_Reward/lifting_object: 153.8180
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 2.29s
                      Time elapsed: 00:34:21
                               ETA: 00:39:34

################################################################################
                     [1m Learning iteration 930/2000 [0m                      

                       Computation: 43025 steps/s (collection: 2.163s, learning 0.122s)
             Mean action noise std: 2.68
          Mean value_function loss: 250.0610
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.2382
                       Mean reward: 756.38
               Mean episode length: 213.09
    Episode_Reward/reaching_object: 1.8713
     Episode_Reward/lifting_object: 152.5684
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 2.28s
                      Time elapsed: 00:34:24
                               ETA: 00:39:32

################################################################################
                     [1m Learning iteration 931/2000 [0m                      

                       Computation: 42203 steps/s (collection: 2.200s, learning 0.130s)
             Mean action noise std: 2.69
          Mean value_function loss: 302.4893
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 74.2445
                       Mean reward: 674.30
               Mean episode length: 193.96
    Episode_Reward/reaching_object: 1.8136
     Episode_Reward/lifting_object: 147.5965
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 2.33s
                      Time elapsed: 00:34:26
                               ETA: 00:39:30

################################################################################
                     [1m Learning iteration 932/2000 [0m                      

                       Computation: 43214 steps/s (collection: 2.154s, learning 0.121s)
             Mean action noise std: 2.69
          Mean value_function loss: 271.9988
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 74.2550
                       Mean reward: 759.01
               Mean episode length: 213.70
    Episode_Reward/reaching_object: 1.8080
     Episode_Reward/lifting_object: 147.0416
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 2.27s
                      Time elapsed: 00:34:28
                               ETA: 00:39:28

################################################################################
                     [1m Learning iteration 933/2000 [0m                      

                       Computation: 44102 steps/s (collection: 2.124s, learning 0.105s)
             Mean action noise std: 2.69
          Mean value_function loss: 277.4781
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 74.2598
                       Mean reward: 738.07
               Mean episode length: 210.07
    Episode_Reward/reaching_object: 1.7578
     Episode_Reward/lifting_object: 142.9329
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 2.23s
                      Time elapsed: 00:34:30
                               ETA: 00:39:25

################################################################################
                     [1m Learning iteration 934/2000 [0m                      

                       Computation: 44454 steps/s (collection: 2.109s, learning 0.103s)
             Mean action noise std: 2.69
          Mean value_function loss: 281.5542
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 74.2639
                       Mean reward: 707.73
               Mean episode length: 200.09
    Episode_Reward/reaching_object: 1.8075
     Episode_Reward/lifting_object: 148.4726
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 2.21s
                      Time elapsed: 00:34:33
                               ETA: 00:39:23

################################################################################
                     [1m Learning iteration 935/2000 [0m                      

                       Computation: 44466 steps/s (collection: 2.110s, learning 0.101s)
             Mean action noise std: 2.69
          Mean value_function loss: 264.8572
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 74.2695
                       Mean reward: 789.20
               Mean episode length: 219.19
    Episode_Reward/reaching_object: 1.9164
     Episode_Reward/lifting_object: 158.2220
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0718
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 2.21s
                      Time elapsed: 00:34:35
                               ETA: 00:39:21

################################################################################
                     [1m Learning iteration 936/2000 [0m                      

                       Computation: 43983 steps/s (collection: 2.137s, learning 0.098s)
             Mean action noise std: 2.69
          Mean value_function loss: 237.4926
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 74.2723
                       Mean reward: 772.09
               Mean episode length: 212.90
    Episode_Reward/reaching_object: 1.8875
     Episode_Reward/lifting_object: 155.6305
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0718
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 2.24s
                      Time elapsed: 00:34:37
                               ETA: 00:39:19

################################################################################
                     [1m Learning iteration 937/2000 [0m                      

                       Computation: 43920 steps/s (collection: 2.139s, learning 0.100s)
             Mean action noise std: 2.69
          Mean value_function loss: 249.6676
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 74.2780
                       Mean reward: 789.12
               Mean episode length: 219.05
    Episode_Reward/reaching_object: 1.8254
     Episode_Reward/lifting_object: 149.6996
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 2.24s
                      Time elapsed: 00:34:39
                               ETA: 00:39:16

################################################################################
                     [1m Learning iteration 938/2000 [0m                      

                       Computation: 43376 steps/s (collection: 2.168s, learning 0.099s)
             Mean action noise std: 2.69
          Mean value_function loss: 232.1473
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 74.2837
                       Mean reward: 816.48
               Mean episode length: 224.13
    Episode_Reward/reaching_object: 1.9070
     Episode_Reward/lifting_object: 157.8477
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 2.27s
                      Time elapsed: 00:34:42
                               ETA: 00:39:14

################################################################################
                     [1m Learning iteration 939/2000 [0m                      

                       Computation: 43873 steps/s (collection: 2.147s, learning 0.094s)
             Mean action noise std: 2.69
          Mean value_function loss: 259.5015
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 74.2853
                       Mean reward: 812.54
               Mean episode length: 222.27
    Episode_Reward/reaching_object: 1.8661
     Episode_Reward/lifting_object: 154.2587
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 2.24s
                      Time elapsed: 00:34:44
                               ETA: 00:39:12

################################################################################
                     [1m Learning iteration 940/2000 [0m                      

                       Computation: 44191 steps/s (collection: 2.124s, learning 0.100s)
             Mean action noise std: 2.69
          Mean value_function loss: 246.7548
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 74.2863
                       Mean reward: 766.56
               Mean episode length: 218.29
    Episode_Reward/reaching_object: 1.8689
     Episode_Reward/lifting_object: 153.5097
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 2.22s
                      Time elapsed: 00:34:46
                               ETA: 00:39:10

################################################################################
                     [1m Learning iteration 941/2000 [0m                      

                       Computation: 44454 steps/s (collection: 2.106s, learning 0.105s)
             Mean action noise std: 2.69
          Mean value_function loss: 264.0358
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 74.2866
                       Mean reward: 777.56
               Mean episode length: 216.81
    Episode_Reward/reaching_object: 1.8730
     Episode_Reward/lifting_object: 154.7732
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0702
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 2.21s
                      Time elapsed: 00:34:48
                               ETA: 00:39:08

################################################################################
                     [1m Learning iteration 942/2000 [0m                      

                       Computation: 43297 steps/s (collection: 2.170s, learning 0.100s)
             Mean action noise std: 2.69
          Mean value_function loss: 273.9052
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 74.2871
                       Mean reward: 768.04
               Mean episode length: 213.72
    Episode_Reward/reaching_object: 1.8755
     Episode_Reward/lifting_object: 154.5423
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 2.27s
                      Time elapsed: 00:34:51
                               ETA: 00:39:06

################################################################################
                     [1m Learning iteration 943/2000 [0m                      

                       Computation: 42998 steps/s (collection: 2.178s, learning 0.108s)
             Mean action noise std: 2.69
          Mean value_function loss: 266.7814
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 74.2886
                       Mean reward: 767.07
               Mean episode length: 216.83
    Episode_Reward/reaching_object: 1.8394
     Episode_Reward/lifting_object: 150.6780
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0694
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 2.29s
                      Time elapsed: 00:34:53
                               ETA: 00:39:03

################################################################################
                     [1m Learning iteration 944/2000 [0m                      

                       Computation: 43724 steps/s (collection: 2.124s, learning 0.124s)
             Mean action noise std: 2.69
          Mean value_function loss: 268.7468
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 74.2909
                       Mean reward: 782.61
               Mean episode length: 218.83
    Episode_Reward/reaching_object: 1.8528
     Episode_Reward/lifting_object: 151.7644
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 2.25s
                      Time elapsed: 00:34:55
                               ETA: 00:39:01

################################################################################
                     [1m Learning iteration 945/2000 [0m                      

                       Computation: 43620 steps/s (collection: 2.152s, learning 0.101s)
             Mean action noise std: 2.69
          Mean value_function loss: 248.2112
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 74.2915
                       Mean reward: 787.55
               Mean episode length: 218.79
    Episode_Reward/reaching_object: 1.8840
     Episode_Reward/lifting_object: 155.2213
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0714
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 2.25s
                      Time elapsed: 00:34:57
                               ETA: 00:38:59

################################################################################
                     [1m Learning iteration 946/2000 [0m                      

                       Computation: 43429 steps/s (collection: 2.167s, learning 0.096s)
             Mean action noise std: 2.69
          Mean value_function loss: 266.6985
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 74.2921
                       Mean reward: 758.22
               Mean episode length: 214.55
    Episode_Reward/reaching_object: 1.8527
     Episode_Reward/lifting_object: 152.1098
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0701
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 2.26s
                      Time elapsed: 00:35:00
                               ETA: 00:38:57

################################################################################
                     [1m Learning iteration 947/2000 [0m                      

                       Computation: 42559 steps/s (collection: 2.210s, learning 0.100s)
             Mean action noise std: 2.69
          Mean value_function loss: 260.0431
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 74.2930
                       Mean reward: 787.01
               Mean episode length: 220.97
    Episode_Reward/reaching_object: 1.8721
     Episode_Reward/lifting_object: 154.1355
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 2.31s
                      Time elapsed: 00:35:02
                               ETA: 00:38:55

################################################################################
                     [1m Learning iteration 948/2000 [0m                      

                       Computation: 43401 steps/s (collection: 2.161s, learning 0.104s)
             Mean action noise std: 2.69
          Mean value_function loss: 277.1893
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 74.2946
                       Mean reward: 723.59
               Mean episode length: 206.33
    Episode_Reward/reaching_object: 1.8434
     Episode_Reward/lifting_object: 150.5659
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 2.26s
                      Time elapsed: 00:35:04
                               ETA: 00:38:53

################################################################################
                     [1m Learning iteration 949/2000 [0m                      

                       Computation: 43667 steps/s (collection: 2.156s, learning 0.096s)
             Mean action noise std: 2.69
          Mean value_function loss: 272.7212
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 74.2983
                       Mean reward: 825.21
               Mean episode length: 229.97
    Episode_Reward/reaching_object: 1.8766
     Episode_Reward/lifting_object: 153.8413
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 2.25s
                      Time elapsed: 00:35:06
                               ETA: 00:38:50

################################################################################
                     [1m Learning iteration 950/2000 [0m                      

                       Computation: 43304 steps/s (collection: 2.153s, learning 0.117s)
             Mean action noise std: 2.69
          Mean value_function loss: 303.4482
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 74.3062
                       Mean reward: 762.31
               Mean episode length: 215.41
    Episode_Reward/reaching_object: 1.8558
     Episode_Reward/lifting_object: 151.4891
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0707
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 2.27s
                      Time elapsed: 00:35:09
                               ETA: 00:38:48

################################################################################
                     [1m Learning iteration 951/2000 [0m                      

                       Computation: 43035 steps/s (collection: 2.176s, learning 0.108s)
             Mean action noise std: 2.69
          Mean value_function loss: 266.4541
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 74.3212
                       Mean reward: 778.60
               Mean episode length: 218.55
    Episode_Reward/reaching_object: 1.7993
     Episode_Reward/lifting_object: 145.7828
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 2.28s
                      Time elapsed: 00:35:11
                               ETA: 00:38:46

################################################################################
                     [1m Learning iteration 952/2000 [0m                      

                       Computation: 43424 steps/s (collection: 2.159s, learning 0.105s)
             Mean action noise std: 2.69
          Mean value_function loss: 278.5002
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 74.3314
                       Mean reward: 770.07
               Mean episode length: 217.62
    Episode_Reward/reaching_object: 1.8995
     Episode_Reward/lifting_object: 156.1907
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0714
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 2.26s
                      Time elapsed: 00:35:13
                               ETA: 00:38:44

################################################################################
                     [1m Learning iteration 953/2000 [0m                      

                       Computation: 43498 steps/s (collection: 2.156s, learning 0.104s)
             Mean action noise std: 2.69
          Mean value_function loss: 273.1549
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 74.3368
                       Mean reward: 790.64
               Mean episode length: 218.52
    Episode_Reward/reaching_object: 1.8818
     Episode_Reward/lifting_object: 154.3126
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 2.26s
                      Time elapsed: 00:35:15
                               ETA: 00:38:42

################################################################################
                     [1m Learning iteration 954/2000 [0m                      

                       Computation: 42825 steps/s (collection: 2.197s, learning 0.099s)
             Mean action noise std: 2.69
          Mean value_function loss: 267.3316
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.3436
                       Mean reward: 791.97
               Mean episode length: 219.54
    Episode_Reward/reaching_object: 1.8850
     Episode_Reward/lifting_object: 154.6210
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0704
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 2.30s
                      Time elapsed: 00:35:18
                               ETA: 00:38:40

################################################################################
                     [1m Learning iteration 955/2000 [0m                      

                       Computation: 43210 steps/s (collection: 2.174s, learning 0.101s)
             Mean action noise std: 2.70
          Mean value_function loss: 287.2178
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 74.3531
                       Mean reward: 747.63
               Mean episode length: 213.53
    Episode_Reward/reaching_object: 1.8610
     Episode_Reward/lifting_object: 152.1404
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0704
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 2.27s
                      Time elapsed: 00:35:20
                               ETA: 00:38:37

################################################################################
                     [1m Learning iteration 956/2000 [0m                      

                       Computation: 42577 steps/s (collection: 2.213s, learning 0.096s)
             Mean action noise std: 2.70
          Mean value_function loss: 297.7507
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 74.3606
                       Mean reward: 790.39
               Mean episode length: 221.31
    Episode_Reward/reaching_object: 1.8375
     Episode_Reward/lifting_object: 150.1311
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 2.31s
                      Time elapsed: 00:35:22
                               ETA: 00:38:35

################################################################################
                     [1m Learning iteration 957/2000 [0m                      

                       Computation: 43634 steps/s (collection: 2.144s, learning 0.109s)
             Mean action noise std: 2.70
          Mean value_function loss: 236.6174
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.3660
                       Mean reward: 825.30
               Mean episode length: 226.11
    Episode_Reward/reaching_object: 1.9558
     Episode_Reward/lifting_object: 160.7900
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 2.25s
                      Time elapsed: 00:35:25
                               ETA: 00:38:33

################################################################################
                     [1m Learning iteration 958/2000 [0m                      

                       Computation: 42727 steps/s (collection: 2.202s, learning 0.099s)
             Mean action noise std: 2.70
          Mean value_function loss: 262.3911
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 74.3735
                       Mean reward: 763.30
               Mean episode length: 212.36
    Episode_Reward/reaching_object: 1.8624
     Episode_Reward/lifting_object: 152.0522
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 2.30s
                      Time elapsed: 00:35:27
                               ETA: 00:38:31

################################################################################
                     [1m Learning iteration 959/2000 [0m                      

                       Computation: 42642 steps/s (collection: 2.192s, learning 0.114s)
             Mean action noise std: 2.70
          Mean value_function loss: 267.6095
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.3784
                       Mean reward: 802.68
               Mean episode length: 223.35
    Episode_Reward/reaching_object: 1.8037
     Episode_Reward/lifting_object: 147.1711
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0692
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 2.31s
                      Time elapsed: 00:35:29
                               ETA: 00:38:29

################################################################################
                     [1m Learning iteration 960/2000 [0m                      

                       Computation: 42798 steps/s (collection: 2.201s, learning 0.096s)
             Mean action noise std: 2.70
          Mean value_function loss: 259.8187
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.3854
                       Mean reward: 794.13
               Mean episode length: 219.82
    Episode_Reward/reaching_object: 1.9037
     Episode_Reward/lifting_object: 156.4879
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 2.30s
                      Time elapsed: 00:35:32
                               ETA: 00:38:27

################################################################################
                     [1m Learning iteration 961/2000 [0m                      

                       Computation: 44220 steps/s (collection: 2.125s, learning 0.098s)
             Mean action noise std: 2.70
          Mean value_function loss: 220.2239
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 74.3920
                       Mean reward: 855.83
               Mean episode length: 232.01
    Episode_Reward/reaching_object: 1.9235
     Episode_Reward/lifting_object: 158.3868
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0722
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 2.22s
                      Time elapsed: 00:35:34
                               ETA: 00:38:25

################################################################################
                     [1m Learning iteration 962/2000 [0m                      

                       Computation: 44033 steps/s (collection: 2.119s, learning 0.114s)
             Mean action noise std: 2.70
          Mean value_function loss: 247.4418
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 74.3967
                       Mean reward: 835.56
               Mean episode length: 228.57
    Episode_Reward/reaching_object: 1.9187
     Episode_Reward/lifting_object: 158.1261
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 2.23s
                      Time elapsed: 00:35:36
                               ETA: 00:38:22

################################################################################
                     [1m Learning iteration 963/2000 [0m                      

                       Computation: 44068 steps/s (collection: 2.125s, learning 0.106s)
             Mean action noise std: 2.70
          Mean value_function loss: 235.2269
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.4019
                       Mean reward: 831.35
               Mean episode length: 227.19
    Episode_Reward/reaching_object: 1.9358
     Episode_Reward/lifting_object: 159.8237
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0722
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 2.23s
                      Time elapsed: 00:35:38
                               ETA: 00:38:20

################################################################################
                     [1m Learning iteration 964/2000 [0m                      

                       Computation: 43887 steps/s (collection: 2.136s, learning 0.104s)
             Mean action noise std: 2.70
          Mean value_function loss: 249.2827
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 74.4106
                       Mean reward: 816.55
               Mean episode length: 222.68
    Episode_Reward/reaching_object: 1.8808
     Episode_Reward/lifting_object: 154.3468
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0723
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 2.24s
                      Time elapsed: 00:35:40
                               ETA: 00:38:18

################################################################################
                     [1m Learning iteration 965/2000 [0m                      

                       Computation: 44410 steps/s (collection: 2.123s, learning 0.091s)
             Mean action noise std: 2.70
          Mean value_function loss: 243.6781
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 74.4222
                       Mean reward: 818.62
               Mean episode length: 225.95
    Episode_Reward/reaching_object: 1.9189
     Episode_Reward/lifting_object: 158.0966
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0726
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 2.21s
                      Time elapsed: 00:35:43
                               ETA: 00:38:16

################################################################################
                     [1m Learning iteration 966/2000 [0m                      

                       Computation: 43992 steps/s (collection: 2.134s, learning 0.101s)
             Mean action noise std: 2.70
          Mean value_function loss: 263.6820
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 74.4301
                       Mean reward: 733.06
               Mean episode length: 205.29
    Episode_Reward/reaching_object: 1.8742
     Episode_Reward/lifting_object: 154.3795
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0710
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 2.23s
                      Time elapsed: 00:35:45
                               ETA: 00:38:14

################################################################################
                     [1m Learning iteration 967/2000 [0m                      

                       Computation: 44148 steps/s (collection: 2.136s, learning 0.091s)
             Mean action noise std: 2.70
          Mean value_function loss: 263.5405
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.4367
                       Mean reward: 773.40
               Mean episode length: 214.36
    Episode_Reward/reaching_object: 1.8868
     Episode_Reward/lifting_object: 154.2359
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 2.23s
                      Time elapsed: 00:35:47
                               ETA: 00:38:11

################################################################################
                     [1m Learning iteration 968/2000 [0m                      

                       Computation: 44051 steps/s (collection: 2.139s, learning 0.092s)
             Mean action noise std: 2.70
          Mean value_function loss: 250.0060
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 74.4412
                       Mean reward: 801.67
               Mean episode length: 221.89
    Episode_Reward/reaching_object: 1.8827
     Episode_Reward/lifting_object: 154.8646
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 2.23s
                      Time elapsed: 00:35:49
                               ETA: 00:38:09

################################################################################
                     [1m Learning iteration 969/2000 [0m                      

                       Computation: 44115 steps/s (collection: 2.134s, learning 0.095s)
             Mean action noise std: 2.70
          Mean value_function loss: 244.3181
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.4469
                       Mean reward: 827.01
               Mean episode length: 227.10
    Episode_Reward/reaching_object: 1.9065
     Episode_Reward/lifting_object: 157.1737
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0717
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 2.23s
                      Time elapsed: 00:35:52
                               ETA: 00:38:07

################################################################################
                     [1m Learning iteration 970/2000 [0m                      

                       Computation: 43390 steps/s (collection: 2.152s, learning 0.114s)
             Mean action noise std: 2.71
          Mean value_function loss: 230.3316
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.4528
                       Mean reward: 804.05
               Mean episode length: 224.45
    Episode_Reward/reaching_object: 1.9189
     Episode_Reward/lifting_object: 158.4193
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 2.27s
                      Time elapsed: 00:35:54
                               ETA: 00:38:05

################################################################################
                     [1m Learning iteration 971/2000 [0m                      

                       Computation: 44211 steps/s (collection: 2.122s, learning 0.102s)
             Mean action noise std: 2.71
          Mean value_function loss: 267.8392
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 74.4589
                       Mean reward: 746.09
               Mean episode length: 208.31
    Episode_Reward/reaching_object: 1.8655
     Episode_Reward/lifting_object: 153.4529
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0721
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 2.22s
                      Time elapsed: 00:35:56
                               ETA: 00:38:03

################################################################################
                     [1m Learning iteration 972/2000 [0m                      

                       Computation: 44119 steps/s (collection: 2.112s, learning 0.116s)
             Mean action noise std: 2.71
          Mean value_function loss: 233.9232
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.4667
                       Mean reward: 805.23
               Mean episode length: 221.73
    Episode_Reward/reaching_object: 1.8761
     Episode_Reward/lifting_object: 154.0695
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 2.23s
                      Time elapsed: 00:35:58
                               ETA: 00:38:00

################################################################################
                     [1m Learning iteration 973/2000 [0m                      

                       Computation: 43710 steps/s (collection: 2.138s, learning 0.111s)
             Mean action noise std: 2.71
          Mean value_function loss: 215.1224
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.4780
                       Mean reward: 823.29
               Mean episode length: 226.25
    Episode_Reward/reaching_object: 1.9593
     Episode_Reward/lifting_object: 162.4399
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0741
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 2.25s
                      Time elapsed: 00:36:01
                               ETA: 00:37:58

################################################################################
                     [1m Learning iteration 974/2000 [0m                      

                       Computation: 44140 steps/s (collection: 2.118s, learning 0.109s)
             Mean action noise std: 2.71
          Mean value_function loss: 251.2914
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 74.4881
                       Mean reward: 801.68
               Mean episode length: 222.94
    Episode_Reward/reaching_object: 1.8818
     Episode_Reward/lifting_object: 154.7528
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0718
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 2.23s
                      Time elapsed: 00:36:03
                               ETA: 00:37:56

################################################################################
                     [1m Learning iteration 975/2000 [0m                      

                       Computation: 43897 steps/s (collection: 2.137s, learning 0.103s)
             Mean action noise std: 2.71
          Mean value_function loss: 236.9701
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 74.4982
                       Mean reward: 794.58
               Mean episode length: 220.66
    Episode_Reward/reaching_object: 1.9723
     Episode_Reward/lifting_object: 163.3916
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0742
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 2.24s
                      Time elapsed: 00:36:05
                               ETA: 00:37:54

################################################################################
                     [1m Learning iteration 976/2000 [0m                      

                       Computation: 44413 steps/s (collection: 2.120s, learning 0.094s)
             Mean action noise std: 2.71
          Mean value_function loss: 223.3910
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.5082
                       Mean reward: 845.40
               Mean episode length: 228.22
    Episode_Reward/reaching_object: 1.9706
     Episode_Reward/lifting_object: 162.9694
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0747
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 2.21s
                      Time elapsed: 00:36:07
                               ETA: 00:37:51

################################################################################
                     [1m Learning iteration 977/2000 [0m                      

                       Computation: 43291 steps/s (collection: 2.161s, learning 0.110s)
             Mean action noise std: 2.71
          Mean value_function loss: 246.5800
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 74.5174
                       Mean reward: 813.29
               Mean episode length: 223.63
    Episode_Reward/reaching_object: 1.9411
     Episode_Reward/lifting_object: 160.2112
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0740
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 2.27s
                      Time elapsed: 00:36:09
                               ETA: 00:37:49

################################################################################
                     [1m Learning iteration 978/2000 [0m                      

                       Computation: 44295 steps/s (collection: 2.123s, learning 0.097s)
             Mean action noise std: 2.71
          Mean value_function loss: 210.7589
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 74.5255
                       Mean reward: 785.61
               Mean episode length: 217.01
    Episode_Reward/reaching_object: 1.9511
     Episode_Reward/lifting_object: 161.4985
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0751
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 2.22s
                      Time elapsed: 00:36:12
                               ETA: 00:37:47

################################################################################
                     [1m Learning iteration 979/2000 [0m                      

                       Computation: 43698 steps/s (collection: 2.155s, learning 0.095s)
             Mean action noise std: 2.71
          Mean value_function loss: 251.9057
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.5309
                       Mean reward: 769.91
               Mean episode length: 213.08
    Episode_Reward/reaching_object: 1.9309
     Episode_Reward/lifting_object: 159.2921
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0742
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 2.25s
                      Time elapsed: 00:36:14
                               ETA: 00:37:45

################################################################################
                     [1m Learning iteration 980/2000 [0m                      

                       Computation: 43622 steps/s (collection: 2.163s, learning 0.090s)
             Mean action noise std: 2.71
          Mean value_function loss: 271.0205
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 74.5392
                       Mean reward: 770.25
               Mean episode length: 213.86
    Episode_Reward/reaching_object: 1.8919
     Episode_Reward/lifting_object: 156.2998
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 2.25s
                      Time elapsed: 00:36:16
                               ETA: 00:37:43

################################################################################
                     [1m Learning iteration 981/2000 [0m                      

                       Computation: 43853 steps/s (collection: 2.142s, learning 0.100s)
             Mean action noise std: 2.72
          Mean value_function loss: 224.3715
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.5524
                       Mean reward: 806.85
               Mean episode length: 225.15
    Episode_Reward/reaching_object: 1.9559
     Episode_Reward/lifting_object: 161.5029
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0753
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 2.24s
                      Time elapsed: 00:36:18
                               ETA: 00:37:41

################################################################################
                     [1m Learning iteration 982/2000 [0m                      

                       Computation: 43954 steps/s (collection: 2.136s, learning 0.100s)
             Mean action noise std: 2.72
          Mean value_function loss: 228.0524
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 74.5630
                       Mean reward: 800.68
               Mean episode length: 223.93
    Episode_Reward/reaching_object: 1.9416
     Episode_Reward/lifting_object: 159.8735
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 2.24s
                      Time elapsed: 00:36:21
                               ETA: 00:37:38

################################################################################
                     [1m Learning iteration 983/2000 [0m                      

                       Computation: 44396 steps/s (collection: 2.123s, learning 0.092s)
             Mean action noise std: 2.72
          Mean value_function loss: 237.9391
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 74.5745
                       Mean reward: 809.83
               Mean episode length: 224.32
    Episode_Reward/reaching_object: 1.9250
     Episode_Reward/lifting_object: 158.5363
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 2.21s
                      Time elapsed: 00:36:23
                               ETA: 00:37:36

################################################################################
                     [1m Learning iteration 984/2000 [0m                      

                       Computation: 43459 steps/s (collection: 2.134s, learning 0.128s)
             Mean action noise std: 2.72
          Mean value_function loss: 233.7983
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 74.5815
                       Mean reward: 816.16
               Mean episode length: 225.01
    Episode_Reward/reaching_object: 1.9505
     Episode_Reward/lifting_object: 161.1988
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 2.26s
                      Time elapsed: 00:36:25
                               ETA: 00:37:34

################################################################################
                     [1m Learning iteration 985/2000 [0m                      

                       Computation: 43826 steps/s (collection: 2.139s, learning 0.104s)
             Mean action noise std: 2.72
          Mean value_function loss: 224.5146
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.5955
                       Mean reward: 837.05
               Mean episode length: 229.64
    Episode_Reward/reaching_object: 1.8985
     Episode_Reward/lifting_object: 155.5328
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0730
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 2.24s
                      Time elapsed: 00:36:27
                               ETA: 00:37:32

################################################################################
                     [1m Learning iteration 986/2000 [0m                      

                       Computation: 43432 steps/s (collection: 2.146s, learning 0.117s)
             Mean action noise std: 2.72
          Mean value_function loss: 211.1196
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 74.6055
                       Mean reward: 747.27
               Mean episode length: 211.65
    Episode_Reward/reaching_object: 1.9301
     Episode_Reward/lifting_object: 159.0953
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0740
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 2.26s
                      Time elapsed: 00:36:30
                               ETA: 00:37:30

################################################################################
                     [1m Learning iteration 987/2000 [0m                      

                       Computation: 43930 steps/s (collection: 2.146s, learning 0.092s)
             Mean action noise std: 2.72
          Mean value_function loss: 265.1470
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 74.6122
                       Mean reward: 759.28
               Mean episode length: 212.96
    Episode_Reward/reaching_object: 1.8747
     Episode_Reward/lifting_object: 153.4428
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0731
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 2.24s
                      Time elapsed: 00:36:32
                               ETA: 00:37:27

################################################################################
                     [1m Learning iteration 988/2000 [0m                      

                       Computation: 44000 steps/s (collection: 2.137s, learning 0.098s)
             Mean action noise std: 2.72
          Mean value_function loss: 209.9897
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 74.6230
                       Mean reward: 812.16
               Mean episode length: 221.99
    Episode_Reward/reaching_object: 1.9326
     Episode_Reward/lifting_object: 158.7087
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0756
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 2.23s
                      Time elapsed: 00:36:34
                               ETA: 00:37:25

################################################################################
                     [1m Learning iteration 989/2000 [0m                      

                       Computation: 42284 steps/s (collection: 2.200s, learning 0.125s)
             Mean action noise std: 2.72
          Mean value_function loss: 250.0078
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 74.6319
                       Mean reward: 811.58
               Mean episode length: 223.35
    Episode_Reward/reaching_object: 1.9105
     Episode_Reward/lifting_object: 157.3113
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 2.32s
                      Time elapsed: 00:36:36
                               ETA: 00:37:23

################################################################################
                     [1m Learning iteration 990/2000 [0m                      

                       Computation: 43426 steps/s (collection: 2.168s, learning 0.096s)
             Mean action noise std: 2.72
          Mean value_function loss: 226.8171
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 74.6391
                       Mean reward: 804.18
               Mean episode length: 223.19
    Episode_Reward/reaching_object: 1.9217
     Episode_Reward/lifting_object: 157.3439
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0756
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 2.26s
                      Time elapsed: 00:36:39
                               ETA: 00:37:21

################################################################################
                     [1m Learning iteration 991/2000 [0m                      

                       Computation: 42204 steps/s (collection: 2.217s, learning 0.113s)
             Mean action noise std: 2.72
          Mean value_function loss: 269.5395
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 74.6488
                       Mean reward: 771.97
               Mean episode length: 215.97
    Episode_Reward/reaching_object: 1.9095
     Episode_Reward/lifting_object: 156.2703
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0737
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 2.33s
                      Time elapsed: 00:36:41
                               ETA: 00:37:19

################################################################################
                     [1m Learning iteration 992/2000 [0m                      

                       Computation: 43860 steps/s (collection: 2.146s, learning 0.095s)
             Mean action noise std: 2.72
          Mean value_function loss: 249.0600
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 74.6589
                       Mean reward: 766.46
               Mean episode length: 212.50
    Episode_Reward/reaching_object: 1.8681
     Episode_Reward/lifting_object: 152.0506
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 2.24s
                      Time elapsed: 00:36:43
                               ETA: 00:37:17

################################################################################
                     [1m Learning iteration 993/2000 [0m                      

                       Computation: 42981 steps/s (collection: 2.182s, learning 0.106s)
             Mean action noise std: 2.73
          Mean value_function loss: 248.6591
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 74.6694
                       Mean reward: 800.54
               Mean episode length: 219.70
    Episode_Reward/reaching_object: 1.8462
     Episode_Reward/lifting_object: 150.0212
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 2.29s
                      Time elapsed: 00:36:46
                               ETA: 00:37:14

################################################################################
                     [1m Learning iteration 994/2000 [0m                      

                       Computation: 43449 steps/s (collection: 2.170s, learning 0.092s)
             Mean action noise std: 2.73
          Mean value_function loss: 264.7998
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 74.6820
                       Mean reward: 778.55
               Mean episode length: 212.98
    Episode_Reward/reaching_object: 1.9046
     Episode_Reward/lifting_object: 154.8849
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0733
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 2.26s
                      Time elapsed: 00:36:48
                               ETA: 00:37:12

################################################################################
                     [1m Learning iteration 995/2000 [0m                      

                       Computation: 42686 steps/s (collection: 2.201s, learning 0.102s)
             Mean action noise std: 2.73
          Mean value_function loss: 258.0447
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 74.6920
                       Mean reward: 719.08
               Mean episode length: 200.17
    Episode_Reward/reaching_object: 1.8704
     Episode_Reward/lifting_object: 152.3336
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0718
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 2.30s
                      Time elapsed: 00:36:50
                               ETA: 00:37:10

################################################################################
                     [1m Learning iteration 996/2000 [0m                      

                       Computation: 43676 steps/s (collection: 2.154s, learning 0.097s)
             Mean action noise std: 2.73
          Mean value_function loss: 233.5646
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 74.7026
                       Mean reward: 832.46
               Mean episode length: 226.73
    Episode_Reward/reaching_object: 1.9779
     Episode_Reward/lifting_object: 161.0815
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 2.25s
                      Time elapsed: 00:36:52
                               ETA: 00:37:08

################################################################################
                     [1m Learning iteration 997/2000 [0m                      

                       Computation: 42747 steps/s (collection: 2.183s, learning 0.117s)
             Mean action noise std: 2.73
          Mean value_function loss: 246.9502
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 74.7079
                       Mean reward: 804.61
               Mean episode length: 220.98
    Episode_Reward/reaching_object: 1.9460
     Episode_Reward/lifting_object: 158.0515
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0740
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 2.30s
                      Time elapsed: 00:36:55
                               ETA: 00:37:06

################################################################################
                     [1m Learning iteration 998/2000 [0m                      

                       Computation: 42433 steps/s (collection: 2.197s, learning 0.120s)
             Mean action noise std: 2.73
          Mean value_function loss: 261.7689
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 74.7111
                       Mean reward: 733.77
               Mean episode length: 204.05
    Episode_Reward/reaching_object: 1.9421
     Episode_Reward/lifting_object: 157.2103
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 2.32s
                      Time elapsed: 00:36:57
                               ETA: 00:37:04

################################################################################
                     [1m Learning iteration 999/2000 [0m                      

                       Computation: 43283 steps/s (collection: 2.158s, learning 0.113s)
             Mean action noise std: 2.73
          Mean value_function loss: 240.3400
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.7178
                       Mean reward: 794.39
               Mean episode length: 219.74
    Episode_Reward/reaching_object: 1.9435
     Episode_Reward/lifting_object: 157.2434
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 2.27s
                      Time elapsed: 00:36:59
                               ETA: 00:37:02

################################################################################
                     [1m Learning iteration 1000/2000 [0m                     

                       Computation: 14336 steps/s (collection: 6.737s, learning 0.120s)
             Mean action noise std: 2.73
          Mean value_function loss: 278.4462
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 74.7241
                       Mean reward: 787.51
               Mean episode length: 216.38
    Episode_Reward/reaching_object: 1.9191
     Episode_Reward/lifting_object: 155.0209
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0725
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 6.86s
                      Time elapsed: 00:37:06
                               ETA: 00:37:04

################################################################################
                     [1m Learning iteration 1001/2000 [0m                     

                       Computation: 13884 steps/s (collection: 6.956s, learning 0.125s)
             Mean action noise std: 2.73
          Mean value_function loss: 222.3030
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 74.7272
                       Mean reward: 797.97
               Mean episode length: 220.26
    Episode_Reward/reaching_object: 1.9782
     Episode_Reward/lifting_object: 159.9547
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 7.08s
                      Time elapsed: 00:37:13
                               ETA: 00:37:07

################################################################################
                     [1m Learning iteration 1002/2000 [0m                     

                       Computation: 14244 steps/s (collection: 6.781s, learning 0.120s)
             Mean action noise std: 2.73
          Mean value_function loss: 219.7784
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 74.7366
                       Mean reward: 787.75
               Mean episode length: 217.61
    Episode_Reward/reaching_object: 1.9505
     Episode_Reward/lifting_object: 156.8480
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 6.90s
                      Time elapsed: 00:37:20
                               ETA: 00:37:09

################################################################################
                     [1m Learning iteration 1003/2000 [0m                     

                       Computation: 14254 steps/s (collection: 6.773s, learning 0.124s)
             Mean action noise std: 2.73
          Mean value_function loss: 218.2298
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.7534
                       Mean reward: 809.70
               Mean episode length: 221.15
    Episode_Reward/reaching_object: 1.9888
     Episode_Reward/lifting_object: 160.9846
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 6.90s
                      Time elapsed: 00:37:27
                               ETA: 00:37:11

################################################################################
                     [1m Learning iteration 1004/2000 [0m                     

                       Computation: 14028 steps/s (collection: 6.879s, learning 0.128s)
             Mean action noise std: 2.73
          Mean value_function loss: 265.5728
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 74.7648
                       Mean reward: 780.14
               Mean episode length: 214.35
    Episode_Reward/reaching_object: 1.9837
     Episode_Reward/lifting_object: 160.7955
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 7.01s
                      Time elapsed: 00:37:34
                               ETA: 00:37:14

################################################################################
                     [1m Learning iteration 1005/2000 [0m                     

                       Computation: 13806 steps/s (collection: 7.000s, learning 0.120s)
             Mean action noise std: 2.73
          Mean value_function loss: 247.1469
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.7720
                       Mean reward: 767.77
               Mean episode length: 212.83
    Episode_Reward/reaching_object: 1.9523
     Episode_Reward/lifting_object: 157.5602
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0734
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 7.12s
                      Time elapsed: 00:37:41
                               ETA: 00:37:16

################################################################################
                     [1m Learning iteration 1006/2000 [0m                     

                       Computation: 14423 steps/s (collection: 6.706s, learning 0.110s)
             Mean action noise std: 2.74
          Mean value_function loss: 216.4470
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.7781
                       Mean reward: 845.04
               Mean episode length: 229.92
    Episode_Reward/reaching_object: 2.0159
     Episode_Reward/lifting_object: 162.9330
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0759
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 6.82s
                      Time elapsed: 00:37:48
                               ETA: 00:37:19

################################################################################
                     [1m Learning iteration 1007/2000 [0m                     

                       Computation: 13670 steps/s (collection: 7.058s, learning 0.134s)
             Mean action noise std: 2.74
          Mean value_function loss: 257.5474
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 74.7842
                       Mean reward: 825.11
               Mean episode length: 225.28
    Episode_Reward/reaching_object: 1.9341
     Episode_Reward/lifting_object: 155.6873
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0726
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 7.19s
                      Time elapsed: 00:37:55
                               ETA: 00:37:21

################################################################################
                     [1m Learning iteration 1008/2000 [0m                     

                       Computation: 15916 steps/s (collection: 6.066s, learning 0.110s)
             Mean action noise std: 2.74
          Mean value_function loss: 261.2845
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 74.7955
                       Mean reward: 758.08
               Mean episode length: 212.84
    Episode_Reward/reaching_object: 1.9165
     Episode_Reward/lifting_object: 154.0622
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0725
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 6.18s
                      Time elapsed: 00:38:01
                               ETA: 00:37:23

################################################################################
                     [1m Learning iteration 1009/2000 [0m                     

                       Computation: 42503 steps/s (collection: 2.203s, learning 0.109s)
             Mean action noise std: 2.74
          Mean value_function loss: 238.6908
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.8038
                       Mean reward: 799.76
               Mean episode length: 220.24
    Episode_Reward/reaching_object: 1.9840
     Episode_Reward/lifting_object: 160.4784
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 2.31s
                      Time elapsed: 00:38:04
                               ETA: 00:37:21

################################################################################
                     [1m Learning iteration 1010/2000 [0m                     

                       Computation: 43948 steps/s (collection: 2.147s, learning 0.090s)
             Mean action noise std: 2.74
          Mean value_function loss: 205.2553
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 74.8144
                       Mean reward: 812.17
               Mean episode length: 223.74
    Episode_Reward/reaching_object: 1.9940
     Episode_Reward/lifting_object: 161.2888
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0747
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 2.24s
                      Time elapsed: 00:38:06
                               ETA: 00:37:18

################################################################################
                     [1m Learning iteration 1011/2000 [0m                     

                       Computation: 44204 steps/s (collection: 2.115s, learning 0.109s)
             Mean action noise std: 2.74
          Mean value_function loss: 229.3466
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 74.8278
                       Mean reward: 829.30
               Mean episode length: 227.00
    Episode_Reward/reaching_object: 1.9291
     Episode_Reward/lifting_object: 155.0842
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0730
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 2.22s
                      Time elapsed: 00:38:08
                               ETA: 00:37:16

################################################################################
                     [1m Learning iteration 1012/2000 [0m                     

                       Computation: 43030 steps/s (collection: 2.175s, learning 0.109s)
             Mean action noise std: 2.74
          Mean value_function loss: 228.7193
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 74.8382
                       Mean reward: 844.45
               Mean episode length: 228.30
    Episode_Reward/reaching_object: 1.9993
     Episode_Reward/lifting_object: 161.6667
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0743
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 2.28s
                      Time elapsed: 00:38:10
                               ETA: 00:37:14

################################################################################
                     [1m Learning iteration 1013/2000 [0m                     

                       Computation: 40227 steps/s (collection: 2.267s, learning 0.177s)
             Mean action noise std: 2.74
          Mean value_function loss: 208.7753
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 74.8547
                       Mean reward: 837.41
               Mean episode length: 226.54
    Episode_Reward/reaching_object: 1.9664
     Episode_Reward/lifting_object: 159.1572
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0739
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 2.44s
                      Time elapsed: 00:38:13
                               ETA: 00:37:12

################################################################################
                     [1m Learning iteration 1014/2000 [0m                     

                       Computation: 40038 steps/s (collection: 2.333s, learning 0.122s)
             Mean action noise std: 2.74
          Mean value_function loss: 225.5063
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.8630
                       Mean reward: 781.11
               Mean episode length: 216.45
    Episode_Reward/reaching_object: 1.9366
     Episode_Reward/lifting_object: 155.8566
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0722
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 2.46s
                      Time elapsed: 00:38:15
                               ETA: 00:37:10

################################################################################
                     [1m Learning iteration 1015/2000 [0m                     

                       Computation: 43095 steps/s (collection: 2.182s, learning 0.099s)
             Mean action noise std: 2.74
          Mean value_function loss: 221.3640
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 74.8717
                       Mean reward: 799.84
               Mean episode length: 220.54
    Episode_Reward/reaching_object: 1.9948
     Episode_Reward/lifting_object: 161.0257
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0742
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 2.28s
                      Time elapsed: 00:38:18
                               ETA: 00:37:07

################################################################################
                     [1m Learning iteration 1016/2000 [0m                     

                       Computation: 42767 steps/s (collection: 2.193s, learning 0.105s)
             Mean action noise std: 2.75
          Mean value_function loss: 232.8129
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.8836
                       Mean reward: 810.67
               Mean episode length: 222.75
    Episode_Reward/reaching_object: 1.9554
     Episode_Reward/lifting_object: 157.3719
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0732
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 2.30s
                      Time elapsed: 00:38:20
                               ETA: 00:37:05

################################################################################
                     [1m Learning iteration 1017/2000 [0m                     

                       Computation: 42755 steps/s (collection: 2.204s, learning 0.095s)
             Mean action noise std: 2.75
          Mean value_function loss: 196.7247
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 74.8938
                       Mean reward: 848.86
               Mean episode length: 228.96
    Episode_Reward/reaching_object: 1.9901
     Episode_Reward/lifting_object: 161.0122
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 2.30s
                      Time elapsed: 00:38:22
                               ETA: 00:37:03

################################################################################
                     [1m Learning iteration 1018/2000 [0m                     

                       Computation: 42264 steps/s (collection: 2.219s, learning 0.107s)
             Mean action noise std: 2.75
          Mean value_function loss: 201.1252
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 74.9059
                       Mean reward: 815.85
               Mean episode length: 221.67
    Episode_Reward/reaching_object: 2.0152
     Episode_Reward/lifting_object: 163.9845
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0732
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 2.33s
                      Time elapsed: 00:38:24
                               ETA: 00:37:01

################################################################################
                     [1m Learning iteration 1019/2000 [0m                     

                       Computation: 43488 steps/s (collection: 2.159s, learning 0.102s)
             Mean action noise std: 2.75
          Mean value_function loss: 208.7930
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.9202
                       Mean reward: 841.33
               Mean episode length: 229.44
    Episode_Reward/reaching_object: 2.0167
     Episode_Reward/lifting_object: 163.3428
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 2.26s
                      Time elapsed: 00:38:27
                               ETA: 00:36:59

################################################################################
                     [1m Learning iteration 1020/2000 [0m                     

                       Computation: 41796 steps/s (collection: 2.255s, learning 0.097s)
             Mean action noise std: 2.75
          Mean value_function loss: 213.0700
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 74.9308
                       Mean reward: 801.84
               Mean episode length: 220.13
    Episode_Reward/reaching_object: 1.9638
     Episode_Reward/lifting_object: 158.5014
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 2.35s
                      Time elapsed: 00:38:29
                               ETA: 00:36:56

################################################################################
                     [1m Learning iteration 1021/2000 [0m                     

                       Computation: 42642 steps/s (collection: 2.211s, learning 0.095s)
             Mean action noise std: 2.75
          Mean value_function loss: 192.0720
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 74.9434
                       Mean reward: 794.87
               Mean episode length: 217.05
    Episode_Reward/reaching_object: 2.0268
     Episode_Reward/lifting_object: 164.0682
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 2.31s
                      Time elapsed: 00:38:31
                               ETA: 00:36:54

################################################################################
                     [1m Learning iteration 1022/2000 [0m                     

                       Computation: 42667 steps/s (collection: 2.200s, learning 0.104s)
             Mean action noise std: 2.75
          Mean value_function loss: 192.5663
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 74.9546
                       Mean reward: 799.90
               Mean episode length: 221.29
    Episode_Reward/reaching_object: 1.9551
     Episode_Reward/lifting_object: 155.8078
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 2.30s
                      Time elapsed: 00:38:34
                               ETA: 00:36:52

################################################################################
                     [1m Learning iteration 1023/2000 [0m                     

                       Computation: 43363 steps/s (collection: 2.177s, learning 0.090s)
             Mean action noise std: 2.75
          Mean value_function loss: 186.7314
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.9718
                       Mean reward: 862.00
               Mean episode length: 231.38
    Episode_Reward/reaching_object: 2.0000
     Episode_Reward/lifting_object: 160.5797
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 2.27s
                      Time elapsed: 00:38:36
                               ETA: 00:36:50

################################################################################
                     [1m Learning iteration 1024/2000 [0m                     

                       Computation: 42236 steps/s (collection: 2.179s, learning 0.148s)
             Mean action noise std: 2.76
          Mean value_function loss: 167.5339
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 74.9858
                       Mean reward: 827.10
               Mean episode length: 225.59
    Episode_Reward/reaching_object: 2.0148
     Episode_Reward/lifting_object: 161.4120
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0742
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 2.33s
                      Time elapsed: 00:38:38
                               ETA: 00:36:47

################################################################################
                     [1m Learning iteration 1025/2000 [0m                     

                       Computation: 42159 steps/s (collection: 2.240s, learning 0.092s)
             Mean action noise std: 2.76
          Mean value_function loss: 201.4228
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.9978
                       Mean reward: 809.76
               Mean episode length: 221.31
    Episode_Reward/reaching_object: 2.0594
     Episode_Reward/lifting_object: 165.3376
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0759
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 2.33s
                      Time elapsed: 00:38:41
                               ETA: 00:36:45

################################################################################
                     [1m Learning iteration 1026/2000 [0m                     

                       Computation: 41548 steps/s (collection: 2.267s, learning 0.099s)
             Mean action noise std: 2.76
          Mean value_function loss: 178.0514
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 75.0085
                       Mean reward: 840.73
               Mean episode length: 228.29
    Episode_Reward/reaching_object: 2.0542
     Episode_Reward/lifting_object: 165.0914
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0762
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 2.37s
                      Time elapsed: 00:38:43
                               ETA: 00:36:43

################################################################################
                     [1m Learning iteration 1027/2000 [0m                     

                       Computation: 41216 steps/s (collection: 2.287s, learning 0.098s)
             Mean action noise std: 2.76
          Mean value_function loss: 226.1198
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 75.0228
                       Mean reward: 767.18
               Mean episode length: 211.57
    Episode_Reward/reaching_object: 1.9652
     Episode_Reward/lifting_object: 157.4083
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0740
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 2.39s
                      Time elapsed: 00:38:45
                               ETA: 00:36:41

################################################################################
                     [1m Learning iteration 1028/2000 [0m                     

                       Computation: 42968 steps/s (collection: 2.196s, learning 0.092s)
             Mean action noise std: 2.76
          Mean value_function loss: 258.9145
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 75.0363
                       Mean reward: 769.33
               Mean episode length: 211.72
    Episode_Reward/reaching_object: 1.9733
     Episode_Reward/lifting_object: 157.2876
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0746
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 2.29s
                      Time elapsed: 00:38:48
                               ETA: 00:36:39

################################################################################
                     [1m Learning iteration 1029/2000 [0m                     

                       Computation: 43015 steps/s (collection: 2.193s, learning 0.093s)
             Mean action noise std: 2.76
          Mean value_function loss: 228.6648
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 75.0551
                       Mean reward: 825.56
               Mean episode length: 225.50
    Episode_Reward/reaching_object: 1.9957
     Episode_Reward/lifting_object: 159.5637
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 2.29s
                      Time elapsed: 00:38:50
                               ETA: 00:36:36

################################################################################
                     [1m Learning iteration 1030/2000 [0m                     

                       Computation: 41869 steps/s (collection: 2.194s, learning 0.154s)
             Mean action noise std: 2.76
          Mean value_function loss: 183.4581
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 75.0652
                       Mean reward: 834.90
               Mean episode length: 227.11
    Episode_Reward/reaching_object: 2.0337
     Episode_Reward/lifting_object: 162.7574
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0765
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 2.35s
                      Time elapsed: 00:38:52
                               ETA: 00:36:34

################################################################################
                     [1m Learning iteration 1031/2000 [0m                     

                       Computation: 42871 steps/s (collection: 2.169s, learning 0.124s)
             Mean action noise std: 2.76
          Mean value_function loss: 200.3783
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 75.0782
                       Mean reward: 739.42
               Mean episode length: 205.31
    Episode_Reward/reaching_object: 1.9942
     Episode_Reward/lifting_object: 159.4926
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0742
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 2.29s
                      Time elapsed: 00:38:55
                               ETA: 00:36:32

################################################################################
                     [1m Learning iteration 1032/2000 [0m                     

                       Computation: 42801 steps/s (collection: 2.204s, learning 0.093s)
             Mean action noise std: 2.77
          Mean value_function loss: 218.3364
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 75.0935
                       Mean reward: 777.48
               Mean episode length: 213.50
    Episode_Reward/reaching_object: 1.9775
     Episode_Reward/lifting_object: 157.3007
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0743
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 2.30s
                      Time elapsed: 00:38:57
                               ETA: 00:36:30

################################################################################
                     [1m Learning iteration 1033/2000 [0m                     

                       Computation: 41568 steps/s (collection: 2.200s, learning 0.165s)
             Mean action noise std: 2.77
          Mean value_function loss: 192.9557
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 75.1074
                       Mean reward: 838.09
               Mean episode length: 226.34
    Episode_Reward/reaching_object: 1.9853
     Episode_Reward/lifting_object: 159.0304
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0743
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 2.36s
                      Time elapsed: 00:38:59
                               ETA: 00:36:28

################################################################################
                     [1m Learning iteration 1034/2000 [0m                     

                       Computation: 43031 steps/s (collection: 2.184s, learning 0.100s)
             Mean action noise std: 2.77
          Mean value_function loss: 234.8950
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 75.1189
                       Mean reward: 843.82
               Mean episode length: 228.41
    Episode_Reward/reaching_object: 1.9637
     Episode_Reward/lifting_object: 155.9887
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 2.28s
                      Time elapsed: 00:39:02
                               ETA: 00:36:25

################################################################################
                     [1m Learning iteration 1035/2000 [0m                     

                       Computation: 42243 steps/s (collection: 2.227s, learning 0.100s)
             Mean action noise std: 2.77
          Mean value_function loss: 201.0824
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 75.1275
                       Mean reward: 779.49
               Mean episode length: 213.88
    Episode_Reward/reaching_object: 2.0215
     Episode_Reward/lifting_object: 161.1146
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0756
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 2.33s
                      Time elapsed: 00:39:04
                               ETA: 00:36:23

################################################################################
                     [1m Learning iteration 1036/2000 [0m                     

                       Computation: 42397 steps/s (collection: 2.220s, learning 0.099s)
             Mean action noise std: 2.77
          Mean value_function loss: 211.3712
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 75.1408
                       Mean reward: 776.95
               Mean episode length: 213.97
    Episode_Reward/reaching_object: 2.0457
     Episode_Reward/lifting_object: 162.9893
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0747
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 2.32s
                      Time elapsed: 00:39:06
                               ETA: 00:36:21

################################################################################
                     [1m Learning iteration 1037/2000 [0m                     

                       Computation: 42050 steps/s (collection: 2.201s, learning 0.137s)
             Mean action noise std: 2.77
          Mean value_function loss: 242.7963
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 75.1529
                       Mean reward: 795.91
               Mean episode length: 218.84
    Episode_Reward/reaching_object: 1.9976
     Episode_Reward/lifting_object: 159.0201
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0743
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 2.34s
                      Time elapsed: 00:39:09
                               ETA: 00:36:19

################################################################################
                     [1m Learning iteration 1038/2000 [0m                     

                       Computation: 41378 steps/s (collection: 2.284s, learning 0.092s)
             Mean action noise std: 2.77
          Mean value_function loss: 257.9342
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 75.1721
                       Mean reward: 813.53
               Mean episode length: 222.58
    Episode_Reward/reaching_object: 2.0183
     Episode_Reward/lifting_object: 160.6639
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 2.38s
                      Time elapsed: 00:39:11
                               ETA: 00:36:17

################################################################################
                     [1m Learning iteration 1039/2000 [0m                     

                       Computation: 43186 steps/s (collection: 2.186s, learning 0.090s)
             Mean action noise std: 2.77
          Mean value_function loss: 221.1753
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 75.1908
                       Mean reward: 789.12
               Mean episode length: 215.19
    Episode_Reward/reaching_object: 2.0478
     Episode_Reward/lifting_object: 163.7179
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0749
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 2.28s
                      Time elapsed: 00:39:13
                               ETA: 00:36:14

################################################################################
                     [1m Learning iteration 1040/2000 [0m                     

                       Computation: 39171 steps/s (collection: 2.401s, learning 0.109s)
             Mean action noise std: 2.77
          Mean value_function loss: 253.3500
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 75.2012
                       Mean reward: 779.22
               Mean episode length: 215.43
    Episode_Reward/reaching_object: 1.9463
     Episode_Reward/lifting_object: 154.0976
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 2.51s
                      Time elapsed: 00:39:16
                               ETA: 00:36:12

################################################################################
                     [1m Learning iteration 1041/2000 [0m                     

                       Computation: 42901 steps/s (collection: 2.200s, learning 0.091s)
             Mean action noise std: 2.78
          Mean value_function loss: 204.4717
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 75.2126
                       Mean reward: 817.15
               Mean episode length: 221.53
    Episode_Reward/reaching_object: 2.0480
     Episode_Reward/lifting_object: 163.5553
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0756
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 2.29s
                      Time elapsed: 00:39:18
                               ETA: 00:36:10

################################################################################
                     [1m Learning iteration 1042/2000 [0m                     

                       Computation: 41913 steps/s (collection: 2.200s, learning 0.145s)
             Mean action noise std: 2.78
          Mean value_function loss: 206.4060
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 75.2239
                       Mean reward: 814.32
               Mean episode length: 219.18
    Episode_Reward/reaching_object: 2.0142
     Episode_Reward/lifting_object: 160.3944
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0730
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 2.35s
                      Time elapsed: 00:39:20
                               ETA: 00:36:08

################################################################################
                     [1m Learning iteration 1043/2000 [0m                     

                       Computation: 42411 steps/s (collection: 2.210s, learning 0.108s)
             Mean action noise std: 2.78
          Mean value_function loss: 183.3638
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 75.2374
                       Mean reward: 803.12
               Mean episode length: 220.92
    Episode_Reward/reaching_object: 2.0127
     Episode_Reward/lifting_object: 159.7280
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0734
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 2.32s
                      Time elapsed: 00:39:23
                               ETA: 00:36:06

################################################################################
                     [1m Learning iteration 1044/2000 [0m                     

                       Computation: 40791 steps/s (collection: 2.305s, learning 0.105s)
             Mean action noise std: 2.78
          Mean value_function loss: 188.8745
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 75.2553
                       Mean reward: 811.86
               Mean episode length: 219.52
    Episode_Reward/reaching_object: 2.0491
     Episode_Reward/lifting_object: 163.4679
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0743
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 2.41s
                      Time elapsed: 00:39:25
                               ETA: 00:36:04

################################################################################
                     [1m Learning iteration 1045/2000 [0m                     

                       Computation: 41246 steps/s (collection: 2.204s, learning 0.179s)
             Mean action noise std: 2.78
          Mean value_function loss: 255.1325
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 75.2702
                       Mean reward: 825.38
               Mean episode length: 220.91
    Episode_Reward/reaching_object: 2.0214
     Episode_Reward/lifting_object: 161.3837
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0730
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 2.38s
                      Time elapsed: 00:39:27
                               ETA: 00:36:01

################################################################################
                     [1m Learning iteration 1046/2000 [0m                     

                       Computation: 42489 steps/s (collection: 2.206s, learning 0.108s)
             Mean action noise std: 2.78
          Mean value_function loss: 206.1358
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 75.2849
                       Mean reward: 835.38
               Mean episode length: 224.85
    Episode_Reward/reaching_object: 2.0300
     Episode_Reward/lifting_object: 161.8678
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 2.31s
                      Time elapsed: 00:39:30
                               ETA: 00:35:59

################################################################################
                     [1m Learning iteration 1047/2000 [0m                     

                       Computation: 43648 steps/s (collection: 2.157s, learning 0.095s)
             Mean action noise std: 2.78
          Mean value_function loss: 208.7567
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 75.2960
                       Mean reward: 801.06
               Mean episode length: 219.98
    Episode_Reward/reaching_object: 2.0319
     Episode_Reward/lifting_object: 161.7433
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0741
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 2.25s
                      Time elapsed: 00:39:32
                               ETA: 00:35:57

################################################################################
                     [1m Learning iteration 1048/2000 [0m                     

                       Computation: 42659 steps/s (collection: 2.182s, learning 0.122s)
             Mean action noise std: 2.78
          Mean value_function loss: 225.6159
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 75.3059
                       Mean reward: 792.50
               Mean episode length: 218.23
    Episode_Reward/reaching_object: 2.0456
     Episode_Reward/lifting_object: 163.0850
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 2.30s
                      Time elapsed: 00:39:34
                               ETA: 00:35:55

################################################################################
                     [1m Learning iteration 1049/2000 [0m                     

                       Computation: 40725 steps/s (collection: 2.326s, learning 0.088s)
             Mean action noise std: 2.79
          Mean value_function loss: 193.9398
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 75.3168
                       Mean reward: 831.37
               Mean episode length: 226.64
    Episode_Reward/reaching_object: 2.0555
     Episode_Reward/lifting_object: 163.9294
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0742
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 2.41s
                      Time elapsed: 00:39:37
                               ETA: 00:35:53

################################################################################
                     [1m Learning iteration 1050/2000 [0m                     

                       Computation: 42754 steps/s (collection: 2.210s, learning 0.090s)
             Mean action noise std: 2.79
          Mean value_function loss: 195.5667
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 75.3353
                       Mean reward: 774.58
               Mean episode length: 214.67
    Episode_Reward/reaching_object: 2.0454
     Episode_Reward/lifting_object: 162.5820
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 2.30s
                      Time elapsed: 00:39:39
                               ETA: 00:35:50

################################################################################
                     [1m Learning iteration 1051/2000 [0m                     

                       Computation: 42746 steps/s (collection: 2.205s, learning 0.095s)
             Mean action noise std: 2.79
          Mean value_function loss: 214.2707
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 75.3488
                       Mean reward: 838.45
               Mean episode length: 226.51
    Episode_Reward/reaching_object: 2.0669
     Episode_Reward/lifting_object: 165.5522
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 2.30s
                      Time elapsed: 00:39:41
                               ETA: 00:35:48

################################################################################
                     [1m Learning iteration 1052/2000 [0m                     

                       Computation: 42373 steps/s (collection: 2.205s, learning 0.115s)
             Mean action noise std: 2.79
          Mean value_function loss: 196.0380
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 75.3652
                       Mean reward: 801.29
               Mean episode length: 219.02
    Episode_Reward/reaching_object: 2.0394
     Episode_Reward/lifting_object: 163.5401
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0731
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 2.32s
                      Time elapsed: 00:39:44
                               ETA: 00:35:46

################################################################################
                     [1m Learning iteration 1053/2000 [0m                     

                       Computation: 42660 steps/s (collection: 2.202s, learning 0.102s)
             Mean action noise std: 2.79
          Mean value_function loss: 181.2790
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 75.3842
                       Mean reward: 796.56
               Mean episode length: 217.95
    Episode_Reward/reaching_object: 2.0341
     Episode_Reward/lifting_object: 163.0229
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0732
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 2.30s
                      Time elapsed: 00:39:46
                               ETA: 00:35:44

################################################################################
                     [1m Learning iteration 1054/2000 [0m                     

                       Computation: 42354 steps/s (collection: 2.208s, learning 0.113s)
             Mean action noise std: 2.79
          Mean value_function loss: 199.1642
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 75.3992
                       Mean reward: 796.67
               Mean episode length: 217.16
    Episode_Reward/reaching_object: 2.0220
     Episode_Reward/lifting_object: 161.9784
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 2.32s
                      Time elapsed: 00:39:48
                               ETA: 00:35:41

################################################################################
                     [1m Learning iteration 1055/2000 [0m                     

                       Computation: 42057 steps/s (collection: 2.232s, learning 0.106s)
             Mean action noise std: 2.79
          Mean value_function loss: 184.2712
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 75.4100
                       Mean reward: 831.80
               Mean episode length: 226.76
    Episode_Reward/reaching_object: 2.0202
     Episode_Reward/lifting_object: 161.6392
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0722
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 2.34s
                      Time elapsed: 00:39:51
                               ETA: 00:35:39

################################################################################
                     [1m Learning iteration 1056/2000 [0m                     

                       Computation: 42064 steps/s (collection: 2.219s, learning 0.118s)
             Mean action noise std: 2.79
          Mean value_function loss: 196.2448
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 75.4200
                       Mean reward: 825.72
               Mean episode length: 224.80
    Episode_Reward/reaching_object: 2.0375
     Episode_Reward/lifting_object: 162.6341
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0730
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 2.34s
                      Time elapsed: 00:39:53
                               ETA: 00:35:37

################################################################################
                     [1m Learning iteration 1057/2000 [0m                     

                       Computation: 42265 steps/s (collection: 2.224s, learning 0.102s)
             Mean action noise std: 2.80
          Mean value_function loss: 194.1260
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 75.4311
                       Mean reward: 855.47
               Mean episode length: 230.65
    Episode_Reward/reaching_object: 2.0675
     Episode_Reward/lifting_object: 165.4144
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.0731
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 2.33s
                      Time elapsed: 00:39:55
                               ETA: 00:35:35

################################################################################
                     [1m Learning iteration 1058/2000 [0m                     

                       Computation: 42260 steps/s (collection: 2.222s, learning 0.105s)
             Mean action noise std: 2.80
          Mean value_function loss: 209.8570
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 75.4431
                       Mean reward: 799.89
               Mean episode length: 218.85
    Episode_Reward/reaching_object: 2.0221
     Episode_Reward/lifting_object: 161.8578
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 2.33s
                      Time elapsed: 00:39:58
                               ETA: 00:35:33

################################################################################
                     [1m Learning iteration 1059/2000 [0m                     

                       Computation: 41415 steps/s (collection: 2.263s, learning 0.111s)
             Mean action noise std: 2.80
          Mean value_function loss: 148.7427
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 75.4516
                       Mean reward: 858.39
               Mean episode length: 232.83
    Episode_Reward/reaching_object: 2.1202
     Episode_Reward/lifting_object: 170.6471
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0746
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 2.37s
                      Time elapsed: 00:40:00
                               ETA: 00:35:30

################################################################################
                     [1m Learning iteration 1060/2000 [0m                     

                       Computation: 43028 steps/s (collection: 2.188s, learning 0.097s)
             Mean action noise std: 2.80
          Mean value_function loss: 202.5634
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 75.4723
                       Mean reward: 813.55
               Mean episode length: 225.38
    Episode_Reward/reaching_object: 2.0485
     Episode_Reward/lifting_object: 163.9600
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 2.28s
                      Time elapsed: 00:40:02
                               ETA: 00:35:28

################################################################################
                     [1m Learning iteration 1061/2000 [0m                     

                       Computation: 42370 steps/s (collection: 2.212s, learning 0.108s)
             Mean action noise std: 2.80
          Mean value_function loss: 198.7260
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 75.4981
                       Mean reward: 831.63
               Mean episode length: 224.80
    Episode_Reward/reaching_object: 2.0355
     Episode_Reward/lifting_object: 162.9976
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 2.32s
                      Time elapsed: 00:40:05
                               ETA: 00:35:26

################################################################################
                     [1m Learning iteration 1062/2000 [0m                     

                       Computation: 40479 steps/s (collection: 2.315s, learning 0.114s)
             Mean action noise std: 2.80
          Mean value_function loss: 178.9176
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 75.5139
                       Mean reward: 823.63
               Mean episode length: 224.01
    Episode_Reward/reaching_object: 2.0935
     Episode_Reward/lifting_object: 168.3347
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 2.43s
                      Time elapsed: 00:40:07
                               ETA: 00:35:24

################################################################################
                     [1m Learning iteration 1063/2000 [0m                     

                       Computation: 41838 steps/s (collection: 2.251s, learning 0.098s)
             Mean action noise std: 2.80
          Mean value_function loss: 208.7057
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 75.5243
                       Mean reward: 811.11
               Mean episode length: 220.75
    Episode_Reward/reaching_object: 2.0069
     Episode_Reward/lifting_object: 159.4582
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 2.35s
                      Time elapsed: 00:40:09
                               ETA: 00:35:22

################################################################################
                     [1m Learning iteration 1064/2000 [0m                     

                       Computation: 43070 steps/s (collection: 2.192s, learning 0.090s)
             Mean action noise std: 2.81
          Mean value_function loss: 201.4299
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 75.5361
                       Mean reward: 819.02
               Mean episode length: 223.28
    Episode_Reward/reaching_object: 2.0360
     Episode_Reward/lifting_object: 162.5586
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 2.28s
                      Time elapsed: 00:40:12
                               ETA: 00:35:19

################################################################################
                     [1m Learning iteration 1065/2000 [0m                     

                       Computation: 42841 steps/s (collection: 2.184s, learning 0.111s)
             Mean action noise std: 2.81
          Mean value_function loss: 232.1586
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 75.5497
                       Mean reward: 794.81
               Mean episode length: 217.09
    Episode_Reward/reaching_object: 1.9930
     Episode_Reward/lifting_object: 158.4998
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0725
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 2.29s
                      Time elapsed: 00:40:14
                               ETA: 00:35:17

################################################################################
                     [1m Learning iteration 1066/2000 [0m                     

                       Computation: 42441 steps/s (collection: 2.226s, learning 0.091s)
             Mean action noise std: 2.81
          Mean value_function loss: 203.5551
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 75.5651
                       Mean reward: 769.85
               Mean episode length: 210.58
    Episode_Reward/reaching_object: 2.0019
     Episode_Reward/lifting_object: 159.7466
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 2.32s
                      Time elapsed: 00:40:16
                               ETA: 00:35:15

################################################################################
                     [1m Learning iteration 1067/2000 [0m                     

                       Computation: 41938 steps/s (collection: 2.224s, learning 0.120s)
             Mean action noise std: 2.81
          Mean value_function loss: 192.8100
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 75.5754
                       Mean reward: 831.11
               Mean episode length: 223.43
    Episode_Reward/reaching_object: 2.0368
     Episode_Reward/lifting_object: 162.6236
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0729
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 2.34s
                      Time elapsed: 00:40:19
                               ETA: 00:35:13

################################################################################
                     [1m Learning iteration 1068/2000 [0m                     

                       Computation: 42501 steps/s (collection: 2.210s, learning 0.103s)
             Mean action noise std: 2.81
          Mean value_function loss: 241.9757
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 75.5858
                       Mean reward: 834.65
               Mean episode length: 227.64
    Episode_Reward/reaching_object: 2.0132
     Episode_Reward/lifting_object: 160.5147
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0725
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 2.31s
                      Time elapsed: 00:40:21
                               ETA: 00:35:11

################################################################################
                     [1m Learning iteration 1069/2000 [0m                     

                       Computation: 42427 steps/s (collection: 2.225s, learning 0.092s)
             Mean action noise std: 2.81
          Mean value_function loss: 200.0826
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 75.6008
                       Mean reward: 804.02
               Mean episode length: 219.44
    Episode_Reward/reaching_object: 2.0250
     Episode_Reward/lifting_object: 161.4043
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 2.32s
                      Time elapsed: 00:40:23
                               ETA: 00:35:08

################################################################################
                     [1m Learning iteration 1070/2000 [0m                     

                       Computation: 41859 steps/s (collection: 2.239s, learning 0.109s)
             Mean action noise std: 2.81
          Mean value_function loss: 210.3238
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 75.6162
                       Mean reward: 815.31
               Mean episode length: 222.07
    Episode_Reward/reaching_object: 2.0441
     Episode_Reward/lifting_object: 163.8222
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0730
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 2.35s
                      Time elapsed: 00:40:26
                               ETA: 00:35:06

################################################################################
                     [1m Learning iteration 1071/2000 [0m                     

                       Computation: 43055 steps/s (collection: 2.194s, learning 0.089s)
             Mean action noise std: 2.81
          Mean value_function loss: 271.2948
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 75.6263
                       Mean reward: 787.88
               Mean episode length: 214.01
    Episode_Reward/reaching_object: 2.0515
     Episode_Reward/lifting_object: 164.2912
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0731
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 2.28s
                      Time elapsed: 00:40:28
                               ETA: 00:35:04

################################################################################
                     [1m Learning iteration 1072/2000 [0m                     

                       Computation: 42236 steps/s (collection: 2.227s, learning 0.101s)
             Mean action noise std: 2.82
          Mean value_function loss: 240.7062
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 75.6399
                       Mean reward: 814.92
               Mean episode length: 222.08
    Episode_Reward/reaching_object: 2.0522
     Episode_Reward/lifting_object: 164.4944
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 2.33s
                      Time elapsed: 00:40:30
                               ETA: 00:35:02

################################################################################
                     [1m Learning iteration 1073/2000 [0m                     

                       Computation: 41828 steps/s (collection: 2.256s, learning 0.094s)
             Mean action noise std: 2.82
          Mean value_function loss: 162.6448
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 75.6540
                       Mean reward: 831.16
               Mean episode length: 224.26
    Episode_Reward/reaching_object: 2.0071
     Episode_Reward/lifting_object: 160.1624
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0721
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 2.35s
                      Time elapsed: 00:40:33
                               ETA: 00:35:00

################################################################################
                     [1m Learning iteration 1074/2000 [0m                     

                       Computation: 42286 steps/s (collection: 2.235s, learning 0.090s)
             Mean action noise std: 2.82
          Mean value_function loss: 183.4854
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 75.6683
                       Mean reward: 815.59
               Mean episode length: 222.44
    Episode_Reward/reaching_object: 2.0176
     Episode_Reward/lifting_object: 160.7696
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 2.32s
                      Time elapsed: 00:40:35
                               ETA: 00:34:57

################################################################################
                     [1m Learning iteration 1075/2000 [0m                     

                       Computation: 42400 steps/s (collection: 2.208s, learning 0.111s)
             Mean action noise std: 2.82
          Mean value_function loss: 181.6709
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 75.6893
                       Mean reward: 892.25
               Mean episode length: 238.94
    Episode_Reward/reaching_object: 2.0823
     Episode_Reward/lifting_object: 166.1555
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0745
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 2.32s
                      Time elapsed: 00:40:37
                               ETA: 00:34:55

################################################################################
                     [1m Learning iteration 1076/2000 [0m                     

                       Computation: 42695 steps/s (collection: 2.202s, learning 0.100s)
             Mean action noise std: 2.82
          Mean value_function loss: 168.7528
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 75.7141
                       Mean reward: 833.29
               Mean episode length: 229.12
    Episode_Reward/reaching_object: 2.0473
     Episode_Reward/lifting_object: 164.0615
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0731
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 2.30s
                      Time elapsed: 00:40:39
                               ETA: 00:34:53

################################################################################
                     [1m Learning iteration 1077/2000 [0m                     

                       Computation: 42727 steps/s (collection: 2.195s, learning 0.106s)
             Mean action noise std: 2.82
          Mean value_function loss: 183.7242
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 75.7308
                       Mean reward: 868.72
               Mean episode length: 233.69
    Episode_Reward/reaching_object: 2.0670
     Episode_Reward/lifting_object: 166.2266
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0730
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 2.30s
                      Time elapsed: 00:40:42
                               ETA: 00:34:51

################################################################################
                     [1m Learning iteration 1078/2000 [0m                     

                       Computation: 41215 steps/s (collection: 2.239s, learning 0.146s)
             Mean action noise std: 2.82
          Mean value_function loss: 164.4516
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 75.7381
                       Mean reward: 840.07
               Mean episode length: 227.32
    Episode_Reward/reaching_object: 2.1056
     Episode_Reward/lifting_object: 169.0403
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0749
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 2.39s
                      Time elapsed: 00:40:44
                               ETA: 00:34:48

################################################################################
                     [1m Learning iteration 1079/2000 [0m                     

                       Computation: 42255 steps/s (collection: 2.229s, learning 0.098s)
             Mean action noise std: 2.82
          Mean value_function loss: 168.5088
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 75.7504
                       Mean reward: 796.29
               Mean episode length: 220.75
    Episode_Reward/reaching_object: 2.0289
     Episode_Reward/lifting_object: 162.1613
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0523
          Episode_Reward/joint_vel: -0.0721
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 2.33s
                      Time elapsed: 00:40:46
                               ETA: 00:34:46

################################################################################
                     [1m Learning iteration 1080/2000 [0m                     

                       Computation: 42282 steps/s (collection: 2.214s, learning 0.111s)
             Mean action noise std: 2.83
          Mean value_function loss: 199.1630
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 75.7635
                       Mean reward: 785.72
               Mean episode length: 214.94
    Episode_Reward/reaching_object: 2.0279
     Episode_Reward/lifting_object: 161.8505
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0732
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 2.32s
                      Time elapsed: 00:40:49
                               ETA: 00:34:44

################################################################################
                     [1m Learning iteration 1081/2000 [0m                     

                       Computation: 42455 steps/s (collection: 2.199s, learning 0.116s)
             Mean action noise std: 2.83
          Mean value_function loss: 208.0722
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 75.7804
                       Mean reward: 838.75
               Mean episode length: 227.05
    Episode_Reward/reaching_object: 2.0564
     Episode_Reward/lifting_object: 164.6796
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 2.32s
                      Time elapsed: 00:40:51
                               ETA: 00:34:42

################################################################################
                     [1m Learning iteration 1082/2000 [0m                     

                       Computation: 42040 steps/s (collection: 2.220s, learning 0.119s)
             Mean action noise std: 2.83
          Mean value_function loss: 209.9684
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 75.7974
                       Mean reward: 845.70
               Mean episode length: 229.09
    Episode_Reward/reaching_object: 2.0191
     Episode_Reward/lifting_object: 161.0710
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0721
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 2.34s
                      Time elapsed: 00:40:53
                               ETA: 00:34:40

################################################################################
                     [1m Learning iteration 1083/2000 [0m                     

                       Computation: 40859 steps/s (collection: 2.295s, learning 0.111s)
             Mean action noise std: 2.83
          Mean value_function loss: 190.0373
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 75.8107
                       Mean reward: 750.52
               Mean episode length: 207.12
    Episode_Reward/reaching_object: 1.9939
     Episode_Reward/lifting_object: 159.0566
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 2.41s
                      Time elapsed: 00:40:56
                               ETA: 00:34:37

################################################################################
                     [1m Learning iteration 1084/2000 [0m                     

                       Computation: 41201 steps/s (collection: 2.268s, learning 0.118s)
             Mean action noise std: 2.83
          Mean value_function loss: 161.7168
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 75.8238
                       Mean reward: 782.82
               Mean episode length: 213.47
    Episode_Reward/reaching_object: 2.0496
     Episode_Reward/lifting_object: 164.4406
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.0726
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 2.39s
                      Time elapsed: 00:40:58
                               ETA: 00:34:35

################################################################################
                     [1m Learning iteration 1085/2000 [0m                     

                       Computation: 41283 steps/s (collection: 2.279s, learning 0.102s)
             Mean action noise std: 2.83
          Mean value_function loss: 194.8872
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 75.8401
                       Mean reward: 814.58
               Mean episode length: 220.64
    Episode_Reward/reaching_object: 2.0560
     Episode_Reward/lifting_object: 164.3205
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0731
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 2.38s
                      Time elapsed: 00:41:01
                               ETA: 00:34:33

################################################################################
                     [1m Learning iteration 1086/2000 [0m                     

                       Computation: 41862 steps/s (collection: 2.256s, learning 0.093s)
             Mean action noise std: 2.83
          Mean value_function loss: 192.7044
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 75.8577
                       Mean reward: 816.32
               Mean episode length: 224.07
    Episode_Reward/reaching_object: 2.0456
     Episode_Reward/lifting_object: 163.4822
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0739
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 2.35s
                      Time elapsed: 00:41:03
                               ETA: 00:34:31

################################################################################
                     [1m Learning iteration 1087/2000 [0m                     

                       Computation: 41296 steps/s (collection: 2.271s, learning 0.109s)
             Mean action noise std: 2.84
          Mean value_function loss: 185.4490
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 75.8717
                       Mean reward: 825.72
               Mean episode length: 223.79
    Episode_Reward/reaching_object: 2.0592
     Episode_Reward/lifting_object: 165.3090
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 2.38s
                      Time elapsed: 00:41:05
                               ETA: 00:34:29

################################################################################
                     [1m Learning iteration 1088/2000 [0m                     

                       Computation: 42180 steps/s (collection: 2.232s, learning 0.098s)
             Mean action noise std: 2.84
          Mean value_function loss: 197.1909
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 75.8819
                       Mean reward: 837.27
               Mean episode length: 223.92
    Episode_Reward/reaching_object: 2.0183
     Episode_Reward/lifting_object: 162.5565
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0523
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 2.33s
                      Time elapsed: 00:41:08
                               ETA: 00:34:27

################################################################################
                     [1m Learning iteration 1089/2000 [0m                     

                       Computation: 41371 steps/s (collection: 2.260s, learning 0.116s)
             Mean action noise std: 2.84
          Mean value_function loss: 175.1287
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 75.8922
                       Mean reward: 852.53
               Mean episode length: 229.73
    Episode_Reward/reaching_object: 2.0251
     Episode_Reward/lifting_object: 162.9259
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0722
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 2.38s
                      Time elapsed: 00:41:10
                               ETA: 00:34:24

################################################################################
                     [1m Learning iteration 1090/2000 [0m                     

                       Computation: 40369 steps/s (collection: 2.326s, learning 0.110s)
             Mean action noise std: 2.84
          Mean value_function loss: 156.3134
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 75.9075
                       Mean reward: 855.63
               Mean episode length: 231.32
    Episode_Reward/reaching_object: 2.0903
     Episode_Reward/lifting_object: 168.1769
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0747
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 2.44s
                      Time elapsed: 00:41:13
                               ETA: 00:34:22

################################################################################
                     [1m Learning iteration 1091/2000 [0m                     

                       Computation: 41052 steps/s (collection: 2.296s, learning 0.099s)
             Mean action noise std: 2.84
          Mean value_function loss: 158.8491
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 75.9254
                       Mean reward: 867.71
               Mean episode length: 233.44
    Episode_Reward/reaching_object: 2.0813
     Episode_Reward/lifting_object: 167.7872
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0733
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 2.39s
                      Time elapsed: 00:41:15
                               ETA: 00:34:20

################################################################################
                     [1m Learning iteration 1092/2000 [0m                     

                       Computation: 39113 steps/s (collection: 2.337s, learning 0.177s)
             Mean action noise std: 2.84
          Mean value_function loss: 168.2970
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 75.9402
                       Mean reward: 844.50
               Mean episode length: 228.06
    Episode_Reward/reaching_object: 2.0832
     Episode_Reward/lifting_object: 168.1821
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0749
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 2.51s
                      Time elapsed: 00:41:17
                               ETA: 00:34:18

################################################################################
                     [1m Learning iteration 1093/2000 [0m                     

                       Computation: 40855 steps/s (collection: 2.311s, learning 0.095s)
             Mean action noise std: 2.84
          Mean value_function loss: 153.5931
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 75.9511
                       Mean reward: 853.68
               Mean episode length: 232.50
    Episode_Reward/reaching_object: 2.0834
     Episode_Reward/lifting_object: 167.5453
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0756
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 2.41s
                      Time elapsed: 00:41:20
                               ETA: 00:34:16

################################################################################
                     [1m Learning iteration 1094/2000 [0m                     

                       Computation: 40309 steps/s (collection: 2.321s, learning 0.118s)
             Mean action noise std: 2.85
          Mean value_function loss: 172.6990
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 75.9674
                       Mean reward: 783.76
               Mean episode length: 216.14
    Episode_Reward/reaching_object: 2.0499
     Episode_Reward/lifting_object: 165.1580
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 2.44s
                      Time elapsed: 00:41:22
                               ETA: 00:34:14

################################################################################
                     [1m Learning iteration 1095/2000 [0m                     

                       Computation: 42350 steps/s (collection: 2.207s, learning 0.114s)
             Mean action noise std: 2.85
          Mean value_function loss: 155.1943
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 75.9802
                       Mean reward: 830.84
               Mean episode length: 224.58
    Episode_Reward/reaching_object: 2.0598
     Episode_Reward/lifting_object: 166.7562
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 2.32s
                      Time elapsed: 00:41:25
                               ETA: 00:34:12

################################################################################
                     [1m Learning iteration 1096/2000 [0m                     

                       Computation: 41261 steps/s (collection: 2.237s, learning 0.146s)
             Mean action noise std: 2.85
          Mean value_function loss: 151.8962
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 75.9866
                       Mean reward: 852.08
               Mean episode length: 230.76
    Episode_Reward/reaching_object: 2.0862
     Episode_Reward/lifting_object: 168.7203
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0745
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 2.38s
                      Time elapsed: 00:41:27
                               ETA: 00:34:09

################################################################################
                     [1m Learning iteration 1097/2000 [0m                     

                       Computation: 41389 steps/s (collection: 2.277s, learning 0.098s)
             Mean action noise std: 2.85
          Mean value_function loss: 168.1151
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 75.9988
                       Mean reward: 844.03
               Mean episode length: 227.10
    Episode_Reward/reaching_object: 2.0797
     Episode_Reward/lifting_object: 168.6715
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 2.38s
                      Time elapsed: 00:41:29
                               ETA: 00:34:07

################################################################################
                     [1m Learning iteration 1098/2000 [0m                     

                       Computation: 42397 steps/s (collection: 2.217s, learning 0.102s)
             Mean action noise std: 2.85
          Mean value_function loss: 180.8027
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 76.0179
                       Mean reward: 867.22
               Mean episode length: 233.21
    Episode_Reward/reaching_object: 2.0042
     Episode_Reward/lifting_object: 161.6396
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 2.32s
                      Time elapsed: 00:41:32
                               ETA: 00:34:05

################################################################################
                     [1m Learning iteration 1099/2000 [0m                     

                       Computation: 40326 steps/s (collection: 2.323s, learning 0.115s)
             Mean action noise std: 2.85
          Mean value_function loss: 173.0649
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 76.0414
                       Mean reward: 849.99
               Mean episode length: 228.32
    Episode_Reward/reaching_object: 2.0554
     Episode_Reward/lifting_object: 167.0013
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0732
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 2.44s
                      Time elapsed: 00:41:34
                               ETA: 00:34:03

################################################################################
                     [1m Learning iteration 1100/2000 [0m                     

                       Computation: 40772 steps/s (collection: 2.258s, learning 0.153s)
             Mean action noise std: 2.85
          Mean value_function loss: 146.6309
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 76.0607
                       Mean reward: 849.61
               Mean episode length: 228.78
    Episode_Reward/reaching_object: 2.0802
     Episode_Reward/lifting_object: 168.9093
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0549
          Episode_Reward/joint_vel: -0.0734
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 2.41s
                      Time elapsed: 00:41:37
                               ETA: 00:34:01

################################################################################
                     [1m Learning iteration 1101/2000 [0m                     

                       Computation: 38595 steps/s (collection: 2.435s, learning 0.112s)
             Mean action noise std: 2.86
          Mean value_function loss: 160.4654
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 76.0753
                       Mean reward: 847.39
               Mean episode length: 227.66
    Episode_Reward/reaching_object: 2.0671
     Episode_Reward/lifting_object: 168.1392
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0734
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 2.55s
                      Time elapsed: 00:41:39
                               ETA: 00:33:59

################################################################################
                     [1m Learning iteration 1102/2000 [0m                     

                       Computation: 39458 steps/s (collection: 2.374s, learning 0.117s)
             Mean action noise std: 2.86
          Mean value_function loss: 184.1037
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 76.0942
                       Mean reward: 784.46
               Mean episode length: 213.76
    Episode_Reward/reaching_object: 1.9820
     Episode_Reward/lifting_object: 160.5958
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0710
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 2.49s
                      Time elapsed: 00:41:42
                               ETA: 00:33:57

################################################################################
                     [1m Learning iteration 1103/2000 [0m                     

                       Computation: 37884 steps/s (collection: 2.460s, learning 0.135s)
             Mean action noise std: 2.86
          Mean value_function loss: 141.2512
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 76.1142
                       Mean reward: 912.05
               Mean episode length: 242.57
    Episode_Reward/reaching_object: 2.0918
     Episode_Reward/lifting_object: 170.5564
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 2.59s
                      Time elapsed: 00:41:44
                               ETA: 00:33:55

################################################################################
                     [1m Learning iteration 1104/2000 [0m                     

                       Computation: 39451 steps/s (collection: 2.396s, learning 0.096s)
             Mean action noise std: 2.86
          Mean value_function loss: 162.0250
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 76.1284
                       Mean reward: 788.26
               Mean episode length: 215.01
    Episode_Reward/reaching_object: 2.0270
     Episode_Reward/lifting_object: 164.5266
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 2.49s
                      Time elapsed: 00:41:47
                               ETA: 00:33:52

################################################################################
                     [1m Learning iteration 1105/2000 [0m                     

                       Computation: 39697 steps/s (collection: 2.386s, learning 0.091s)
             Mean action noise std: 2.86
          Mean value_function loss: 156.2287
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 76.1483
                       Mean reward: 828.31
               Mean episode length: 225.14
    Episode_Reward/reaching_object: 1.9979
     Episode_Reward/lifting_object: 162.2453
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 2.48s
                      Time elapsed: 00:41:49
                               ETA: 00:33:50

################################################################################
                     [1m Learning iteration 1106/2000 [0m                     

                       Computation: 39689 steps/s (collection: 2.347s, learning 0.130s)
             Mean action noise std: 2.86
          Mean value_function loss: 193.7249
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 76.1637
                       Mean reward: 814.49
               Mean episode length: 221.00
    Episode_Reward/reaching_object: 2.0112
     Episode_Reward/lifting_object: 163.9553
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0721
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 2.48s
                      Time elapsed: 00:41:52
                               ETA: 00:33:48

################################################################################
                     [1m Learning iteration 1107/2000 [0m                     

                       Computation: 40864 steps/s (collection: 2.316s, learning 0.090s)
             Mean action noise std: 2.87
          Mean value_function loss: 193.5520
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 76.1782
                       Mean reward: 835.81
               Mean episode length: 225.17
    Episode_Reward/reaching_object: 2.0454
     Episode_Reward/lifting_object: 166.4778
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 2.41s
                      Time elapsed: 00:41:54
                               ETA: 00:33:46

################################################################################
                     [1m Learning iteration 1108/2000 [0m                     

                       Computation: 40516 steps/s (collection: 2.268s, learning 0.158s)
             Mean action noise std: 2.87
          Mean value_function loss: 167.7676
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 76.1973
                       Mean reward: 815.45
               Mean episode length: 221.04
    Episode_Reward/reaching_object: 2.0391
     Episode_Reward/lifting_object: 166.0569
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 2.43s
                      Time elapsed: 00:41:56
                               ETA: 00:33:44

################################################################################
                     [1m Learning iteration 1109/2000 [0m                     

                       Computation: 42266 steps/s (collection: 2.225s, learning 0.101s)
             Mean action noise std: 2.87
          Mean value_function loss: 166.6908
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 76.2132
                       Mean reward: 829.75
               Mean episode length: 223.70
    Episode_Reward/reaching_object: 2.0784
     Episode_Reward/lifting_object: 169.3461
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0749
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 2.33s
                      Time elapsed: 00:41:59
                               ETA: 00:33:42

################################################################################
                     [1m Learning iteration 1110/2000 [0m                     

                       Computation: 42741 steps/s (collection: 2.212s, learning 0.088s)
             Mean action noise std: 2.87
          Mean value_function loss: 132.8697
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 76.2298
                       Mean reward: 858.39
               Mean episode length: 231.59
    Episode_Reward/reaching_object: 2.0872
     Episode_Reward/lifting_object: 170.2973
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0746
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 2.30s
                      Time elapsed: 00:42:01
                               ETA: 00:33:39

################################################################################
                     [1m Learning iteration 1111/2000 [0m                     

                       Computation: 41355 steps/s (collection: 2.274s, learning 0.103s)
             Mean action noise std: 2.87
          Mean value_function loss: 159.9138
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 76.2462
                       Mean reward: 829.78
               Mean episode length: 225.13
    Episode_Reward/reaching_object: 2.0183
     Episode_Reward/lifting_object: 164.0307
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 2.38s
                      Time elapsed: 00:42:03
                               ETA: 00:33:37

################################################################################
                     [1m Learning iteration 1112/2000 [0m                     

                       Computation: 42361 steps/s (collection: 2.232s, learning 0.089s)
             Mean action noise std: 2.87
          Mean value_function loss: 146.7044
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 76.2674
                       Mean reward: 866.12
               Mean episode length: 232.53
    Episode_Reward/reaching_object: 2.0800
     Episode_Reward/lifting_object: 169.7970
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0741
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 2.32s
                      Time elapsed: 00:42:06
                               ETA: 00:33:35

################################################################################
                     [1m Learning iteration 1113/2000 [0m                     

                       Computation: 41511 steps/s (collection: 2.275s, learning 0.094s)
             Mean action noise std: 2.88
          Mean value_function loss: 161.3445
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 76.2870
                       Mean reward: 836.83
               Mean episode length: 225.63
    Episode_Reward/reaching_object: 2.0586
     Episode_Reward/lifting_object: 166.9486
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0743
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 2.37s
                      Time elapsed: 00:42:08
                               ETA: 00:33:33

################################################################################
                     [1m Learning iteration 1114/2000 [0m                     

                       Computation: 42148 steps/s (collection: 2.211s, learning 0.122s)
             Mean action noise std: 2.88
          Mean value_function loss: 175.5929
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 76.3060
                       Mean reward: 824.92
               Mean episode length: 224.63
    Episode_Reward/reaching_object: 1.9976
     Episode_Reward/lifting_object: 161.1656
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 2.33s
                      Time elapsed: 00:42:10
                               ETA: 00:33:31

################################################################################
                     [1m Learning iteration 1115/2000 [0m                     

                       Computation: 40631 steps/s (collection: 2.284s, learning 0.135s)
             Mean action noise std: 2.88
          Mean value_function loss: 152.0049
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 76.3284
                       Mean reward: 837.90
               Mean episode length: 226.46
    Episode_Reward/reaching_object: 2.0048
     Episode_Reward/lifting_object: 162.0641
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0722
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 2.42s
                      Time elapsed: 00:42:13
                               ETA: 00:33:28

################################################################################
                     [1m Learning iteration 1116/2000 [0m                     

                       Computation: 41923 steps/s (collection: 2.258s, learning 0.087s)
             Mean action noise std: 2.88
          Mean value_function loss: 142.5748
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 76.3467
                       Mean reward: 870.40
               Mean episode length: 234.28
    Episode_Reward/reaching_object: 2.0553
     Episode_Reward/lifting_object: 166.2048
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 2.34s
                      Time elapsed: 00:42:15
                               ETA: 00:33:26

################################################################################
                     [1m Learning iteration 1117/2000 [0m                     

                       Computation: 39285 steps/s (collection: 2.300s, learning 0.202s)
             Mean action noise std: 2.88
          Mean value_function loss: 194.8830
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 76.3658
                       Mean reward: 849.87
               Mean episode length: 228.45
    Episode_Reward/reaching_object: 2.0008
     Episode_Reward/lifting_object: 161.5975
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0718
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 2.50s
                      Time elapsed: 00:42:18
                               ETA: 00:33:24

################################################################################
                     [1m Learning iteration 1118/2000 [0m                     

                       Computation: 41691 steps/s (collection: 2.266s, learning 0.092s)
             Mean action noise std: 2.89
          Mean value_function loss: 169.4568
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 76.3887
                       Mean reward: 822.46
               Mean episode length: 221.65
    Episode_Reward/reaching_object: 2.0582
     Episode_Reward/lifting_object: 165.9311
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 2.36s
                      Time elapsed: 00:42:20
                               ETA: 00:33:22

################################################################################
                     [1m Learning iteration 1119/2000 [0m                     

                       Computation: 40039 steps/s (collection: 2.297s, learning 0.158s)
             Mean action noise std: 2.89
          Mean value_function loss: 147.9090
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 76.4059
                       Mean reward: 866.93
               Mean episode length: 233.55
    Episode_Reward/reaching_object: 2.1244
     Episode_Reward/lifting_object: 171.7254
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0572
          Episode_Reward/joint_vel: -0.0757
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 2.46s
                      Time elapsed: 00:42:23
                               ETA: 00:33:20

################################################################################
                     [1m Learning iteration 1120/2000 [0m                     

                       Computation: 41345 steps/s (collection: 2.275s, learning 0.103s)
             Mean action noise std: 2.89
          Mean value_function loss: 178.7334
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 76.4243
                       Mean reward: 830.71
               Mean episode length: 224.77
    Episode_Reward/reaching_object: 2.1009
     Episode_Reward/lifting_object: 169.6249
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0750
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 2.38s
                      Time elapsed: 00:42:25
                               ETA: 00:33:18

################################################################################
                     [1m Learning iteration 1121/2000 [0m                     

                       Computation: 42587 steps/s (collection: 2.219s, learning 0.089s)
             Mean action noise std: 2.89
          Mean value_function loss: 183.1778
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 76.4418
                       Mean reward: 831.75
               Mean episode length: 223.77
    Episode_Reward/reaching_object: 1.9771
     Episode_Reward/lifting_object: 158.7133
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0710
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 2.31s
                      Time elapsed: 00:42:27
                               ETA: 00:33:15

################################################################################
                     [1m Learning iteration 1122/2000 [0m                     

                       Computation: 39576 steps/s (collection: 2.368s, learning 0.116s)
             Mean action noise std: 2.89
          Mean value_function loss: 170.9706
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 76.4595
                       Mean reward: 843.63
               Mean episode length: 226.90
    Episode_Reward/reaching_object: 2.0793
     Episode_Reward/lifting_object: 168.5611
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0740
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 2.48s
                      Time elapsed: 00:42:30
                               ETA: 00:33:13

################################################################################
                     [1m Learning iteration 1123/2000 [0m                     

                       Computation: 41389 steps/s (collection: 2.260s, learning 0.115s)
             Mean action noise std: 2.89
          Mean value_function loss: 172.8883
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 76.4667
                       Mean reward: 851.67
               Mean episode length: 228.05
    Episode_Reward/reaching_object: 2.0226
     Episode_Reward/lifting_object: 163.5966
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 2.38s
                      Time elapsed: 00:42:32
                               ETA: 00:33:11

################################################################################
                     [1m Learning iteration 1124/2000 [0m                     

                       Computation: 41954 steps/s (collection: 2.232s, learning 0.112s)
             Mean action noise std: 2.89
          Mean value_function loss: 176.0118
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 76.4773
                       Mean reward: 805.04
               Mean episode length: 217.36
    Episode_Reward/reaching_object: 2.0113
     Episode_Reward/lifting_object: 162.7619
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 2.34s
                      Time elapsed: 00:42:34
                               ETA: 00:33:09

################################################################################
                     [1m Learning iteration 1125/2000 [0m                     

                       Computation: 41626 steps/s (collection: 2.271s, learning 0.091s)
             Mean action noise std: 2.89
          Mean value_function loss: 142.5494
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 76.4939
                       Mean reward: 877.89
               Mean episode length: 233.96
    Episode_Reward/reaching_object: 2.0709
     Episode_Reward/lifting_object: 167.8367
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 2.36s
                      Time elapsed: 00:42:37
                               ETA: 00:33:07

################################################################################
                     [1m Learning iteration 1126/2000 [0m                     

                       Computation: 42326 steps/s (collection: 2.223s, learning 0.100s)
             Mean action noise std: 2.90
          Mean value_function loss: 158.1867
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 76.5125
                       Mean reward: 832.04
               Mean episode length: 224.46
    Episode_Reward/reaching_object: 2.0112
     Episode_Reward/lifting_object: 162.9670
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0718
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 2.32s
                      Time elapsed: 00:42:39
                               ETA: 00:33:04

################################################################################
                     [1m Learning iteration 1127/2000 [0m                     

                       Computation: 38235 steps/s (collection: 2.472s, learning 0.099s)
             Mean action noise std: 2.90
          Mean value_function loss: 173.7140
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 76.5314
                       Mean reward: 890.34
               Mean episode length: 237.81
    Episode_Reward/reaching_object: 2.0896
     Episode_Reward/lifting_object: 169.5077
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0740
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 2.57s
                      Time elapsed: 00:42:42
                               ETA: 00:33:02

################################################################################
                     [1m Learning iteration 1128/2000 [0m                     

                       Computation: 38695 steps/s (collection: 2.443s, learning 0.098s)
             Mean action noise std: 2.90
          Mean value_function loss: 147.7663
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 76.5433
                       Mean reward: 882.48
               Mean episode length: 234.89
    Episode_Reward/reaching_object: 2.0873
     Episode_Reward/lifting_object: 169.3875
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0741
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 2.54s
                      Time elapsed: 00:42:44
                               ETA: 00:33:00

################################################################################
                     [1m Learning iteration 1129/2000 [0m                     

                       Computation: 41691 steps/s (collection: 2.253s, learning 0.105s)
             Mean action noise std: 2.90
          Mean value_function loss: 173.6025
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 76.5574
                       Mean reward: 861.06
               Mean episode length: 230.66
    Episode_Reward/reaching_object: 2.0334
     Episode_Reward/lifting_object: 164.7184
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 2.36s
                      Time elapsed: 00:42:47
                               ETA: 00:32:58

################################################################################
                     [1m Learning iteration 1130/2000 [0m                     

                       Computation: 41660 steps/s (collection: 2.256s, learning 0.104s)
             Mean action noise std: 2.90
          Mean value_function loss: 156.8921
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 76.5760
                       Mean reward: 850.64
               Mean episode length: 229.03
    Episode_Reward/reaching_object: 2.0799
     Episode_Reward/lifting_object: 169.0743
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0746
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 2.36s
                      Time elapsed: 00:42:49
                               ETA: 00:32:56

################################################################################
                     [1m Learning iteration 1131/2000 [0m                     

                       Computation: 42906 steps/s (collection: 2.196s, learning 0.096s)
             Mean action noise std: 2.90
          Mean value_function loss: 168.6852
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 76.5915
                       Mean reward: 862.70
               Mean episode length: 228.95
    Episode_Reward/reaching_object: 2.0128
     Episode_Reward/lifting_object: 163.3239
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 2.29s
                      Time elapsed: 00:42:51
                               ETA: 00:32:54

################################################################################
                     [1m Learning iteration 1132/2000 [0m                     

                       Computation: 41413 steps/s (collection: 2.230s, learning 0.144s)
             Mean action noise std: 2.91
          Mean value_function loss: 172.4394
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 76.6112
                       Mean reward: 859.74
               Mean episode length: 229.19
    Episode_Reward/reaching_object: 2.0478
     Episode_Reward/lifting_object: 166.1394
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0743
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 2.37s
                      Time elapsed: 00:42:54
                               ETA: 00:32:52

################################################################################
                     [1m Learning iteration 1133/2000 [0m                     

                       Computation: 42114 steps/s (collection: 2.246s, learning 0.089s)
             Mean action noise std: 2.91
          Mean value_function loss: 171.0360
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 76.6263
                       Mean reward: 835.68
               Mean episode length: 227.47
    Episode_Reward/reaching_object: 2.0022
     Episode_Reward/lifting_object: 161.6604
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 2.33s
                      Time elapsed: 00:42:56
                               ETA: 00:32:49

################################################################################
                     [1m Learning iteration 1134/2000 [0m                     

                       Computation: 41729 steps/s (collection: 2.236s, learning 0.120s)
             Mean action noise std: 2.91
          Mean value_function loss: 186.2737
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 76.6379
                       Mean reward: 811.20
               Mean episode length: 219.66
    Episode_Reward/reaching_object: 2.0148
     Episode_Reward/lifting_object: 163.6633
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0739
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 2.36s
                      Time elapsed: 00:42:58
                               ETA: 00:32:47

################################################################################
                     [1m Learning iteration 1135/2000 [0m                     

                       Computation: 41380 steps/s (collection: 2.226s, learning 0.150s)
             Mean action noise std: 2.91
          Mean value_function loss: 182.2959
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 76.6556
                       Mean reward: 813.96
               Mean episode length: 219.53
    Episode_Reward/reaching_object: 1.9729
     Episode_Reward/lifting_object: 159.6847
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 2.38s
                      Time elapsed: 00:43:01
                               ETA: 00:32:45

################################################################################
                     [1m Learning iteration 1136/2000 [0m                     

                       Computation: 42489 steps/s (collection: 2.225s, learning 0.089s)
             Mean action noise std: 2.91
          Mean value_function loss: 167.1412
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 76.6728
                       Mean reward: 848.58
               Mean episode length: 228.27
    Episode_Reward/reaching_object: 2.0093
     Episode_Reward/lifting_object: 163.5614
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0740
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 2.31s
                      Time elapsed: 00:43:03
                               ETA: 00:32:43

################################################################################
                     [1m Learning iteration 1137/2000 [0m                     

                       Computation: 42500 steps/s (collection: 2.216s, learning 0.097s)
             Mean action noise std: 2.91
          Mean value_function loss: 155.4803
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 76.6838
                       Mean reward: 853.16
               Mean episode length: 228.58
    Episode_Reward/reaching_object: 2.0461
     Episode_Reward/lifting_object: 166.7950
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0564
          Episode_Reward/joint_vel: -0.0752
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 2.31s
                      Time elapsed: 00:43:05
                               ETA: 00:32:40

################################################################################
                     [1m Learning iteration 1138/2000 [0m                     

                       Computation: 42668 steps/s (collection: 2.186s, learning 0.118s)
             Mean action noise std: 2.91
          Mean value_function loss: 161.8958
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 76.6916
                       Mean reward: 872.48
               Mean episode length: 234.06
    Episode_Reward/reaching_object: 2.0516
     Episode_Reward/lifting_object: 167.2809
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0763
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 2.30s
                      Time elapsed: 00:43:08
                               ETA: 00:32:38

################################################################################
                     [1m Learning iteration 1139/2000 [0m                     

                       Computation: 42717 steps/s (collection: 2.208s, learning 0.093s)
             Mean action noise std: 2.91
          Mean value_function loss: 162.9111
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 76.6993
                       Mean reward: 831.35
               Mean episode length: 223.20
    Episode_Reward/reaching_object: 2.0230
     Episode_Reward/lifting_object: 164.8662
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0746
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 2.30s
                      Time elapsed: 00:43:10
                               ETA: 00:32:36

################################################################################
                     [1m Learning iteration 1140/2000 [0m                     

                       Computation: 42043 steps/s (collection: 2.249s, learning 0.090s)
             Mean action noise std: 2.92
          Mean value_function loss: 178.7006
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 76.7111
                       Mean reward: 851.92
               Mean episode length: 229.00
    Episode_Reward/reaching_object: 2.0031
     Episode_Reward/lifting_object: 162.4716
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0746
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 2.34s
                      Time elapsed: 00:43:12
                               ETA: 00:32:34

################################################################################
                     [1m Learning iteration 1141/2000 [0m                     

                       Computation: 42985 steps/s (collection: 2.200s, learning 0.087s)
             Mean action noise std: 2.92
          Mean value_function loss: 151.2714
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 76.7346
                       Mean reward: 865.26
               Mean episode length: 231.26
    Episode_Reward/reaching_object: 2.0518
     Episode_Reward/lifting_object: 167.0258
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0768
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 2.29s
                      Time elapsed: 00:43:15
                               ETA: 00:32:31

################################################################################
                     [1m Learning iteration 1142/2000 [0m                     

                       Computation: 42075 steps/s (collection: 2.224s, learning 0.112s)
             Mean action noise std: 2.92
          Mean value_function loss: 151.1210
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 76.7550
                       Mean reward: 862.25
               Mean episode length: 230.69
    Episode_Reward/reaching_object: 2.0551
     Episode_Reward/lifting_object: 167.8641
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0759
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 2.34s
                      Time elapsed: 00:43:17
                               ETA: 00:32:29

################################################################################
                     [1m Learning iteration 1143/2000 [0m                     

                       Computation: 41981 steps/s (collection: 2.249s, learning 0.093s)
             Mean action noise std: 2.92
          Mean value_function loss: 175.4435
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 76.7678
                       Mean reward: 832.00
               Mean episode length: 225.33
    Episode_Reward/reaching_object: 2.0030
     Episode_Reward/lifting_object: 162.5014
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0738
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 2.34s
                      Time elapsed: 00:43:19
                               ETA: 00:32:27

################################################################################
                     [1m Learning iteration 1144/2000 [0m                     

                       Computation: 43035 steps/s (collection: 2.197s, learning 0.088s)
             Mean action noise std: 2.92
          Mean value_function loss: 150.0912
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 76.7852
                       Mean reward: 887.97
               Mean episode length: 237.58
    Episode_Reward/reaching_object: 2.0938
     Episode_Reward/lifting_object: 170.6591
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0774
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 2.28s
                      Time elapsed: 00:43:21
                               ETA: 00:32:25

################################################################################
                     [1m Learning iteration 1145/2000 [0m                     

                       Computation: 42280 steps/s (collection: 2.202s, learning 0.123s)
             Mean action noise std: 2.92
          Mean value_function loss: 138.3951
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 76.8016
                       Mean reward: 874.71
               Mean episode length: 233.90
    Episode_Reward/reaching_object: 2.0781
     Episode_Reward/lifting_object: 168.8098
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0768
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 2.33s
                      Time elapsed: 00:43:24
                               ETA: 00:32:22

################################################################################
                     [1m Learning iteration 1146/2000 [0m                     

                       Computation: 42047 steps/s (collection: 2.245s, learning 0.093s)
             Mean action noise std: 2.93
          Mean value_function loss: 166.3770
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 76.8208
                       Mean reward: 830.35
               Mean episode length: 223.34
    Episode_Reward/reaching_object: 2.0272
     Episode_Reward/lifting_object: 164.7446
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0745
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 2.34s
                      Time elapsed: 00:43:26
                               ETA: 00:32:20

################################################################################
                     [1m Learning iteration 1147/2000 [0m                     

                       Computation: 41166 steps/s (collection: 2.227s, learning 0.161s)
             Mean action noise std: 2.93
          Mean value_function loss: 161.2904
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 76.8345
                       Mean reward: 889.45
               Mean episode length: 235.00
    Episode_Reward/reaching_object: 2.0379
     Episode_Reward/lifting_object: 165.9038
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0565
          Episode_Reward/joint_vel: -0.0756
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 2.39s
                      Time elapsed: 00:43:29
                               ETA: 00:32:18

################################################################################
                     [1m Learning iteration 1148/2000 [0m                     

                       Computation: 41901 steps/s (collection: 2.240s, learning 0.106s)
             Mean action noise std: 2.93
          Mean value_function loss: 166.0654
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 76.8504
                       Mean reward: 836.17
               Mean episode length: 225.04
    Episode_Reward/reaching_object: 2.0607
     Episode_Reward/lifting_object: 167.6547
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0757
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 2.35s
                      Time elapsed: 00:43:31
                               ETA: 00:32:16

################################################################################
                     [1m Learning iteration 1149/2000 [0m                     

                       Computation: 42393 steps/s (collection: 2.223s, learning 0.096s)
             Mean action noise std: 2.93
          Mean value_function loss: 168.9685
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 76.8758
                       Mean reward: 787.31
               Mean episode length: 216.62
    Episode_Reward/reaching_object: 2.0260
     Episode_Reward/lifting_object: 163.9961
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0755
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 2.32s
                      Time elapsed: 00:43:33
                               ETA: 00:32:14

################################################################################
                     [1m Learning iteration 1150/2000 [0m                     

                       Computation: 42909 steps/s (collection: 2.203s, learning 0.088s)
             Mean action noise std: 2.93
          Mean value_function loss: 135.3542
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 76.8965
                       Mean reward: 902.50
               Mean episode length: 241.06
    Episode_Reward/reaching_object: 2.0710
     Episode_Reward/lifting_object: 168.5837
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0764
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 2.29s
                      Time elapsed: 00:43:35
                               ETA: 00:32:11

################################################################################
                     [1m Learning iteration 1151/2000 [0m                     

                       Computation: 39741 steps/s (collection: 2.326s, learning 0.148s)
             Mean action noise std: 2.93
          Mean value_function loss: 140.5491
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 76.9104
                       Mean reward: 807.30
               Mean episode length: 220.70
    Episode_Reward/reaching_object: 2.0672
     Episode_Reward/lifting_object: 167.5854
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0774
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 2.47s
                      Time elapsed: 00:43:38
                               ETA: 00:32:09

################################################################################
                     [1m Learning iteration 1152/2000 [0m                     

                       Computation: 42648 steps/s (collection: 2.210s, learning 0.095s)
             Mean action noise std: 2.94
          Mean value_function loss: 149.4774
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 76.9266
                       Mean reward: 836.74
               Mean episode length: 227.08
    Episode_Reward/reaching_object: 2.0299
     Episode_Reward/lifting_object: 164.6998
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0755
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 2.30s
                      Time elapsed: 00:43:40
                               ETA: 00:32:07

################################################################################
                     [1m Learning iteration 1153/2000 [0m                     

                       Computation: 43200 steps/s (collection: 2.187s, learning 0.089s)
             Mean action noise std: 2.94
          Mean value_function loss: 149.9551
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 76.9505
                       Mean reward: 853.28
               Mean episode length: 231.03
    Episode_Reward/reaching_object: 2.0743
     Episode_Reward/lifting_object: 168.2086
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0775
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 2.28s
                      Time elapsed: 00:43:43
                               ETA: 00:32:05

################################################################################
                     [1m Learning iteration 1154/2000 [0m                     

                       Computation: 41781 steps/s (collection: 2.219s, learning 0.134s)
             Mean action noise std: 2.94
          Mean value_function loss: 164.0339
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 76.9746
                       Mean reward: 790.25
               Mean episode length: 215.20
    Episode_Reward/reaching_object: 2.0202
     Episode_Reward/lifting_object: 164.1163
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0751
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 2.35s
                      Time elapsed: 00:43:45
                               ETA: 00:32:03

################################################################################
                     [1m Learning iteration 1155/2000 [0m                     

                       Computation: 41559 steps/s (collection: 2.250s, learning 0.115s)
             Mean action noise std: 2.94
          Mean value_function loss: 148.8816
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 76.9921
                       Mean reward: 900.07
               Mean episode length: 238.49
    Episode_Reward/reaching_object: 2.0846
     Episode_Reward/lifting_object: 169.9961
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0777
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 2.37s
                      Time elapsed: 00:43:47
                               ETA: 00:32:00

################################################################################
                     [1m Learning iteration 1156/2000 [0m                     

                       Computation: 41582 steps/s (collection: 2.264s, learning 0.101s)
             Mean action noise std: 2.94
          Mean value_function loss: 155.4861
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 77.0101
                       Mean reward: 841.61
               Mean episode length: 228.11
    Episode_Reward/reaching_object: 2.0183
     Episode_Reward/lifting_object: 163.1417
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0764
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 2.36s
                      Time elapsed: 00:43:50
                               ETA: 00:31:58

################################################################################
                     [1m Learning iteration 1157/2000 [0m                     

                       Computation: 42501 steps/s (collection: 2.200s, learning 0.113s)
             Mean action noise std: 2.95
          Mean value_function loss: 169.4592
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 77.0282
                       Mean reward: 833.02
               Mean episode length: 223.75
    Episode_Reward/reaching_object: 2.0345
     Episode_Reward/lifting_object: 165.2803
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0763
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 2.31s
                      Time elapsed: 00:43:52
                               ETA: 00:31:56

################################################################################
                     [1m Learning iteration 1158/2000 [0m                     

                       Computation: 43070 steps/s (collection: 2.194s, learning 0.089s)
             Mean action noise std: 2.95
          Mean value_function loss: 154.7069
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 77.0427
                       Mean reward: 803.91
               Mean episode length: 217.02
    Episode_Reward/reaching_object: 2.0531
     Episode_Reward/lifting_object: 167.0429
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0771
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 2.28s
                      Time elapsed: 00:43:54
                               ETA: 00:31:54

################################################################################
                     [1m Learning iteration 1159/2000 [0m                     

                       Computation: 42299 steps/s (collection: 2.222s, learning 0.102s)
             Mean action noise std: 2.95
          Mean value_function loss: 211.8071
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 77.0620
                       Mean reward: 836.81
               Mean episode length: 224.10
    Episode_Reward/reaching_object: 2.0242
     Episode_Reward/lifting_object: 164.0262
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0766
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 2.32s
                      Time elapsed: 00:43:57
                               ETA: 00:31:51

################################################################################
                     [1m Learning iteration 1160/2000 [0m                     

                       Computation: 41695 steps/s (collection: 2.245s, learning 0.112s)
             Mean action noise std: 2.95
          Mean value_function loss: 189.4500
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 77.0832
                       Mean reward: 831.90
               Mean episode length: 224.96
    Episode_Reward/reaching_object: 2.0461
     Episode_Reward/lifting_object: 165.9449
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0774
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 2.36s
                      Time elapsed: 00:43:59
                               ETA: 00:31:49

################################################################################
                     [1m Learning iteration 1161/2000 [0m                     

                       Computation: 42209 steps/s (collection: 2.236s, learning 0.093s)
             Mean action noise std: 2.95
          Mean value_function loss: 146.9459
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 77.1009
                       Mean reward: 848.82
               Mean episode length: 228.83
    Episode_Reward/reaching_object: 2.0988
     Episode_Reward/lifting_object: 170.7946
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0785
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 2.33s
                      Time elapsed: 00:44:01
                               ETA: 00:31:47

################################################################################
                     [1m Learning iteration 1162/2000 [0m                     

                       Computation: 42829 steps/s (collection: 2.198s, learning 0.097s)
             Mean action noise std: 2.95
          Mean value_function loss: 156.4376
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 77.1155
                       Mean reward: 859.78
               Mean episode length: 230.87
    Episode_Reward/reaching_object: 2.0655
     Episode_Reward/lifting_object: 167.7519
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0784
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 2.30s
                      Time elapsed: 00:44:04
                               ETA: 00:31:45

################################################################################
                     [1m Learning iteration 1163/2000 [0m                     

                       Computation: 41200 steps/s (collection: 2.253s, learning 0.133s)
             Mean action noise std: 2.96
          Mean value_function loss: 193.8510
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 77.1327
                       Mean reward: 807.92
               Mean episode length: 219.01
    Episode_Reward/reaching_object: 1.9928
     Episode_Reward/lifting_object: 161.0570
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0767
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 2.39s
                      Time elapsed: 00:44:06
                               ETA: 00:31:42

################################################################################
                     [1m Learning iteration 1164/2000 [0m                     

                       Computation: 39248 steps/s (collection: 2.373s, learning 0.132s)
             Mean action noise std: 2.96
          Mean value_function loss: 192.0344
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 77.1459
                       Mean reward: 773.86
               Mean episode length: 212.99
    Episode_Reward/reaching_object: 1.9973
     Episode_Reward/lifting_object: 162.0005
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0761
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 2.50s
                      Time elapsed: 00:44:08
                               ETA: 00:31:40

################################################################################
                     [1m Learning iteration 1165/2000 [0m                     

                       Computation: 41234 steps/s (collection: 2.289s, learning 0.095s)
             Mean action noise std: 2.96
          Mean value_function loss: 182.3751
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 77.1622
                       Mean reward: 827.15
               Mean episode length: 223.72
    Episode_Reward/reaching_object: 2.0481
     Episode_Reward/lifting_object: 166.6172
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0584
          Episode_Reward/joint_vel: -0.0777
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 2.38s
                      Time elapsed: 00:44:11
                               ETA: 00:31:38

################################################################################
                     [1m Learning iteration 1166/2000 [0m                     

                       Computation: 39469 steps/s (collection: 2.399s, learning 0.092s)
             Mean action noise std: 2.96
          Mean value_function loss: 179.1006
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 77.1737
                       Mean reward: 791.96
               Mean episode length: 214.99
    Episode_Reward/reaching_object: 1.9972
     Episode_Reward/lifting_object: 162.3293
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0753
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 2.49s
                      Time elapsed: 00:44:13
                               ETA: 00:31:36

################################################################################
                     [1m Learning iteration 1167/2000 [0m                     

                       Computation: 42506 steps/s (collection: 2.210s, learning 0.103s)
             Mean action noise std: 2.96
          Mean value_function loss: 188.5707
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 77.1899
                       Mean reward: 827.37
               Mean episode length: 222.25
    Episode_Reward/reaching_object: 2.0076
     Episode_Reward/lifting_object: 162.8698
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0759
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 2.31s
                      Time elapsed: 00:44:16
                               ETA: 00:31:34

################################################################################
                     [1m Learning iteration 1168/2000 [0m                     

                       Computation: 42433 steps/s (collection: 2.216s, learning 0.101s)
             Mean action noise std: 2.96
          Mean value_function loss: 180.6008
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 77.2053
                       Mean reward: 862.30
               Mean episode length: 231.10
    Episode_Reward/reaching_object: 1.9997
     Episode_Reward/lifting_object: 161.3426
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0763
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 2.32s
                      Time elapsed: 00:44:18
                               ETA: 00:31:32

################################################################################
                     [1m Learning iteration 1169/2000 [0m                     

                       Computation: 42434 steps/s (collection: 2.222s, learning 0.095s)
             Mean action noise std: 2.97
          Mean value_function loss: 171.2365
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 77.2234
                       Mean reward: 776.11
               Mean episode length: 212.79
    Episode_Reward/reaching_object: 2.0231
     Episode_Reward/lifting_object: 163.3737
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0766
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 2.32s
                      Time elapsed: 00:44:20
                               ETA: 00:31:29

################################################################################
                     [1m Learning iteration 1170/2000 [0m                     

                       Computation: 41428 steps/s (collection: 2.258s, learning 0.115s)
             Mean action noise std: 2.97
          Mean value_function loss: 183.9488
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 77.2404
                       Mean reward: 824.73
               Mean episode length: 222.25
    Episode_Reward/reaching_object: 2.0619
     Episode_Reward/lifting_object: 167.1926
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0772
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 2.37s
                      Time elapsed: 00:44:23
                               ETA: 00:31:27

################################################################################
                     [1m Learning iteration 1171/2000 [0m                     

                       Computation: 41554 steps/s (collection: 2.245s, learning 0.121s)
             Mean action noise std: 2.97
          Mean value_function loss: 171.3329
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 77.2602
                       Mean reward: 846.52
               Mean episode length: 227.93
    Episode_Reward/reaching_object: 1.9600
     Episode_Reward/lifting_object: 157.3979
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0745
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 2.37s
                      Time elapsed: 00:44:25
                               ETA: 00:31:25

################################################################################
                     [1m Learning iteration 1172/2000 [0m                     

                       Computation: 42560 steps/s (collection: 2.216s, learning 0.094s)
             Mean action noise std: 2.97
          Mean value_function loss: 176.9878
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 77.2808
                       Mean reward: 823.12
               Mean episode length: 221.74
    Episode_Reward/reaching_object: 2.0350
     Episode_Reward/lifting_object: 164.4848
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0584
          Episode_Reward/joint_vel: -0.0768
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 2.31s
                      Time elapsed: 00:44:27
                               ETA: 00:31:23

################################################################################
                     [1m Learning iteration 1173/2000 [0m                     

                       Computation: 42816 steps/s (collection: 2.208s, learning 0.088s)
             Mean action noise std: 2.97
          Mean value_function loss: 155.5596
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 77.2981
                       Mean reward: 841.70
               Mean episode length: 224.53
    Episode_Reward/reaching_object: 2.0495
     Episode_Reward/lifting_object: 165.5171
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0762
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 2.30s
                      Time elapsed: 00:44:30
                               ETA: 00:31:20

################################################################################
                     [1m Learning iteration 1174/2000 [0m                     

                       Computation: 41301 steps/s (collection: 2.222s, learning 0.159s)
             Mean action noise std: 2.97
          Mean value_function loss: 147.6339
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 77.3149
                       Mean reward: 841.90
               Mean episode length: 227.27
    Episode_Reward/reaching_object: 2.0535
     Episode_Reward/lifting_object: 164.8687
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0774
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 2.38s
                      Time elapsed: 00:44:32
                               ETA: 00:31:18

################################################################################
                     [1m Learning iteration 1175/2000 [0m                     

                       Computation: 42961 steps/s (collection: 2.197s, learning 0.091s)
             Mean action noise std: 2.98
          Mean value_function loss: 166.0007
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 77.3469
                       Mean reward: 830.32
               Mean episode length: 222.82
    Episode_Reward/reaching_object: 2.0727
     Episode_Reward/lifting_object: 167.3290
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0770
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 2.29s
                      Time elapsed: 00:44:34
                               ETA: 00:31:16

################################################################################
                     [1m Learning iteration 1176/2000 [0m                     

                       Computation: 41114 steps/s (collection: 2.221s, learning 0.170s)
             Mean action noise std: 2.98
          Mean value_function loss: 165.9032
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 77.3725
                       Mean reward: 847.58
               Mean episode length: 227.31
    Episode_Reward/reaching_object: 2.0726
     Episode_Reward/lifting_object: 166.8713
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0781
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 2.39s
                      Time elapsed: 00:44:37
                               ETA: 00:31:14

################################################################################
                     [1m Learning iteration 1177/2000 [0m                     

                       Computation: 42561 steps/s (collection: 2.216s, learning 0.094s)
             Mean action noise std: 2.98
          Mean value_function loss: 129.3866
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 77.3758
                       Mean reward: 869.72
               Mean episode length: 233.35
    Episode_Reward/reaching_object: 2.1019
     Episode_Reward/lifting_object: 169.8974
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0791
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 2.31s
                      Time elapsed: 00:44:39
                               ETA: 00:31:11

################################################################################
                     [1m Learning iteration 1178/2000 [0m                     

                       Computation: 42697 steps/s (collection: 2.211s, learning 0.091s)
             Mean action noise std: 2.98
          Mean value_function loss: 154.7248
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 77.3825
                       Mean reward: 867.95
               Mean episode length: 232.96
    Episode_Reward/reaching_object: 2.0584
     Episode_Reward/lifting_object: 165.4529
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0781
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 2.30s
                      Time elapsed: 00:44:41
                               ETA: 00:31:09

################################################################################
                     [1m Learning iteration 1179/2000 [0m                     

                       Computation: 42176 steps/s (collection: 2.217s, learning 0.114s)
             Mean action noise std: 2.98
          Mean value_function loss: 164.1782
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 77.3937
                       Mean reward: 853.45
               Mean episode length: 228.53
    Episode_Reward/reaching_object: 2.0654
     Episode_Reward/lifting_object: 166.5592
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0769
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 2.33s
                      Time elapsed: 00:44:44
                               ETA: 00:31:07

################################################################################
                     [1m Learning iteration 1180/2000 [0m                     

                       Computation: 42186 steps/s (collection: 2.237s, learning 0.094s)
             Mean action noise std: 2.98
          Mean value_function loss: 203.0329
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 77.4069
                       Mean reward: 783.08
               Mean episode length: 212.39
    Episode_Reward/reaching_object: 2.0080
     Episode_Reward/lifting_object: 161.2536
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0768
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 2.33s
                      Time elapsed: 00:44:46
                               ETA: 00:31:05

################################################################################
                     [1m Learning iteration 1181/2000 [0m                     

                       Computation: 41620 steps/s (collection: 2.270s, learning 0.092s)
             Mean action noise std: 2.98
          Mean value_function loss: 183.7102
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 77.4252
                       Mean reward: 849.56
               Mean episode length: 226.92
    Episode_Reward/reaching_object: 2.0289
     Episode_Reward/lifting_object: 164.0197
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0584
          Episode_Reward/joint_vel: -0.0766
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 2.36s
                      Time elapsed: 00:44:48
                               ETA: 00:31:03

################################################################################
                     [1m Learning iteration 1182/2000 [0m                     

                       Computation: 42970 steps/s (collection: 2.196s, learning 0.092s)
             Mean action noise std: 2.99
          Mean value_function loss: 171.5141
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 77.4423
                       Mean reward: 787.71
               Mean episode length: 215.55
    Episode_Reward/reaching_object: 2.0303
     Episode_Reward/lifting_object: 163.7517
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0771
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 2.29s
                      Time elapsed: 00:44:51
                               ETA: 00:31:00

################################################################################
                     [1m Learning iteration 1183/2000 [0m                     

                       Computation: 41947 steps/s (collection: 2.231s, learning 0.112s)
             Mean action noise std: 2.99
          Mean value_function loss: 172.4503
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 77.4577
                       Mean reward: 863.91
               Mean episode length: 232.53
    Episode_Reward/reaching_object: 2.0769
     Episode_Reward/lifting_object: 167.9033
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0779
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 2.34s
                      Time elapsed: 00:44:53
                               ETA: 00:30:58

################################################################################
                     [1m Learning iteration 1184/2000 [0m                     

                       Computation: 43260 steps/s (collection: 2.180s, learning 0.092s)
             Mean action noise std: 2.99
          Mean value_function loss: 188.6731
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 77.4665
                       Mean reward: 844.84
               Mean episode length: 226.63
    Episode_Reward/reaching_object: 1.9852
     Episode_Reward/lifting_object: 160.2906
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0756
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 2.27s
                      Time elapsed: 00:44:55
                               ETA: 00:30:56

################################################################################
                     [1m Learning iteration 1185/2000 [0m                     

                       Computation: 40620 steps/s (collection: 2.280s, learning 0.140s)
             Mean action noise std: 2.99
          Mean value_function loss: 186.1318
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 77.4791
                       Mean reward: 823.93
               Mean episode length: 223.97
    Episode_Reward/reaching_object: 2.0161
     Episode_Reward/lifting_object: 163.6230
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0757
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 2.42s
                      Time elapsed: 00:44:58
                               ETA: 00:30:54

################################################################################
                     [1m Learning iteration 1186/2000 [0m                     

                       Computation: 41641 steps/s (collection: 2.251s, learning 0.110s)
             Mean action noise std: 2.99
          Mean value_function loss: 168.8940
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 77.5014
                       Mean reward: 830.43
               Mean episode length: 222.63
    Episode_Reward/reaching_object: 2.0447
     Episode_Reward/lifting_object: 166.2437
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0765
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 2.36s
                      Time elapsed: 00:45:00
                               ETA: 00:30:51

################################################################################
                     [1m Learning iteration 1187/2000 [0m                     

                       Computation: 36759 steps/s (collection: 2.478s, learning 0.197s)
             Mean action noise std: 2.99
          Mean value_function loss: 184.7262
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 77.5218
                       Mean reward: 858.77
               Mean episode length: 229.65
    Episode_Reward/reaching_object: 2.0071
     Episode_Reward/lifting_object: 162.6062
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0760
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 2.67s
                      Time elapsed: 00:45:03
                               ETA: 00:30:49

################################################################################
                     [1m Learning iteration 1188/2000 [0m                     

                       Computation: 40072 steps/s (collection: 2.355s, learning 0.098s)
             Mean action noise std: 3.00
          Mean value_function loss: 137.6619
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 77.5444
                       Mean reward: 854.76
               Mean episode length: 229.68
    Episode_Reward/reaching_object: 2.0724
     Episode_Reward/lifting_object: 169.0552
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.0784
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 2.45s
                      Time elapsed: 00:45:05
                               ETA: 00:30:47

################################################################################
                     [1m Learning iteration 1189/2000 [0m                     

                       Computation: 37635 steps/s (collection: 2.473s, learning 0.139s)
             Mean action noise std: 3.00
          Mean value_function loss: 143.1107
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 77.5564
                       Mean reward: 857.73
               Mean episode length: 229.82
    Episode_Reward/reaching_object: 2.0465
     Episode_Reward/lifting_object: 167.2974
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0763
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 2.61s
                      Time elapsed: 00:45:08
                               ETA: 00:30:45

################################################################################
                     [1m Learning iteration 1190/2000 [0m                     

                       Computation: 41639 steps/s (collection: 2.268s, learning 0.093s)
             Mean action noise std: 3.00
          Mean value_function loss: 135.4679
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 77.5711
                       Mean reward: 875.56
               Mean episode length: 234.48
    Episode_Reward/reaching_object: 2.1017
     Episode_Reward/lifting_object: 172.0067
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.0785
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 2.36s
                      Time elapsed: 00:45:10
                               ETA: 00:30:43

################################################################################
                     [1m Learning iteration 1191/2000 [0m                     

                       Computation: 41865 steps/s (collection: 2.246s, learning 0.102s)
             Mean action noise std: 3.00
          Mean value_function loss: 136.3290
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 77.5929
                       Mean reward: 898.08
               Mean episode length: 240.64
    Episode_Reward/reaching_object: 2.0761
     Episode_Reward/lifting_object: 169.0108
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0777
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 2.35s
                      Time elapsed: 00:45:12
                               ETA: 00:30:41

################################################################################
                     [1m Learning iteration 1192/2000 [0m                     

                       Computation: 41952 steps/s (collection: 2.242s, learning 0.102s)
             Mean action noise std: 3.00
          Mean value_function loss: 161.5032
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 77.6135
                       Mean reward: 812.30
               Mean episode length: 221.50
    Episode_Reward/reaching_object: 2.0335
     Episode_Reward/lifting_object: 165.6543
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0778
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 2.34s
                      Time elapsed: 00:45:15
                               ETA: 00:30:38

################################################################################
                     [1m Learning iteration 1193/2000 [0m                     

                       Computation: 40913 steps/s (collection: 2.267s, learning 0.135s)
             Mean action noise std: 3.00
          Mean value_function loss: 145.2063
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 77.6274
                       Mean reward: 801.84
               Mean episode length: 217.21
    Episode_Reward/reaching_object: 2.0385
     Episode_Reward/lifting_object: 165.9481
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0776
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 2.40s
                      Time elapsed: 00:45:17
                               ETA: 00:30:36

################################################################################
                     [1m Learning iteration 1194/2000 [0m                     

                       Computation: 41386 steps/s (collection: 2.253s, learning 0.123s)
             Mean action noise std: 3.01
          Mean value_function loss: 121.4017
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 77.6429
                       Mean reward: 873.93
               Mean episode length: 235.23
    Episode_Reward/reaching_object: 2.1030
     Episode_Reward/lifting_object: 171.5687
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0616
          Episode_Reward/joint_vel: -0.0790
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 2.38s
                      Time elapsed: 00:45:20
                               ETA: 00:30:34

################################################################################
                     [1m Learning iteration 1195/2000 [0m                     

                       Computation: 38194 steps/s (collection: 2.425s, learning 0.149s)
             Mean action noise std: 3.01
          Mean value_function loss: 180.7764
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 77.6624
                       Mean reward: 819.76
               Mean episode length: 221.28
    Episode_Reward/reaching_object: 2.0415
     Episode_Reward/lifting_object: 166.2348
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0768
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 2.57s
                      Time elapsed: 00:45:22
                               ETA: 00:30:32

################################################################################
                     [1m Learning iteration 1196/2000 [0m                     

                       Computation: 39710 steps/s (collection: 2.360s, learning 0.115s)
             Mean action noise std: 3.01
          Mean value_function loss: 122.9369
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 77.6767
                       Mean reward: 890.01
               Mean episode length: 238.15
    Episode_Reward/reaching_object: 2.0896
     Episode_Reward/lifting_object: 170.1253
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0613
          Episode_Reward/joint_vel: -0.0777
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 2.48s
                      Time elapsed: 00:45:25
                               ETA: 00:30:30

################################################################################
                     [1m Learning iteration 1197/2000 [0m                     

                       Computation: 41267 steps/s (collection: 2.294s, learning 0.088s)
             Mean action noise std: 3.01
          Mean value_function loss: 130.9005
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 77.6968
                       Mean reward: 860.33
               Mean episode length: 230.15
    Episode_Reward/reaching_object: 2.1142
     Episode_Reward/lifting_object: 172.6895
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0781
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 2.38s
                      Time elapsed: 00:45:27
                               ETA: 00:30:28

################################################################################
                     [1m Learning iteration 1198/2000 [0m                     

                       Computation: 42066 steps/s (collection: 2.241s, learning 0.096s)
             Mean action noise std: 3.01
          Mean value_function loss: 145.8787
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 77.7096
                       Mean reward: 828.81
               Mean episode length: 224.13
    Episode_Reward/reaching_object: 2.0420
     Episode_Reward/lifting_object: 165.9003
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0764
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 2.34s
                      Time elapsed: 00:45:29
                               ETA: 00:30:25

################################################################################
                     [1m Learning iteration 1199/2000 [0m                     

                       Computation: 41870 steps/s (collection: 2.238s, learning 0.110s)
             Mean action noise std: 3.01
          Mean value_function loss: 130.7876
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 77.7268
                       Mean reward: 868.32
               Mean episode length: 232.62
    Episode_Reward/reaching_object: 2.0725
     Episode_Reward/lifting_object: 167.1182
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0773
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 2.35s
                      Time elapsed: 00:45:32
                               ETA: 00:30:23

################################################################################
                     [1m Learning iteration 1200/2000 [0m                     

                       Computation: 41101 steps/s (collection: 2.278s, learning 0.114s)
             Mean action noise std: 3.02
          Mean value_function loss: 130.8148
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 77.7487
                       Mean reward: 882.68
               Mean episode length: 235.43
    Episode_Reward/reaching_object: 2.1053
     Episode_Reward/lifting_object: 170.8963
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0616
          Episode_Reward/joint_vel: -0.0776
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 2.39s
                      Time elapsed: 00:45:34
                               ETA: 00:30:21

################################################################################
                     [1m Learning iteration 1201/2000 [0m                     

                       Computation: 39735 steps/s (collection: 2.366s, learning 0.108s)
             Mean action noise std: 3.02
          Mean value_function loss: 141.4583
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 77.7631
                       Mean reward: 837.37
               Mean episode length: 226.70
    Episode_Reward/reaching_object: 2.1078
     Episode_Reward/lifting_object: 170.7128
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0779
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 2.47s
                      Time elapsed: 00:45:37
                               ETA: 00:30:19

################################################################################
                     [1m Learning iteration 1202/2000 [0m                     

                       Computation: 41489 steps/s (collection: 2.272s, learning 0.098s)
             Mean action noise std: 3.02
          Mean value_function loss: 154.7311
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 77.7797
                       Mean reward: 833.09
               Mean episode length: 226.11
    Episode_Reward/reaching_object: 2.1202
     Episode_Reward/lifting_object: 171.3426
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0783
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 2.37s
                      Time elapsed: 00:45:39
                               ETA: 00:30:17

################################################################################
                     [1m Learning iteration 1203/2000 [0m                     

                       Computation: 42830 steps/s (collection: 2.203s, learning 0.093s)
             Mean action noise std: 3.02
          Mean value_function loss: 166.6972
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 77.8082
                       Mean reward: 832.52
               Mean episode length: 227.05
    Episode_Reward/reaching_object: 2.0285
     Episode_Reward/lifting_object: 163.4213
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0763
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 2.30s
                      Time elapsed: 00:45:41
                               ETA: 00:30:14

################################################################################
                     [1m Learning iteration 1204/2000 [0m                     

                       Computation: 39297 steps/s (collection: 2.373s, learning 0.128s)
             Mean action noise std: 3.03
          Mean value_function loss: 176.8359
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 77.8351
                       Mean reward: 829.86
               Mean episode length: 223.65
    Episode_Reward/reaching_object: 2.0776
     Episode_Reward/lifting_object: 168.3583
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0764
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 2.50s
                      Time elapsed: 00:45:44
                               ETA: 00:30:12

################################################################################
                     [1m Learning iteration 1205/2000 [0m                     

                       Computation: 42677 steps/s (collection: 2.206s, learning 0.098s)
             Mean action noise std: 3.03
          Mean value_function loss: 173.1606
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 77.8572
                       Mean reward: 840.05
               Mean episode length: 227.32
    Episode_Reward/reaching_object: 2.0620
     Episode_Reward/lifting_object: 167.1947
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0608
          Episode_Reward/joint_vel: -0.0754
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 2.30s
                      Time elapsed: 00:45:46
                               ETA: 00:30:10

################################################################################
                     [1m Learning iteration 1206/2000 [0m                     

                       Computation: 43428 steps/s (collection: 2.176s, learning 0.088s)
             Mean action noise std: 3.03
          Mean value_function loss: 170.8448
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 77.8851
                       Mean reward: 828.65
               Mean episode length: 224.75
    Episode_Reward/reaching_object: 2.0548
     Episode_Reward/lifting_object: 165.9181
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 2.26s
                      Time elapsed: 00:45:48
                               ETA: 00:30:08

################################################################################
                     [1m Learning iteration 1207/2000 [0m                     

                       Computation: 42671 steps/s (collection: 2.212s, learning 0.092s)
             Mean action noise std: 3.03
          Mean value_function loss: 151.2621
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 77.9048
                       Mean reward: 845.30
               Mean episode length: 227.56
    Episode_Reward/reaching_object: 2.0616
     Episode_Reward/lifting_object: 167.3159
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0608
          Episode_Reward/joint_vel: -0.0741
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 2.30s
                      Time elapsed: 00:45:51
                               ETA: 00:30:05

################################################################################
                     [1m Learning iteration 1208/2000 [0m                     

                       Computation: 41898 steps/s (collection: 2.232s, learning 0.114s)
             Mean action noise std: 3.03
          Mean value_function loss: 171.5497
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 77.9201
                       Mean reward: 817.82
               Mean episode length: 221.10
    Episode_Reward/reaching_object: 1.9850
     Episode_Reward/lifting_object: 160.2180
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0590
          Episode_Reward/joint_vel: -0.0721
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 2.35s
                      Time elapsed: 00:45:53
                               ETA: 00:30:03

################################################################################
                     [1m Learning iteration 1209/2000 [0m                     

                       Computation: 42821 steps/s (collection: 2.209s, learning 0.087s)
             Mean action noise std: 3.03
          Mean value_function loss: 158.8807
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 77.9296
                       Mean reward: 848.82
               Mean episode length: 228.61
    Episode_Reward/reaching_object: 2.0785
     Episode_Reward/lifting_object: 168.0972
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0616
          Episode_Reward/joint_vel: -0.0753
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 2.30s
                      Time elapsed: 00:45:55
                               ETA: 00:30:01

################################################################################
                     [1m Learning iteration 1210/2000 [0m                     

                       Computation: 43078 steps/s (collection: 2.192s, learning 0.090s)
             Mean action noise std: 3.04
          Mean value_function loss: 132.7196
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 77.9448
                       Mean reward: 854.91
               Mean episode length: 229.78
    Episode_Reward/reaching_object: 2.0878
     Episode_Reward/lifting_object: 169.6518
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0617
          Episode_Reward/joint_vel: -0.0749
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 2.28s
                      Time elapsed: 00:45:57
                               ETA: 00:29:59

################################################################################
                     [1m Learning iteration 1211/2000 [0m                     

                       Computation: 40841 steps/s (collection: 2.243s, learning 0.164s)
             Mean action noise std: 3.04
          Mean value_function loss: 135.3508
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 77.9600
                       Mean reward: 865.80
               Mean episode length: 232.33
    Episode_Reward/reaching_object: 2.0985
     Episode_Reward/lifting_object: 170.4118
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0750
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 2.41s
                      Time elapsed: 00:46:00
                               ETA: 00:29:56

################################################################################
                     [1m Learning iteration 1212/2000 [0m                     

                       Computation: 40724 steps/s (collection: 2.317s, learning 0.097s)
             Mean action noise std: 3.04
          Mean value_function loss: 176.9604
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 77.9719
                       Mean reward: 744.95
               Mean episode length: 204.39
    Episode_Reward/reaching_object: 1.9992
     Episode_Reward/lifting_object: 161.5507
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0722
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 2.41s
                      Time elapsed: 00:46:02
                               ETA: 00:29:54

################################################################################
                     [1m Learning iteration 1213/2000 [0m                     

                       Computation: 35858 steps/s (collection: 2.613s, learning 0.128s)
             Mean action noise std: 3.04
          Mean value_function loss: 147.0688
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 77.9907
                       Mean reward: 878.78
               Mean episode length: 234.78
    Episode_Reward/reaching_object: 2.1106
     Episode_Reward/lifting_object: 171.0559
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0765
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 2.74s
                      Time elapsed: 00:46:05
                               ETA: 00:29:52

################################################################################
                     [1m Learning iteration 1214/2000 [0m                     

                       Computation: 35898 steps/s (collection: 2.577s, learning 0.161s)
             Mean action noise std: 3.04
          Mean value_function loss: 153.5729
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 78.0132
                       Mean reward: 882.16
               Mean episode length: 233.92
    Episode_Reward/reaching_object: 2.0428
     Episode_Reward/lifting_object: 165.4618
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0737
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 2.74s
                      Time elapsed: 00:46:08
                               ETA: 00:29:50

################################################################################
                     [1m Learning iteration 1215/2000 [0m                     

                       Computation: 40350 steps/s (collection: 2.338s, learning 0.098s)
             Mean action noise std: 3.04
          Mean value_function loss: 154.1987
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 78.0222
                       Mean reward: 820.22
               Mean episode length: 221.13
    Episode_Reward/reaching_object: 2.0481
     Episode_Reward/lifting_object: 165.6266
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0747
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 2.44s
                      Time elapsed: 00:46:10
                               ETA: 00:29:48

################################################################################
                     [1m Learning iteration 1216/2000 [0m                     

                       Computation: 40955 steps/s (collection: 2.293s, learning 0.107s)
             Mean action noise std: 3.05
          Mean value_function loss: 157.0581
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 78.0331
                       Mean reward: 877.25
               Mean episode length: 235.36
    Episode_Reward/reaching_object: 2.1052
     Episode_Reward/lifting_object: 170.7797
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0764
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 2.40s
                      Time elapsed: 00:46:13
                               ETA: 00:29:46

################################################################################
                     [1m Learning iteration 1217/2000 [0m                     

                       Computation: 41607 steps/s (collection: 2.241s, learning 0.122s)
             Mean action noise std: 3.05
          Mean value_function loss: 147.1046
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 78.0511
                       Mean reward: 842.69
               Mean episode length: 226.36
    Episode_Reward/reaching_object: 2.0853
     Episode_Reward/lifting_object: 169.3564
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0757
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 2.36s
                      Time elapsed: 00:46:15
                               ETA: 00:29:44

################################################################################
                     [1m Learning iteration 1218/2000 [0m                     

                       Computation: 42241 steps/s (collection: 2.232s, learning 0.095s)
             Mean action noise std: 3.05
          Mean value_function loss: 176.2318
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 78.0650
                       Mean reward: 809.56
               Mean episode length: 219.59
    Episode_Reward/reaching_object: 2.0249
     Episode_Reward/lifting_object: 163.8741
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0605
          Episode_Reward/joint_vel: -0.0739
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 2.33s
                      Time elapsed: 00:46:17
                               ETA: 00:29:41

################################################################################
                     [1m Learning iteration 1219/2000 [0m                     

                       Computation: 39930 steps/s (collection: 2.362s, learning 0.100s)
             Mean action noise std: 3.05
          Mean value_function loss: 138.8197
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 78.0721
                       Mean reward: 853.52
               Mean episode length: 228.27
    Episode_Reward/reaching_object: 2.0482
     Episode_Reward/lifting_object: 166.0429
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 2.46s
                      Time elapsed: 00:46:20
                               ETA: 00:29:39

################################################################################
                     [1m Learning iteration 1220/2000 [0m                     

                       Computation: 40655 steps/s (collection: 2.280s, learning 0.138s)
             Mean action noise std: 3.05
          Mean value_function loss: 116.6841
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 78.0871
                       Mean reward: 893.42
               Mean episode length: 238.60
    Episode_Reward/reaching_object: 2.1109
     Episode_Reward/lifting_object: 171.0894
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0762
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 2.42s
                      Time elapsed: 00:46:22
                               ETA: 00:29:37

################################################################################
                     [1m Learning iteration 1221/2000 [0m                     

                       Computation: 41077 steps/s (collection: 2.282s, learning 0.112s)
             Mean action noise std: 3.05
          Mean value_function loss: 143.2848
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 78.1004
                       Mean reward: 858.68
               Mean episode length: 229.90
    Episode_Reward/reaching_object: 2.0611
     Episode_Reward/lifting_object: 167.3590
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0616
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 2.39s
                      Time elapsed: 00:46:25
                               ETA: 00:29:35

################################################################################
                     [1m Learning iteration 1222/2000 [0m                     

                       Computation: 41314 steps/s (collection: 2.287s, learning 0.093s)
             Mean action noise std: 3.05
          Mean value_function loss: 151.9315
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 78.1169
                       Mean reward: 849.34
               Mean episode length: 227.45
    Episode_Reward/reaching_object: 2.0527
     Episode_Reward/lifting_object: 166.4198
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0615
          Episode_Reward/joint_vel: -0.0754
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 2.38s
                      Time elapsed: 00:46:27
                               ETA: 00:29:33

################################################################################
                     [1m Learning iteration 1223/2000 [0m                     

                       Computation: 40717 steps/s (collection: 2.287s, learning 0.128s)
             Mean action noise std: 3.06
          Mean value_function loss: 128.6376
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 78.1367
                       Mean reward: 909.45
               Mean episode length: 241.86
    Episode_Reward/reaching_object: 2.0952
     Episode_Reward/lifting_object: 170.5522
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0764
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 2.41s
                      Time elapsed: 00:46:29
                               ETA: 00:29:31

################################################################################
                     [1m Learning iteration 1224/2000 [0m                     

                       Computation: 39274 steps/s (collection: 2.319s, learning 0.184s)
             Mean action noise std: 3.06
          Mean value_function loss: 146.3531
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 78.1535
                       Mean reward: 875.59
               Mean episode length: 235.39
    Episode_Reward/reaching_object: 2.0842
     Episode_Reward/lifting_object: 169.2457
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0773
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 2.50s
                      Time elapsed: 00:46:32
                               ETA: 00:29:28

################################################################################
                     [1m Learning iteration 1225/2000 [0m                     

                       Computation: 37967 steps/s (collection: 2.484s, learning 0.105s)
             Mean action noise std: 3.06
          Mean value_function loss: 136.7208
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 78.1785
                       Mean reward: 862.00
               Mean episode length: 231.01
    Episode_Reward/reaching_object: 2.1006
     Episode_Reward/lifting_object: 170.3776
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0774
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 2.59s
                      Time elapsed: 00:46:34
                               ETA: 00:29:26

################################################################################
                     [1m Learning iteration 1226/2000 [0m                     

                       Computation: 39866 steps/s (collection: 2.334s, learning 0.132s)
             Mean action noise std: 3.06
          Mean value_function loss: 172.4851
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 78.1934
                       Mean reward: 781.09
               Mean episode length: 212.50
    Episode_Reward/reaching_object: 2.0377
     Episode_Reward/lifting_object: 164.5845
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.0765
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 2.47s
                      Time elapsed: 00:46:37
                               ETA: 00:29:24

################################################################################
                     [1m Learning iteration 1227/2000 [0m                     

                       Computation: 38939 steps/s (collection: 2.413s, learning 0.112s)
             Mean action noise std: 3.06
          Mean value_function loss: 159.1367
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 78.2026
                       Mean reward: 832.45
               Mean episode length: 223.62
    Episode_Reward/reaching_object: 2.0860
     Episode_Reward/lifting_object: 169.1710
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0787
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 2.52s
                      Time elapsed: 00:46:39
                               ETA: 00:29:22

################################################################################
                     [1m Learning iteration 1228/2000 [0m                     

                       Computation: 40884 steps/s (collection: 2.284s, learning 0.121s)
             Mean action noise std: 3.06
          Mean value_function loss: 155.1100
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 78.2169
                       Mean reward: 863.56
               Mean episode length: 229.82
    Episode_Reward/reaching_object: 2.1146
     Episode_Reward/lifting_object: 171.7238
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0798
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 2.40s
                      Time elapsed: 00:46:42
                               ETA: 00:29:20

################################################################################
                     [1m Learning iteration 1229/2000 [0m                     

                       Computation: 40741 steps/s (collection: 2.292s, learning 0.121s)
             Mean action noise std: 3.06
          Mean value_function loss: 142.6649
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 78.2298
                       Mean reward: 855.06
               Mean episode length: 229.14
    Episode_Reward/reaching_object: 2.0863
     Episode_Reward/lifting_object: 169.1167
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0788
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 2.41s
                      Time elapsed: 00:46:44
                               ETA: 00:29:18

################################################################################
                     [1m Learning iteration 1230/2000 [0m                     

                       Computation: 41334 steps/s (collection: 2.260s, learning 0.118s)
             Mean action noise std: 3.07
          Mean value_function loss: 198.9739
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 78.2439
                       Mean reward: 795.10
               Mean episode length: 215.85
    Episode_Reward/reaching_object: 2.0502
     Episode_Reward/lifting_object: 165.1134
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0782
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 2.38s
                      Time elapsed: 00:46:47
                               ETA: 00:29:15

################################################################################
                     [1m Learning iteration 1231/2000 [0m                     

                       Computation: 40383 steps/s (collection: 2.326s, learning 0.109s)
             Mean action noise std: 3.07
          Mean value_function loss: 166.8131
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 78.2589
                       Mean reward: 792.28
               Mean episode length: 214.20
    Episode_Reward/reaching_object: 2.0501
     Episode_Reward/lifting_object: 165.4883
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0780
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 2.43s
                      Time elapsed: 00:46:49
                               ETA: 00:29:13

################################################################################
                     [1m Learning iteration 1232/2000 [0m                     

                       Computation: 41402 steps/s (collection: 2.284s, learning 0.091s)
             Mean action noise std: 3.07
          Mean value_function loss: 119.3253
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 78.2826
                       Mean reward: 861.52
               Mean episode length: 229.34
    Episode_Reward/reaching_object: 2.0799
     Episode_Reward/lifting_object: 167.9583
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0794
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 2.37s
                      Time elapsed: 00:46:51
                               ETA: 00:29:11

################################################################################
                     [1m Learning iteration 1233/2000 [0m                     

                       Computation: 37942 steps/s (collection: 2.496s, learning 0.095s)
             Mean action noise std: 3.07
          Mean value_function loss: 155.2737
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 78.3000
                       Mean reward: 786.00
               Mean episode length: 214.15
    Episode_Reward/reaching_object: 2.0767
     Episode_Reward/lifting_object: 167.3326
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0791
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 2.59s
                      Time elapsed: 00:46:54
                               ETA: 00:29:09

################################################################################
                     [1m Learning iteration 1234/2000 [0m                     

                       Computation: 41265 steps/s (collection: 2.283s, learning 0.099s)
             Mean action noise std: 3.07
          Mean value_function loss: 143.2543
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 78.3119
                       Mean reward: 884.78
               Mean episode length: 235.99
    Episode_Reward/reaching_object: 2.0731
     Episode_Reward/lifting_object: 167.6398
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0786
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 2.38s
                      Time elapsed: 00:46:56
                               ETA: 00:29:07

################################################################################
                     [1m Learning iteration 1235/2000 [0m                     

                       Computation: 37059 steps/s (collection: 2.549s, learning 0.104s)
             Mean action noise std: 3.08
          Mean value_function loss: 165.1528
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 78.3324
                       Mean reward: 883.67
               Mean episode length: 236.93
    Episode_Reward/reaching_object: 2.0779
     Episode_Reward/lifting_object: 167.8770
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0781
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 2.65s
                      Time elapsed: 00:46:59
                               ETA: 00:29:05

################################################################################
                     [1m Learning iteration 1236/2000 [0m                     

                       Computation: 40407 steps/s (collection: 2.307s, learning 0.126s)
             Mean action noise std: 3.08
          Mean value_function loss: 188.0737
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 78.3506
                       Mean reward: 830.23
               Mean episode length: 223.94
    Episode_Reward/reaching_object: 2.0402
     Episode_Reward/lifting_object: 164.4220
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0770
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 2.43s
                      Time elapsed: 00:47:02
                               ETA: 00:29:02

################################################################################
                     [1m Learning iteration 1237/2000 [0m                     

                       Computation: 40222 steps/s (collection: 2.348s, learning 0.096s)
             Mean action noise std: 3.08
          Mean value_function loss: 139.3368
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 78.3719
                       Mean reward: 851.06
               Mean episode length: 228.02
    Episode_Reward/reaching_object: 2.1226
     Episode_Reward/lifting_object: 171.7286
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0645
          Episode_Reward/joint_vel: -0.0798
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 2.44s
                      Time elapsed: 00:47:04
                               ETA: 00:29:00

################################################################################
                     [1m Learning iteration 1238/2000 [0m                     

                       Computation: 39377 steps/s (collection: 2.394s, learning 0.102s)
             Mean action noise std: 3.08
          Mean value_function loss: 157.7695
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 78.3858
                       Mean reward: 868.49
               Mean episode length: 232.74
    Episode_Reward/reaching_object: 2.0669
     Episode_Reward/lifting_object: 167.0468
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0770
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 2.50s
                      Time elapsed: 00:47:06
                               ETA: 00:28:58

################################################################################
                     [1m Learning iteration 1239/2000 [0m                     

                       Computation: 38687 steps/s (collection: 2.438s, learning 0.103s)
             Mean action noise std: 3.08
          Mean value_function loss: 140.1002
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 78.3884
                       Mean reward: 851.38
               Mean episode length: 229.32
    Episode_Reward/reaching_object: 2.1072
     Episode_Reward/lifting_object: 171.0444
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0778
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 2.54s
                      Time elapsed: 00:47:09
                               ETA: 00:28:56

################################################################################
                     [1m Learning iteration 1240/2000 [0m                     

                       Computation: 40406 steps/s (collection: 2.342s, learning 0.091s)
             Mean action noise std: 3.08
          Mean value_function loss: 153.3512
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 78.3963
                       Mean reward: 848.57
               Mean episode length: 227.56
    Episode_Reward/reaching_object: 2.0669
     Episode_Reward/lifting_object: 167.3614
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0770
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 2.43s
                      Time elapsed: 00:47:11
                               ETA: 00:28:54

################################################################################
                     [1m Learning iteration 1241/2000 [0m                     

                       Computation: 39995 steps/s (collection: 2.332s, learning 0.126s)
             Mean action noise std: 3.08
          Mean value_function loss: 130.1778
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 78.4112
                       Mean reward: 852.19
               Mean episode length: 227.41
    Episode_Reward/reaching_object: 2.0584
     Episode_Reward/lifting_object: 167.3251
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0762
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 2.46s
                      Time elapsed: 00:47:14
                               ETA: 00:28:52

################################################################################
                     [1m Learning iteration 1242/2000 [0m                     

                       Computation: 41497 steps/s (collection: 2.277s, learning 0.092s)
             Mean action noise std: 3.09
          Mean value_function loss: 135.5049
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 78.4301
                       Mean reward: 879.21
               Mean episode length: 234.28
    Episode_Reward/reaching_object: 2.1050
     Episode_Reward/lifting_object: 171.5074
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0772
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 2.37s
                      Time elapsed: 00:47:16
                               ETA: 00:28:49

################################################################################
                     [1m Learning iteration 1243/2000 [0m                     

                       Computation: 40071 steps/s (collection: 2.310s, learning 0.144s)
             Mean action noise std: 3.09
          Mean value_function loss: 163.0244
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 78.4531
                       Mean reward: 842.73
               Mean episode length: 224.60
    Episode_Reward/reaching_object: 2.0364
     Episode_Reward/lifting_object: 165.5395
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0756
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 2.45s
                      Time elapsed: 00:47:19
                               ETA: 00:28:47

################################################################################
                     [1m Learning iteration 1244/2000 [0m                     

                       Computation: 40845 steps/s (collection: 2.303s, learning 0.104s)
             Mean action noise std: 3.09
          Mean value_function loss: 155.0328
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 78.4672
                       Mean reward: 843.38
               Mean episode length: 227.84
    Episode_Reward/reaching_object: 2.0650
     Episode_Reward/lifting_object: 167.8147
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0771
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 2.41s
                      Time elapsed: 00:47:21
                               ETA: 00:28:45

################################################################################
                     [1m Learning iteration 1245/2000 [0m                     

                       Computation: 41367 steps/s (collection: 2.268s, learning 0.109s)
             Mean action noise std: 3.09
          Mean value_function loss: 138.1502
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 78.4787
                       Mean reward: 855.78
               Mean episode length: 229.16
    Episode_Reward/reaching_object: 2.0884
     Episode_Reward/lifting_object: 170.2974
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0779
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 2.38s
                      Time elapsed: 00:47:23
                               ETA: 00:28:43

################################################################################
                     [1m Learning iteration 1246/2000 [0m                     

                       Computation: 40043 steps/s (collection: 2.353s, learning 0.102s)
             Mean action noise std: 3.09
          Mean value_function loss: 141.4798
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 78.4923
                       Mean reward: 858.16
               Mean episode length: 230.04
    Episode_Reward/reaching_object: 2.0670
     Episode_Reward/lifting_object: 167.7673
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0780
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 2.45s
                      Time elapsed: 00:47:26
                               ETA: 00:28:41

################################################################################
                     [1m Learning iteration 1247/2000 [0m                     

                       Computation: 37757 steps/s (collection: 2.511s, learning 0.092s)
             Mean action noise std: 3.09
          Mean value_function loss: 139.3238
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 78.5081
                       Mean reward: 845.70
               Mean episode length: 227.15
    Episode_Reward/reaching_object: 2.0713
     Episode_Reward/lifting_object: 168.9645
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0772
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 2.60s
                      Time elapsed: 00:47:29
                               ETA: 00:28:39

################################################################################
                     [1m Learning iteration 1248/2000 [0m                     

                       Computation: 35311 steps/s (collection: 2.658s, learning 0.126s)
             Mean action noise std: 3.09
          Mean value_function loss: 150.5496
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 78.5202
                       Mean reward: 864.79
               Mean episode length: 233.88
    Episode_Reward/reaching_object: 2.0593
     Episode_Reward/lifting_object: 167.1875
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0773
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 2.78s
                      Time elapsed: 00:47:31
                               ETA: 00:28:37

################################################################################
                     [1m Learning iteration 1249/2000 [0m                     

                       Computation: 38305 steps/s (collection: 2.383s, learning 0.183s)
             Mean action noise std: 3.10
          Mean value_function loss: 132.6441
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 78.5382
                       Mean reward: 860.80
               Mean episode length: 230.55
    Episode_Reward/reaching_object: 2.0689
     Episode_Reward/lifting_object: 168.2221
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0772
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 2.57s
                      Time elapsed: 00:47:34
                               ETA: 00:28:34

################################################################################
                     [1m Learning iteration 1250/2000 [0m                     

                       Computation: 39017 steps/s (collection: 2.387s, learning 0.132s)
             Mean action noise std: 3.10
          Mean value_function loss: 144.8762
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 78.5648
                       Mean reward: 842.20
               Mean episode length: 226.30
    Episode_Reward/reaching_object: 2.0069
     Episode_Reward/lifting_object: 162.9983
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0757
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 2.52s
                      Time elapsed: 00:47:36
                               ETA: 00:28:32

################################################################################
                     [1m Learning iteration 1251/2000 [0m                     

                       Computation: 38433 steps/s (collection: 2.457s, learning 0.100s)
             Mean action noise std: 3.10
          Mean value_function loss: 148.5041
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 78.5887
                       Mean reward: 867.42
               Mean episode length: 231.24
    Episode_Reward/reaching_object: 1.9938
     Episode_Reward/lifting_object: 161.5428
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0753
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 2.56s
                      Time elapsed: 00:47:39
                               ETA: 00:28:30

################################################################################
                     [1m Learning iteration 1252/2000 [0m                     

                       Computation: 35928 steps/s (collection: 2.607s, learning 0.129s)
             Mean action noise std: 3.10
          Mean value_function loss: 143.0888
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 78.6074
                       Mean reward: 868.73
               Mean episode length: 233.81
    Episode_Reward/reaching_object: 2.0362
     Episode_Reward/lifting_object: 165.4331
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0765
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 2.74s
                      Time elapsed: 00:47:42
                               ETA: 00:28:28

################################################################################
                     [1m Learning iteration 1253/2000 [0m                     

                       Computation: 39611 steps/s (collection: 2.361s, learning 0.121s)
             Mean action noise std: 3.10
          Mean value_function loss: 166.5589
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 78.6245
                       Mean reward: 884.46
               Mean episode length: 235.99
    Episode_Reward/reaching_object: 2.0819
     Episode_Reward/lifting_object: 169.2442
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0771
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 2.48s
                      Time elapsed: 00:47:44
                               ETA: 00:28:26

################################################################################
                     [1m Learning iteration 1254/2000 [0m                     

                       Computation: 39274 steps/s (collection: 2.381s, learning 0.122s)
             Mean action noise std: 3.11
          Mean value_function loss: 153.8949
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 78.6428
                       Mean reward: 852.70
               Mean episode length: 229.62
    Episode_Reward/reaching_object: 2.0553
     Episode_Reward/lifting_object: 166.1145
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0768
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 2.50s
                      Time elapsed: 00:47:47
                               ETA: 00:28:24

################################################################################
                     [1m Learning iteration 1255/2000 [0m                     

                       Computation: 39516 steps/s (collection: 2.372s, learning 0.116s)
             Mean action noise std: 3.11
          Mean value_function loss: 157.2876
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 78.6682
                       Mean reward: 823.84
               Mean episode length: 219.88
    Episode_Reward/reaching_object: 2.0617
     Episode_Reward/lifting_object: 167.2770
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0769
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 2.49s
                      Time elapsed: 00:47:49
                               ETA: 00:28:22

################################################################################
                     [1m Learning iteration 1256/2000 [0m                     

                       Computation: 41059 steps/s (collection: 2.301s, learning 0.093s)
             Mean action noise std: 3.11
          Mean value_function loss: 142.4707
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 78.6875
                       Mean reward: 854.89
               Mean episode length: 228.52
    Episode_Reward/reaching_object: 2.0788
     Episode_Reward/lifting_object: 168.7953
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0787
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 2.39s
                      Time elapsed: 00:47:52
                               ETA: 00:28:19

################################################################################
                     [1m Learning iteration 1257/2000 [0m                     

                       Computation: 40164 steps/s (collection: 2.344s, learning 0.104s)
             Mean action noise std: 3.11
          Mean value_function loss: 146.7441
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 78.7039
                       Mean reward: 840.52
               Mean episode length: 225.87
    Episode_Reward/reaching_object: 2.0748
     Episode_Reward/lifting_object: 168.4054
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0648
          Episode_Reward/joint_vel: -0.0783
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 2.45s
                      Time elapsed: 00:47:54
                               ETA: 00:28:17

################################################################################
                     [1m Learning iteration 1258/2000 [0m                     

                       Computation: 40240 steps/s (collection: 2.313s, learning 0.130s)
             Mean action noise std: 3.11
          Mean value_function loss: 153.7760
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 78.7190
                       Mean reward: 865.69
               Mean episode length: 229.61
    Episode_Reward/reaching_object: 2.0468
     Episode_Reward/lifting_object: 166.0574
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0774
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 2.44s
                      Time elapsed: 00:47:56
                               ETA: 00:28:15

################################################################################
                     [1m Learning iteration 1259/2000 [0m                     

                       Computation: 41107 steps/s (collection: 2.295s, learning 0.097s)
             Mean action noise std: 3.12
          Mean value_function loss: 130.5970
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 78.7260
                       Mean reward: 872.53
               Mean episode length: 233.07
    Episode_Reward/reaching_object: 2.0812
     Episode_Reward/lifting_object: 169.3685
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0651
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 2.39s
                      Time elapsed: 00:47:59
                               ETA: 00:28:13

################################################################################
                     [1m Learning iteration 1260/2000 [0m                     

                       Computation: 40050 steps/s (collection: 2.321s, learning 0.133s)
             Mean action noise std: 3.12
          Mean value_function loss: 153.6480
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 78.7411
                       Mean reward: 834.93
               Mean episode length: 225.65
    Episode_Reward/reaching_object: 2.0697
     Episode_Reward/lifting_object: 168.4756
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0648
          Episode_Reward/joint_vel: -0.0793
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 2.45s
                      Time elapsed: 00:48:01
                               ETA: 00:28:11

################################################################################
                     [1m Learning iteration 1261/2000 [0m                     

                       Computation: 39805 steps/s (collection: 2.364s, learning 0.106s)
             Mean action noise std: 3.12
          Mean value_function loss: 146.5942
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 78.7524
                       Mean reward: 841.32
               Mean episode length: 226.50
    Episode_Reward/reaching_object: 2.0535
     Episode_Reward/lifting_object: 166.9523
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0784
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 2.47s
                      Time elapsed: 00:48:04
                               ETA: 00:28:08

################################################################################
                     [1m Learning iteration 1262/2000 [0m                     

                       Computation: 40803 steps/s (collection: 2.314s, learning 0.095s)
             Mean action noise std: 3.12
          Mean value_function loss: 129.8084
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 78.7654
                       Mean reward: 830.86
               Mean episode length: 223.91
    Episode_Reward/reaching_object: 2.0608
     Episode_Reward/lifting_object: 167.9673
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0649
          Episode_Reward/joint_vel: -0.0789
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 2.41s
                      Time elapsed: 00:48:06
                               ETA: 00:28:06

################################################################################
                     [1m Learning iteration 1263/2000 [0m                     

                       Computation: 40472 steps/s (collection: 2.332s, learning 0.097s)
             Mean action noise std: 3.12
          Mean value_function loss: 144.5528
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 78.7810
                       Mean reward: 846.76
               Mean episode length: 226.36
    Episode_Reward/reaching_object: 2.0948
     Episode_Reward/lifting_object: 171.0858
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0660
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 2.43s
                      Time elapsed: 00:48:09
                               ETA: 00:28:04

################################################################################
                     [1m Learning iteration 1264/2000 [0m                     

                       Computation: 38951 steps/s (collection: 2.342s, learning 0.182s)
             Mean action noise std: 3.12
          Mean value_function loss: 153.4843
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 78.8024
                       Mean reward: 836.47
               Mean episode length: 225.13
    Episode_Reward/reaching_object: 2.0528
     Episode_Reward/lifting_object: 167.4565
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0651
          Episode_Reward/joint_vel: -0.0801
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 2.52s
                      Time elapsed: 00:48:11
                               ETA: 00:28:02

################################################################################
                     [1m Learning iteration 1265/2000 [0m                     

                       Computation: 39924 steps/s (collection: 2.347s, learning 0.115s)
             Mean action noise std: 3.13
          Mean value_function loss: 154.7559
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 78.8295
                       Mean reward: 829.44
               Mean episode length: 223.35
    Episode_Reward/reaching_object: 2.0684
     Episode_Reward/lifting_object: 169.4332
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0808
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 2.46s
                      Time elapsed: 00:48:14
                               ETA: 00:28:00

################################################################################
                     [1m Learning iteration 1266/2000 [0m                     

                       Computation: 41744 steps/s (collection: 2.261s, learning 0.094s)
             Mean action noise std: 3.13
          Mean value_function loss: 147.1865
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 78.8502
                       Mean reward: 820.91
               Mean episode length: 222.47
    Episode_Reward/reaching_object: 2.0512
     Episode_Reward/lifting_object: 167.6989
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0654
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 2.35s
                      Time elapsed: 00:48:16
                               ETA: 00:27:57

################################################################################
                     [1m Learning iteration 1267/2000 [0m                     

                       Computation: 41040 steps/s (collection: 2.286s, learning 0.110s)
             Mean action noise std: 3.13
          Mean value_function loss: 134.3337
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 78.8596
                       Mean reward: 892.21
               Mean episode length: 237.54
    Episode_Reward/reaching_object: 2.0662
     Episode_Reward/lifting_object: 169.6851
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0806
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 2.40s
                      Time elapsed: 00:48:18
                               ETA: 00:27:55

################################################################################
                     [1m Learning iteration 1268/2000 [0m                     

                       Computation: 41030 steps/s (collection: 2.296s, learning 0.100s)
             Mean action noise std: 3.13
          Mean value_function loss: 133.9072
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 78.8755
                       Mean reward: 898.21
               Mean episode length: 238.63
    Episode_Reward/reaching_object: 2.1059
     Episode_Reward/lifting_object: 173.5069
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0672
          Episode_Reward/joint_vel: -0.0822
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 2.40s
                      Time elapsed: 00:48:21
                               ETA: 00:27:53

################################################################################
                     [1m Learning iteration 1269/2000 [0m                     

                       Computation: 41604 steps/s (collection: 2.268s, learning 0.095s)
             Mean action noise std: 3.13
          Mean value_function loss: 146.6276
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 78.8967
                       Mean reward: 849.41
               Mean episode length: 227.83
    Episode_Reward/reaching_object: 2.0664
     Episode_Reward/lifting_object: 169.4294
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0659
          Episode_Reward/joint_vel: -0.0809
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 2.36s
                      Time elapsed: 00:48:23
                               ETA: 00:27:51

################################################################################
                     [1m Learning iteration 1270/2000 [0m                     

                       Computation: 40338 steps/s (collection: 2.324s, learning 0.113s)
             Mean action noise std: 3.13
          Mean value_function loss: 184.0511
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 78.9141
                       Mean reward: 835.48
               Mean episode length: 226.06
    Episode_Reward/reaching_object: 2.0127
     Episode_Reward/lifting_object: 164.9394
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0649
          Episode_Reward/joint_vel: -0.0805
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 2.44s
                      Time elapsed: 00:48:26
                               ETA: 00:27:49

################################################################################
                     [1m Learning iteration 1271/2000 [0m                     

                       Computation: 39757 steps/s (collection: 2.376s, learning 0.096s)
             Mean action noise std: 3.14
          Mean value_function loss: 142.3533
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 78.9313
                       Mean reward: 848.95
               Mean episode length: 226.69
    Episode_Reward/reaching_object: 2.0306
     Episode_Reward/lifting_object: 166.6281
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.0805
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 2.47s
                      Time elapsed: 00:48:28
                               ETA: 00:27:46

################################################################################
                     [1m Learning iteration 1272/2000 [0m                     

                       Computation: 40278 steps/s (collection: 2.286s, learning 0.155s)
             Mean action noise std: 3.14
          Mean value_function loss: 136.2372
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 78.9541
                       Mean reward: 883.96
               Mean episode length: 236.62
    Episode_Reward/reaching_object: 2.0899
     Episode_Reward/lifting_object: 171.5257
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0670
          Episode_Reward/joint_vel: -0.0823
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 2.44s
                      Time elapsed: 00:48:30
                               ETA: 00:27:44

################################################################################
                     [1m Learning iteration 1273/2000 [0m                     

                       Computation: 38376 steps/s (collection: 2.456s, learning 0.106s)
             Mean action noise std: 3.14
          Mean value_function loss: 132.1137
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 78.9765
                       Mean reward: 874.31
               Mean episode length: 232.29
    Episode_Reward/reaching_object: 2.0288
     Episode_Reward/lifting_object: 166.8145
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0803
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 2.56s
                      Time elapsed: 00:48:33
                               ETA: 00:27:42

################################################################################
                     [1m Learning iteration 1274/2000 [0m                     

                       Computation: 40937 steps/s (collection: 2.297s, learning 0.105s)
             Mean action noise std: 3.14
          Mean value_function loss: 153.4667
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 78.9859
                       Mean reward: 892.77
               Mean episode length: 237.69
    Episode_Reward/reaching_object: 2.0629
     Episode_Reward/lifting_object: 169.3262
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0666
          Episode_Reward/joint_vel: -0.0822
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 2.40s
                      Time elapsed: 00:48:35
                               ETA: 00:27:40

################################################################################
                     [1m Learning iteration 1275/2000 [0m                     

                       Computation: 35389 steps/s (collection: 2.680s, learning 0.098s)
             Mean action noise std: 3.14
          Mean value_function loss: 150.9037
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 79.0024
                       Mean reward: 837.67
               Mean episode length: 224.95
    Episode_Reward/reaching_object: 2.0670
     Episode_Reward/lifting_object: 169.1659
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0667
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 2.78s
                      Time elapsed: 00:48:38
                               ETA: 00:27:38

################################################################################
                     [1m Learning iteration 1276/2000 [0m                     

                       Computation: 40192 steps/s (collection: 2.299s, learning 0.147s)
             Mean action noise std: 3.15
          Mean value_function loss: 136.6934
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 79.0201
                       Mean reward: 858.19
               Mean episode length: 230.27
    Episode_Reward/reaching_object: 2.0581
     Episode_Reward/lifting_object: 168.1554
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0665
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 2.45s
                      Time elapsed: 00:48:41
                               ETA: 00:27:36

################################################################################
                     [1m Learning iteration 1277/2000 [0m                     

                       Computation: 40612 steps/s (collection: 2.324s, learning 0.097s)
             Mean action noise std: 3.15
          Mean value_function loss: 171.3019
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 79.0394
                       Mean reward: 841.01
               Mean episode length: 226.37
    Episode_Reward/reaching_object: 2.0621
     Episode_Reward/lifting_object: 169.0482
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0669
          Episode_Reward/joint_vel: -0.0830
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 2.42s
                      Time elapsed: 00:48:43
                               ETA: 00:27:33

################################################################################
                     [1m Learning iteration 1278/2000 [0m                     

                       Computation: 41392 steps/s (collection: 2.281s, learning 0.094s)
             Mean action noise std: 3.15
          Mean value_function loss: 158.1602
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 79.0566
                       Mean reward: 837.86
               Mean episode length: 226.29
    Episode_Reward/reaching_object: 2.0274
     Episode_Reward/lifting_object: 166.0408
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0660
          Episode_Reward/joint_vel: -0.0822
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 2.37s
                      Time elapsed: 00:48:45
                               ETA: 00:27:31

################################################################################
                     [1m Learning iteration 1279/2000 [0m                     

                       Computation: 36380 steps/s (collection: 2.588s, learning 0.114s)
             Mean action noise std: 3.15
          Mean value_function loss: 160.3014
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 79.0755
                       Mean reward: 858.18
               Mean episode length: 230.27
    Episode_Reward/reaching_object: 2.0316
     Episode_Reward/lifting_object: 166.0446
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0660
          Episode_Reward/joint_vel: -0.0808
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 2.70s
                      Time elapsed: 00:48:48
                               ETA: 00:27:29

################################################################################
                     [1m Learning iteration 1280/2000 [0m                     

                       Computation: 39366 steps/s (collection: 2.355s, learning 0.142s)
             Mean action noise std: 3.15
          Mean value_function loss: 136.4554
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 79.0946
                       Mean reward: 829.86
               Mean episode length: 224.73
    Episode_Reward/reaching_object: 2.0464
     Episode_Reward/lifting_object: 167.5282
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0666
          Episode_Reward/joint_vel: -0.0832
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 2.50s
                      Time elapsed: 00:48:51
                               ETA: 00:27:27

################################################################################
                     [1m Learning iteration 1281/2000 [0m                     

                       Computation: 39119 steps/s (collection: 2.414s, learning 0.099s)
             Mean action noise std: 3.15
          Mean value_function loss: 189.4244
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 79.1125
                       Mean reward: 830.00
               Mean episode length: 225.38
    Episode_Reward/reaching_object: 2.0377
     Episode_Reward/lifting_object: 167.3153
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0663
          Episode_Reward/joint_vel: -0.0815
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 2.51s
                      Time elapsed: 00:48:53
                               ETA: 00:27:25

################################################################################
                     [1m Learning iteration 1282/2000 [0m                     

                       Computation: 39992 steps/s (collection: 2.315s, learning 0.143s)
             Mean action noise std: 3.16
          Mean value_function loss: 151.4302
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 79.1322
                       Mean reward: 841.41
               Mean episode length: 225.54
    Episode_Reward/reaching_object: 2.0405
     Episode_Reward/lifting_object: 167.7378
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0665
          Episode_Reward/joint_vel: -0.0825
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 2.46s
                      Time elapsed: 00:48:56
                               ETA: 00:27:23

################################################################################
                     [1m Learning iteration 1283/2000 [0m                     

                       Computation: 38337 steps/s (collection: 2.465s, learning 0.100s)
             Mean action noise std: 3.16
          Mean value_function loss: 158.6650
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 79.1533
                       Mean reward: 851.90
               Mean episode length: 229.11
    Episode_Reward/reaching_object: 2.0328
     Episode_Reward/lifting_object: 166.5803
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0665
          Episode_Reward/joint_vel: -0.0823
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 2.56s
                      Time elapsed: 00:48:58
                               ETA: 00:27:20

################################################################################
                     [1m Learning iteration 1284/2000 [0m                     

                       Computation: 39696 steps/s (collection: 2.340s, learning 0.136s)
             Mean action noise std: 3.16
          Mean value_function loss: 128.8622
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 79.1740
                       Mean reward: 855.90
               Mean episode length: 228.64
    Episode_Reward/reaching_object: 2.0302
     Episode_Reward/lifting_object: 167.2441
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0665
          Episode_Reward/joint_vel: -0.0815
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 2.48s
                      Time elapsed: 00:49:01
                               ETA: 00:27:18

################################################################################
                     [1m Learning iteration 1285/2000 [0m                     

                       Computation: 39503 steps/s (collection: 2.392s, learning 0.097s)
             Mean action noise std: 3.16
          Mean value_function loss: 181.2255
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 79.1925
                       Mean reward: 852.20
               Mean episode length: 228.18
    Episode_Reward/reaching_object: 2.0151
     Episode_Reward/lifting_object: 165.3055
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0665
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 2.49s
                      Time elapsed: 00:49:03
                               ETA: 00:27:16

################################################################################
                     [1m Learning iteration 1286/2000 [0m                     

                       Computation: 40388 steps/s (collection: 2.319s, learning 0.115s)
             Mean action noise std: 3.16
          Mean value_function loss: 218.9671
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 79.2047
                       Mean reward: 863.49
               Mean episode length: 231.22
    Episode_Reward/reaching_object: 1.9979
     Episode_Reward/lifting_object: 164.2055
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0661
          Episode_Reward/joint_vel: -0.0803
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 2.43s
                      Time elapsed: 00:49:06
                               ETA: 00:27:14

################################################################################
                     [1m Learning iteration 1287/2000 [0m                     

                       Computation: 41062 steps/s (collection: 2.283s, learning 0.111s)
             Mean action noise std: 3.17
          Mean value_function loss: 157.0166
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 79.2166
                       Mean reward: 885.73
               Mean episode length: 236.92
    Episode_Reward/reaching_object: 2.0408
     Episode_Reward/lifting_object: 168.5574
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0670
          Episode_Reward/joint_vel: -0.0814
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 2.39s
                      Time elapsed: 00:49:08
                               ETA: 00:27:12

################################################################################
                     [1m Learning iteration 1288/2000 [0m                     

                       Computation: 41757 steps/s (collection: 2.264s, learning 0.091s)
             Mean action noise std: 3.17
          Mean value_function loss: 127.8062
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 79.2360
                       Mean reward: 877.54
               Mean episode length: 233.96
    Episode_Reward/reaching_object: 2.0767
     Episode_Reward/lifting_object: 172.0008
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0683
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 2.35s
                      Time elapsed: 00:49:10
                               ETA: 00:27:09

################################################################################
                     [1m Learning iteration 1289/2000 [0m                     

                       Computation: 37326 steps/s (collection: 2.467s, learning 0.167s)
             Mean action noise std: 3.17
          Mean value_function loss: 157.2899
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 79.2566
                       Mean reward: 806.66
               Mean episode length: 217.87
    Episode_Reward/reaching_object: 2.0092
     Episode_Reward/lifting_object: 165.8983
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0806
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 2.63s
                      Time elapsed: 00:49:13
                               ETA: 00:27:07

################################################################################
                     [1m Learning iteration 1290/2000 [0m                     

                       Computation: 37951 steps/s (collection: 2.451s, learning 0.139s)
             Mean action noise std: 3.17
          Mean value_function loss: 134.2526
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 79.2758
                       Mean reward: 872.61
               Mean episode length: 233.07
    Episode_Reward/reaching_object: 2.0690
     Episode_Reward/lifting_object: 171.4571
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0816
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 2.59s
                      Time elapsed: 00:49:16
                               ETA: 00:27:05

################################################################################
                     [1m Learning iteration 1291/2000 [0m                     

                       Computation: 37385 steps/s (collection: 2.530s, learning 0.099s)
             Mean action noise std: 3.17
          Mean value_function loss: 187.3604
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 79.2902
                       Mean reward: 855.71
               Mean episode length: 230.28
    Episode_Reward/reaching_object: 2.0575
     Episode_Reward/lifting_object: 168.9917
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0679
          Episode_Reward/joint_vel: -0.0818
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 2.63s
                      Time elapsed: 00:49:18
                               ETA: 00:27:03

################################################################################
                     [1m Learning iteration 1292/2000 [0m                     

                       Computation: 40593 steps/s (collection: 2.293s, learning 0.129s)
             Mean action noise std: 3.18
          Mean value_function loss: 135.4101
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 79.3115
                       Mean reward: 900.19
               Mean episode length: 240.59
    Episode_Reward/reaching_object: 2.0502
     Episode_Reward/lifting_object: 169.1554
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0818
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 2.42s
                      Time elapsed: 00:49:21
                               ETA: 00:27:01

################################################################################
                     [1m Learning iteration 1293/2000 [0m                     

                       Computation: 37438 steps/s (collection: 2.527s, learning 0.099s)
             Mean action noise std: 3.18
          Mean value_function loss: 142.9737
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 79.3369
                       Mean reward: 874.18
               Mean episode length: 234.04
    Episode_Reward/reaching_object: 2.0323
     Episode_Reward/lifting_object: 167.4100
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0674
          Episode_Reward/joint_vel: -0.0816
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 2.63s
                      Time elapsed: 00:49:23
                               ETA: 00:26:59

################################################################################
                     [1m Learning iteration 1294/2000 [0m                     

                       Computation: 37956 steps/s (collection: 2.412s, learning 0.178s)
             Mean action noise std: 3.18
          Mean value_function loss: 124.2599
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 79.3592
                       Mean reward: 894.66
               Mean episode length: 238.52
    Episode_Reward/reaching_object: 2.0632
     Episode_Reward/lifting_object: 170.1071
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0685
          Episode_Reward/joint_vel: -0.0822
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 2.59s
                      Time elapsed: 00:49:26
                               ETA: 00:26:57

################################################################################
                     [1m Learning iteration 1295/2000 [0m                     

                       Computation: 39693 steps/s (collection: 2.387s, learning 0.090s)
             Mean action noise std: 3.18
          Mean value_function loss: 130.5433
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 79.3834
                       Mean reward: 861.81
               Mean episode length: 229.71
    Episode_Reward/reaching_object: 2.0768
     Episode_Reward/lifting_object: 171.9104
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0690
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 2.48s
                      Time elapsed: 00:49:28
                               ETA: 00:26:54

################################################################################
                     [1m Learning iteration 1296/2000 [0m                     

                       Computation: 38655 steps/s (collection: 2.376s, learning 0.167s)
             Mean action noise std: 3.18
          Mean value_function loss: 165.1008
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 79.3991
                       Mean reward: 829.34
               Mean episode length: 225.39
    Episode_Reward/reaching_object: 2.0278
     Episode_Reward/lifting_object: 166.9468
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0673
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 2.54s
                      Time elapsed: 00:49:31
                               ETA: 00:26:52

################################################################################
                     [1m Learning iteration 1297/2000 [0m                     

                       Computation: 39784 steps/s (collection: 2.366s, learning 0.105s)
             Mean action noise std: 3.19
          Mean value_function loss: 154.2535
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 79.4160
                       Mean reward: 839.37
               Mean episode length: 226.70
    Episode_Reward/reaching_object: 2.0563
     Episode_Reward/lifting_object: 169.2946
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0830
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 2.47s
                      Time elapsed: 00:49:33
                               ETA: 00:26:50

################################################################################
                     [1m Learning iteration 1298/2000 [0m                     

                       Computation: 41378 steps/s (collection: 2.270s, learning 0.106s)
             Mean action noise std: 3.19
          Mean value_function loss: 180.4569
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 79.4331
                       Mean reward: 797.77
               Mean episode length: 217.03
    Episode_Reward/reaching_object: 2.0159
     Episode_Reward/lifting_object: 165.8279
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0676
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 2.38s
                      Time elapsed: 00:49:36
                               ETA: 00:26:48

################################################################################
                     [1m Learning iteration 1299/2000 [0m                     

                       Computation: 37864 steps/s (collection: 2.483s, learning 0.114s)
             Mean action noise std: 3.19
          Mean value_function loss: 154.8544
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 79.4438
                       Mean reward: 818.07
               Mean episode length: 220.14
    Episode_Reward/reaching_object: 2.0188
     Episode_Reward/lifting_object: 166.5068
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0830
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 2.60s
                      Time elapsed: 00:49:38
                               ETA: 00:26:46

################################################################################
                     [1m Learning iteration 1300/2000 [0m                     

                       Computation: 41214 steps/s (collection: 2.274s, learning 0.111s)
             Mean action noise std: 3.19
          Mean value_function loss: 142.2616
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 79.4590
                       Mean reward: 824.42
               Mean episode length: 220.78
    Episode_Reward/reaching_object: 2.0524
     Episode_Reward/lifting_object: 169.8403
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0686
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 2.39s
                      Time elapsed: 00:49:41
                               ETA: 00:26:44

################################################################################
                     [1m Learning iteration 1301/2000 [0m                     

                       Computation: 40984 steps/s (collection: 2.297s, learning 0.101s)
             Mean action noise std: 3.19
          Mean value_function loss: 129.4682
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 79.4769
                       Mean reward: 907.85
               Mean episode length: 241.38
    Episode_Reward/reaching_object: 2.0312
     Episode_Reward/lifting_object: 168.0377
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0683
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 2.40s
                      Time elapsed: 00:49:43
                               ETA: 00:26:41

################################################################################
                     [1m Learning iteration 1302/2000 [0m                     

                       Computation: 41019 steps/s (collection: 2.294s, learning 0.103s)
             Mean action noise std: 3.19
          Mean value_function loss: 142.9618
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 79.4943
                       Mean reward: 862.77
               Mean episode length: 231.23
    Episode_Reward/reaching_object: 2.0552
     Episode_Reward/lifting_object: 169.7293
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0693
          Episode_Reward/joint_vel: -0.0856
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 2.40s
                      Time elapsed: 00:49:45
                               ETA: 00:26:39

################################################################################
                     [1m Learning iteration 1303/2000 [0m                     

                       Computation: 38980 steps/s (collection: 2.396s, learning 0.126s)
             Mean action noise std: 3.20
          Mean value_function loss: 151.5827
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 79.5068
                       Mean reward: 843.28
               Mean episode length: 226.75
    Episode_Reward/reaching_object: 2.0482
     Episode_Reward/lifting_object: 169.0104
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0689
          Episode_Reward/joint_vel: -0.0843
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 2.52s
                      Time elapsed: 00:49:48
                               ETA: 00:26:37

################################################################################
                     [1m Learning iteration 1304/2000 [0m                     

                       Computation: 40481 steps/s (collection: 2.318s, learning 0.110s)
             Mean action noise std: 3.20
          Mean value_function loss: 171.4366
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 79.5216
                       Mean reward: 822.97
               Mean episode length: 223.34
    Episode_Reward/reaching_object: 1.9984
     Episode_Reward/lifting_object: 164.6955
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0831
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 2.43s
                      Time elapsed: 00:49:50
                               ETA: 00:26:35

################################################################################
                     [1m Learning iteration 1305/2000 [0m                     

                       Computation: 39889 steps/s (collection: 2.363s, learning 0.102s)
             Mean action noise std: 3.20
          Mean value_function loss: 165.2196
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 79.5367
                       Mean reward: 854.79
               Mean episode length: 229.99
    Episode_Reward/reaching_object: 1.9842
     Episode_Reward/lifting_object: 163.2224
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0674
          Episode_Reward/joint_vel: -0.0823
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 2.46s
                      Time elapsed: 00:49:53
                               ETA: 00:26:32

################################################################################
                     [1m Learning iteration 1306/2000 [0m                     

                       Computation: 40005 steps/s (collection: 2.336s, learning 0.122s)
             Mean action noise std: 3.20
          Mean value_function loss: 149.9014
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 79.5467
                       Mean reward: 829.20
               Mean episode length: 222.74
    Episode_Reward/reaching_object: 2.0399
     Episode_Reward/lifting_object: 168.3624
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0690
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 2.46s
                      Time elapsed: 00:49:55
                               ETA: 00:26:30

################################################################################
                     [1m Learning iteration 1307/2000 [0m                     

                       Computation: 40262 steps/s (collection: 2.337s, learning 0.104s)
             Mean action noise std: 3.20
          Mean value_function loss: 144.0430
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 79.5585
                       Mean reward: 874.76
               Mean episode length: 233.48
    Episode_Reward/reaching_object: 2.0351
     Episode_Reward/lifting_object: 167.8838
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0689
          Episode_Reward/joint_vel: -0.0845
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 2.44s
                      Time elapsed: 00:49:58
                               ETA: 00:26:28

################################################################################
                     [1m Learning iteration 1308/2000 [0m                     

                       Computation: 40690 steps/s (collection: 2.290s, learning 0.126s)
             Mean action noise std: 3.20
          Mean value_function loss: 160.8376
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 79.5684
                       Mean reward: 787.65
               Mean episode length: 213.24
    Episode_Reward/reaching_object: 2.0088
     Episode_Reward/lifting_object: 165.8891
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0680
          Episode_Reward/joint_vel: -0.0836
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 2.42s
                      Time elapsed: 00:50:00
                               ETA: 00:26:26

################################################################################
                     [1m Learning iteration 1309/2000 [0m                     

                       Computation: 39234 steps/s (collection: 2.414s, learning 0.092s)
             Mean action noise std: 3.20
          Mean value_function loss: 136.7710
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 79.5839
                       Mean reward: 821.83
               Mean episode length: 222.42
    Episode_Reward/reaching_object: 2.0542
     Episode_Reward/lifting_object: 169.7453
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 2.51s
                      Time elapsed: 00:50:03
                               ETA: 00:26:24

################################################################################
                     [1m Learning iteration 1310/2000 [0m                     

                       Computation: 41264 steps/s (collection: 2.277s, learning 0.105s)
             Mean action noise std: 3.20
          Mean value_function loss: 146.5562
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 79.5988
                       Mean reward: 866.13
               Mean episode length: 231.41
    Episode_Reward/reaching_object: 2.0370
     Episode_Reward/lifting_object: 168.3587
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 2.38s
                      Time elapsed: 00:50:05
                               ETA: 00:26:21

################################################################################
                     [1m Learning iteration 1311/2000 [0m                     

                       Computation: 39842 steps/s (collection: 2.375s, learning 0.092s)
             Mean action noise std: 3.21
          Mean value_function loss: 158.7643
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 79.6078
                       Mean reward: 865.69
               Mean episode length: 230.68
    Episode_Reward/reaching_object: 2.0274
     Episode_Reward/lifting_object: 167.6208
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0690
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 2.47s
                      Time elapsed: 00:50:08
                               ETA: 00:26:19

################################################################################
                     [1m Learning iteration 1312/2000 [0m                     

                       Computation: 41155 steps/s (collection: 2.257s, learning 0.132s)
             Mean action noise std: 3.21
          Mean value_function loss: 133.0567
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 79.6233
                       Mean reward: 856.74
               Mean episode length: 229.55
    Episode_Reward/reaching_object: 2.0301
     Episode_Reward/lifting_object: 167.8452
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 2.39s
                      Time elapsed: 00:50:10
                               ETA: 00:26:17

################################################################################
                     [1m Learning iteration 1313/2000 [0m                     

                       Computation: 38036 steps/s (collection: 2.362s, learning 0.222s)
             Mean action noise std: 3.21
          Mean value_function loss: 118.3805
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 79.6399
                       Mean reward: 828.79
               Mean episode length: 223.87
    Episode_Reward/reaching_object: 2.0648
     Episode_Reward/lifting_object: 171.1567
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0705
          Episode_Reward/joint_vel: -0.0861
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 2.58s
                      Time elapsed: 00:50:13
                               ETA: 00:26:15

################################################################################
                     [1m Learning iteration 1314/2000 [0m                     

                       Computation: 37050 steps/s (collection: 2.486s, learning 0.167s)
             Mean action noise std: 3.21
          Mean value_function loss: 105.0504
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 79.6586
                       Mean reward: 877.56
               Mean episode length: 233.81
    Episode_Reward/reaching_object: 2.1175
     Episode_Reward/lifting_object: 176.5219
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0718
          Episode_Reward/joint_vel: -0.0868
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 2.65s
                      Time elapsed: 00:50:15
                               ETA: 00:26:13

################################################################################
                     [1m Learning iteration 1315/2000 [0m                     

                       Computation: 38527 steps/s (collection: 2.450s, learning 0.102s)
             Mean action noise std: 3.21
          Mean value_function loss: 135.0654
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 79.6755
                       Mean reward: 841.96
               Mean episode length: 225.73
    Episode_Reward/reaching_object: 1.9983
     Episode_Reward/lifting_object: 165.9302
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0821
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 2.55s
                      Time elapsed: 00:50:18
                               ETA: 00:26:11

################################################################################
                     [1m Learning iteration 1316/2000 [0m                     

                       Computation: 39969 steps/s (collection: 2.335s, learning 0.124s)
             Mean action noise std: 3.21
          Mean value_function loss: 153.3320
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 79.6878
                       Mean reward: 829.57
               Mean episode length: 221.42
    Episode_Reward/reaching_object: 2.0178
     Episode_Reward/lifting_object: 167.9760
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0689
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 2.46s
                      Time elapsed: 00:50:20
                               ETA: 00:26:08

################################################################################
                     [1m Learning iteration 1317/2000 [0m                     

                       Computation: 41101 steps/s (collection: 2.279s, learning 0.113s)
             Mean action noise std: 3.22
          Mean value_function loss: 138.4552
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 79.7073
                       Mean reward: 833.22
               Mean episode length: 223.15
    Episode_Reward/reaching_object: 2.0186
     Episode_Reward/lifting_object: 168.5322
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0692
          Episode_Reward/joint_vel: -0.0816
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 2.39s
                      Time elapsed: 00:50:23
                               ETA: 00:26:06

################################################################################
                     [1m Learning iteration 1318/2000 [0m                     

                       Computation: 41692 steps/s (collection: 2.262s, learning 0.096s)
             Mean action noise std: 3.22
          Mean value_function loss: 140.8634
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 79.7259
                       Mean reward: 853.31
               Mean episode length: 229.56
    Episode_Reward/reaching_object: 2.0407
     Episode_Reward/lifting_object: 169.8117
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 2.36s
                      Time elapsed: 00:50:25
                               ETA: 00:26:04

################################################################################
                     [1m Learning iteration 1319/2000 [0m                     

                       Computation: 39872 steps/s (collection: 2.346s, learning 0.119s)
             Mean action noise std: 3.22
          Mean value_function loss: 150.0908
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 79.7447
                       Mean reward: 810.10
               Mean episode length: 218.21
    Episode_Reward/reaching_object: 2.0103
     Episode_Reward/lifting_object: 167.8356
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0687
          Episode_Reward/joint_vel: -0.0802
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 2.47s
                      Time elapsed: 00:50:27
                               ETA: 00:26:02

################################################################################
                     [1m Learning iteration 1320/2000 [0m                     

                       Computation: 40814 steps/s (collection: 2.293s, learning 0.116s)
             Mean action noise std: 3.22
          Mean value_function loss: 170.8028
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 79.7560
                       Mean reward: 844.98
               Mean episode length: 227.66
    Episode_Reward/reaching_object: 2.0174
     Episode_Reward/lifting_object: 167.6891
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0692
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 2.41s
                      Time elapsed: 00:50:30
                               ETA: 00:25:59

################################################################################
                     [1m Learning iteration 1321/2000 [0m                     

                       Computation: 38996 steps/s (collection: 2.409s, learning 0.112s)
             Mean action noise std: 3.22
          Mean value_function loss: 139.9942
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 79.7708
                       Mean reward: 846.83
               Mean episode length: 226.57
    Episode_Reward/reaching_object: 2.0456
     Episode_Reward/lifting_object: 170.9145
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.0805
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 2.52s
                      Time elapsed: 00:50:32
                               ETA: 00:25:57

################################################################################
                     [1m Learning iteration 1322/2000 [0m                     

                       Computation: 41643 steps/s (collection: 2.260s, learning 0.101s)
             Mean action noise std: 3.22
          Mean value_function loss: 146.0913
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 79.7831
                       Mean reward: 828.76
               Mean episode length: 225.68
    Episode_Reward/reaching_object: 1.9888
     Episode_Reward/lifting_object: 165.3525
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0683
          Episode_Reward/joint_vel: -0.0786
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 2.36s
                      Time elapsed: 00:50:35
                               ETA: 00:25:55

################################################################################
                     [1m Learning iteration 1323/2000 [0m                     

                       Computation: 42249 steps/s (collection: 2.224s, learning 0.103s)
             Mean action noise std: 3.23
          Mean value_function loss: 159.9430
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 79.7896
                       Mean reward: 834.70
               Mean episode length: 223.95
    Episode_Reward/reaching_object: 1.9884
     Episode_Reward/lifting_object: 165.5038
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0681
          Episode_Reward/joint_vel: -0.0771
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 2.33s
                      Time elapsed: 00:50:37
                               ETA: 00:25:53

################################################################################
                     [1m Learning iteration 1324/2000 [0m                     

                       Computation: 41778 steps/s (collection: 2.247s, learning 0.106s)
             Mean action noise std: 3.23
          Mean value_function loss: 154.3896
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 79.8018
                       Mean reward: 842.46
               Mean episode length: 226.89
    Episode_Reward/reaching_object: 1.9666
     Episode_Reward/lifting_object: 163.1221
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0676
          Episode_Reward/joint_vel: -0.0766
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 2.35s
                      Time elapsed: 00:50:39
                               ETA: 00:25:50

################################################################################
                     [1m Learning iteration 1325/2000 [0m                     

                       Computation: 41107 steps/s (collection: 2.291s, learning 0.100s)
             Mean action noise std: 3.23
          Mean value_function loss: 148.3849
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 79.8254
                       Mean reward: 864.29
               Mean episode length: 230.89
    Episode_Reward/reaching_object: 2.0010
     Episode_Reward/lifting_object: 166.8203
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0688
          Episode_Reward/joint_vel: -0.0779
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 2.39s
                      Time elapsed: 00:50:42
                               ETA: 00:25:48

################################################################################
                     [1m Learning iteration 1326/2000 [0m                     

                       Computation: 42001 steps/s (collection: 2.232s, learning 0.108s)
             Mean action noise std: 3.23
          Mean value_function loss: 150.8405
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 79.8446
                       Mean reward: 833.93
               Mean episode length: 224.91
    Episode_Reward/reaching_object: 2.0498
     Episode_Reward/lifting_object: 170.6840
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0702
          Episode_Reward/joint_vel: -0.0779
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 2.34s
                      Time elapsed: 00:50:44
                               ETA: 00:25:46

################################################################################
                     [1m Learning iteration 1327/2000 [0m                     

                       Computation: 40843 steps/s (collection: 2.281s, learning 0.126s)
             Mean action noise std: 3.23
          Mean value_function loss: 140.0311
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 79.8584
                       Mean reward: 872.93
               Mean episode length: 234.12
    Episode_Reward/reaching_object: 2.0736
     Episode_Reward/lifting_object: 173.1916
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0709
          Episode_Reward/joint_vel: -0.0797
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 2.41s
                      Time elapsed: 00:50:47
                               ETA: 00:25:44

################################################################################
                     [1m Learning iteration 1328/2000 [0m                     

                       Computation: 42681 steps/s (collection: 2.204s, learning 0.099s)
             Mean action noise std: 3.23
          Mean value_function loss: 158.7690
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 79.8772
                       Mean reward: 821.68
               Mean episode length: 223.22
    Episode_Reward/reaching_object: 2.0015
     Episode_Reward/lifting_object: 166.6112
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0688
          Episode_Reward/joint_vel: -0.0767
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 2.30s
                      Time elapsed: 00:50:49
                               ETA: 00:25:41

################################################################################
                     [1m Learning iteration 1329/2000 [0m                     

                       Computation: 39539 steps/s (collection: 2.254s, learning 0.232s)
             Mean action noise std: 3.24
          Mean value_function loss: 132.0765
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 79.8952
                       Mean reward: 831.11
               Mean episode length: 225.47
    Episode_Reward/reaching_object: 2.0315
     Episode_Reward/lifting_object: 168.8633
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0701
          Episode_Reward/joint_vel: -0.0796
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 2.49s
                      Time elapsed: 00:50:51
                               ETA: 00:25:39

################################################################################
                     [1m Learning iteration 1330/2000 [0m                     

                       Computation: 41006 steps/s (collection: 2.299s, learning 0.098s)
             Mean action noise std: 3.24
          Mean value_function loss: 138.6406
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 79.9182
                       Mean reward: 874.18
               Mean episode length: 233.52
    Episode_Reward/reaching_object: 2.0536
     Episode_Reward/lifting_object: 171.0817
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0706
          Episode_Reward/joint_vel: -0.0784
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 2.40s
                      Time elapsed: 00:50:54
                               ETA: 00:25:37

################################################################################
                     [1m Learning iteration 1331/2000 [0m                     

                       Computation: 40442 steps/s (collection: 2.273s, learning 0.158s)
             Mean action noise std: 3.24
          Mean value_function loss: 136.5087
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 79.9356
                       Mean reward: 893.48
               Mean episode length: 238.29
    Episode_Reward/reaching_object: 2.0541
     Episode_Reward/lifting_object: 171.3581
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0705
          Episode_Reward/joint_vel: -0.0785
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 2.43s
                      Time elapsed: 00:50:56
                               ETA: 00:25:35

################################################################################
                     [1m Learning iteration 1332/2000 [0m                     

                       Computation: 41381 steps/s (collection: 2.253s, learning 0.123s)
             Mean action noise std: 3.24
          Mean value_function loss: 136.3776
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 79.9502
                       Mean reward: 853.17
               Mean episode length: 227.67
    Episode_Reward/reaching_object: 2.0204
     Episode_Reward/lifting_object: 168.1302
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0698
          Episode_Reward/joint_vel: -0.0777
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 2.38s
                      Time elapsed: 00:50:58
                               ETA: 00:25:32

################################################################################
                     [1m Learning iteration 1333/2000 [0m                     

                       Computation: 17910 steps/s (collection: 5.350s, learning 0.139s)
             Mean action noise std: 3.24
          Mean value_function loss: 147.2656
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 79.9637
                       Mean reward: 864.59
               Mean episode length: 232.57
    Episode_Reward/reaching_object: 2.0155
     Episode_Reward/lifting_object: 167.5948
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0697
          Episode_Reward/joint_vel: -0.0776
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 5.49s
                      Time elapsed: 00:51:04
                               ETA: 00:25:32

################################################################################
                     [1m Learning iteration 1334/2000 [0m                     

                       Computation: 13449 steps/s (collection: 7.177s, learning 0.132s)
             Mean action noise std: 3.25
          Mean value_function loss: 151.0388
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 79.9807
                       Mean reward: 828.95
               Mean episode length: 223.37
    Episode_Reward/reaching_object: 2.0231
     Episode_Reward/lifting_object: 168.3532
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0698
          Episode_Reward/joint_vel: -0.0787
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 7.31s
                      Time elapsed: 00:51:11
                               ETA: 00:25:32

################################################################################
                     [1m Learning iteration 1335/2000 [0m                     

                       Computation: 13908 steps/s (collection: 6.953s, learning 0.115s)
             Mean action noise std: 3.25
          Mean value_function loss: 152.5346
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 79.9907
                       Mean reward: 787.31
               Mean episode length: 215.18
    Episode_Reward/reaching_object: 1.9896
     Episode_Reward/lifting_object: 164.3431
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0689
          Episode_Reward/joint_vel: -0.0782
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 7.07s
                      Time elapsed: 00:51:18
                               ETA: 00:25:32

################################################################################
                     [1m Learning iteration 1336/2000 [0m                     

                       Computation: 13571 steps/s (collection: 7.096s, learning 0.148s)
             Mean action noise std: 3.25
          Mean value_function loss: 126.2630
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 80.0062
                       Mean reward: 858.87
               Mean episode length: 231.48
    Episode_Reward/reaching_object: 2.0203
     Episode_Reward/lifting_object: 167.1345
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0700
          Episode_Reward/joint_vel: -0.0791
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 7.24s
                      Time elapsed: 00:51:26
                               ETA: 00:25:32

################################################################################
                     [1m Learning iteration 1337/2000 [0m                     

                       Computation: 13928 steps/s (collection: 6.931s, learning 0.127s)
             Mean action noise std: 3.25
          Mean value_function loss: 118.3953
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 80.0231
                       Mean reward: 872.11
               Mean episode length: 233.51
    Episode_Reward/reaching_object: 2.0876
     Episode_Reward/lifting_object: 173.4651
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0718
          Episode_Reward/joint_vel: -0.0800
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 7.06s
                      Time elapsed: 00:51:33
                               ETA: 00:25:32

################################################################################
                     [1m Learning iteration 1338/2000 [0m                     

                       Computation: 13624 steps/s (collection: 7.084s, learning 0.131s)
             Mean action noise std: 3.25
          Mean value_function loss: 135.8807
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 80.0391
                       Mean reward: 838.25
               Mean episode length: 224.13
    Episode_Reward/reaching_object: 2.0232
     Episode_Reward/lifting_object: 167.6563
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0700
          Episode_Reward/joint_vel: -0.0783
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 7.22s
                      Time elapsed: 00:51:40
                               ETA: 00:25:32

################################################################################
                     [1m Learning iteration 1339/2000 [0m                     

                       Computation: 13699 steps/s (collection: 7.062s, learning 0.114s)
             Mean action noise std: 3.25
          Mean value_function loss: 132.1726
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 80.0585
                       Mean reward: 897.64
               Mean episode length: 238.55
    Episode_Reward/reaching_object: 2.0567
     Episode_Reward/lifting_object: 170.4270
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0711
          Episode_Reward/joint_vel: -0.0800
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 7.18s
                      Time elapsed: 00:51:47
                               ETA: 00:25:32

################################################################################
                     [1m Learning iteration 1340/2000 [0m                     

                       Computation: 13651 steps/s (collection: 7.081s, learning 0.120s)
             Mean action noise std: 3.25
          Mean value_function loss: 155.9942
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 80.0685
                       Mean reward: 861.56
               Mean episode length: 230.39
    Episode_Reward/reaching_object: 2.0221
     Episode_Reward/lifting_object: 167.5589
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0700
          Episode_Reward/joint_vel: -0.0788
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 7.20s
                      Time elapsed: 00:51:54
                               ETA: 00:25:32

################################################################################
                     [1m Learning iteration 1341/2000 [0m                     

                       Computation: 12658 steps/s (collection: 7.648s, learning 0.118s)
             Mean action noise std: 3.26
          Mean value_function loss: 137.1870
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 80.0794
                       Mean reward: 877.35
               Mean episode length: 232.42
    Episode_Reward/reaching_object: 2.0747
     Episode_Reward/lifting_object: 172.0859
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0718
          Episode_Reward/joint_vel: -0.0805
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 7.77s
                      Time elapsed: 00:52:02
                               ETA: 00:25:33

################################################################################
                     [1m Learning iteration 1342/2000 [0m                     

                       Computation: 42382 steps/s (collection: 2.231s, learning 0.088s)
             Mean action noise std: 3.26
          Mean value_function loss: 155.7176
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 80.0969
                       Mean reward: 830.36
               Mean episode length: 223.89
    Episode_Reward/reaching_object: 2.0105
     Episode_Reward/lifting_object: 166.4833
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.0786
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 2.32s
                      Time elapsed: 00:52:04
                               ETA: 00:25:31

################################################################################
                     [1m Learning iteration 1343/2000 [0m                     

                       Computation: 42928 steps/s (collection: 2.182s, learning 0.108s)
             Mean action noise std: 3.26
          Mean value_function loss: 163.8265
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 80.1066
                       Mean reward: 795.67
               Mean episode length: 215.69
    Episode_Reward/reaching_object: 1.9985
     Episode_Reward/lifting_object: 164.9077
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0696
          Episode_Reward/joint_vel: -0.0786
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 2.29s
                      Time elapsed: 00:52:07
                               ETA: 00:25:28

################################################################################
                     [1m Learning iteration 1344/2000 [0m                     

                       Computation: 43996 steps/s (collection: 2.142s, learning 0.093s)
             Mean action noise std: 3.26
          Mean value_function loss: 108.2117
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 80.1173
                       Mean reward: 901.82
               Mean episode length: 239.51
    Episode_Reward/reaching_object: 2.0900
     Episode_Reward/lifting_object: 173.7467
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0724
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 2.23s
                      Time elapsed: 00:52:09
                               ETA: 00:25:26

################################################################################
                     [1m Learning iteration 1345/2000 [0m                     

                       Computation: 43698 steps/s (collection: 2.164s, learning 0.086s)
             Mean action noise std: 3.26
          Mean value_function loss: 115.1309
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 80.1263
                       Mean reward: 869.87
               Mean episode length: 231.99
    Episode_Reward/reaching_object: 2.0506
     Episode_Reward/lifting_object: 170.0802
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0713
          Episode_Reward/joint_vel: -0.0798
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 2.25s
                      Time elapsed: 00:52:11
                               ETA: 00:25:23

################################################################################
                     [1m Learning iteration 1346/2000 [0m                     

                       Computation: 43259 steps/s (collection: 2.184s, learning 0.089s)
             Mean action noise std: 3.26
          Mean value_function loss: 153.6457
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 80.1410
                       Mean reward: 853.09
               Mean episode length: 228.44
    Episode_Reward/reaching_object: 2.0358
     Episode_Reward/lifting_object: 168.9757
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0707
          Episode_Reward/joint_vel: -0.0796
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 2.27s
                      Time elapsed: 00:52:13
                               ETA: 00:25:21

################################################################################
                     [1m Learning iteration 1347/2000 [0m                     

                       Computation: 42367 steps/s (collection: 2.203s, learning 0.117s)
             Mean action noise std: 3.26
          Mean value_function loss: 172.2074
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 80.1562
                       Mean reward: 813.60
               Mean episode length: 217.95
    Episode_Reward/reaching_object: 1.9939
     Episode_Reward/lifting_object: 165.3904
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0696
          Episode_Reward/joint_vel: -0.0778
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 2.32s
                      Time elapsed: 00:52:16
                               ETA: 00:25:19

################################################################################
                     [1m Learning iteration 1348/2000 [0m                     

                       Computation: 42252 steps/s (collection: 2.210s, learning 0.117s)
             Mean action noise std: 3.26
          Mean value_function loss: 138.4995
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 80.1650
                       Mean reward: 841.44
               Mean episode length: 224.56
    Episode_Reward/reaching_object: 2.0696
     Episode_Reward/lifting_object: 171.9632
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0721
          Episode_Reward/joint_vel: -0.0803
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 2.33s
                      Time elapsed: 00:52:18
                               ETA: 00:25:16

################################################################################
                     [1m Learning iteration 1349/2000 [0m                     

                       Computation: 41855 steps/s (collection: 2.252s, learning 0.097s)
             Mean action noise std: 3.27
          Mean value_function loss: 155.6785
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 80.1760
                       Mean reward: 861.52
               Mean episode length: 230.48
    Episode_Reward/reaching_object: 2.0492
     Episode_Reward/lifting_object: 170.4953
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0714
          Episode_Reward/joint_vel: -0.0794
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 2.35s
                      Time elapsed: 00:52:20
                               ETA: 00:25:14

################################################################################
                     [1m Learning iteration 1350/2000 [0m                     

                       Computation: 43387 steps/s (collection: 2.164s, learning 0.102s)
             Mean action noise std: 3.27
          Mean value_function loss: 145.8665
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 80.1951
                       Mean reward: 848.57
               Mean episode length: 227.98
    Episode_Reward/reaching_object: 2.0264
     Episode_Reward/lifting_object: 168.1234
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0709
          Episode_Reward/joint_vel: -0.0803
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 2.27s
                      Time elapsed: 00:52:23
                               ETA: 00:25:12

################################################################################
                     [1m Learning iteration 1351/2000 [0m                     

                       Computation: 42717 steps/s (collection: 2.191s, learning 0.110s)
             Mean action noise std: 3.27
          Mean value_function loss: 146.9375
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 80.2146
                       Mean reward: 844.65
               Mean episode length: 228.16
    Episode_Reward/reaching_object: 2.0403
     Episode_Reward/lifting_object: 169.2335
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0712
          Episode_Reward/joint_vel: -0.0793
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 2.30s
                      Time elapsed: 00:52:25
                               ETA: 00:25:09

################################################################################
                     [1m Learning iteration 1352/2000 [0m                     

                       Computation: 43669 steps/s (collection: 2.155s, learning 0.096s)
             Mean action noise std: 3.27
          Mean value_function loss: 148.1387
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 80.2366
                       Mean reward: 823.06
               Mean episode length: 223.00
    Episode_Reward/reaching_object: 2.0260
     Episode_Reward/lifting_object: 168.3054
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0708
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 2.25s
                      Time elapsed: 00:52:27
                               ETA: 00:25:07

################################################################################
                     [1m Learning iteration 1353/2000 [0m                     

                       Computation: 43237 steps/s (collection: 2.165s, learning 0.109s)
             Mean action noise std: 3.27
          Mean value_function loss: 160.4703
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 80.2540
                       Mean reward: 854.61
               Mean episode length: 230.87
    Episode_Reward/reaching_object: 2.0130
     Episode_Reward/lifting_object: 166.7554
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0703
          Episode_Reward/joint_vel: -0.0783
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 2.27s
                      Time elapsed: 00:52:29
                               ETA: 00:25:05

################################################################################
                     [1m Learning iteration 1354/2000 [0m                     

                       Computation: 42204 steps/s (collection: 2.231s, learning 0.099s)
             Mean action noise std: 3.28
          Mean value_function loss: 140.6814
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 80.2688
                       Mean reward: 849.75
               Mean episode length: 227.29
    Episode_Reward/reaching_object: 2.0465
     Episode_Reward/lifting_object: 169.9642
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0717
          Episode_Reward/joint_vel: -0.0791
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 2.33s
                      Time elapsed: 00:52:32
                               ETA: 00:25:02

################################################################################
                     [1m Learning iteration 1355/2000 [0m                     

                       Computation: 42695 steps/s (collection: 2.205s, learning 0.098s)
             Mean action noise std: 3.28
          Mean value_function loss: 145.0494
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 80.2867
                       Mean reward: 836.87
               Mean episode length: 226.30
    Episode_Reward/reaching_object: 1.9858
     Episode_Reward/lifting_object: 164.2477
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0695
          Episode_Reward/joint_vel: -0.0775
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 2.30s
                      Time elapsed: 00:52:34
                               ETA: 00:25:00

################################################################################
                     [1m Learning iteration 1356/2000 [0m                     

                       Computation: 42983 steps/s (collection: 2.196s, learning 0.091s)
             Mean action noise std: 3.28
          Mean value_function loss: 127.9965
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 80.3016
                       Mean reward: 884.18
               Mean episode length: 234.78
    Episode_Reward/reaching_object: 2.0905
     Episode_Reward/lifting_object: 174.2905
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0730
          Episode_Reward/joint_vel: -0.0802
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 2.29s
                      Time elapsed: 00:52:36
                               ETA: 00:24:58

################################################################################
                     [1m Learning iteration 1357/2000 [0m                     

                       Computation: 42634 steps/s (collection: 2.205s, learning 0.101s)
             Mean action noise std: 3.28
          Mean value_function loss: 112.8477
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 80.3192
                       Mean reward: 879.52
               Mean episode length: 235.86
    Episode_Reward/reaching_object: 2.0683
     Episode_Reward/lifting_object: 172.0246
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0722
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 2.31s
                      Time elapsed: 00:52:39
                               ETA: 00:24:55

################################################################################
                     [1m Learning iteration 1358/2000 [0m                     

                       Computation: 41738 steps/s (collection: 2.248s, learning 0.108s)
             Mean action noise std: 3.28
          Mean value_function loss: 129.5130
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 80.3356
                       Mean reward: 869.74
               Mean episode length: 232.34
    Episode_Reward/reaching_object: 2.0629
     Episode_Reward/lifting_object: 171.3598
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0724
          Episode_Reward/joint_vel: -0.0796
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 2.36s
                      Time elapsed: 00:52:41
                               ETA: 00:24:53

################################################################################
                     [1m Learning iteration 1359/2000 [0m                     

                       Computation: 41128 steps/s (collection: 2.298s, learning 0.092s)
             Mean action noise std: 3.29
          Mean value_function loss: 116.5978
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 80.3545
                       Mean reward: 865.19
               Mean episode length: 232.22
    Episode_Reward/reaching_object: 2.0712
     Episode_Reward/lifting_object: 172.0810
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.0793
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 2.39s
                      Time elapsed: 00:52:43
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 1360/2000 [0m                     

                       Computation: 40761 steps/s (collection: 2.251s, learning 0.161s)
             Mean action noise std: 3.29
          Mean value_function loss: 122.9969
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 80.3692
                       Mean reward: 857.72
               Mean episode length: 230.17
    Episode_Reward/reaching_object: 2.0671
     Episode_Reward/lifting_object: 171.9887
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0727
          Episode_Reward/joint_vel: -0.0786
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 2.41s
                      Time elapsed: 00:52:46
                               ETA: 00:24:48

################################################################################
                     [1m Learning iteration 1361/2000 [0m                     

                       Computation: 40844 steps/s (collection: 2.310s, learning 0.097s)
             Mean action noise std: 3.29
          Mean value_function loss: 119.1037
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 80.3861
                       Mean reward: 840.38
               Mean episode length: 226.95
    Episode_Reward/reaching_object: 2.0675
     Episode_Reward/lifting_object: 171.6347
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0729
          Episode_Reward/joint_vel: -0.0797
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 2.41s
                      Time elapsed: 00:52:48
                               ETA: 00:24:46

################################################################################
                     [1m Learning iteration 1362/2000 [0m                     

                       Computation: 40376 steps/s (collection: 2.318s, learning 0.116s)
             Mean action noise std: 3.29
          Mean value_function loss: 110.6754
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 80.4059
                       Mean reward: 873.77
               Mean episode length: 233.48
    Episode_Reward/reaching_object: 2.0864
     Episode_Reward/lifting_object: 174.2155
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0734
          Episode_Reward/joint_vel: -0.0805
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 2.43s
                      Time elapsed: 00:52:51
                               ETA: 00:24:44

################################################################################
                     [1m Learning iteration 1363/2000 [0m                     

                       Computation: 42083 steps/s (collection: 2.198s, learning 0.138s)
             Mean action noise std: 3.29
          Mean value_function loss: 117.8880
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 80.4257
                       Mean reward: 902.93
               Mean episode length: 240.34
    Episode_Reward/reaching_object: 2.0724
     Episode_Reward/lifting_object: 173.2390
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0730
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 2.34s
                      Time elapsed: 00:52:53
                               ETA: 00:24:42

################################################################################
                     [1m Learning iteration 1364/2000 [0m                     

                       Computation: 40749 steps/s (collection: 2.206s, learning 0.207s)
             Mean action noise std: 3.30
          Mean value_function loss: 131.6031
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 80.4448
                       Mean reward: 906.48
               Mean episode length: 242.74
    Episode_Reward/reaching_object: 2.0480
     Episode_Reward/lifting_object: 170.6448
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0726
          Episode_Reward/joint_vel: -0.0785
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 2.41s
                      Time elapsed: 00:52:55
                               ETA: 00:24:39

################################################################################
                     [1m Learning iteration 1365/2000 [0m                     

                       Computation: 39447 steps/s (collection: 2.384s, learning 0.108s)
             Mean action noise std: 3.30
          Mean value_function loss: 111.2251
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 80.4566
                       Mean reward: 894.41
               Mean episode length: 237.89
    Episode_Reward/reaching_object: 2.0815
     Episode_Reward/lifting_object: 174.2233
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0789
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 2.49s
                      Time elapsed: 00:52:58
                               ETA: 00:24:37

################################################################################
                     [1m Learning iteration 1366/2000 [0m                     

                       Computation: 39558 steps/s (collection: 2.345s, learning 0.140s)
             Mean action noise std: 3.30
          Mean value_function loss: 114.3300
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 80.4715
                       Mean reward: 864.33
               Mean episode length: 229.78
    Episode_Reward/reaching_object: 2.0498
     Episode_Reward/lifting_object: 171.3121
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.0782
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 2.49s
                      Time elapsed: 00:53:00
                               ETA: 00:24:35

################################################################################
                     [1m Learning iteration 1367/2000 [0m                     

                       Computation: 38911 steps/s (collection: 2.403s, learning 0.123s)
             Mean action noise std: 3.30
          Mean value_function loss: 135.4316
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 80.4887
                       Mean reward: 865.76
               Mean episode length: 234.80
    Episode_Reward/reaching_object: 2.0256
     Episode_Reward/lifting_object: 169.0762
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0723
          Episode_Reward/joint_vel: -0.0776
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 2.53s
                      Time elapsed: 00:53:03
                               ETA: 00:24:33

################################################################################
                     [1m Learning iteration 1368/2000 [0m                     

                       Computation: 41475 steps/s (collection: 2.257s, learning 0.113s)
             Mean action noise std: 3.30
          Mean value_function loss: 124.7567
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 80.5060
                       Mean reward: 818.06
               Mean episode length: 220.56
    Episode_Reward/reaching_object: 2.0726
     Episode_Reward/lifting_object: 173.2639
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0787
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 2.37s
                      Time elapsed: 00:53:05
                               ETA: 00:24:30

################################################################################
                     [1m Learning iteration 1369/2000 [0m                     

                       Computation: 41552 steps/s (collection: 2.255s, learning 0.110s)
             Mean action noise std: 3.31
          Mean value_function loss: 120.4243
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 80.5294
                       Mean reward: 868.61
               Mean episode length: 233.17
    Episode_Reward/reaching_object: 2.0551
     Episode_Reward/lifting_object: 171.6939
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.0777
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 2.37s
                      Time elapsed: 00:53:08
                               ETA: 00:24:28

################################################################################
                     [1m Learning iteration 1370/2000 [0m                     

                       Computation: 41406 steps/s (collection: 2.232s, learning 0.143s)
             Mean action noise std: 3.31
          Mean value_function loss: 157.8746
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 80.5490
                       Mean reward: 883.87
               Mean episode length: 236.97
    Episode_Reward/reaching_object: 2.0602
     Episode_Reward/lifting_object: 172.1987
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0778
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 2.37s
                      Time elapsed: 00:53:10
                               ETA: 00:24:26

################################################################################
                     [1m Learning iteration 1371/2000 [0m                     

                       Computation: 41118 steps/s (collection: 2.268s, learning 0.123s)
             Mean action noise std: 3.31
          Mean value_function loss: 137.6565
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 80.5612
                       Mean reward: 842.45
               Mean episode length: 226.75
    Episode_Reward/reaching_object: 2.0159
     Episode_Reward/lifting_object: 168.3069
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0719
          Episode_Reward/joint_vel: -0.0764
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 2.39s
                      Time elapsed: 00:53:12
                               ETA: 00:24:23

################################################################################
                     [1m Learning iteration 1372/2000 [0m                     

                       Computation: 41077 steps/s (collection: 2.279s, learning 0.114s)
             Mean action noise std: 3.31
          Mean value_function loss: 133.9776
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 80.5828
                       Mean reward: 866.37
               Mean episode length: 233.45
    Episode_Reward/reaching_object: 2.0864
     Episode_Reward/lifting_object: 174.7220
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0741
          Episode_Reward/joint_vel: -0.0782
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 2.39s
                      Time elapsed: 00:53:15
                               ETA: 00:24:21

################################################################################
                     [1m Learning iteration 1373/2000 [0m                     

                       Computation: 41275 steps/s (collection: 2.267s, learning 0.115s)
             Mean action noise std: 3.31
          Mean value_function loss: 123.5610
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 80.6071
                       Mean reward: 870.63
               Mean episode length: 233.34
    Episode_Reward/reaching_object: 2.0391
     Episode_Reward/lifting_object: 170.1458
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0730
          Episode_Reward/joint_vel: -0.0782
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 2.38s
                      Time elapsed: 00:53:17
                               ETA: 00:24:19

################################################################################
                     [1m Learning iteration 1374/2000 [0m                     

                       Computation: 41905 steps/s (collection: 2.246s, learning 0.100s)
             Mean action noise std: 3.31
          Mean value_function loss: 142.5431
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 80.6244
                       Mean reward: 839.02
               Mean episode length: 226.53
    Episode_Reward/reaching_object: 2.0333
     Episode_Reward/lifting_object: 169.5902
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0727
          Episode_Reward/joint_vel: -0.0779
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 2.35s
                      Time elapsed: 00:53:20
                               ETA: 00:24:16

################################################################################
                     [1m Learning iteration 1375/2000 [0m                     

                       Computation: 42476 steps/s (collection: 2.203s, learning 0.112s)
             Mean action noise std: 3.32
          Mean value_function loss: 128.7315
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 80.6353
                       Mean reward: 838.64
               Mean episode length: 227.31
    Episode_Reward/reaching_object: 2.0319
     Episode_Reward/lifting_object: 169.1747
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0727
          Episode_Reward/joint_vel: -0.0777
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 2.31s
                      Time elapsed: 00:53:22
                               ETA: 00:24:14

################################################################################
                     [1m Learning iteration 1376/2000 [0m                     

                       Computation: 41646 steps/s (collection: 2.243s, learning 0.117s)
             Mean action noise std: 3.32
          Mean value_function loss: 126.2229
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 80.6488
                       Mean reward: 815.36
               Mean episode length: 222.40
    Episode_Reward/reaching_object: 2.0332
     Episode_Reward/lifting_object: 168.8099
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0731
          Episode_Reward/joint_vel: -0.0790
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 2.36s
                      Time elapsed: 00:53:24
                               ETA: 00:24:12

################################################################################
                     [1m Learning iteration 1377/2000 [0m                     

                       Computation: 38670 steps/s (collection: 2.415s, learning 0.128s)
             Mean action noise std: 3.32
          Mean value_function loss: 115.7448
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 80.6668
                       Mean reward: 823.57
               Mean episode length: 221.68
    Episode_Reward/reaching_object: 2.0645
     Episode_Reward/lifting_object: 172.2119
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0741
          Episode_Reward/joint_vel: -0.0800
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 2.54s
                      Time elapsed: 00:53:27
                               ETA: 00:24:10

################################################################################
                     [1m Learning iteration 1378/2000 [0m                     

                       Computation: 41973 steps/s (collection: 2.234s, learning 0.108s)
             Mean action noise std: 3.32
          Mean value_function loss: 146.7803
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 80.6859
                       Mean reward: 847.01
               Mean episode length: 226.80
    Episode_Reward/reaching_object: 2.0038
     Episode_Reward/lifting_object: 166.7878
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0721
          Episode_Reward/joint_vel: -0.0777
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 2.34s
                      Time elapsed: 00:53:29
                               ETA: 00:24:07

################################################################################
                     [1m Learning iteration 1379/2000 [0m                     

                       Computation: 41453 steps/s (collection: 2.274s, learning 0.097s)
             Mean action noise std: 3.33
          Mean value_function loss: 143.4701
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 80.7071
                       Mean reward: 868.59
               Mean episode length: 233.94
    Episode_Reward/reaching_object: 2.0340
     Episode_Reward/lifting_object: 169.4342
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0731
          Episode_Reward/joint_vel: -0.0784
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 2.37s
                      Time elapsed: 00:53:32
                               ETA: 00:24:05

################################################################################
                     [1m Learning iteration 1380/2000 [0m                     

                       Computation: 42125 steps/s (collection: 2.236s, learning 0.098s)
             Mean action noise std: 3.33
          Mean value_function loss: 141.3883
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 80.7302
                       Mean reward: 837.84
               Mean episode length: 225.13
    Episode_Reward/reaching_object: 2.0437
     Episode_Reward/lifting_object: 170.1457
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0786
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 2.33s
                      Time elapsed: 00:53:34
                               ETA: 00:24:03

################################################################################
                     [1m Learning iteration 1381/2000 [0m                     

                       Computation: 41742 steps/s (collection: 2.235s, learning 0.120s)
             Mean action noise std: 3.33
          Mean value_function loss: 111.3072
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 80.7424
                       Mean reward: 865.65
               Mean episode length: 233.28
    Episode_Reward/reaching_object: 2.0607
     Episode_Reward/lifting_object: 171.4740
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0743
          Episode_Reward/joint_vel: -0.0811
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 2.36s
                      Time elapsed: 00:53:36
                               ETA: 00:24:00

################################################################################
                     [1m Learning iteration 1382/2000 [0m                     

                       Computation: 43022 steps/s (collection: 2.193s, learning 0.092s)
             Mean action noise std: 3.33
          Mean value_function loss: 124.5774
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 80.7618
                       Mean reward: 891.63
               Mean episode length: 236.73
    Episode_Reward/reaching_object: 2.0425
     Episode_Reward/lifting_object: 169.9982
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0804
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 2.28s
                      Time elapsed: 00:53:38
                               ETA: 00:23:58

################################################################################
                     [1m Learning iteration 1383/2000 [0m                     

                       Computation: 40902 steps/s (collection: 2.267s, learning 0.136s)
             Mean action noise std: 3.33
          Mean value_function loss: 142.1091
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 80.7855
                       Mean reward: 874.53
               Mean episode length: 233.75
    Episode_Reward/reaching_object: 2.0587
     Episode_Reward/lifting_object: 171.2033
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0744
          Episode_Reward/joint_vel: -0.0821
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 2.40s
                      Time elapsed: 00:53:41
                               ETA: 00:23:56

################################################################################
                     [1m Learning iteration 1384/2000 [0m                     

                       Computation: 40651 steps/s (collection: 2.322s, learning 0.096s)
             Mean action noise std: 3.33
          Mean value_function loss: 117.7676
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 80.8068
                       Mean reward: 874.90
               Mean episode length: 233.55
    Episode_Reward/reaching_object: 2.0401
     Episode_Reward/lifting_object: 170.2582
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0801
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 2.42s
                      Time elapsed: 00:53:43
                               ETA: 00:23:53

################################################################################
                     [1m Learning iteration 1385/2000 [0m                     

                       Computation: 41964 steps/s (collection: 2.235s, learning 0.108s)
             Mean action noise std: 3.34
          Mean value_function loss: 153.9883
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 80.8149
                       Mean reward: 856.44
               Mean episode length: 231.22
    Episode_Reward/reaching_object: 2.0566
     Episode_Reward/lifting_object: 171.1521
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0748
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 2.34s
                      Time elapsed: 00:53:46
                               ETA: 00:23:51

################################################################################
                     [1m Learning iteration 1386/2000 [0m                     

                       Computation: 41123 steps/s (collection: 2.254s, learning 0.136s)
             Mean action noise std: 3.34
          Mean value_function loss: 172.0194
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 80.8311
                       Mean reward: 844.08
               Mean episode length: 225.35
    Episode_Reward/reaching_object: 2.0557
     Episode_Reward/lifting_object: 171.6468
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0748
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 2.39s
                      Time elapsed: 00:53:48
                               ETA: 00:23:49

################################################################################
                     [1m Learning iteration 1387/2000 [0m                     

                       Computation: 42362 steps/s (collection: 2.226s, learning 0.094s)
             Mean action noise std: 3.34
          Mean value_function loss: 172.6674
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 80.8514
                       Mean reward: 828.85
               Mean episode length: 223.97
    Episode_Reward/reaching_object: 1.9854
     Episode_Reward/lifting_object: 165.4054
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.0806
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 2.32s
                      Time elapsed: 00:53:50
                               ETA: 00:23:46

################################################################################
                     [1m Learning iteration 1388/2000 [0m                     

                       Computation: 42899 steps/s (collection: 2.204s, learning 0.087s)
             Mean action noise std: 3.34
          Mean value_function loss: 140.3045
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 80.8700
                       Mean reward: 866.07
               Mean episode length: 230.26
    Episode_Reward/reaching_object: 2.0262
     Episode_Reward/lifting_object: 169.7490
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0739
          Episode_Reward/joint_vel: -0.0805
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 2.29s
                      Time elapsed: 00:53:53
                               ETA: 00:23:44

################################################################################
                     [1m Learning iteration 1389/2000 [0m                     

                       Computation: 42586 steps/s (collection: 2.213s, learning 0.095s)
             Mean action noise std: 3.34
          Mean value_function loss: 146.5970
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 80.8896
                       Mean reward: 861.21
               Mean episode length: 231.37
    Episode_Reward/reaching_object: 2.0148
     Episode_Reward/lifting_object: 168.7497
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 2.31s
                      Time elapsed: 00:53:55
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 1390/2000 [0m                     

                       Computation: 42045 steps/s (collection: 2.220s, learning 0.118s)
             Mean action noise std: 3.35
          Mean value_function loss: 131.5908
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 80.9134
                       Mean reward: 888.54
               Mean episode length: 238.25
    Episode_Reward/reaching_object: 2.0399
     Episode_Reward/lifting_object: 171.1374
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0750
          Episode_Reward/joint_vel: -0.0819
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 2.34s
                      Time elapsed: 00:53:57
                               ETA: 00:23:39

################################################################################
                     [1m Learning iteration 1391/2000 [0m                     

                       Computation: 42598 steps/s (collection: 2.207s, learning 0.101s)
             Mean action noise std: 3.35
          Mean value_function loss: 148.2602
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 80.9312
                       Mean reward: 843.49
               Mean episode length: 225.71
    Episode_Reward/reaching_object: 1.9934
     Episode_Reward/lifting_object: 167.4950
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0740
          Episode_Reward/joint_vel: -0.0815
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 2.31s
                      Time elapsed: 00:54:00
                               ETA: 00:23:37

################################################################################
                     [1m Learning iteration 1392/2000 [0m                     

                       Computation: 42683 steps/s (collection: 2.211s, learning 0.092s)
             Mean action noise std: 3.35
          Mean value_function loss: 163.3516
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 80.9464
                       Mean reward: 828.70
               Mean episode length: 221.87
    Episode_Reward/reaching_object: 1.9858
     Episode_Reward/lifting_object: 166.9430
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0806
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 2.30s
                      Time elapsed: 00:54:02
                               ETA: 00:23:35

################################################################################
                     [1m Learning iteration 1393/2000 [0m                     

                       Computation: 42118 steps/s (collection: 2.201s, learning 0.133s)
             Mean action noise std: 3.35
          Mean value_function loss: 141.7135
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 80.9613
                       Mean reward: 856.91
               Mean episode length: 229.24
    Episode_Reward/reaching_object: 2.0087
     Episode_Reward/lifting_object: 168.6830
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0742
          Episode_Reward/joint_vel: -0.0813
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 2.33s
                      Time elapsed: 00:54:04
                               ETA: 00:23:32

################################################################################
                     [1m Learning iteration 1394/2000 [0m                     

                       Computation: 40969 steps/s (collection: 2.302s, learning 0.097s)
             Mean action noise std: 3.35
          Mean value_function loss: 163.6584
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 80.9809
                       Mean reward: 775.14
               Mean episode length: 210.27
    Episode_Reward/reaching_object: 1.9780
     Episode_Reward/lifting_object: 165.9273
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0735
          Episode_Reward/joint_vel: -0.0809
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 2.40s
                      Time elapsed: 00:54:07
                               ETA: 00:23:30

################################################################################
                     [1m Learning iteration 1395/2000 [0m                     

                       Computation: 42243 steps/s (collection: 2.224s, learning 0.103s)
             Mean action noise std: 3.36
          Mean value_function loss: 140.1764
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 80.9951
                       Mean reward: 864.73
               Mean episode length: 231.03
    Episode_Reward/reaching_object: 2.0300
     Episode_Reward/lifting_object: 170.8573
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0753
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 2.33s
                      Time elapsed: 00:54:09
                               ETA: 00:23:28

################################################################################
                     [1m Learning iteration 1396/2000 [0m                     

                       Computation: 41302 steps/s (collection: 2.295s, learning 0.086s)
             Mean action noise std: 3.36
          Mean value_function loss: 125.6826
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 81.0130
                       Mean reward: 843.26
               Mean episode length: 225.33
    Episode_Reward/reaching_object: 2.0491
     Episode_Reward/lifting_object: 172.4656
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0761
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 2.38s
                      Time elapsed: 00:54:11
                               ETA: 00:23:25

################################################################################
                     [1m Learning iteration 1397/2000 [0m                     

                       Computation: 43012 steps/s (collection: 2.189s, learning 0.097s)
             Mean action noise std: 3.36
          Mean value_function loss: 144.0437
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 81.0311
                       Mean reward: 863.93
               Mean episode length: 229.35
    Episode_Reward/reaching_object: 1.9742
     Episode_Reward/lifting_object: 165.5461
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0735
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 2.29s
                      Time elapsed: 00:54:14
                               ETA: 00:23:23

################################################################################
                     [1m Learning iteration 1398/2000 [0m                     

                       Computation: 43047 steps/s (collection: 2.195s, learning 0.089s)
             Mean action noise std: 3.36
          Mean value_function loss: 161.5386
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 81.0524
                       Mean reward: 852.94
               Mean episode length: 228.15
    Episode_Reward/reaching_object: 1.9732
     Episode_Reward/lifting_object: 165.0144
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0735
          Episode_Reward/joint_vel: -0.0810
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 2.28s
                      Time elapsed: 00:54:16
                               ETA: 00:23:21

################################################################################
                     [1m Learning iteration 1399/2000 [0m                     

                       Computation: 42912 steps/s (collection: 2.178s, learning 0.112s)
             Mean action noise std: 3.36
          Mean value_function loss: 146.4203
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 81.0656
                       Mean reward: 864.25
               Mean episode length: 229.87
    Episode_Reward/reaching_object: 2.0105
     Episode_Reward/lifting_object: 168.9484
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0748
          Episode_Reward/joint_vel: -0.0818
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 2.29s
                      Time elapsed: 00:54:18
                               ETA: 00:23:18

################################################################################
                     [1m Learning iteration 1400/2000 [0m                     

                       Computation: 42016 steps/s (collection: 2.220s, learning 0.120s)
             Mean action noise std: 3.36
          Mean value_function loss: 142.7189
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 81.0723
                       Mean reward: 872.70
               Mean episode length: 234.00
    Episode_Reward/reaching_object: 2.0220
     Episode_Reward/lifting_object: 169.5296
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0753
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 2.34s
                      Time elapsed: 00:54:21
                               ETA: 00:23:16

################################################################################
                     [1m Learning iteration 1401/2000 [0m                     

                       Computation: 41436 steps/s (collection: 2.252s, learning 0.121s)
             Mean action noise std: 3.37
          Mean value_function loss: 114.7134
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 81.0820
                       Mean reward: 917.08
               Mean episode length: 243.41
    Episode_Reward/reaching_object: 2.0560
     Episode_Reward/lifting_object: 173.1753
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0767
          Episode_Reward/joint_vel: -0.0843
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 2.37s
                      Time elapsed: 00:54:23
                               ETA: 00:23:14

################################################################################
                     [1m Learning iteration 1402/2000 [0m                     

                       Computation: 41251 steps/s (collection: 2.226s, learning 0.157s)
             Mean action noise std: 3.37
          Mean value_function loss: 191.9762
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 81.0982
                       Mean reward: 794.71
               Mean episode length: 213.63
    Episode_Reward/reaching_object: 1.9587
     Episode_Reward/lifting_object: 165.0056
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0795
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 2.38s
                      Time elapsed: 00:54:25
                               ETA: 00:23:11

################################################################################
                     [1m Learning iteration 1403/2000 [0m                     

                       Computation: 41444 steps/s (collection: 2.238s, learning 0.134s)
             Mean action noise std: 3.37
          Mean value_function loss: 142.9333
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 81.1193
                       Mean reward: 867.39
               Mean episode length: 233.19
    Episode_Reward/reaching_object: 2.0414
     Episode_Reward/lifting_object: 171.4375
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0764
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 2.37s
                      Time elapsed: 00:54:28
                               ETA: 00:23:09

################################################################################
                     [1m Learning iteration 1404/2000 [0m                     

                       Computation: 42500 steps/s (collection: 2.200s, learning 0.113s)
             Mean action noise std: 3.37
          Mean value_function loss: 127.1410
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 81.1365
                       Mean reward: 892.17
               Mean episode length: 238.24
    Episode_Reward/reaching_object: 2.0519
     Episode_Reward/lifting_object: 172.3523
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0766
          Episode_Reward/joint_vel: -0.0830
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 2.31s
                      Time elapsed: 00:54:30
                               ETA: 00:23:07

################################################################################
                     [1m Learning iteration 1405/2000 [0m                     

                       Computation: 41520 steps/s (collection: 2.253s, learning 0.114s)
             Mean action noise std: 3.37
          Mean value_function loss: 130.5932
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 81.1565
                       Mean reward: 880.82
               Mean episode length: 235.45
    Episode_Reward/reaching_object: 2.0284
     Episode_Reward/lifting_object: 170.4067
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0761
          Episode_Reward/joint_vel: -0.0830
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 2.37s
                      Time elapsed: 00:54:32
                               ETA: 00:23:05

################################################################################
                     [1m Learning iteration 1406/2000 [0m                     

                       Computation: 42834 steps/s (collection: 2.204s, learning 0.091s)
             Mean action noise std: 3.38
          Mean value_function loss: 120.1657
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 81.1773
                       Mean reward: 862.66
               Mean episode length: 230.51
    Episode_Reward/reaching_object: 2.0302
     Episode_Reward/lifting_object: 171.0919
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0761
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 2.29s
                      Time elapsed: 00:54:35
                               ETA: 00:23:02

################################################################################
                     [1m Learning iteration 1407/2000 [0m                     

                       Computation: 42902 steps/s (collection: 2.199s, learning 0.092s)
             Mean action noise std: 3.38
          Mean value_function loss: 141.4070
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 81.1961
                       Mean reward: 871.83
               Mean episode length: 232.12
    Episode_Reward/reaching_object: 1.9922
     Episode_Reward/lifting_object: 167.6542
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0751
          Episode_Reward/joint_vel: -0.0819
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 2.29s
                      Time elapsed: 00:54:37
                               ETA: 00:23:00

################################################################################
                     [1m Learning iteration 1408/2000 [0m                     

                       Computation: 42135 steps/s (collection: 2.220s, learning 0.113s)
             Mean action noise std: 3.38
          Mean value_function loss: 142.9072
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 81.2129
                       Mean reward: 861.36
               Mean episode length: 229.07
    Episode_Reward/reaching_object: 1.9851
     Episode_Reward/lifting_object: 167.2219
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0746
          Episode_Reward/joint_vel: -0.0809
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 2.33s
                      Time elapsed: 00:54:39
                               ETA: 00:22:58

################################################################################
                     [1m Learning iteration 1409/2000 [0m                     

                       Computation: 42182 steps/s (collection: 2.232s, learning 0.098s)
             Mean action noise std: 3.38
          Mean value_function loss: 144.7481
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 81.2321
                       Mean reward: 844.01
               Mean episode length: 224.32
    Episode_Reward/reaching_object: 1.9881
     Episode_Reward/lifting_object: 167.3805
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0747
          Episode_Reward/joint_vel: -0.0806
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 2.33s
                      Time elapsed: 00:54:42
                               ETA: 00:22:55

################################################################################
                     [1m Learning iteration 1410/2000 [0m                     

                       Computation: 42607 steps/s (collection: 2.208s, learning 0.099s)
             Mean action noise std: 3.38
          Mean value_function loss: 126.6459
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 81.2475
                       Mean reward: 886.83
               Mean episode length: 236.41
    Episode_Reward/reaching_object: 2.0677
     Episode_Reward/lifting_object: 174.8876
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0779
          Episode_Reward/joint_vel: -0.0843
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 2.31s
                      Time elapsed: 00:54:44
                               ETA: 00:22:53

################################################################################
                     [1m Learning iteration 1411/2000 [0m                     

                       Computation: 42990 steps/s (collection: 2.181s, learning 0.105s)
             Mean action noise std: 3.39
          Mean value_function loss: 120.1215
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 81.2675
                       Mean reward: 867.44
               Mean episode length: 231.84
    Episode_Reward/reaching_object: 2.0322
     Episode_Reward/lifting_object: 171.3757
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0768
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 2.29s
                      Time elapsed: 00:54:46
                               ETA: 00:22:51

################################################################################
                     [1m Learning iteration 1412/2000 [0m                     

                       Computation: 41805 steps/s (collection: 2.254s, learning 0.097s)
             Mean action noise std: 3.39
          Mean value_function loss: 115.3090
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 81.2842
                       Mean reward: 878.59
               Mean episode length: 233.79
    Episode_Reward/reaching_object: 2.0226
     Episode_Reward/lifting_object: 171.2710
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0765
          Episode_Reward/joint_vel: -0.0818
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 2.35s
                      Time elapsed: 00:54:49
                               ETA: 00:22:48

################################################################################
                     [1m Learning iteration 1413/2000 [0m                     

                       Computation: 43009 steps/s (collection: 2.187s, learning 0.099s)
             Mean action noise std: 3.39
          Mean value_function loss: 116.6354
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 81.2996
                       Mean reward: 869.03
               Mean episode length: 232.81
    Episode_Reward/reaching_object: 2.0258
     Episode_Reward/lifting_object: 171.3438
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0769
          Episode_Reward/joint_vel: -0.0828
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 2.29s
                      Time elapsed: 00:54:51
                               ETA: 00:22:46

################################################################################
                     [1m Learning iteration 1414/2000 [0m                     

                       Computation: 42702 steps/s (collection: 2.200s, learning 0.103s)
             Mean action noise std: 3.39
          Mean value_function loss: 128.7474
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 81.3189
                       Mean reward: 839.61
               Mean episode length: 225.42
    Episode_Reward/reaching_object: 2.0082
     Episode_Reward/lifting_object: 169.6341
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0763
          Episode_Reward/joint_vel: -0.0818
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 2.30s
                      Time elapsed: 00:54:53
                               ETA: 00:22:44

################################################################################
                     [1m Learning iteration 1415/2000 [0m                     

                       Computation: 41500 steps/s (collection: 2.231s, learning 0.138s)
             Mean action noise std: 3.39
          Mean value_function loss: 132.3569
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 81.3399
                       Mean reward: 881.65
               Mean episode length: 234.06
    Episode_Reward/reaching_object: 2.0205
     Episode_Reward/lifting_object: 171.9712
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0768
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 2.37s
                      Time elapsed: 00:54:55
                               ETA: 00:22:41

################################################################################
                     [1m Learning iteration 1416/2000 [0m                     

                       Computation: 42893 steps/s (collection: 2.203s, learning 0.088s)
             Mean action noise std: 3.40
          Mean value_function loss: 120.0668
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 81.3676
                       Mean reward: 879.07
               Mean episode length: 233.85
    Episode_Reward/reaching_object: 2.0302
     Episode_Reward/lifting_object: 172.4579
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0775
          Episode_Reward/joint_vel: -0.0831
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 2.29s
                      Time elapsed: 00:54:58
                               ETA: 00:22:39

################################################################################
                     [1m Learning iteration 1417/2000 [0m                     

                       Computation: 42624 steps/s (collection: 2.217s, learning 0.089s)
             Mean action noise std: 3.40
          Mean value_function loss: 135.4679
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 81.3870
                       Mean reward: 887.95
               Mean episode length: 237.89
    Episode_Reward/reaching_object: 2.0258
     Episode_Reward/lifting_object: 171.9876
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0774
          Episode_Reward/joint_vel: -0.0828
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 2.31s
                      Time elapsed: 00:55:00
                               ETA: 00:22:37

################################################################################
                     [1m Learning iteration 1418/2000 [0m                     

                       Computation: 42393 steps/s (collection: 2.214s, learning 0.105s)
             Mean action noise std: 3.40
          Mean value_function loss: 134.4930
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 81.4076
                       Mean reward: 846.54
               Mean episode length: 227.77
    Episode_Reward/reaching_object: 1.9934
     Episode_Reward/lifting_object: 169.1597
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0766
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 2.32s
                      Time elapsed: 00:55:02
                               ETA: 00:22:34

################################################################################
                     [1m Learning iteration 1419/2000 [0m                     

                       Computation: 42388 steps/s (collection: 2.220s, learning 0.099s)
             Mean action noise std: 3.40
          Mean value_function loss: 120.6979
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 81.4272
                       Mean reward: 882.19
               Mean episode length: 236.48
    Episode_Reward/reaching_object: 2.0293
     Episode_Reward/lifting_object: 172.6897
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0780
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 2.32s
                      Time elapsed: 00:55:05
                               ETA: 00:22:32

################################################################################
                     [1m Learning iteration 1420/2000 [0m                     

                       Computation: 41957 steps/s (collection: 2.228s, learning 0.115s)
             Mean action noise std: 3.41
          Mean value_function loss: 134.8032
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 81.4404
                       Mean reward: 852.23
               Mean episode length: 228.47
    Episode_Reward/reaching_object: 1.9919
     Episode_Reward/lifting_object: 169.2860
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0766
          Episode_Reward/joint_vel: -0.0810
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 2.34s
                      Time elapsed: 00:55:07
                               ETA: 00:22:30

################################################################################
                     [1m Learning iteration 1421/2000 [0m                     

                       Computation: 41975 steps/s (collection: 2.238s, learning 0.104s)
             Mean action noise std: 3.41
          Mean value_function loss: 145.2969
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 81.4541
                       Mean reward: 768.83
               Mean episode length: 209.47
    Episode_Reward/reaching_object: 1.9908
     Episode_Reward/lifting_object: 168.8665
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0768
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 2.34s
                      Time elapsed: 00:55:09
                               ETA: 00:22:27

################################################################################
                     [1m Learning iteration 1422/2000 [0m                     

                       Computation: 42174 steps/s (collection: 2.229s, learning 0.102s)
             Mean action noise std: 3.41
          Mean value_function loss: 153.6272
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 81.4699
                       Mean reward: 828.31
               Mean episode length: 224.29
    Episode_Reward/reaching_object: 1.9877
     Episode_Reward/lifting_object: 168.8448
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0767
          Episode_Reward/joint_vel: -0.0822
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 2.33s
                      Time elapsed: 00:55:12
                               ETA: 00:22:25

################################################################################
                     [1m Learning iteration 1423/2000 [0m                     

                       Computation: 42382 steps/s (collection: 2.222s, learning 0.098s)
             Mean action noise std: 3.41
          Mean value_function loss: 165.6788
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 81.4823
                       Mean reward: 819.63
               Mean episode length: 220.85
    Episode_Reward/reaching_object: 1.9892
     Episode_Reward/lifting_object: 168.8438
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0766
          Episode_Reward/joint_vel: -0.0825
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 2.32s
                      Time elapsed: 00:55:14
                               ETA: 00:22:23

################################################################################
                     [1m Learning iteration 1424/2000 [0m                     

                       Computation: 41448 steps/s (collection: 2.250s, learning 0.122s)
             Mean action noise std: 3.41
          Mean value_function loss: 116.8749
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 81.4992
                       Mean reward: 917.29
               Mean episode length: 242.34
    Episode_Reward/reaching_object: 2.0538
     Episode_Reward/lifting_object: 174.9650
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0787
          Episode_Reward/joint_vel: -0.0835
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 2.37s
                      Time elapsed: 00:55:16
                               ETA: 00:22:20

################################################################################
                     [1m Learning iteration 1425/2000 [0m                     

                       Computation: 42327 steps/s (collection: 2.227s, learning 0.095s)
             Mean action noise std: 3.41
          Mean value_function loss: 126.2665
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 81.5143
                       Mean reward: 887.86
               Mean episode length: 237.07
    Episode_Reward/reaching_object: 2.0202
     Episode_Reward/lifting_object: 171.8466
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0777
          Episode_Reward/joint_vel: -0.0823
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 2.32s
                      Time elapsed: 00:55:19
                               ETA: 00:22:18

################################################################################
                     [1m Learning iteration 1426/2000 [0m                     

                       Computation: 41762 steps/s (collection: 2.247s, learning 0.107s)
             Mean action noise std: 3.41
          Mean value_function loss: 130.0004
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 81.5277
                       Mean reward: 894.20
               Mean episode length: 238.53
    Episode_Reward/reaching_object: 2.0189
     Episode_Reward/lifting_object: 170.8287
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0777
          Episode_Reward/joint_vel: -0.0829
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 2.35s
                      Time elapsed: 00:55:21
                               ETA: 00:22:16

################################################################################
                     [1m Learning iteration 1427/2000 [0m                     

                       Computation: 42224 steps/s (collection: 2.199s, learning 0.129s)
             Mean action noise std: 3.42
          Mean value_function loss: 127.9462
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 81.5411
                       Mean reward: 858.01
               Mean episode length: 229.48
    Episode_Reward/reaching_object: 1.9906
     Episode_Reward/lifting_object: 168.7848
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0769
          Episode_Reward/joint_vel: -0.0818
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 2.33s
                      Time elapsed: 00:55:23
                               ETA: 00:22:13

################################################################################
                     [1m Learning iteration 1428/2000 [0m                     

                       Computation: 41724 steps/s (collection: 2.265s, learning 0.091s)
             Mean action noise std: 3.42
          Mean value_function loss: 143.3073
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 81.5579
                       Mean reward: 860.35
               Mean episode length: 230.35
    Episode_Reward/reaching_object: 1.9767
     Episode_Reward/lifting_object: 168.0800
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0761
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 2.36s
                      Time elapsed: 00:55:26
                               ETA: 00:22:11

################################################################################
                     [1m Learning iteration 1429/2000 [0m                     

                       Computation: 41505 steps/s (collection: 2.271s, learning 0.098s)
             Mean action noise std: 3.42
          Mean value_function loss: 137.1409
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 81.5701
                       Mean reward: 870.17
               Mean episode length: 232.03
    Episode_Reward/reaching_object: 2.0156
     Episode_Reward/lifting_object: 170.2483
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0779
          Episode_Reward/joint_vel: -0.0832
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 2.37s
                      Time elapsed: 00:55:28
                               ETA: 00:22:09

################################################################################
                     [1m Learning iteration 1430/2000 [0m                     

                       Computation: 39361 steps/s (collection: 2.349s, learning 0.149s)
             Mean action noise std: 3.42
          Mean value_function loss: 132.6078
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 81.5819
                       Mean reward: 888.13
               Mean episode length: 237.35
    Episode_Reward/reaching_object: 2.0630
     Episode_Reward/lifting_object: 174.8043
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0793
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 2.50s
                      Time elapsed: 00:55:31
                               ETA: 00:22:06

################################################################################
                     [1m Learning iteration 1431/2000 [0m                     

                       Computation: 40792 steps/s (collection: 2.299s, learning 0.111s)
             Mean action noise std: 3.42
          Mean value_function loss: 103.5206
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 81.5979
                       Mean reward: 906.19
               Mean episode length: 240.77
    Episode_Reward/reaching_object: 2.0105
     Episode_Reward/lifting_object: 169.6965
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0777
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 2.41s
                      Time elapsed: 00:55:33
                               ETA: 00:22:04

################################################################################
                     [1m Learning iteration 1432/2000 [0m                     

                       Computation: 38312 steps/s (collection: 2.416s, learning 0.150s)
             Mean action noise std: 3.43
          Mean value_function loss: 127.2954
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 81.6175
                       Mean reward: 854.43
               Mean episode length: 229.66
    Episode_Reward/reaching_object: 1.9955
     Episode_Reward/lifting_object: 168.1699
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 2.57s
                      Time elapsed: 00:55:36
                               ETA: 00:22:02

################################################################################
                     [1m Learning iteration 1433/2000 [0m                     

                       Computation: 40858 steps/s (collection: 2.287s, learning 0.119s)
             Mean action noise std: 3.43
          Mean value_function loss: 131.7116
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 81.6318
                       Mean reward: 883.76
               Mean episode length: 235.89
    Episode_Reward/reaching_object: 2.0336
     Episode_Reward/lifting_object: 171.4371
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0784
          Episode_Reward/joint_vel: -0.0830
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 2.41s
                      Time elapsed: 00:55:38
                               ETA: 00:22:00

################################################################################
                     [1m Learning iteration 1434/2000 [0m                     

                       Computation: 40902 steps/s (collection: 2.288s, learning 0.115s)
             Mean action noise std: 3.43
          Mean value_function loss: 142.4569
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 81.6476
                       Mean reward: 869.54
               Mean episode length: 232.24
    Episode_Reward/reaching_object: 2.0064
     Episode_Reward/lifting_object: 169.1552
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0775
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 2.40s
                      Time elapsed: 00:55:40
                               ETA: 00:21:57

################################################################################
                     [1m Learning iteration 1435/2000 [0m                     

                       Computation: 41711 steps/s (collection: 2.262s, learning 0.095s)
             Mean action noise std: 3.43
          Mean value_function loss: 134.4480
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 81.6695
                       Mean reward: 847.04
               Mean episode length: 228.17
    Episode_Reward/reaching_object: 2.0006
     Episode_Reward/lifting_object: 168.1459
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0774
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 2.36s
                      Time elapsed: 00:55:43
                               ETA: 00:21:55

################################################################################
                     [1m Learning iteration 1436/2000 [0m                     

                       Computation: 41527 steps/s (collection: 2.267s, learning 0.100s)
             Mean action noise std: 3.43
          Mean value_function loss: 150.8258
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 81.6917
                       Mean reward: 828.79
               Mean episode length: 222.67
    Episode_Reward/reaching_object: 1.9885
     Episode_Reward/lifting_object: 166.6873
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0769
          Episode_Reward/joint_vel: -0.0822
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 2.37s
                      Time elapsed: 00:55:45
                               ETA: 00:21:53

################################################################################
                     [1m Learning iteration 1437/2000 [0m                     

                       Computation: 41834 steps/s (collection: 2.241s, learning 0.109s)
             Mean action noise std: 3.44
          Mean value_function loss: 125.2181
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 81.7124
                       Mean reward: 902.60
               Mean episode length: 239.23
    Episode_Reward/reaching_object: 2.0413
     Episode_Reward/lifting_object: 171.8071
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0787
          Episode_Reward/joint_vel: -0.0836
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 2.35s
                      Time elapsed: 00:55:48
                               ETA: 00:21:50

################################################################################
                     [1m Learning iteration 1438/2000 [0m                     

                       Computation: 42187 steps/s (collection: 2.227s, learning 0.103s)
             Mean action noise std: 3.44
          Mean value_function loss: 144.9477
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 81.7274
                       Mean reward: 820.77
               Mean episode length: 223.02
    Episode_Reward/reaching_object: 1.9897
     Episode_Reward/lifting_object: 165.8637
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 2.33s
                      Time elapsed: 00:55:50
                               ETA: 00:21:48

################################################################################
                     [1m Learning iteration 1439/2000 [0m                     

                       Computation: 42143 steps/s (collection: 2.230s, learning 0.103s)
             Mean action noise std: 3.44
          Mean value_function loss: 123.7816
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 81.7407
                       Mean reward: 850.01
               Mean episode length: 228.30
    Episode_Reward/reaching_object: 2.0467
     Episode_Reward/lifting_object: 172.2416
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0790
          Episode_Reward/joint_vel: -0.0836
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 2.33s
                      Time elapsed: 00:55:52
                               ETA: 00:21:46

################################################################################
                     [1m Learning iteration 1440/2000 [0m                     

                       Computation: 41536 steps/s (collection: 2.247s, learning 0.120s)
             Mean action noise std: 3.44
          Mean value_function loss: 121.1906
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 81.7506
                       Mean reward: 867.54
               Mean episode length: 232.32
    Episode_Reward/reaching_object: 2.0206
     Episode_Reward/lifting_object: 170.0707
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0783
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 2.37s
                      Time elapsed: 00:55:55
                               ETA: 00:21:43

################################################################################
                     [1m Learning iteration 1441/2000 [0m                     

                       Computation: 42631 steps/s (collection: 2.215s, learning 0.091s)
             Mean action noise std: 3.44
          Mean value_function loss: 163.7422
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 81.7625
                       Mean reward: 836.70
               Mean episode length: 225.12
    Episode_Reward/reaching_object: 1.9758
     Episode_Reward/lifting_object: 165.7430
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0768
          Episode_Reward/joint_vel: -0.0822
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 2.31s
                      Time elapsed: 00:55:57
                               ETA: 00:21:41

################################################################################
                     [1m Learning iteration 1442/2000 [0m                     

                       Computation: 41400 steps/s (collection: 2.277s, learning 0.098s)
             Mean action noise std: 3.44
          Mean value_function loss: 143.7795
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 81.7771
                       Mean reward: 871.86
               Mean episode length: 233.65
    Episode_Reward/reaching_object: 2.0324
     Episode_Reward/lifting_object: 170.1779
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0791
          Episode_Reward/joint_vel: -0.0844
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 2.37s
                      Time elapsed: 00:55:59
                               ETA: 00:21:39

################################################################################
                     [1m Learning iteration 1443/2000 [0m                     

                       Computation: 41833 steps/s (collection: 2.232s, learning 0.118s)
             Mean action noise std: 3.45
          Mean value_function loss: 142.3530
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 81.7967
                       Mean reward: 873.42
               Mean episode length: 235.89
    Episode_Reward/reaching_object: 2.0357
     Episode_Reward/lifting_object: 170.2235
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0791
          Episode_Reward/joint_vel: -0.0854
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 2.35s
                      Time elapsed: 00:56:02
                               ETA: 00:21:36

################################################################################
                     [1m Learning iteration 1444/2000 [0m                     

                       Computation: 42115 steps/s (collection: 2.243s, learning 0.091s)
             Mean action noise std: 3.45
          Mean value_function loss: 149.0250
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 81.8153
                       Mean reward: 842.44
               Mean episode length: 226.67
    Episode_Reward/reaching_object: 1.9922
     Episode_Reward/lifting_object: 166.8783
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0778
          Episode_Reward/joint_vel: -0.0836
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 2.33s
                      Time elapsed: 00:56:04
                               ETA: 00:21:34

################################################################################
                     [1m Learning iteration 1445/2000 [0m                     

                       Computation: 42405 steps/s (collection: 2.229s, learning 0.089s)
             Mean action noise std: 3.45
          Mean value_function loss: 137.1654
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 81.8321
                       Mean reward: 847.18
               Mean episode length: 229.34
    Episode_Reward/reaching_object: 2.0212
     Episode_Reward/lifting_object: 169.0271
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0785
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 2.32s
                      Time elapsed: 00:56:06
                               ETA: 00:21:32

################################################################################
                     [1m Learning iteration 1446/2000 [0m                     

                       Computation: 42615 steps/s (collection: 2.216s, learning 0.091s)
             Mean action noise std: 3.45
          Mean value_function loss: 135.0409
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 81.8405
                       Mean reward: 864.13
               Mean episode length: 231.12
    Episode_Reward/reaching_object: 2.0376
     Episode_Reward/lifting_object: 170.9803
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0792
          Episode_Reward/joint_vel: -0.0845
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 2.31s
                      Time elapsed: 00:56:09
                               ETA: 00:21:29

################################################################################
                     [1m Learning iteration 1447/2000 [0m                     

                       Computation: 41616 steps/s (collection: 2.267s, learning 0.095s)
             Mean action noise std: 3.45
          Mean value_function loss: 158.6445
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 81.8507
                       Mean reward: 826.35
               Mean episode length: 221.28
    Episode_Reward/reaching_object: 2.0016
     Episode_Reward/lifting_object: 166.9218
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0779
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 2.36s
                      Time elapsed: 00:56:11
                               ETA: 00:21:27

################################################################################
                     [1m Learning iteration 1448/2000 [0m                     

                       Computation: 42450 steps/s (collection: 2.223s, learning 0.093s)
             Mean action noise std: 3.45
          Mean value_function loss: 170.9840
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 81.8734
                       Mean reward: 820.48
               Mean episode length: 219.17
    Episode_Reward/reaching_object: 2.0149
     Episode_Reward/lifting_object: 168.0696
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0783
          Episode_Reward/joint_vel: -0.0842
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 2.32s
                      Time elapsed: 00:56:13
                               ETA: 00:21:25

################################################################################
                     [1m Learning iteration 1449/2000 [0m                     

                       Computation: 41476 steps/s (collection: 2.281s, learning 0.090s)
             Mean action noise std: 3.46
          Mean value_function loss: 149.5158
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 81.8972
                       Mean reward: 830.07
               Mean episode length: 222.36
    Episode_Reward/reaching_object: 2.0182
     Episode_Reward/lifting_object: 168.8830
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0786
          Episode_Reward/joint_vel: -0.0842
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 2.37s
                      Time elapsed: 00:56:16
                               ETA: 00:21:22

################################################################################
                     [1m Learning iteration 1450/2000 [0m                     

                       Computation: 42338 steps/s (collection: 2.231s, learning 0.091s)
             Mean action noise std: 3.46
          Mean value_function loss: 157.5581
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 81.9148
                       Mean reward: 889.75
               Mean episode length: 237.54
    Episode_Reward/reaching_object: 2.0146
     Episode_Reward/lifting_object: 167.7037
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0786
          Episode_Reward/joint_vel: -0.0849
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 2.32s
                      Time elapsed: 00:56:18
                               ETA: 00:21:20

################################################################################
                     [1m Learning iteration 1451/2000 [0m                     

                       Computation: 40866 steps/s (collection: 2.309s, learning 0.097s)
             Mean action noise std: 3.46
          Mean value_function loss: 143.3627
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 81.9368
                       Mean reward: 837.66
               Mean episode length: 226.37
    Episode_Reward/reaching_object: 1.9985
     Episode_Reward/lifting_object: 166.3503
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0780
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 2.41s
                      Time elapsed: 00:56:20
                               ETA: 00:21:18

################################################################################
                     [1m Learning iteration 1452/2000 [0m                     

                       Computation: 42224 steps/s (collection: 2.237s, learning 0.092s)
             Mean action noise std: 3.46
          Mean value_function loss: 125.7365
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 81.9590
                       Mean reward: 883.56
               Mean episode length: 234.44
    Episode_Reward/reaching_object: 2.0534
     Episode_Reward/lifting_object: 171.7833
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0798
          Episode_Reward/joint_vel: -0.0859
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 2.33s
                      Time elapsed: 00:56:23
                               ETA: 00:21:15

################################################################################
                     [1m Learning iteration 1453/2000 [0m                     

                       Computation: 41888 steps/s (collection: 2.249s, learning 0.098s)
             Mean action noise std: 3.47
          Mean value_function loss: 143.7454
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 81.9779
                       Mean reward: 816.42
               Mean episode length: 219.50
    Episode_Reward/reaching_object: 1.9892
     Episode_Reward/lifting_object: 165.9078
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0776
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 2.35s
                      Time elapsed: 00:56:25
                               ETA: 00:21:13

################################################################################
                     [1m Learning iteration 1454/2000 [0m                     

                       Computation: 41628 steps/s (collection: 2.259s, learning 0.102s)
             Mean action noise std: 3.47
          Mean value_function loss: 159.2456
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 81.9929
                       Mean reward: 868.49
               Mean episode length: 231.75
    Episode_Reward/reaching_object: 2.0448
     Episode_Reward/lifting_object: 170.7940
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0796
          Episode_Reward/joint_vel: -0.0857
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 2.36s
                      Time elapsed: 00:56:27
                               ETA: 00:21:11

################################################################################
                     [1m Learning iteration 1455/2000 [0m                     

                       Computation: 41930 steps/s (collection: 2.226s, learning 0.118s)
             Mean action noise std: 3.47
          Mean value_function loss: 154.1622
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 82.0072
                       Mean reward: 810.81
               Mean episode length: 218.84
    Episode_Reward/reaching_object: 2.0181
     Episode_Reward/lifting_object: 168.0309
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0787
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 2.34s
                      Time elapsed: 00:56:30
                               ETA: 00:21:08

################################################################################
                     [1m Learning iteration 1456/2000 [0m                     

                       Computation: 39622 steps/s (collection: 2.382s, learning 0.099s)
             Mean action noise std: 3.47
          Mean value_function loss: 151.4312
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 82.0224
                       Mean reward: 849.45
               Mean episode length: 227.76
    Episode_Reward/reaching_object: 2.0339
     Episode_Reward/lifting_object: 169.7098
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0791
          Episode_Reward/joint_vel: -0.0855
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 2.48s
                      Time elapsed: 00:56:32
                               ETA: 00:21:06

################################################################################
                     [1m Learning iteration 1457/2000 [0m                     

                       Computation: 41113 steps/s (collection: 2.239s, learning 0.152s)
             Mean action noise std: 3.47
          Mean value_function loss: 136.9969
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 82.0299
                       Mean reward: 839.27
               Mean episode length: 225.05
    Episode_Reward/reaching_object: 2.0394
     Episode_Reward/lifting_object: 169.8572
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0795
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 2.39s
                      Time elapsed: 00:56:35
                               ETA: 00:21:04

################################################################################
                     [1m Learning iteration 1458/2000 [0m                     

                       Computation: 39993 steps/s (collection: 2.322s, learning 0.136s)
             Mean action noise std: 3.47
          Mean value_function loss: 142.7142
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 82.0412
                       Mean reward: 858.38
               Mean episode length: 227.84
    Episode_Reward/reaching_object: 2.0511
     Episode_Reward/lifting_object: 171.1158
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0800
          Episode_Reward/joint_vel: -0.0857
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 2.46s
                      Time elapsed: 00:56:37
                               ETA: 00:21:02

################################################################################
                     [1m Learning iteration 1459/2000 [0m                     

                       Computation: 41826 steps/s (collection: 2.230s, learning 0.121s)
             Mean action noise std: 3.47
          Mean value_function loss: 154.1255
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 82.0564
                       Mean reward: 814.35
               Mean episode length: 219.67
    Episode_Reward/reaching_object: 1.9986
     Episode_Reward/lifting_object: 166.1249
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0783
          Episode_Reward/joint_vel: -0.0843
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 2.35s
                      Time elapsed: 00:56:39
                               ETA: 00:20:59

################################################################################
                     [1m Learning iteration 1460/2000 [0m                     

                       Computation: 42336 steps/s (collection: 2.218s, learning 0.104s)
             Mean action noise std: 3.48
          Mean value_function loss: 149.1712
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 82.0747
                       Mean reward: 830.77
               Mean episode length: 223.20
    Episode_Reward/reaching_object: 2.0296
     Episode_Reward/lifting_object: 168.3983
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0794
          Episode_Reward/joint_vel: -0.0848
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 2.32s
                      Time elapsed: 00:56:42
                               ETA: 00:20:57

################################################################################
                     [1m Learning iteration 1461/2000 [0m                     

                       Computation: 41873 steps/s (collection: 2.235s, learning 0.113s)
             Mean action noise std: 3.48
          Mean value_function loss: 133.1912
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 82.0907
                       Mean reward: 875.89
               Mean episode length: 232.51
    Episode_Reward/reaching_object: 2.0761
     Episode_Reward/lifting_object: 172.8528
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0812
          Episode_Reward/joint_vel: -0.0873
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 2.35s
                      Time elapsed: 00:56:44
                               ETA: 00:20:55

################################################################################
                     [1m Learning iteration 1462/2000 [0m                     

                       Computation: 41575 steps/s (collection: 2.271s, learning 0.094s)
             Mean action noise std: 3.48
          Mean value_function loss: 133.0697
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 82.1078
                       Mean reward: 839.61
               Mean episode length: 225.60
    Episode_Reward/reaching_object: 2.0375
     Episode_Reward/lifting_object: 169.7601
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0798
          Episode_Reward/joint_vel: -0.0854
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 2.36s
                      Time elapsed: 00:56:46
                               ETA: 00:20:52

################################################################################
                     [1m Learning iteration 1463/2000 [0m                     

                       Computation: 41540 steps/s (collection: 2.254s, learning 0.113s)
             Mean action noise std: 3.48
          Mean value_function loss: 138.5656
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 82.1171
                       Mean reward: 890.89
               Mean episode length: 237.03
    Episode_Reward/reaching_object: 2.0435
     Episode_Reward/lifting_object: 170.0651
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0800
          Episode_Reward/joint_vel: -0.0861
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 2.37s
                      Time elapsed: 00:56:49
                               ETA: 00:20:50

################################################################################
                     [1m Learning iteration 1464/2000 [0m                     

                       Computation: 41926 steps/s (collection: 2.238s, learning 0.107s)
             Mean action noise std: 3.48
          Mean value_function loss: 131.6692
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 82.1327
                       Mean reward: 864.54
               Mean episode length: 232.15
    Episode_Reward/reaching_object: 2.0604
     Episode_Reward/lifting_object: 171.4904
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0809
          Episode_Reward/joint_vel: -0.0856
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 2.34s
                      Time elapsed: 00:56:51
                               ETA: 00:20:48

################################################################################
                     [1m Learning iteration 1465/2000 [0m                     

                       Computation: 42305 steps/s (collection: 2.229s, learning 0.095s)
             Mean action noise std: 3.49
          Mean value_function loss: 140.8787
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 82.1505
                       Mean reward: 838.03
               Mean episode length: 226.06
    Episode_Reward/reaching_object: 2.0552
     Episode_Reward/lifting_object: 170.7982
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0809
          Episode_Reward/joint_vel: -0.0871
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 2.32s
                      Time elapsed: 00:56:53
                               ETA: 00:20:45

################################################################################
                     [1m Learning iteration 1466/2000 [0m                     

                       Computation: 42180 steps/s (collection: 2.231s, learning 0.100s)
             Mean action noise std: 3.49
          Mean value_function loss: 138.3120
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 82.1671
                       Mean reward: 863.36
               Mean episode length: 231.06
    Episode_Reward/reaching_object: 2.0390
     Episode_Reward/lifting_object: 169.4647
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0805
          Episode_Reward/joint_vel: -0.0864
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 2.33s
                      Time elapsed: 00:56:56
                               ETA: 00:20:43

################################################################################
                     [1m Learning iteration 1467/2000 [0m                     

                       Computation: 41175 steps/s (collection: 2.230s, learning 0.157s)
             Mean action noise std: 3.49
          Mean value_function loss: 130.5641
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 82.1765
                       Mean reward: 903.91
               Mean episode length: 239.20
    Episode_Reward/reaching_object: 2.0479
     Episode_Reward/lifting_object: 171.1562
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0805
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 2.39s
                      Time elapsed: 00:56:58
                               ETA: 00:20:41

################################################################################
                     [1m Learning iteration 1468/2000 [0m                     

                       Computation: 42546 steps/s (collection: 2.204s, learning 0.107s)
             Mean action noise std: 3.49
          Mean value_function loss: 151.9357
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 82.1813
                       Mean reward: 829.51
               Mean episode length: 225.00
    Episode_Reward/reaching_object: 2.0136
     Episode_Reward/lifting_object: 167.1252
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0796
          Episode_Reward/joint_vel: -0.0854
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 2.31s
                      Time elapsed: 00:57:00
                               ETA: 00:20:38

################################################################################
                     [1m Learning iteration 1469/2000 [0m                     

                       Computation: 42477 steps/s (collection: 2.213s, learning 0.102s)
             Mean action noise std: 3.49
          Mean value_function loss: 133.8927
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 82.1873
                       Mean reward: 856.30
               Mean episode length: 229.33
    Episode_Reward/reaching_object: 2.0153
     Episode_Reward/lifting_object: 167.9730
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0796
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 2.31s
                      Time elapsed: 00:57:03
                               ETA: 00:20:36

################################################################################
                     [1m Learning iteration 1470/2000 [0m                     

                       Computation: 42277 steps/s (collection: 2.230s, learning 0.095s)
             Mean action noise std: 3.49
          Mean value_function loss: 110.8618
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 82.1999
                       Mean reward: 869.28
               Mean episode length: 231.83
    Episode_Reward/reaching_object: 2.1109
     Episode_Reward/lifting_object: 176.4238
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0830
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 2.33s
                      Time elapsed: 00:57:05
                               ETA: 00:20:34

################################################################################
                     [1m Learning iteration 1471/2000 [0m                     

                       Computation: 42163 steps/s (collection: 2.229s, learning 0.103s)
             Mean action noise std: 3.49
          Mean value_function loss: 134.1810
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 82.2197
                       Mean reward: 873.78
               Mean episode length: 233.79
    Episode_Reward/reaching_object: 2.0265
     Episode_Reward/lifting_object: 168.5241
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0801
          Episode_Reward/joint_vel: -0.0860
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 2.33s
                      Time elapsed: 00:57:07
                               ETA: 00:20:31

################################################################################
                     [1m Learning iteration 1472/2000 [0m                     

                       Computation: 41949 steps/s (collection: 2.246s, learning 0.098s)
             Mean action noise std: 3.50
          Mean value_function loss: 158.3272
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 82.2385
                       Mean reward: 791.52
               Mean episode length: 213.59
    Episode_Reward/reaching_object: 2.0110
     Episode_Reward/lifting_object: 167.5656
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0797
          Episode_Reward/joint_vel: -0.0857
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 2.34s
                      Time elapsed: 00:57:10
                               ETA: 00:20:29

################################################################################
                     [1m Learning iteration 1473/2000 [0m                     

                       Computation: 42162 steps/s (collection: 2.239s, learning 0.092s)
             Mean action noise std: 3.50
          Mean value_function loss: 131.3671
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 82.2579
                       Mean reward: 807.38
               Mean episode length: 218.83
    Episode_Reward/reaching_object: 2.0179
     Episode_Reward/lifting_object: 168.1157
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0799
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 2.33s
                      Time elapsed: 00:57:12
                               ETA: 00:20:27

################################################################################
                     [1m Learning iteration 1474/2000 [0m                     

                       Computation: 41529 steps/s (collection: 2.258s, learning 0.110s)
             Mean action noise std: 3.50
          Mean value_function loss: 118.1888
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 82.2776
                       Mean reward: 876.08
               Mean episode length: 232.75
    Episode_Reward/reaching_object: 2.0982
     Episode_Reward/lifting_object: 175.1049
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0831
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 2.37s
                      Time elapsed: 00:57:14
                               ETA: 00:20:24

################################################################################
                     [1m Learning iteration 1475/2000 [0m                     

                       Computation: 41551 steps/s (collection: 2.263s, learning 0.103s)
             Mean action noise std: 3.50
          Mean value_function loss: 130.9328
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 82.2933
                       Mean reward: 839.91
               Mean episode length: 224.95
    Episode_Reward/reaching_object: 2.0364
     Episode_Reward/lifting_object: 169.5681
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0806
          Episode_Reward/joint_vel: -0.0856
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 2.37s
                      Time elapsed: 00:57:17
                               ETA: 00:20:22

################################################################################
                     [1m Learning iteration 1476/2000 [0m                     

                       Computation: 42457 steps/s (collection: 2.228s, learning 0.087s)
             Mean action noise std: 3.50
          Mean value_function loss: 135.6226
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 82.3120
                       Mean reward: 852.40
               Mean episode length: 226.84
    Episode_Reward/reaching_object: 2.0555
     Episode_Reward/lifting_object: 171.4269
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0814
          Episode_Reward/joint_vel: -0.0856
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 2.32s
                      Time elapsed: 00:57:19
                               ETA: 00:20:20

################################################################################
                     [1m Learning iteration 1477/2000 [0m                     

                       Computation: 42034 steps/s (collection: 2.248s, learning 0.091s)
             Mean action noise std: 3.51
          Mean value_function loss: 138.4134
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 82.3275
                       Mean reward: 832.10
               Mean episode length: 222.09
    Episode_Reward/reaching_object: 2.0314
     Episode_Reward/lifting_object: 169.0630
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0804
          Episode_Reward/joint_vel: -0.0850
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 2.34s
                      Time elapsed: 00:57:22
                               ETA: 00:20:17

################################################################################
                     [1m Learning iteration 1478/2000 [0m                     

                       Computation: 41310 steps/s (collection: 2.284s, learning 0.096s)
             Mean action noise std: 3.51
          Mean value_function loss: 139.4888
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 82.3421
                       Mean reward: 861.09
               Mean episode length: 230.26
    Episode_Reward/reaching_object: 2.0044
     Episode_Reward/lifting_object: 166.0347
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0797
          Episode_Reward/joint_vel: -0.0842
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 2.38s
                      Time elapsed: 00:57:24
                               ETA: 00:20:15

################################################################################
                     [1m Learning iteration 1479/2000 [0m                     

                       Computation: 40576 steps/s (collection: 2.249s, learning 0.174s)
             Mean action noise std: 3.51
          Mean value_function loss: 122.1764
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 82.3649
                       Mean reward: 869.89
               Mean episode length: 231.37
    Episode_Reward/reaching_object: 2.0364
     Episode_Reward/lifting_object: 170.1592
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0807
          Episode_Reward/joint_vel: -0.0844
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 2.42s
                      Time elapsed: 00:57:26
                               ETA: 00:20:13

################################################################################
                     [1m Learning iteration 1480/2000 [0m                     

                       Computation: 41336 steps/s (collection: 2.280s, learning 0.099s)
             Mean action noise std: 3.51
          Mean value_function loss: 141.6981
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 82.3877
                       Mean reward: 879.29
               Mean episode length: 232.75
    Episode_Reward/reaching_object: 2.0158
     Episode_Reward/lifting_object: 167.5472
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0800
          Episode_Reward/joint_vel: -0.0830
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 2.38s
                      Time elapsed: 00:57:29
                               ETA: 00:20:11

################################################################################
                     [1m Learning iteration 1481/2000 [0m                     

                       Computation: 41916 steps/s (collection: 2.243s, learning 0.103s)
             Mean action noise std: 3.51
          Mean value_function loss: 145.5264
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 82.4037
                       Mean reward: 857.44
               Mean episode length: 228.96
    Episode_Reward/reaching_object: 2.0127
     Episode_Reward/lifting_object: 166.7035
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0801
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 2.35s
                      Time elapsed: 00:57:31
                               ETA: 00:20:08

################################################################################
                     [1m Learning iteration 1482/2000 [0m                     

                       Computation: 42161 steps/s (collection: 2.236s, learning 0.096s)
             Mean action noise std: 3.52
          Mean value_function loss: 111.3078
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 82.4181
                       Mean reward: 882.39
               Mean episode length: 235.58
    Episode_Reward/reaching_object: 2.0781
     Episode_Reward/lifting_object: 172.8688
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0826
          Episode_Reward/joint_vel: -0.0856
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 2.33s
                      Time elapsed: 00:57:33
                               ETA: 00:20:06

################################################################################
                     [1m Learning iteration 1483/2000 [0m                     

                       Computation: 42573 steps/s (collection: 2.220s, learning 0.090s)
             Mean action noise std: 3.52
          Mean value_function loss: 134.9098
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 82.4314
                       Mean reward: 867.41
               Mean episode length: 231.31
    Episode_Reward/reaching_object: 2.0015
     Episode_Reward/lifting_object: 165.5100
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0799
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 2.31s
                      Time elapsed: 00:57:36
                               ETA: 00:20:04

################################################################################
                     [1m Learning iteration 1484/2000 [0m                     

                       Computation: 40167 steps/s (collection: 2.344s, learning 0.103s)
             Mean action noise std: 3.52
          Mean value_function loss: 152.9417
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 82.4526
                       Mean reward: 868.08
               Mean episode length: 231.37
    Episode_Reward/reaching_object: 2.0220
     Episode_Reward/lifting_object: 167.7238
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0805
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 2.45s
                      Time elapsed: 00:57:38
                               ETA: 00:20:01

################################################################################
                     [1m Learning iteration 1485/2000 [0m                     

                       Computation: 41522 steps/s (collection: 2.267s, learning 0.101s)
             Mean action noise std: 3.52
          Mean value_function loss: 131.2745
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 82.4718
                       Mean reward: 875.97
               Mean episode length: 234.29
    Episode_Reward/reaching_object: 2.0444
     Episode_Reward/lifting_object: 168.9240
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0815
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 2.37s
                      Time elapsed: 00:57:40
                               ETA: 00:19:59

################################################################################
                     [1m Learning iteration 1486/2000 [0m                     

                       Computation: 41337 steps/s (collection: 2.280s, learning 0.098s)
             Mean action noise std: 3.52
          Mean value_function loss: 136.4683
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 82.4839
                       Mean reward: 887.16
               Mean episode length: 236.65
    Episode_Reward/reaching_object: 2.0737
     Episode_Reward/lifting_object: 172.0199
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0826
          Episode_Reward/joint_vel: -0.0851
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 2.38s
                      Time elapsed: 00:57:43
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 1487/2000 [0m                     

                       Computation: 40096 steps/s (collection: 2.324s, learning 0.128s)
             Mean action noise std: 3.52
          Mean value_function loss: 130.9085
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 82.4918
                       Mean reward: 852.59
               Mean episode length: 227.48
    Episode_Reward/reaching_object: 2.0551
     Episode_Reward/lifting_object: 170.7364
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0821
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 2.45s
                      Time elapsed: 00:57:45
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 1488/2000 [0m                     

                       Computation: 41006 steps/s (collection: 2.280s, learning 0.118s)
             Mean action noise std: 3.53
          Mean value_function loss: 153.0930
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 82.5058
                       Mean reward: 869.05
               Mean episode length: 233.47
    Episode_Reward/reaching_object: 2.0006
     Episode_Reward/lifting_object: 165.7570
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0801
          Episode_Reward/joint_vel: -0.0822
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 2.40s
                      Time elapsed: 00:57:48
                               ETA: 00:19:52

################################################################################
                     [1m Learning iteration 1489/2000 [0m                     

                       Computation: 41380 steps/s (collection: 2.243s, learning 0.133s)
             Mean action noise std: 3.53
          Mean value_function loss: 134.7050
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 82.5235
                       Mean reward: 819.46
               Mean episode length: 220.36
    Episode_Reward/reaching_object: 2.0331
     Episode_Reward/lifting_object: 169.3327
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0816
          Episode_Reward/joint_vel: -0.0842
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 2.38s
                      Time elapsed: 00:57:50
                               ETA: 00:19:50

################################################################################
                     [1m Learning iteration 1490/2000 [0m                     

                       Computation: 41569 steps/s (collection: 2.277s, learning 0.088s)
             Mean action noise std: 3.53
          Mean value_function loss: 154.6071
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 82.5352
                       Mean reward: 826.95
               Mean episode length: 223.71
    Episode_Reward/reaching_object: 2.0469
     Episode_Reward/lifting_object: 170.2193
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0821
          Episode_Reward/joint_vel: -0.0853
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 2.36s
                      Time elapsed: 00:57:52
                               ETA: 00:19:47

################################################################################
                     [1m Learning iteration 1491/2000 [0m                     

                       Computation: 42057 steps/s (collection: 2.244s, learning 0.093s)
             Mean action noise std: 3.53
          Mean value_function loss: 133.1498
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 82.5462
                       Mean reward: 884.62
               Mean episode length: 235.04
    Episode_Reward/reaching_object: 2.0575
     Episode_Reward/lifting_object: 171.7153
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0824
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 2.34s
                      Time elapsed: 00:57:55
                               ETA: 00:19:45

################################################################################
                     [1m Learning iteration 1492/2000 [0m                     

                       Computation: 41718 steps/s (collection: 2.229s, learning 0.127s)
             Mean action noise std: 3.53
          Mean value_function loss: 119.7773
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 82.5569
                       Mean reward: 910.15
               Mean episode length: 240.48
    Episode_Reward/reaching_object: 2.0783
     Episode_Reward/lifting_object: 173.1438
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0838
          Episode_Reward/joint_vel: -0.0860
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 2.36s
                      Time elapsed: 00:57:57
                               ETA: 00:19:43

################################################################################
                     [1m Learning iteration 1493/2000 [0m                     

                       Computation: 41140 steps/s (collection: 2.230s, learning 0.159s)
             Mean action noise std: 3.53
          Mean value_function loss: 134.8612
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 82.5701
                       Mean reward: 879.87
               Mean episode length: 235.00
    Episode_Reward/reaching_object: 2.0764
     Episode_Reward/lifting_object: 173.5700
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0837
          Episode_Reward/joint_vel: -0.0857
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 2.39s
                      Time elapsed: 00:58:00
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 1494/2000 [0m                     

                       Computation: 42092 steps/s (collection: 2.242s, learning 0.094s)
             Mean action noise std: 3.54
          Mean value_function loss: 159.5707
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 82.5868
                       Mean reward: 875.00
               Mean episode length: 233.43
    Episode_Reward/reaching_object: 2.0368
     Episode_Reward/lifting_object: 170.5610
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0825
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 2.34s
                      Time elapsed: 00:58:02
                               ETA: 00:19:38

################################################################################
                     [1m Learning iteration 1495/2000 [0m                     

                       Computation: 40797 steps/s (collection: 2.295s, learning 0.115s)
             Mean action noise std: 3.54
          Mean value_function loss: 125.2854
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 82.6031
                       Mean reward: 848.86
               Mean episode length: 227.39
    Episode_Reward/reaching_object: 2.0546
     Episode_Reward/lifting_object: 172.2546
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0832
          Episode_Reward/joint_vel: -0.0852
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 2.41s
                      Time elapsed: 00:58:04
                               ETA: 00:19:36

################################################################################
                     [1m Learning iteration 1496/2000 [0m                     

                       Computation: 42290 steps/s (collection: 2.217s, learning 0.107s)
             Mean action noise std: 3.54
          Mean value_function loss: 149.9901
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 82.6200
                       Mean reward: 839.60
               Mean episode length: 225.05
    Episode_Reward/reaching_object: 2.0277
     Episode_Reward/lifting_object: 170.3774
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0822
          Episode_Reward/joint_vel: -0.0830
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 2.32s
                      Time elapsed: 00:58:07
                               ETA: 00:19:34

################################################################################
                     [1m Learning iteration 1497/2000 [0m                     

                       Computation: 41985 steps/s (collection: 2.250s, learning 0.091s)
             Mean action noise std: 3.54
          Mean value_function loss: 148.2585
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 82.6322
                       Mean reward: 827.55
               Mean episode length: 224.61
    Episode_Reward/reaching_object: 2.0244
     Episode_Reward/lifting_object: 169.0675
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0823
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 2.34s
                      Time elapsed: 00:58:09
                               ETA: 00:19:31

################################################################################
                     [1m Learning iteration 1498/2000 [0m                     

                       Computation: 42403 steps/s (collection: 2.230s, learning 0.088s)
             Mean action noise std: 3.54
          Mean value_function loss: 125.0364
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 82.6426
                       Mean reward: 877.77
               Mean episode length: 233.99
    Episode_Reward/reaching_object: 2.1014
     Episode_Reward/lifting_object: 176.2275
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0852
          Episode_Reward/joint_vel: -0.0858
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 2.32s
                      Time elapsed: 00:58:11
                               ETA: 00:19:29

################################################################################
                     [1m Learning iteration 1499/2000 [0m                     

                       Computation: 41763 steps/s (collection: 2.250s, learning 0.104s)
             Mean action noise std: 3.54
          Mean value_function loss: 130.4875
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 82.6538
                       Mean reward: 864.27
               Mean episode length: 228.74
    Episode_Reward/reaching_object: 2.0835
     Episode_Reward/lifting_object: 174.4983
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0841
          Episode_Reward/joint_vel: -0.0845
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 2.35s
                      Time elapsed: 00:58:14
                               ETA: 00:19:27

################################################################################
                     [1m Learning iteration 1500/2000 [0m                     

                       Computation: 41122 steps/s (collection: 2.275s, learning 0.116s)
             Mean action noise std: 3.55
          Mean value_function loss: 146.4171
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 82.6699
                       Mean reward: 853.01
               Mean episode length: 228.68
    Episode_Reward/reaching_object: 2.0105
     Episode_Reward/lifting_object: 167.7480
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0829
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 2.39s
                      Time elapsed: 00:58:16
                               ETA: 00:19:24

################################################################################
                     [1m Learning iteration 1501/2000 [0m                     

                       Computation: 41569 steps/s (collection: 2.268s, learning 0.097s)
             Mean action noise std: 3.55
          Mean value_function loss: 125.9600
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 82.6801
                       Mean reward: 818.63
               Mean episode length: 220.70
    Episode_Reward/reaching_object: 2.0275
     Episode_Reward/lifting_object: 168.9797
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0824
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 2.36s
                      Time elapsed: 00:58:18
                               ETA: 00:19:22

################################################################################
                     [1m Learning iteration 1502/2000 [0m                     

                       Computation: 41004 steps/s (collection: 2.306s, learning 0.092s)
             Mean action noise std: 3.55
          Mean value_function loss: 149.2571
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 82.6974
                       Mean reward: 817.68
               Mean episode length: 222.05
    Episode_Reward/reaching_object: 1.9957
     Episode_Reward/lifting_object: 166.0628
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0815
          Episode_Reward/joint_vel: -0.0830
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 2.40s
                      Time elapsed: 00:58:21
                               ETA: 00:19:20

################################################################################
                     [1m Learning iteration 1503/2000 [0m                     

                       Computation: 41323 steps/s (collection: 2.283s, learning 0.096s)
             Mean action noise std: 3.55
          Mean value_function loss: 140.2564
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 82.7168
                       Mean reward: 869.75
               Mean episode length: 233.75
    Episode_Reward/reaching_object: 2.0534
     Episode_Reward/lifting_object: 171.5056
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0836
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 2.38s
                      Time elapsed: 00:58:23
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 1504/2000 [0m                     

                       Computation: 40169 steps/s (collection: 2.312s, learning 0.135s)
             Mean action noise std: 3.55
          Mean value_function loss: 155.2747
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 82.7339
                       Mean reward: 876.81
               Mean episode length: 236.34
    Episode_Reward/reaching_object: 2.0343
     Episode_Reward/lifting_object: 168.8560
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0830
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 2.45s
                      Time elapsed: 00:58:26
                               ETA: 00:19:15

################################################################################
                     [1m Learning iteration 1505/2000 [0m                     

                       Computation: 40142 steps/s (collection: 2.321s, learning 0.128s)
             Mean action noise std: 3.56
          Mean value_function loss: 168.3513
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 82.7549
                       Mean reward: 841.75
               Mean episode length: 227.59
    Episode_Reward/reaching_object: 2.0294
     Episode_Reward/lifting_object: 168.6548
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0827
          Episode_Reward/joint_vel: -0.0835
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 2.45s
                      Time elapsed: 00:58:28
                               ETA: 00:19:13

################################################################################
                     [1m Learning iteration 1506/2000 [0m                     

                       Computation: 41017 steps/s (collection: 2.305s, learning 0.092s)
             Mean action noise std: 3.56
          Mean value_function loss: 165.3350
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 82.7766
                       Mean reward: 855.17
               Mean episode length: 228.48
    Episode_Reward/reaching_object: 1.9770
     Episode_Reward/lifting_object: 164.4768
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0808
          Episode_Reward/joint_vel: -0.0821
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 2.40s
                      Time elapsed: 00:58:30
                               ETA: 00:19:10

################################################################################
                     [1m Learning iteration 1507/2000 [0m                     

                       Computation: 37449 steps/s (collection: 2.507s, learning 0.118s)
             Mean action noise std: 3.56
          Mean value_function loss: 149.4277
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 82.7931
                       Mean reward: 866.97
               Mean episode length: 230.89
    Episode_Reward/reaching_object: 2.0258
     Episode_Reward/lifting_object: 168.3947
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0827
          Episode_Reward/joint_vel: -0.0836
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 2.62s
                      Time elapsed: 00:58:33
                               ETA: 00:19:08

################################################################################
                     [1m Learning iteration 1508/2000 [0m                     

                       Computation: 41406 steps/s (collection: 2.262s, learning 0.112s)
             Mean action noise std: 3.56
          Mean value_function loss: 154.5664
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 82.8056
                       Mean reward: 855.90
               Mean episode length: 229.14
    Episode_Reward/reaching_object: 1.9940
     Episode_Reward/lifting_object: 166.5948
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0816
          Episode_Reward/joint_vel: -0.0822
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 2.37s
                      Time elapsed: 00:58:35
                               ETA: 00:19:06

################################################################################
                     [1m Learning iteration 1509/2000 [0m                     

                       Computation: 40327 steps/s (collection: 2.342s, learning 0.096s)
             Mean action noise std: 3.56
          Mean value_function loss: 123.6142
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 82.8206
                       Mean reward: 881.51
               Mean episode length: 235.38
    Episode_Reward/reaching_object: 2.0523
     Episode_Reward/lifting_object: 171.7145
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0838
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 2.44s
                      Time elapsed: 00:58:38
                               ETA: 00:19:04

################################################################################
                     [1m Learning iteration 1510/2000 [0m                     

                       Computation: 40349 steps/s (collection: 2.316s, learning 0.121s)
             Mean action noise std: 3.56
          Mean value_function loss: 139.7225
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 82.8302
                       Mean reward: 844.80
               Mean episode length: 226.59
    Episode_Reward/reaching_object: 2.0212
     Episode_Reward/lifting_object: 169.1159
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0829
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 2.44s
                      Time elapsed: 00:58:40
                               ETA: 00:19:01

################################################################################
                     [1m Learning iteration 1511/2000 [0m                     

                       Computation: 40322 steps/s (collection: 2.319s, learning 0.119s)
             Mean action noise std: 3.57
          Mean value_function loss: 166.3251
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 82.8467
                       Mean reward: 830.43
               Mean episode length: 222.28
    Episode_Reward/reaching_object: 2.0092
     Episode_Reward/lifting_object: 168.4202
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0826
          Episode_Reward/joint_vel: -0.0823
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 2.44s
                      Time elapsed: 00:58:43
                               ETA: 00:18:59

################################################################################
                     [1m Learning iteration 1512/2000 [0m                     

                       Computation: 40840 steps/s (collection: 2.316s, learning 0.091s)
             Mean action noise std: 3.57
          Mean value_function loss: 115.2990
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 82.8764
                       Mean reward: 871.78
               Mean episode length: 233.42
    Episode_Reward/reaching_object: 2.0489
     Episode_Reward/lifting_object: 171.8638
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0839
          Episode_Reward/joint_vel: -0.0835
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 2.41s
                      Time elapsed: 00:58:45
                               ETA: 00:18:57

################################################################################
                     [1m Learning iteration 1513/2000 [0m                     

                       Computation: 40810 steps/s (collection: 2.266s, learning 0.143s)
             Mean action noise std: 3.57
          Mean value_function loss: 151.5686
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 82.8897
                       Mean reward: 830.30
               Mean episode length: 224.02
    Episode_Reward/reaching_object: 2.0233
     Episode_Reward/lifting_object: 169.1960
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0831
          Episode_Reward/joint_vel: -0.0832
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 2.41s
                      Time elapsed: 00:58:48
                               ETA: 00:18:54

################################################################################
                     [1m Learning iteration 1514/2000 [0m                     

                       Computation: 39378 steps/s (collection: 2.394s, learning 0.103s)
             Mean action noise std: 3.57
          Mean value_function loss: 156.4513
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 82.9033
                       Mean reward: 898.26
               Mean episode length: 238.51
    Episode_Reward/reaching_object: 2.0381
     Episode_Reward/lifting_object: 170.1477
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0842
          Episode_Reward/joint_vel: -0.0844
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 2.50s
                      Time elapsed: 00:58:50
                               ETA: 00:18:52

################################################################################
                     [1m Learning iteration 1515/2000 [0m                     

                       Computation: 41094 steps/s (collection: 2.295s, learning 0.097s)
             Mean action noise std: 3.57
          Mean value_function loss: 137.3656
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 82.9234
                       Mean reward: 867.94
               Mean episode length: 232.25
    Episode_Reward/reaching_object: 2.0348
     Episode_Reward/lifting_object: 170.0029
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0839
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 2.39s
                      Time elapsed: 00:58:52
                               ETA: 00:18:50

################################################################################
                     [1m Learning iteration 1516/2000 [0m                     

                       Computation: 40382 steps/s (collection: 2.332s, learning 0.102s)
             Mean action noise std: 3.58
          Mean value_function loss: 188.6093
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 82.9459
                       Mean reward: 822.23
               Mean episode length: 221.68
    Episode_Reward/reaching_object: 1.9572
     Episode_Reward/lifting_object: 163.3346
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0811
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 2.43s
                      Time elapsed: 00:58:55
                               ETA: 00:18:47

################################################################################
                     [1m Learning iteration 1517/2000 [0m                     

                       Computation: 39834 steps/s (collection: 2.368s, learning 0.100s)
             Mean action noise std: 3.58
          Mean value_function loss: 185.1258
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 82.9640
                       Mean reward: 865.96
               Mean episode length: 229.98
    Episode_Reward/reaching_object: 1.9971
     Episode_Reward/lifting_object: 167.0067
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0823
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 2.47s
                      Time elapsed: 00:58:57
                               ETA: 00:18:45

################################################################################
                     [1m Learning iteration 1518/2000 [0m                     

                       Computation: 40668 steps/s (collection: 2.273s, learning 0.144s)
             Mean action noise std: 3.58
          Mean value_function loss: 165.9296
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 82.9815
                       Mean reward: 831.38
               Mean episode length: 221.84
    Episode_Reward/reaching_object: 1.9812
     Episode_Reward/lifting_object: 165.6000
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 2.42s
                      Time elapsed: 00:59:00
                               ETA: 00:18:43

################################################################################
                     [1m Learning iteration 1519/2000 [0m                     

                       Computation: 39682 steps/s (collection: 2.365s, learning 0.112s)
             Mean action noise std: 3.58
          Mean value_function loss: 175.7760
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 82.9985
                       Mean reward: 792.79
               Mean episode length: 215.51
    Episode_Reward/reaching_object: 1.9757
     Episode_Reward/lifting_object: 164.9029
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0819
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 2.48s
                      Time elapsed: 00:59:02
                               ETA: 00:18:41

################################################################################
                     [1m Learning iteration 1520/2000 [0m                     

                       Computation: 40084 steps/s (collection: 2.348s, learning 0.104s)
             Mean action noise std: 3.59
          Mean value_function loss: 182.1578
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 83.0159
                       Mean reward: 790.74
               Mean episode length: 214.31
    Episode_Reward/reaching_object: 1.9492
     Episode_Reward/lifting_object: 162.0885
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0807
          Episode_Reward/joint_vel: -0.0814
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 2.45s
                      Time elapsed: 00:59:05
                               ETA: 00:18:38

################################################################################
                     [1m Learning iteration 1521/2000 [0m                     

                       Computation: 40452 steps/s (collection: 2.319s, learning 0.112s)
             Mean action noise std: 3.59
          Mean value_function loss: 170.6407
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 83.0350
                       Mean reward: 872.03
               Mean episode length: 233.51
    Episode_Reward/reaching_object: 2.0084
     Episode_Reward/lifting_object: 167.3929
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0834
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 2.43s
                      Time elapsed: 00:59:07
                               ETA: 00:18:36

################################################################################
                     [1m Learning iteration 1522/2000 [0m                     

                       Computation: 37967 steps/s (collection: 2.466s, learning 0.124s)
             Mean action noise std: 3.59
          Mean value_function loss: 153.4945
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 83.0488
                       Mean reward: 844.15
               Mean episode length: 225.89
    Episode_Reward/reaching_object: 1.9996
     Episode_Reward/lifting_object: 167.1554
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0826
          Episode_Reward/joint_vel: -0.0828
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 2.59s
                      Time elapsed: 00:59:10
                               ETA: 00:18:34

################################################################################
                     [1m Learning iteration 1523/2000 [0m                     

                       Computation: 39909 steps/s (collection: 2.366s, learning 0.098s)
             Mean action noise std: 3.59
          Mean value_function loss: 189.9551
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 83.0689
                       Mean reward: 787.77
               Mean episode length: 211.87
    Episode_Reward/reaching_object: 1.9330
     Episode_Reward/lifting_object: 160.5758
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0804
          Episode_Reward/joint_vel: -0.0818
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 2.46s
                      Time elapsed: 00:59:12
                               ETA: 00:18:31

################################################################################
                     [1m Learning iteration 1524/2000 [0m                     

                       Computation: 40132 steps/s (collection: 2.346s, learning 0.103s)
             Mean action noise std: 3.59
          Mean value_function loss: 160.7509
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 83.0937
                       Mean reward: 852.84
               Mean episode length: 228.89
    Episode_Reward/reaching_object: 1.9851
     Episode_Reward/lifting_object: 165.5133
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0828
          Episode_Reward/joint_vel: -0.0842
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 2.45s
                      Time elapsed: 00:59:15
                               ETA: 00:18:29

################################################################################
                     [1m Learning iteration 1525/2000 [0m                     

                       Computation: 38449 steps/s (collection: 2.438s, learning 0.119s)
             Mean action noise std: 3.60
          Mean value_function loss: 142.2039
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 83.1122
                       Mean reward: 898.25
               Mean episode length: 238.89
    Episode_Reward/reaching_object: 2.0077
     Episode_Reward/lifting_object: 168.0035
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0837
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 2.56s
                      Time elapsed: 00:59:17
                               ETA: 00:18:27

################################################################################
                     [1m Learning iteration 1526/2000 [0m                     

                       Computation: 40540 steps/s (collection: 2.329s, learning 0.096s)
             Mean action noise std: 3.60
          Mean value_function loss: 171.7388
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 83.1321
                       Mean reward: 857.02
               Mean episode length: 228.48
    Episode_Reward/reaching_object: 2.0180
     Episode_Reward/lifting_object: 169.1674
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0842
          Episode_Reward/joint_vel: -0.0849
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 2.42s
                      Time elapsed: 00:59:20
                               ETA: 00:18:25

################################################################################
                     [1m Learning iteration 1527/2000 [0m                     

                       Computation: 39743 steps/s (collection: 2.354s, learning 0.120s)
             Mean action noise std: 3.60
          Mean value_function loss: 162.2095
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 83.1492
                       Mean reward: 869.26
               Mean episode length: 232.00
    Episode_Reward/reaching_object: 2.0177
     Episode_Reward/lifting_object: 169.4653
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0843
          Episode_Reward/joint_vel: -0.0843
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 2.47s
                      Time elapsed: 00:59:22
                               ETA: 00:18:22

################################################################################
                     [1m Learning iteration 1528/2000 [0m                     

                       Computation: 40855 steps/s (collection: 2.292s, learning 0.115s)
             Mean action noise std: 3.60
          Mean value_function loss: 150.0798
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 83.1655
                       Mean reward: 812.35
               Mean episode length: 218.73
    Episode_Reward/reaching_object: 2.0538
     Episode_Reward/lifting_object: 172.2232
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0859
          Episode_Reward/joint_vel: -0.0860
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 2.41s
                      Time elapsed: 00:59:25
                               ETA: 00:18:20

################################################################################
                     [1m Learning iteration 1529/2000 [0m                     

                       Computation: 39131 steps/s (collection: 2.394s, learning 0.118s)
             Mean action noise std: 3.61
          Mean value_function loss: 198.5689
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 83.1885
                       Mean reward: 865.56
               Mean episode length: 228.91
    Episode_Reward/reaching_object: 1.9409
     Episode_Reward/lifting_object: 162.6304
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0816
          Episode_Reward/joint_vel: -0.0813
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 2.51s
                      Time elapsed: 00:59:27
                               ETA: 00:18:18

################################################################################
                     [1m Learning iteration 1530/2000 [0m                     

                       Computation: 40485 steps/s (collection: 2.308s, learning 0.121s)
             Mean action noise std: 3.61
          Mean value_function loss: 158.4637
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 83.2076
                       Mean reward: 875.92
               Mean episode length: 233.67
    Episode_Reward/reaching_object: 1.9959
     Episode_Reward/lifting_object: 167.2541
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0840
          Episode_Reward/joint_vel: -0.0842
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 2.43s
                      Time elapsed: 00:59:29
                               ETA: 00:18:15

################################################################################
                     [1m Learning iteration 1531/2000 [0m                     

                       Computation: 40208 steps/s (collection: 2.332s, learning 0.113s)
             Mean action noise std: 3.61
          Mean value_function loss: 142.5677
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 83.2318
                       Mean reward: 872.42
               Mean episode length: 232.72
    Episode_Reward/reaching_object: 2.0022
     Episode_Reward/lifting_object: 167.4493
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0841
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 2.44s
                      Time elapsed: 00:59:32
                               ETA: 00:18:13

################################################################################
                     [1m Learning iteration 1532/2000 [0m                     

                       Computation: 39803 steps/s (collection: 2.346s, learning 0.124s)
             Mean action noise std: 3.61
          Mean value_function loss: 148.2940
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 83.2533
                       Mean reward: 875.13
               Mean episode length: 232.59
    Episode_Reward/reaching_object: 2.0050
     Episode_Reward/lifting_object: 168.3058
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0847
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 2.47s
                      Time elapsed: 00:59:34
                               ETA: 00:18:11

################################################################################
                     [1m Learning iteration 1533/2000 [0m                     

                       Computation: 40533 steps/s (collection: 2.248s, learning 0.178s)
             Mean action noise std: 3.61
          Mean value_function loss: 214.5290
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 83.2670
                       Mean reward: 817.56
               Mean episode length: 219.65
    Episode_Reward/reaching_object: 1.9365
     Episode_Reward/lifting_object: 161.7194
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0819
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 2.43s
                      Time elapsed: 00:59:37
                               ETA: 00:18:09

################################################################################
                     [1m Learning iteration 1534/2000 [0m                     

                       Computation: 40965 steps/s (collection: 2.297s, learning 0.103s)
             Mean action noise std: 3.61
          Mean value_function loss: 202.6202
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 83.2769
                       Mean reward: 771.50
               Mean episode length: 208.66
    Episode_Reward/reaching_object: 1.9255
     Episode_Reward/lifting_object: 160.9982
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0816
          Episode_Reward/joint_vel: -0.0823
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 2.40s
                      Time elapsed: 00:59:39
                               ETA: 00:18:06

################################################################################
                     [1m Learning iteration 1535/2000 [0m                     

                       Computation: 41228 steps/s (collection: 2.274s, learning 0.110s)
             Mean action noise std: 3.62
          Mean value_function loss: 194.8404
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 83.2820
                       Mean reward: 816.38
               Mean episode length: 217.97
    Episode_Reward/reaching_object: 1.9388
     Episode_Reward/lifting_object: 162.2690
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0825
          Episode_Reward/joint_vel: -0.0825
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 2.38s
                      Time elapsed: 00:59:42
                               ETA: 00:18:04

################################################################################
                     [1m Learning iteration 1536/2000 [0m                     

                       Computation: 38965 steps/s (collection: 2.406s, learning 0.117s)
             Mean action noise std: 3.62
          Mean value_function loss: 171.6886
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 83.2924
                       Mean reward: 854.16
               Mean episode length: 228.27
    Episode_Reward/reaching_object: 1.9898
     Episode_Reward/lifting_object: 166.9590
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0843
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 2.52s
                      Time elapsed: 00:59:44
                               ETA: 00:18:02

################################################################################
                     [1m Learning iteration 1537/2000 [0m                     

                       Computation: 42053 steps/s (collection: 2.248s, learning 0.090s)
             Mean action noise std: 3.62
          Mean value_function loss: 172.6065
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 83.3040
                       Mean reward: 770.15
               Mean episode length: 208.74
    Episode_Reward/reaching_object: 1.9835
     Episode_Reward/lifting_object: 165.6857
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0842
          Episode_Reward/joint_vel: -0.0848
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 2.34s
                      Time elapsed: 00:59:46
                               ETA: 00:17:59

################################################################################
                     [1m Learning iteration 1538/2000 [0m                     

                       Computation: 37796 steps/s (collection: 2.462s, learning 0.139s)
             Mean action noise std: 3.62
          Mean value_function loss: 166.3487
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 83.3220
                       Mean reward: 820.10
               Mean episode length: 218.80
    Episode_Reward/reaching_object: 1.9889
     Episode_Reward/lifting_object: 166.7757
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0845
          Episode_Reward/joint_vel: -0.0849
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 2.60s
                      Time elapsed: 00:59:49
                               ETA: 00:17:57

################################################################################
                     [1m Learning iteration 1539/2000 [0m                     

                       Computation: 41892 steps/s (collection: 2.247s, learning 0.100s)
             Mean action noise std: 3.62
          Mean value_function loss: 183.6950
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 83.3430
                       Mean reward: 773.04
               Mean episode length: 209.10
    Episode_Reward/reaching_object: 1.9693
     Episode_Reward/lifting_object: 164.2366
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0842
          Episode_Reward/joint_vel: -0.0850
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 2.35s
                      Time elapsed: 00:59:51
                               ETA: 00:17:55

################################################################################
                     [1m Learning iteration 1540/2000 [0m                     

                       Computation: 41864 steps/s (collection: 2.255s, learning 0.094s)
             Mean action noise std: 3.63
          Mean value_function loss: 204.8485
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 83.3639
                       Mean reward: 808.00
               Mean episode length: 220.13
    Episode_Reward/reaching_object: 1.9147
     Episode_Reward/lifting_object: 159.8376
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0820
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 2.35s
                      Time elapsed: 00:59:54
                               ETA: 00:17:52

################################################################################
                     [1m Learning iteration 1541/2000 [0m                     

                       Computation: 39828 steps/s (collection: 2.331s, learning 0.138s)
             Mean action noise std: 3.63
          Mean value_function loss: 190.3748
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 83.3818
                       Mean reward: 814.23
               Mean episode length: 220.71
    Episode_Reward/reaching_object: 1.9331
     Episode_Reward/lifting_object: 161.3330
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0831
          Episode_Reward/joint_vel: -0.0850
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 2.47s
                      Time elapsed: 00:59:56
                               ETA: 00:17:50

################################################################################
                     [1m Learning iteration 1542/2000 [0m                     

                       Computation: 41143 steps/s (collection: 2.300s, learning 0.090s)
             Mean action noise std: 3.63
          Mean value_function loss: 159.3037
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 83.3920
                       Mean reward: 808.74
               Mean episode length: 218.39
    Episode_Reward/reaching_object: 1.9644
     Episode_Reward/lifting_object: 163.6947
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0838
          Episode_Reward/joint_vel: -0.0855
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 2.39s
                      Time elapsed: 00:59:59
                               ETA: 00:17:48

################################################################################
                     [1m Learning iteration 1543/2000 [0m                     

                       Computation: 41813 steps/s (collection: 2.232s, learning 0.119s)
             Mean action noise std: 3.63
          Mean value_function loss: 155.3383
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 83.4008
                       Mean reward: 821.65
               Mean episode length: 220.24
    Episode_Reward/reaching_object: 1.9593
     Episode_Reward/lifting_object: 164.4693
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0837
          Episode_Reward/joint_vel: -0.0853
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 2.35s
                      Time elapsed: 01:00:01
                               ETA: 00:17:45

################################################################################
                     [1m Learning iteration 1544/2000 [0m                     

                       Computation: 41622 steps/s (collection: 2.256s, learning 0.106s)
             Mean action noise std: 3.63
          Mean value_function loss: 173.4368
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 83.4069
                       Mean reward: 841.36
               Mean episode length: 224.44
    Episode_Reward/reaching_object: 1.9920
     Episode_Reward/lifting_object: 166.8857
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0853
          Episode_Reward/joint_vel: -0.0872
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 2.36s
                      Time elapsed: 01:00:03
                               ETA: 00:17:43

################################################################################
                     [1m Learning iteration 1545/2000 [0m                     

                       Computation: 40971 steps/s (collection: 2.293s, learning 0.106s)
             Mean action noise std: 3.63
          Mean value_function loss: 175.3892
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 83.4130
                       Mean reward: 803.92
               Mean episode length: 216.25
    Episode_Reward/reaching_object: 1.9670
     Episode_Reward/lifting_object: 164.5497
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0841
          Episode_Reward/joint_vel: -0.0852
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 2.40s
                      Time elapsed: 01:00:06
                               ETA: 00:17:41

################################################################################
                     [1m Learning iteration 1546/2000 [0m                     

                       Computation: 39677 steps/s (collection: 2.373s, learning 0.105s)
             Mean action noise std: 3.63
          Mean value_function loss: 173.8555
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 83.4247
                       Mean reward: 738.07
               Mean episode length: 201.95
    Episode_Reward/reaching_object: 1.9651
     Episode_Reward/lifting_object: 164.3026
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0842
          Episode_Reward/joint_vel: -0.0853
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 2.48s
                      Time elapsed: 01:00:08
                               ETA: 00:17:39

################################################################################
                     [1m Learning iteration 1547/2000 [0m                     

                       Computation: 41728 steps/s (collection: 2.253s, learning 0.102s)
             Mean action noise std: 3.63
          Mean value_function loss: 170.9656
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 83.4357
                       Mean reward: 793.63
               Mean episode length: 213.80
    Episode_Reward/reaching_object: 1.9611
     Episode_Reward/lifting_object: 163.8332
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0841
          Episode_Reward/joint_vel: -0.0855
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 2.36s
                      Time elapsed: 01:00:11
                               ETA: 00:17:36

################################################################################
                     [1m Learning iteration 1548/2000 [0m                     

                       Computation: 41144 steps/s (collection: 2.251s, learning 0.138s)
             Mean action noise std: 3.64
          Mean value_function loss: 172.1514
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 83.4523
                       Mean reward: 842.02
               Mean episode length: 227.91
    Episode_Reward/reaching_object: 1.9928
     Episode_Reward/lifting_object: 166.0259
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0855
          Episode_Reward/joint_vel: -0.0873
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 2.39s
                      Time elapsed: 01:00:13
                               ETA: 00:17:34

################################################################################
                     [1m Learning iteration 1549/2000 [0m                     

                       Computation: 38230 steps/s (collection: 2.476s, learning 0.096s)
             Mean action noise std: 3.64
          Mean value_function loss: 207.5367
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 83.4629
                       Mean reward: 825.13
               Mean episode length: 221.83
    Episode_Reward/reaching_object: 1.9446
     Episode_Reward/lifting_object: 162.0805
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0835
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 2.57s
                      Time elapsed: 01:00:15
                               ETA: 00:17:32

################################################################################
                     [1m Learning iteration 1550/2000 [0m                     

                       Computation: 39197 steps/s (collection: 2.378s, learning 0.130s)
             Mean action noise std: 3.64
          Mean value_function loss: 192.6164
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 83.4779
                       Mean reward: 816.10
               Mean episode length: 218.32
    Episode_Reward/reaching_object: 1.9635
     Episode_Reward/lifting_object: 164.0103
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0842
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 2.51s
                      Time elapsed: 01:00:18
                               ETA: 00:17:29

################################################################################
                     [1m Learning iteration 1551/2000 [0m                     

                       Computation: 40673 steps/s (collection: 2.311s, learning 0.106s)
             Mean action noise std: 3.64
          Mean value_function loss: 196.2813
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 83.4845
                       Mean reward: 853.64
               Mean episode length: 228.57
    Episode_Reward/reaching_object: 1.9407
     Episode_Reward/lifting_object: 161.5380
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0834
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 2.42s
                      Time elapsed: 01:00:20
                               ETA: 00:17:27

################################################################################
                     [1m Learning iteration 1552/2000 [0m                     

                       Computation: 40205 steps/s (collection: 2.348s, learning 0.097s)
             Mean action noise std: 3.64
          Mean value_function loss: 190.1083
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 83.4929
                       Mean reward: 836.54
               Mean episode length: 223.55
    Episode_Reward/reaching_object: 1.9171
     Episode_Reward/lifting_object: 159.5721
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0826
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 2.45s
                      Time elapsed: 01:00:23
                               ETA: 00:17:25

################################################################################
                     [1m Learning iteration 1553/2000 [0m                     

                       Computation: 41337 steps/s (collection: 2.278s, learning 0.101s)
             Mean action noise std: 3.64
          Mean value_function loss: 185.4785
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 83.5013
                       Mean reward: 809.26
               Mean episode length: 219.78
    Episode_Reward/reaching_object: 1.9636
     Episode_Reward/lifting_object: 163.2222
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0843
          Episode_Reward/joint_vel: -0.0858
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 2.38s
                      Time elapsed: 01:00:25
                               ETA: 00:17:22

################################################################################
                     [1m Learning iteration 1554/2000 [0m                     

                       Computation: 39203 steps/s (collection: 2.386s, learning 0.121s)
             Mean action noise std: 3.64
          Mean value_function loss: 158.9434
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 83.5151
                       Mean reward: 847.87
               Mean episode length: 227.02
    Episode_Reward/reaching_object: 2.0199
     Episode_Reward/lifting_object: 168.3285
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0866
          Episode_Reward/joint_vel: -0.0886
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 2.51s
                      Time elapsed: 01:00:28
                               ETA: 00:17:20

################################################################################
                     [1m Learning iteration 1555/2000 [0m                     

                       Computation: 39435 steps/s (collection: 2.396s, learning 0.097s)
             Mean action noise std: 3.65
          Mean value_function loss: 183.4653
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 83.5264
                       Mean reward: 825.63
               Mean episode length: 222.82
    Episode_Reward/reaching_object: 1.9639
     Episode_Reward/lifting_object: 163.2166
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0844
          Episode_Reward/joint_vel: -0.0863
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 2.49s
                      Time elapsed: 01:00:30
                               ETA: 00:17:18

################################################################################
                     [1m Learning iteration 1556/2000 [0m                     

                       Computation: 38756 steps/s (collection: 2.429s, learning 0.108s)
             Mean action noise std: 3.65
          Mean value_function loss: 178.1642
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 83.5395
                       Mean reward: 813.94
               Mean episode length: 218.76
    Episode_Reward/reaching_object: 1.9700
     Episode_Reward/lifting_object: 163.3518
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0851
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 2.54s
                      Time elapsed: 01:00:33
                               ETA: 00:17:16

################################################################################
                     [1m Learning iteration 1557/2000 [0m                     

                       Computation: 40809 steps/s (collection: 2.276s, learning 0.133s)
             Mean action noise std: 3.65
          Mean value_function loss: 199.5727
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 83.5539
                       Mean reward: 830.57
               Mean episode length: 223.26
    Episode_Reward/reaching_object: 1.9447
     Episode_Reward/lifting_object: 161.0369
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0841
          Episode_Reward/joint_vel: -0.0879
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 2.41s
                      Time elapsed: 01:00:35
                               ETA: 00:17:13

################################################################################
                     [1m Learning iteration 1558/2000 [0m                     

                       Computation: 39298 steps/s (collection: 2.402s, learning 0.100s)
             Mean action noise std: 3.65
          Mean value_function loss: 174.3527
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 83.5605
                       Mean reward: 824.35
               Mean episode length: 222.19
    Episode_Reward/reaching_object: 1.9765
     Episode_Reward/lifting_object: 164.2424
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0854
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 2.50s
                      Time elapsed: 01:00:38
                               ETA: 00:17:11

################################################################################
                     [1m Learning iteration 1559/2000 [0m                     

                       Computation: 41019 steps/s (collection: 2.281s, learning 0.116s)
             Mean action noise std: 3.65
          Mean value_function loss: 164.0863
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 83.5756
                       Mean reward: 827.63
               Mean episode length: 222.85
    Episode_Reward/reaching_object: 1.9812
     Episode_Reward/lifting_object: 164.4409
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0857
          Episode_Reward/joint_vel: -0.0889
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 2.40s
                      Time elapsed: 01:00:40
                               ETA: 00:17:09

################################################################################
                     [1m Learning iteration 1560/2000 [0m                     

                       Computation: 40898 steps/s (collection: 2.297s, learning 0.107s)
             Mean action noise std: 3.65
          Mean value_function loss: 155.1380
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 83.5919
                       Mean reward: 858.77
               Mean episode length: 229.22
    Episode_Reward/reaching_object: 2.0428
     Episode_Reward/lifting_object: 170.0283
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0878
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 2.40s
                      Time elapsed: 01:00:42
                               ETA: 00:17:06

################################################################################
                     [1m Learning iteration 1561/2000 [0m                     

                       Computation: 39856 steps/s (collection: 2.350s, learning 0.116s)
             Mean action noise std: 3.65
          Mean value_function loss: 163.7875
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 83.6023
                       Mean reward: 855.17
               Mean episode length: 228.03
    Episode_Reward/reaching_object: 1.9724
     Episode_Reward/lifting_object: 163.9393
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0854
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 2.47s
                      Time elapsed: 01:00:45
                               ETA: 00:17:04

################################################################################
                     [1m Learning iteration 1562/2000 [0m                     

                       Computation: 38120 steps/s (collection: 2.475s, learning 0.104s)
             Mean action noise std: 3.66
          Mean value_function loss: 161.2088
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 83.6162
                       Mean reward: 853.01
               Mean episode length: 227.82
    Episode_Reward/reaching_object: 2.0071
     Episode_Reward/lifting_object: 166.5925
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0864
          Episode_Reward/joint_vel: -0.0882
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 2.58s
                      Time elapsed: 01:00:48
                               ETA: 00:17:02

################################################################################
                     [1m Learning iteration 1563/2000 [0m                     

                       Computation: 38861 steps/s (collection: 2.399s, learning 0.131s)
             Mean action noise std: 3.66
          Mean value_function loss: 179.0467
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 83.6349
                       Mean reward: 857.00
               Mean episode length: 227.78
    Episode_Reward/reaching_object: 2.0295
     Episode_Reward/lifting_object: 168.7548
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0878
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 2.53s
                      Time elapsed: 01:00:50
                               ETA: 00:17:00

################################################################################
                     [1m Learning iteration 1564/2000 [0m                     

                       Computation: 39017 steps/s (collection: 2.377s, learning 0.143s)
             Mean action noise std: 3.66
          Mean value_function loss: 178.8526
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 83.6524
                       Mean reward: 848.65
               Mean episode length: 227.14
    Episode_Reward/reaching_object: 1.9663
     Episode_Reward/lifting_object: 162.3679
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0857
          Episode_Reward/joint_vel: -0.0886
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 2.52s
                      Time elapsed: 01:00:53
                               ETA: 00:16:57

################################################################################
                     [1m Learning iteration 1565/2000 [0m                     

                       Computation: 39417 steps/s (collection: 2.387s, learning 0.107s)
             Mean action noise std: 3.66
          Mean value_function loss: 162.5654
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 83.6645
                       Mean reward: 844.71
               Mean episode length: 225.68
    Episode_Reward/reaching_object: 2.0243
     Episode_Reward/lifting_object: 167.9851
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0878
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 2.49s
                      Time elapsed: 01:00:55
                               ETA: 00:16:55

################################################################################
                     [1m Learning iteration 1566/2000 [0m                     

                       Computation: 41082 steps/s (collection: 2.302s, learning 0.091s)
             Mean action noise std: 3.66
          Mean value_function loss: 153.7885
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 83.6772
                       Mean reward: 847.81
               Mean episode length: 226.73
    Episode_Reward/reaching_object: 2.0046
     Episode_Reward/lifting_object: 165.7658
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0869
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 2.39s
                      Time elapsed: 01:00:57
                               ETA: 00:16:53

################################################################################
                     [1m Learning iteration 1567/2000 [0m                     

                       Computation: 41377 steps/s (collection: 2.286s, learning 0.090s)
             Mean action noise std: 3.66
          Mean value_function loss: 181.4289
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 83.6818
                       Mean reward: 807.34
               Mean episode length: 217.74
    Episode_Reward/reaching_object: 1.9874
     Episode_Reward/lifting_object: 164.6299
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0863
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 2.38s
                      Time elapsed: 01:01:00
                               ETA: 00:16:50

################################################################################
                     [1m Learning iteration 1568/2000 [0m                     

                       Computation: 39453 steps/s (collection: 2.392s, learning 0.100s)
             Mean action noise std: 3.67
          Mean value_function loss: 146.9598
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 83.6919
                       Mean reward: 862.29
               Mean episode length: 229.91
    Episode_Reward/reaching_object: 2.0171
     Episode_Reward/lifting_object: 167.5801
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0876
          Episode_Reward/joint_vel: -0.0899
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 2.49s
                      Time elapsed: 01:01:02
                               ETA: 00:16:48

################################################################################
                     [1m Learning iteration 1569/2000 [0m                     

                       Computation: 38169 steps/s (collection: 2.444s, learning 0.131s)
             Mean action noise std: 3.67
          Mean value_function loss: 185.7790
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 83.7095
                       Mean reward: 806.43
               Mean episode length: 216.47
    Episode_Reward/reaching_object: 1.9961
     Episode_Reward/lifting_object: 164.8906
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0869
          Episode_Reward/joint_vel: -0.0885
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 2.58s
                      Time elapsed: 01:01:05
                               ETA: 00:16:46

################################################################################
                     [1m Learning iteration 1570/2000 [0m                     

                       Computation: 40070 steps/s (collection: 2.352s, learning 0.102s)
             Mean action noise std: 3.67
          Mean value_function loss: 144.1611
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 83.7305
                       Mean reward: 882.29
               Mean episode length: 233.78
    Episode_Reward/reaching_object: 2.0432
     Episode_Reward/lifting_object: 169.7050
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0890
          Episode_Reward/joint_vel: -0.0908
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 2.45s
                      Time elapsed: 01:01:07
                               ETA: 00:16:43

################################################################################
                     [1m Learning iteration 1571/2000 [0m                     

                       Computation: 40477 steps/s (collection: 2.330s, learning 0.099s)
             Mean action noise std: 3.67
          Mean value_function loss: 129.6170
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 83.7517
                       Mean reward: 894.55
               Mean episode length: 236.88
    Episode_Reward/reaching_object: 2.0401
     Episode_Reward/lifting_object: 169.4400
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0887
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 2.43s
                      Time elapsed: 01:01:10
                               ETA: 00:16:41

################################################################################
                     [1m Learning iteration 1572/2000 [0m                     

                       Computation: 40016 steps/s (collection: 2.321s, learning 0.136s)
             Mean action noise std: 3.67
          Mean value_function loss: 156.8103
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 83.7711
                       Mean reward: 840.10
               Mean episode length: 225.11
    Episode_Reward/reaching_object: 2.0096
     Episode_Reward/lifting_object: 166.4972
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0877
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 2.46s
                      Time elapsed: 01:01:12
                               ETA: 00:16:39

################################################################################
                     [1m Learning iteration 1573/2000 [0m                     

                       Computation: 40218 steps/s (collection: 2.341s, learning 0.103s)
             Mean action noise std: 3.68
          Mean value_function loss: 145.5579
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 83.7869
                       Mean reward: 882.22
               Mean episode length: 234.97
    Episode_Reward/reaching_object: 2.0776
     Episode_Reward/lifting_object: 173.2579
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0904
          Episode_Reward/joint_vel: -0.0922
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 2.44s
                      Time elapsed: 01:01:15
                               ETA: 00:16:37

################################################################################
                     [1m Learning iteration 1574/2000 [0m                     

                       Computation: 39878 steps/s (collection: 2.337s, learning 0.128s)
             Mean action noise std: 3.68
          Mean value_function loss: 143.8310
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 83.8024
                       Mean reward: 849.11
               Mean episode length: 228.54
    Episode_Reward/reaching_object: 2.0087
     Episode_Reward/lifting_object: 166.5708
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0878
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 2.47s
                      Time elapsed: 01:01:17
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 1575/2000 [0m                     

                       Computation: 39159 steps/s (collection: 2.392s, learning 0.119s)
             Mean action noise std: 3.68
          Mean value_function loss: 146.9908
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 83.8176
                       Mean reward: 833.86
               Mean episode length: 225.17
    Episode_Reward/reaching_object: 2.0286
     Episode_Reward/lifting_object: 168.5689
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0890
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 2.51s
                      Time elapsed: 01:01:20
                               ETA: 00:16:32

################################################################################
                     [1m Learning iteration 1576/2000 [0m                     

                       Computation: 40362 steps/s (collection: 2.339s, learning 0.097s)
             Mean action noise std: 3.68
          Mean value_function loss: 159.1281
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 83.8325
                       Mean reward: 811.79
               Mean episode length: 217.35
    Episode_Reward/reaching_object: 1.9853
     Episode_Reward/lifting_object: 165.0722
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0872
          Episode_Reward/joint_vel: -0.0891
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 2.44s
                      Time elapsed: 01:01:22
                               ETA: 00:16:30

################################################################################
                     [1m Learning iteration 1577/2000 [0m                     

                       Computation: 39810 steps/s (collection: 2.344s, learning 0.125s)
             Mean action noise std: 3.68
          Mean value_function loss: 129.7148
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 83.8424
                       Mean reward: 871.46
               Mean episode length: 233.94
    Episode_Reward/reaching_object: 2.0619
     Episode_Reward/lifting_object: 171.9509
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0904
          Episode_Reward/joint_vel: -0.0916
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 2.47s
                      Time elapsed: 01:01:25
                               ETA: 00:16:27

################################################################################
                     [1m Learning iteration 1578/2000 [0m                     

                       Computation: 40048 steps/s (collection: 2.332s, learning 0.123s)
             Mean action noise std: 3.68
          Mean value_function loss: 155.0849
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 83.8527
                       Mean reward: 833.64
               Mean episode length: 221.31
    Episode_Reward/reaching_object: 2.0092
     Episode_Reward/lifting_object: 167.7132
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0880
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 2.45s
                      Time elapsed: 01:01:27
                               ETA: 00:16:25

################################################################################
                     [1m Learning iteration 1579/2000 [0m                     

                       Computation: 37856 steps/s (collection: 2.501s, learning 0.095s)
             Mean action noise std: 3.69
          Mean value_function loss: 164.2565
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 83.8618
                       Mean reward: 841.95
               Mean episode length: 224.62
    Episode_Reward/reaching_object: 2.0017
     Episode_Reward/lifting_object: 166.5507
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0883
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 2.60s
                      Time elapsed: 01:01:30
                               ETA: 00:16:23

################################################################################
                     [1m Learning iteration 1580/2000 [0m                     

                       Computation: 40724 steps/s (collection: 2.316s, learning 0.098s)
             Mean action noise std: 3.69
          Mean value_function loss: 173.6183
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 83.8719
                       Mean reward: 806.65
               Mean episode length: 215.04
    Episode_Reward/reaching_object: 1.9566
     Episode_Reward/lifting_object: 162.7725
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0863
          Episode_Reward/joint_vel: -0.0879
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 2.41s
                      Time elapsed: 01:01:32
                               ETA: 00:16:20

################################################################################
                     [1m Learning iteration 1581/2000 [0m                     

                       Computation: 39702 steps/s (collection: 2.345s, learning 0.131s)
             Mean action noise std: 3.69
          Mean value_function loss: 165.7881
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 83.8841
                       Mean reward: 814.72
               Mean episode length: 220.82
    Episode_Reward/reaching_object: 1.9902
     Episode_Reward/lifting_object: 164.9858
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0881
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 2.48s
                      Time elapsed: 01:01:35
                               ETA: 00:16:18

################################################################################
                     [1m Learning iteration 1582/2000 [0m                     

                       Computation: 39830 steps/s (collection: 2.337s, learning 0.131s)
             Mean action noise std: 3.69
          Mean value_function loss: 206.9875
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 83.8983
                       Mean reward: 850.14
               Mean episode length: 227.00
    Episode_Reward/reaching_object: 1.9920
     Episode_Reward/lifting_object: 166.1180
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0879
          Episode_Reward/joint_vel: -0.0885
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 2.47s
                      Time elapsed: 01:01:37
                               ETA: 00:16:16

################################################################################
                     [1m Learning iteration 1583/2000 [0m                     

                       Computation: 37509 steps/s (collection: 2.517s, learning 0.104s)
             Mean action noise std: 3.69
          Mean value_function loss: 172.5692
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 83.9093
                       Mean reward: 837.41
               Mean episode length: 225.41
    Episode_Reward/reaching_object: 1.9482
     Episode_Reward/lifting_object: 162.2946
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0864
          Episode_Reward/joint_vel: -0.0877
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 2.62s
                      Time elapsed: 01:01:40
                               ETA: 00:16:14

################################################################################
                     [1m Learning iteration 1584/2000 [0m                     

                       Computation: 39240 steps/s (collection: 2.411s, learning 0.095s)
             Mean action noise std: 3.69
          Mean value_function loss: 188.6346
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 83.9208
                       Mean reward: 842.29
               Mean episode length: 225.29
    Episode_Reward/reaching_object: 1.9144
     Episode_Reward/lifting_object: 159.2408
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0848
          Episode_Reward/joint_vel: -0.0859
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 2.51s
                      Time elapsed: 01:01:42
                               ETA: 00:16:11

################################################################################
                     [1m Learning iteration 1585/2000 [0m                     

                       Computation: 39949 steps/s (collection: 2.360s, learning 0.101s)
             Mean action noise std: 3.69
          Mean value_function loss: 155.5763
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 83.9331
                       Mean reward: 854.78
               Mean episode length: 227.33
    Episode_Reward/reaching_object: 1.9773
     Episode_Reward/lifting_object: 165.0468
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0876
          Episode_Reward/joint_vel: -0.0891
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 2.46s
                      Time elapsed: 01:01:45
                               ETA: 00:16:09

################################################################################
                     [1m Learning iteration 1586/2000 [0m                     

                       Computation: 39642 steps/s (collection: 2.359s, learning 0.121s)
             Mean action noise std: 3.70
          Mean value_function loss: 156.0685
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 83.9460
                       Mean reward: 878.85
               Mean episode length: 233.16
    Episode_Reward/reaching_object: 2.0043
     Episode_Reward/lifting_object: 167.5944
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 2.48s
                      Time elapsed: 01:01:47
                               ETA: 00:16:07

################################################################################
                     [1m Learning iteration 1587/2000 [0m                     

                       Computation: 40246 steps/s (collection: 2.339s, learning 0.104s)
             Mean action noise std: 3.70
          Mean value_function loss: 161.1408
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 83.9602
                       Mean reward: 880.50
               Mean episode length: 234.72
    Episode_Reward/reaching_object: 2.0148
     Episode_Reward/lifting_object: 168.7086
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0891
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 2.44s
                      Time elapsed: 01:01:49
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 1588/2000 [0m                     

                       Computation: 41098 steps/s (collection: 2.298s, learning 0.094s)
             Mean action noise std: 3.70
          Mean value_function loss: 157.9152
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 83.9730
                       Mean reward: 840.16
               Mean episode length: 225.18
    Episode_Reward/reaching_object: 2.0091
     Episode_Reward/lifting_object: 167.7023
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0893
          Episode_Reward/joint_vel: -0.0914
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 2.39s
                      Time elapsed: 01:01:52
                               ETA: 00:16:02

################################################################################
                     [1m Learning iteration 1589/2000 [0m                     

                       Computation: 36091 steps/s (collection: 2.530s, learning 0.194s)
             Mean action noise std: 3.70
          Mean value_function loss: 186.6777
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 83.9934
                       Mean reward: 810.94
               Mean episode length: 218.66
    Episode_Reward/reaching_object: 1.9834
     Episode_Reward/lifting_object: 166.0098
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0883
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 2.72s
                      Time elapsed: 01:01:55
                               ETA: 00:16:00

################################################################################
                     [1m Learning iteration 1590/2000 [0m                     

                       Computation: 40698 steps/s (collection: 2.302s, learning 0.114s)
             Mean action noise std: 3.70
          Mean value_function loss: 154.6085
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 84.0098
                       Mean reward: 850.99
               Mean episode length: 228.16
    Episode_Reward/reaching_object: 2.0331
     Episode_Reward/lifting_object: 170.4886
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0903
          Episode_Reward/joint_vel: -0.0909
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 2.42s
                      Time elapsed: 01:01:57
                               ETA: 00:15:58

################################################################################
                     [1m Learning iteration 1591/2000 [0m                     

                       Computation: 40984 steps/s (collection: 2.302s, learning 0.097s)
             Mean action noise std: 3.70
          Mean value_function loss: 165.4438
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 84.0126
                       Mean reward: 839.78
               Mean episode length: 225.45
    Episode_Reward/reaching_object: 1.9619
     Episode_Reward/lifting_object: 164.4742
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0876
          Episode_Reward/joint_vel: -0.0887
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 2.40s
                      Time elapsed: 01:01:59
                               ETA: 00:15:55

################################################################################
                     [1m Learning iteration 1592/2000 [0m                     

                       Computation: 39781 steps/s (collection: 2.379s, learning 0.092s)
             Mean action noise std: 3.70
          Mean value_function loss: 155.6122
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 84.0191
                       Mean reward: 872.32
               Mean episode length: 234.13
    Episode_Reward/reaching_object: 2.0180
     Episode_Reward/lifting_object: 169.1586
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0904
          Episode_Reward/joint_vel: -0.0918
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 2.47s
                      Time elapsed: 01:02:02
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 1593/2000 [0m                     

                       Computation: 40879 steps/s (collection: 2.290s, learning 0.115s)
             Mean action noise std: 3.71
          Mean value_function loss: 144.1933
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 84.0341
                       Mean reward: 822.99
               Mean episode length: 221.35
    Episode_Reward/reaching_object: 2.0343
     Episode_Reward/lifting_object: 171.2207
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0909
          Episode_Reward/joint_vel: -0.0919
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 2.40s
                      Time elapsed: 01:02:04
                               ETA: 00:15:51

################################################################################
                     [1m Learning iteration 1594/2000 [0m                     

                       Computation: 40385 steps/s (collection: 2.329s, learning 0.106s)
             Mean action noise std: 3.71
          Mean value_function loss: 165.6065
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 84.0472
                       Mean reward: 853.24
               Mean episode length: 228.64
    Episode_Reward/reaching_object: 2.0162
     Episode_Reward/lifting_object: 169.2962
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0903
          Episode_Reward/joint_vel: -0.0915
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 2.43s
                      Time elapsed: 01:02:07
                               ETA: 00:15:48

################################################################################
                     [1m Learning iteration 1595/2000 [0m                     

                       Computation: 37198 steps/s (collection: 2.503s, learning 0.140s)
             Mean action noise std: 3.71
          Mean value_function loss: 160.7069
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 84.0571
                       Mean reward: 851.63
               Mean episode length: 227.87
    Episode_Reward/reaching_object: 1.9994
     Episode_Reward/lifting_object: 168.9933
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0898
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 2.64s
                      Time elapsed: 01:02:09
                               ETA: 00:15:46

################################################################################
                     [1m Learning iteration 1596/2000 [0m                     

                       Computation: 40246 steps/s (collection: 2.329s, learning 0.113s)
             Mean action noise std: 3.71
          Mean value_function loss: 120.6221
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 84.0706
                       Mean reward: 920.66
               Mean episode length: 244.92
    Episode_Reward/reaching_object: 2.0252
     Episode_Reward/lifting_object: 170.8427
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0910
          Episode_Reward/joint_vel: -0.0911
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 2.44s
                      Time elapsed: 01:02:12
                               ETA: 00:15:44

################################################################################
                     [1m Learning iteration 1597/2000 [0m                     

                       Computation: 40867 steps/s (collection: 2.310s, learning 0.096s)
             Mean action noise std: 3.71
          Mean value_function loss: 140.9154
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 84.0838
                       Mean reward: 879.39
               Mean episode length: 234.17
    Episode_Reward/reaching_object: 1.9949
     Episode_Reward/lifting_object: 168.7955
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0895
          Episode_Reward/joint_vel: -0.0885
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 2.41s
                      Time elapsed: 01:02:14
                               ETA: 00:15:41

################################################################################
                     [1m Learning iteration 1598/2000 [0m                     

                       Computation: 40395 steps/s (collection: 2.298s, learning 0.135s)
             Mean action noise std: 3.71
          Mean value_function loss: 163.6613
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 84.0954
                       Mean reward: 825.37
               Mean episode length: 222.35
    Episode_Reward/reaching_object: 1.9798
     Episode_Reward/lifting_object: 166.2005
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0896
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 2.43s
                      Time elapsed: 01:02:17
                               ETA: 00:15:39

################################################################################
                     [1m Learning iteration 1599/2000 [0m                     

                       Computation: 40615 steps/s (collection: 2.302s, learning 0.118s)
             Mean action noise std: 3.72
          Mean value_function loss: 174.0984
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 84.1131
                       Mean reward: 811.90
               Mean episode length: 218.33
    Episode_Reward/reaching_object: 1.9814
     Episode_Reward/lifting_object: 167.4866
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0894
          Episode_Reward/joint_vel: -0.0887
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 2.42s
                      Time elapsed: 01:02:19
                               ETA: 00:15:37

################################################################################
                     [1m Learning iteration 1600/2000 [0m                     

                       Computation: 40979 steps/s (collection: 2.301s, learning 0.098s)
             Mean action noise std: 3.72
          Mean value_function loss: 224.5921
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 84.1246
                       Mean reward: 770.70
               Mean episode length: 208.82
    Episode_Reward/reaching_object: 1.9195
     Episode_Reward/lifting_object: 162.1245
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0872
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 2.40s
                      Time elapsed: 01:02:21
                               ETA: 00:15:34

################################################################################
                     [1m Learning iteration 1601/2000 [0m                     

                       Computation: 37847 steps/s (collection: 2.451s, learning 0.146s)
             Mean action noise std: 3.72
          Mean value_function loss: 135.0804
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 84.1323
                       Mean reward: 902.75
               Mean episode length: 238.43
    Episode_Reward/reaching_object: 2.0295
     Episode_Reward/lifting_object: 172.4620
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0916
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 2.60s
                      Time elapsed: 01:02:24
                               ETA: 00:15:32

################################################################################
                     [1m Learning iteration 1602/2000 [0m                     

                       Computation: 39476 steps/s (collection: 2.376s, learning 0.115s)
             Mean action noise std: 3.72
          Mean value_function loss: 150.2356
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 84.1485
                       Mean reward: 846.85
               Mean episode length: 229.67
    Episode_Reward/reaching_object: 1.9838
     Episode_Reward/lifting_object: 167.7522
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0901
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 2.49s
                      Time elapsed: 01:02:27
                               ETA: 00:15:30

################################################################################
                     [1m Learning iteration 1603/2000 [0m                     

                       Computation: 37704 steps/s (collection: 2.446s, learning 0.162s)
             Mean action noise std: 3.72
          Mean value_function loss: 152.2681
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 84.1722
                       Mean reward: 831.23
               Mean episode length: 224.02
    Episode_Reward/reaching_object: 1.9661
     Episode_Reward/lifting_object: 166.3135
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0897
          Episode_Reward/joint_vel: -0.0885
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 2.61s
                      Time elapsed: 01:02:29
                               ETA: 00:15:28

################################################################################
                     [1m Learning iteration 1604/2000 [0m                     

                       Computation: 38911 steps/s (collection: 2.398s, learning 0.129s)
             Mean action noise std: 3.72
          Mean value_function loss: 152.5241
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 84.1844
                       Mean reward: 857.75
               Mean episode length: 230.26
    Episode_Reward/reaching_object: 1.9848
     Episode_Reward/lifting_object: 168.3133
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0904
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 2.53s
                      Time elapsed: 01:02:32
                               ETA: 00:15:25

################################################################################
                     [1m Learning iteration 1605/2000 [0m                     

                       Computation: 39593 steps/s (collection: 2.385s, learning 0.098s)
             Mean action noise std: 3.73
          Mean value_function loss: 146.1396
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 84.1968
                       Mean reward: 873.35
               Mean episode length: 233.81
    Episode_Reward/reaching_object: 2.0452
     Episode_Reward/lifting_object: 173.7544
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0927
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 2.48s
                      Time elapsed: 01:02:34
                               ETA: 00:15:23

################################################################################
                     [1m Learning iteration 1606/2000 [0m                     

                       Computation: 39727 steps/s (collection: 2.372s, learning 0.102s)
             Mean action noise std: 3.73
          Mean value_function loss: 189.0074
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 84.2084
                       Mean reward: 818.34
               Mean episode length: 219.17
    Episode_Reward/reaching_object: 1.9475
     Episode_Reward/lifting_object: 165.0618
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0889
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 2.47s
                      Time elapsed: 01:02:37
                               ETA: 00:15:21

################################################################################
                     [1m Learning iteration 1607/2000 [0m                     

                       Computation: 39774 steps/s (collection: 2.344s, learning 0.128s)
             Mean action noise std: 3.73
          Mean value_function loss: 146.5153
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 84.2274
                       Mean reward: 846.84
               Mean episode length: 226.55
    Episode_Reward/reaching_object: 1.9979
     Episode_Reward/lifting_object: 169.6772
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0914
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 2.47s
                      Time elapsed: 01:02:39
                               ETA: 00:15:18

################################################################################
                     [1m Learning iteration 1608/2000 [0m                     

                       Computation: 38755 steps/s (collection: 2.426s, learning 0.111s)
             Mean action noise std: 3.73
          Mean value_function loss: 200.1734
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 84.2400
                       Mean reward: 819.28
               Mean episode length: 220.90
    Episode_Reward/reaching_object: 1.9089
     Episode_Reward/lifting_object: 161.6363
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0875
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 2.54s
                      Time elapsed: 01:02:42
                               ETA: 00:15:16

################################################################################
                     [1m Learning iteration 1609/2000 [0m                     

                       Computation: 39605 steps/s (collection: 2.373s, learning 0.109s)
             Mean action noise std: 3.73
          Mean value_function loss: 170.3492
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 84.2503
                       Mean reward: 882.11
               Mean episode length: 235.22
    Episode_Reward/reaching_object: 1.9653
     Episode_Reward/lifting_object: 166.9840
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0903
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 2.48s
                      Time elapsed: 01:02:44
                               ETA: 00:15:14

################################################################################
                     [1m Learning iteration 1610/2000 [0m                     

                       Computation: 40385 steps/s (collection: 2.340s, learning 0.094s)
             Mean action noise std: 3.74
          Mean value_function loss: 163.4037
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 84.2630
                       Mean reward: 807.48
               Mean episode length: 219.40
    Episode_Reward/reaching_object: 1.9205
     Episode_Reward/lifting_object: 162.4343
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0882
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 2.43s
                      Time elapsed: 01:02:47
                               ETA: 00:15:11

################################################################################
                     [1m Learning iteration 1611/2000 [0m                     

                       Computation: 37813 steps/s (collection: 2.419s, learning 0.181s)
             Mean action noise std: 3.74
          Mean value_function loss: 168.6086
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 84.2849
                       Mean reward: 846.50
               Mean episode length: 225.34
    Episode_Reward/reaching_object: 1.9769
     Episode_Reward/lifting_object: 167.0458
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0907
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 2.60s
                      Time elapsed: 01:02:49
                               ETA: 00:15:09

################################################################################
                     [1m Learning iteration 1612/2000 [0m                     

                       Computation: 37546 steps/s (collection: 2.458s, learning 0.160s)
             Mean action noise std: 3.74
          Mean value_function loss: 156.4198
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 84.3017
                       Mean reward: 847.59
               Mean episode length: 226.53
    Episode_Reward/reaching_object: 1.9607
     Episode_Reward/lifting_object: 166.0560
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0900
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 2.62s
                      Time elapsed: 01:02:52
                               ETA: 00:15:07

################################################################################
                     [1m Learning iteration 1613/2000 [0m                     

                       Computation: 39969 steps/s (collection: 2.360s, learning 0.099s)
             Mean action noise std: 3.74
          Mean value_function loss: 169.3496
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 84.3120
                       Mean reward: 866.85
               Mean episode length: 232.72
    Episode_Reward/reaching_object: 1.9718
     Episode_Reward/lifting_object: 167.0487
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0906
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 2.46s
                      Time elapsed: 01:02:54
                               ETA: 00:15:05

################################################################################
                     [1m Learning iteration 1614/2000 [0m                     

                       Computation: 39377 steps/s (collection: 2.349s, learning 0.148s)
             Mean action noise std: 3.74
          Mean value_function loss: 151.7840
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 84.3263
                       Mean reward: 844.52
               Mean episode length: 226.34
    Episode_Reward/reaching_object: 1.9677
     Episode_Reward/lifting_object: 165.8200
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0902
          Episode_Reward/joint_vel: -0.0898
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 2.50s
                      Time elapsed: 01:02:57
                               ETA: 00:15:02

################################################################################
                     [1m Learning iteration 1615/2000 [0m                     

                       Computation: 37070 steps/s (collection: 2.535s, learning 0.117s)
             Mean action noise std: 3.75
          Mean value_function loss: 152.7990
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 84.3478
                       Mean reward: 841.32
               Mean episode length: 224.22
    Episode_Reward/reaching_object: 1.9625
     Episode_Reward/lifting_object: 165.8136
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0895
          Episode_Reward/joint_vel: -0.0884
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 2.65s
                      Time elapsed: 01:02:59
                               ETA: 00:15:00

################################################################################
                     [1m Learning iteration 1616/2000 [0m                     

                       Computation: 40257 steps/s (collection: 2.339s, learning 0.103s)
             Mean action noise std: 3.75
          Mean value_function loss: 151.6435
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 84.3657
                       Mean reward: 855.10
               Mean episode length: 227.91
    Episode_Reward/reaching_object: 1.9712
     Episode_Reward/lifting_object: 166.5600
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0900
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 2.44s
                      Time elapsed: 01:03:02
                               ETA: 00:14:58

################################################################################
                     [1m Learning iteration 1617/2000 [0m                     

                       Computation: 40598 steps/s (collection: 2.324s, learning 0.097s)
             Mean action noise std: 3.75
          Mean value_function loss: 139.1229
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 84.3816
                       Mean reward: 838.90
               Mean episode length: 226.13
    Episode_Reward/reaching_object: 2.0371
     Episode_Reward/lifting_object: 171.7399
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0933
          Episode_Reward/joint_vel: -0.0922
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 2.42s
                      Time elapsed: 01:03:04
                               ETA: 00:14:55

################################################################################
                     [1m Learning iteration 1618/2000 [0m                     

                       Computation: 40910 steps/s (collection: 2.308s, learning 0.095s)
             Mean action noise std: 3.75
          Mean value_function loss: 175.9212
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 84.4000
                       Mean reward: 826.22
               Mean episode length: 221.67
    Episode_Reward/reaching_object: 1.9632
     Episode_Reward/lifting_object: 165.8665
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0896
          Episode_Reward/joint_vel: -0.0877
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 2.40s
                      Time elapsed: 01:03:07
                               ETA: 00:14:53

################################################################################
                     [1m Learning iteration 1619/2000 [0m                     

                       Computation: 39464 steps/s (collection: 2.356s, learning 0.135s)
             Mean action noise std: 3.75
          Mean value_function loss: 155.8203
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 84.4130
                       Mean reward: 832.92
               Mean episode length: 222.48
    Episode_Reward/reaching_object: 1.9991
     Episode_Reward/lifting_object: 168.1993
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0912
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 2.49s
                      Time elapsed: 01:03:09
                               ETA: 00:14:51

################################################################################
                     [1m Learning iteration 1620/2000 [0m                     

                       Computation: 40190 steps/s (collection: 2.351s, learning 0.095s)
             Mean action noise std: 3.75
          Mean value_function loss: 188.1933
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 84.4244
                       Mean reward: 846.10
               Mean episode length: 225.51
    Episode_Reward/reaching_object: 1.9689
     Episode_Reward/lifting_object: 165.5545
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0900
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 2.45s
                      Time elapsed: 01:03:12
                               ETA: 00:14:48

################################################################################
                     [1m Learning iteration 1621/2000 [0m                     

                       Computation: 40381 steps/s (collection: 2.331s, learning 0.104s)
             Mean action noise std: 3.76
          Mean value_function loss: 169.0638
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 84.4418
                       Mean reward: 886.04
               Mean episode length: 234.32
    Episode_Reward/reaching_object: 2.0015
     Episode_Reward/lifting_object: 168.3932
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0913
          Episode_Reward/joint_vel: -0.0889
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 2.43s
                      Time elapsed: 01:03:14
                               ETA: 00:14:46

################################################################################
                     [1m Learning iteration 1622/2000 [0m                     

                       Computation: 40454 steps/s (collection: 2.316s, learning 0.114s)
             Mean action noise std: 3.76
          Mean value_function loss: 142.1228
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 84.4565
                       Mean reward: 873.21
               Mean episode length: 233.06
    Episode_Reward/reaching_object: 1.9963
     Episode_Reward/lifting_object: 167.3738
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0912
          Episode_Reward/joint_vel: -0.0888
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 2.43s
                      Time elapsed: 01:03:16
                               ETA: 00:14:44

################################################################################
                     [1m Learning iteration 1623/2000 [0m                     

                       Computation: 39306 steps/s (collection: 2.389s, learning 0.112s)
             Mean action noise std: 3.76
          Mean value_function loss: 173.5846
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 84.4669
                       Mean reward: 799.24
               Mean episode length: 215.12
    Episode_Reward/reaching_object: 1.9923
     Episode_Reward/lifting_object: 167.5407
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0906
          Episode_Reward/joint_vel: -0.0886
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 2.50s
                      Time elapsed: 01:03:19
                               ETA: 00:14:42

################################################################################
                     [1m Learning iteration 1624/2000 [0m                     

                       Computation: 39522 steps/s (collection: 2.386s, learning 0.102s)
             Mean action noise std: 3.76
          Mean value_function loss: 190.3388
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 84.4725
                       Mean reward: 789.02
               Mean episode length: 212.87
    Episode_Reward/reaching_object: 1.9719
     Episode_Reward/lifting_object: 165.3041
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0903
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 2.49s
                      Time elapsed: 01:03:21
                               ETA: 00:14:39

################################################################################
                     [1m Learning iteration 1625/2000 [0m                     

                       Computation: 39776 steps/s (collection: 2.351s, learning 0.120s)
             Mean action noise std: 3.76
          Mean value_function loss: 191.8357
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 84.4743
                       Mean reward: 793.33
               Mean episode length: 216.60
    Episode_Reward/reaching_object: 2.0109
     Episode_Reward/lifting_object: 168.2754
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0918
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 2.47s
                      Time elapsed: 01:03:24
                               ETA: 00:14:37

################################################################################
                     [1m Learning iteration 1626/2000 [0m                     

                       Computation: 40021 steps/s (collection: 2.344s, learning 0.112s)
             Mean action noise std: 3.76
          Mean value_function loss: 171.8675
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 84.4758
                       Mean reward: 844.95
               Mean episode length: 224.99
    Episode_Reward/reaching_object: 1.9518
     Episode_Reward/lifting_object: 163.0137
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0899
          Episode_Reward/joint_vel: -0.0869
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 2.46s
                      Time elapsed: 01:03:26
                               ETA: 00:14:35

################################################################################
                     [1m Learning iteration 1627/2000 [0m                     

                       Computation: 39664 steps/s (collection: 2.329s, learning 0.149s)
             Mean action noise std: 3.76
          Mean value_function loss: 143.4664
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 84.4783
                       Mean reward: 889.29
               Mean episode length: 235.89
    Episode_Reward/reaching_object: 2.0279
     Episode_Reward/lifting_object: 170.4580
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0929
          Episode_Reward/joint_vel: -0.0908
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 2.48s
                      Time elapsed: 01:03:29
                               ETA: 00:14:32

################################################################################
                     [1m Learning iteration 1628/2000 [0m                     

                       Computation: 33623 steps/s (collection: 2.805s, learning 0.119s)
             Mean action noise std: 3.76
          Mean value_function loss: 148.2965
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 84.4813
                       Mean reward: 874.44
               Mean episode length: 234.19
    Episode_Reward/reaching_object: 2.0016
     Episode_Reward/lifting_object: 167.6987
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0921
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 2.92s
                      Time elapsed: 01:03:32
                               ETA: 00:14:30

################################################################################
                     [1m Learning iteration 1629/2000 [0m                     

                       Computation: 38014 steps/s (collection: 2.482s, learning 0.104s)
             Mean action noise std: 3.76
          Mean value_function loss: 168.5480
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 84.4869
                       Mean reward: 834.64
               Mean episode length: 223.97
    Episode_Reward/reaching_object: 1.9657
     Episode_Reward/lifting_object: 165.0492
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0905
          Episode_Reward/joint_vel: -0.0873
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 2.59s
                      Time elapsed: 01:03:34
                               ETA: 00:14:28

################################################################################
                     [1m Learning iteration 1630/2000 [0m                     

                       Computation: 37110 steps/s (collection: 2.511s, learning 0.138s)
             Mean action noise std: 3.76
          Mean value_function loss: 160.3473
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 84.4996
                       Mean reward: 890.50
               Mean episode length: 237.22
    Episode_Reward/reaching_object: 2.0010
     Episode_Reward/lifting_object: 168.2150
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0921
          Episode_Reward/joint_vel: -0.0889
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 2.65s
                      Time elapsed: 01:03:37
                               ETA: 00:14:26

################################################################################
                     [1m Learning iteration 1631/2000 [0m                     

                       Computation: 39662 steps/s (collection: 2.379s, learning 0.100s)
             Mean action noise std: 3.77
          Mean value_function loss: 194.7665
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 84.5159
                       Mean reward: 873.31
               Mean episode length: 234.04
    Episode_Reward/reaching_object: 1.9529
     Episode_Reward/lifting_object: 163.5316
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0904
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 2.48s
                      Time elapsed: 01:03:40
                               ETA: 00:14:23

################################################################################
                     [1m Learning iteration 1632/2000 [0m                     

                       Computation: 37488 steps/s (collection: 2.466s, learning 0.157s)
             Mean action noise std: 3.77
          Mean value_function loss: 211.4371
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 84.5305
                       Mean reward: 848.49
               Mean episode length: 226.04
    Episode_Reward/reaching_object: 1.9116
     Episode_Reward/lifting_object: 160.0352
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.0864
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 2.62s
                      Time elapsed: 01:03:42
                               ETA: 00:14:21

################################################################################
                     [1m Learning iteration 1633/2000 [0m                     

                       Computation: 38726 steps/s (collection: 2.434s, learning 0.104s)
             Mean action noise std: 3.77
          Mean value_function loss: 210.3200
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 84.5427
                       Mean reward: 801.32
               Mean episode length: 214.08
    Episode_Reward/reaching_object: 1.9173
     Episode_Reward/lifting_object: 161.4602
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0888
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 2.54s
                      Time elapsed: 01:03:45
                               ETA: 00:14:19

################################################################################
                     [1m Learning iteration 1634/2000 [0m                     

                       Computation: 39733 steps/s (collection: 2.361s, learning 0.113s)
             Mean action noise std: 3.77
          Mean value_function loss: 215.6971
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 84.5523
                       Mean reward: 815.25
               Mean episode length: 218.62
    Episode_Reward/reaching_object: 1.9328
     Episode_Reward/lifting_object: 162.3262
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0897
          Episode_Reward/joint_vel: -0.0873
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 2.47s
                      Time elapsed: 01:03:47
                               ETA: 00:14:16

################################################################################
                     [1m Learning iteration 1635/2000 [0m                     

                       Computation: 39063 steps/s (collection: 2.412s, learning 0.105s)
             Mean action noise std: 3.77
          Mean value_function loss: 185.5131
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 84.5636
                       Mean reward: 837.97
               Mean episode length: 223.19
    Episode_Reward/reaching_object: 1.9576
     Episode_Reward/lifting_object: 163.9424
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0907
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 2.52s
                      Time elapsed: 01:03:50
                               ETA: 00:14:14

################################################################################
                     [1m Learning iteration 1636/2000 [0m                     

                       Computation: 39676 steps/s (collection: 2.370s, learning 0.108s)
             Mean action noise std: 3.77
          Mean value_function loss: 186.2253
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 84.5775
                       Mean reward: 833.45
               Mean episode length: 223.05
    Episode_Reward/reaching_object: 1.9671
     Episode_Reward/lifting_object: 165.0680
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0913
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 2.48s
                      Time elapsed: 01:03:52
                               ETA: 00:14:12

################################################################################
                     [1m Learning iteration 1637/2000 [0m                     

                       Computation: 39089 steps/s (collection: 2.393s, learning 0.122s)
             Mean action noise std: 3.78
          Mean value_function loss: 179.2388
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 84.5909
                       Mean reward: 846.75
               Mean episode length: 224.61
    Episode_Reward/reaching_object: 1.9434
     Episode_Reward/lifting_object: 163.7528
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0899
          Episode_Reward/joint_vel: -0.0861
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 2.51s
                      Time elapsed: 01:03:55
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 1638/2000 [0m                     

                       Computation: 39505 steps/s (collection: 2.371s, learning 0.118s)
             Mean action noise std: 3.78
          Mean value_function loss: 162.8746
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 84.6018
                       Mean reward: 835.70
               Mean episode length: 223.17
    Episode_Reward/reaching_object: 1.9982
     Episode_Reward/lifting_object: 167.7966
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0926
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 2.49s
                      Time elapsed: 01:03:57
                               ETA: 00:14:07

################################################################################
                     [1m Learning iteration 1639/2000 [0m                     

                       Computation: 39282 steps/s (collection: 2.404s, learning 0.098s)
             Mean action noise std: 3.78
          Mean value_function loss: 163.5513
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 84.6193
                       Mean reward: 838.10
               Mean episode length: 226.85
    Episode_Reward/reaching_object: 1.9485
     Episode_Reward/lifting_object: 163.1755
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0907
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 2.50s
                      Time elapsed: 01:04:00
                               ETA: 00:14:05

################################################################################
                     [1m Learning iteration 1640/2000 [0m                     

                       Computation: 37934 steps/s (collection: 2.437s, learning 0.154s)
             Mean action noise std: 3.78
          Mean value_function loss: 169.5315
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 84.6302
                       Mean reward: 837.74
               Mean episode length: 223.50
    Episode_Reward/reaching_object: 1.9490
     Episode_Reward/lifting_object: 163.3625
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0909
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 2.59s
                      Time elapsed: 01:04:02
                               ETA: 00:14:03

################################################################################
                     [1m Learning iteration 1641/2000 [0m                     

                       Computation: 38819 steps/s (collection: 2.413s, learning 0.120s)
             Mean action noise std: 3.78
          Mean value_function loss: 153.8960
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 84.6394
                       Mean reward: 832.66
               Mean episode length: 223.57
    Episode_Reward/reaching_object: 2.0238
     Episode_Reward/lifting_object: 170.1835
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0936
          Episode_Reward/joint_vel: -0.0908
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 2.53s
                      Time elapsed: 01:04:05
                               ETA: 00:14:00

################################################################################
                     [1m Learning iteration 1642/2000 [0m                     

                       Computation: 39909 steps/s (collection: 2.370s, learning 0.093s)
             Mean action noise std: 3.78
          Mean value_function loss: 251.9792
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 84.6538
                       Mean reward: 829.85
               Mean episode length: 222.38
    Episode_Reward/reaching_object: 1.9752
     Episode_Reward/lifting_object: 165.8402
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0917
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 2.46s
                      Time elapsed: 01:04:07
                               ETA: 00:13:58

################################################################################
                     [1m Learning iteration 1643/2000 [0m                     

                       Computation: 34968 steps/s (collection: 2.668s, learning 0.144s)
             Mean action noise std: 3.79
          Mean value_function loss: 246.1766
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 84.6730
                       Mean reward: 804.99
               Mean episode length: 217.47
    Episode_Reward/reaching_object: 1.9884
     Episode_Reward/lifting_object: 167.0023
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0924
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 2.81s
                      Time elapsed: 01:04:10
                               ETA: 00:13:56

################################################################################
                     [1m Learning iteration 1644/2000 [0m                     

                       Computation: 37144 steps/s (collection: 2.508s, learning 0.139s)
             Mean action noise std: 3.79
          Mean value_function loss: 189.8257
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 84.6896
                       Mean reward: 841.33
               Mean episode length: 224.20
    Episode_Reward/reaching_object: 1.9263
     Episode_Reward/lifting_object: 161.5059
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0901
          Episode_Reward/joint_vel: -0.0882
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 2.65s
                      Time elapsed: 01:04:13
                               ETA: 00:13:53

################################################################################
                     [1m Learning iteration 1645/2000 [0m                     

                       Computation: 38558 steps/s (collection: 2.413s, learning 0.137s)
             Mean action noise std: 3.79
          Mean value_function loss: 183.7334
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 84.7060
                       Mean reward: 819.29
               Mean episode length: 219.24
    Episode_Reward/reaching_object: 1.9668
     Episode_Reward/lifting_object: 165.7046
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0918
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 2.55s
                      Time elapsed: 01:04:15
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 1646/2000 [0m                     

                       Computation: 38397 steps/s (collection: 2.462s, learning 0.098s)
             Mean action noise std: 3.79
          Mean value_function loss: 188.2021
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 84.7198
                       Mean reward: 778.94
               Mean episode length: 210.05
    Episode_Reward/reaching_object: 1.9014
     Episode_Reward/lifting_object: 159.3625
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0892
          Episode_Reward/joint_vel: -0.0871
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 2.56s
                      Time elapsed: 01:04:18
                               ETA: 00:13:49

################################################################################
                     [1m Learning iteration 1647/2000 [0m                     

                       Computation: 38520 steps/s (collection: 2.372s, learning 0.180s)
             Mean action noise std: 3.79
          Mean value_function loss: 212.0704
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 84.7309
                       Mean reward: 818.58
               Mean episode length: 221.50
    Episode_Reward/reaching_object: 1.9628
     Episode_Reward/lifting_object: 164.1700
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0919
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 2.55s
                      Time elapsed: 01:04:20
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 1648/2000 [0m                     

                       Computation: 38481 steps/s (collection: 2.460s, learning 0.095s)
             Mean action noise std: 3.80
          Mean value_function loss: 148.9000
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 84.7467
                       Mean reward: 836.59
               Mean episode length: 224.99
    Episode_Reward/reaching_object: 1.9789
     Episode_Reward/lifting_object: 165.3734
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 2.55s
                      Time elapsed: 01:04:23
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 1649/2000 [0m                     

                       Computation: 39874 steps/s (collection: 2.349s, learning 0.117s)
             Mean action noise std: 3.80
          Mean value_function loss: 189.0414
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 84.7600
                       Mean reward: 834.70
               Mean episode length: 224.99
    Episode_Reward/reaching_object: 1.9673
     Episode_Reward/lifting_object: 165.5530
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0924
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 2.47s
                      Time elapsed: 01:04:25
                               ETA: 00:13:42

################################################################################
                     [1m Learning iteration 1650/2000 [0m                     

                       Computation: 38796 steps/s (collection: 2.418s, learning 0.116s)
             Mean action noise std: 3.80
          Mean value_function loss: 152.4123
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 84.7626
                       Mean reward: 886.52
               Mean episode length: 235.68
    Episode_Reward/reaching_object: 2.0156
     Episode_Reward/lifting_object: 169.8159
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0945
          Episode_Reward/joint_vel: -0.0921
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 2.53s
                      Time elapsed: 01:04:28
                               ETA: 00:13:40

################################################################################
                     [1m Learning iteration 1651/2000 [0m                     

                       Computation: 38718 steps/s (collection: 2.387s, learning 0.152s)
             Mean action noise std: 3.80
          Mean value_function loss: 155.0356
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 84.7695
                       Mean reward: 841.04
               Mean episode length: 225.22
    Episode_Reward/reaching_object: 1.9841
     Episode_Reward/lifting_object: 166.8895
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0932
          Episode_Reward/joint_vel: -0.0918
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 2.54s
                      Time elapsed: 01:04:30
                               ETA: 00:13:37

################################################################################
                     [1m Learning iteration 1652/2000 [0m                     

                       Computation: 39784 steps/s (collection: 2.362s, learning 0.109s)
             Mean action noise std: 3.80
          Mean value_function loss: 172.7287
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 84.7842
                       Mean reward: 820.85
               Mean episode length: 221.55
    Episode_Reward/reaching_object: 1.9635
     Episode_Reward/lifting_object: 164.9877
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0924
          Episode_Reward/joint_vel: -0.0889
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 2.47s
                      Time elapsed: 01:04:33
                               ETA: 00:13:35

################################################################################
                     [1m Learning iteration 1653/2000 [0m                     

                       Computation: 38884 steps/s (collection: 2.381s, learning 0.148s)
             Mean action noise std: 3.80
          Mean value_function loss: 178.6136
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 84.8011
                       Mean reward: 851.69
               Mean episode length: 227.26
    Episode_Reward/reaching_object: 1.9830
     Episode_Reward/lifting_object: 167.0554
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0934
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 2.53s
                      Time elapsed: 01:04:35
                               ETA: 00:13:33

################################################################################
                     [1m Learning iteration 1654/2000 [0m                     

                       Computation: 40475 steps/s (collection: 2.308s, learning 0.121s)
             Mean action noise std: 3.80
          Mean value_function loss: 166.9547
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 84.8131
                       Mean reward: 852.53
               Mean episode length: 230.87
    Episode_Reward/reaching_object: 1.9753
     Episode_Reward/lifting_object: 165.7235
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0934
          Episode_Reward/joint_vel: -0.0913
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 2.43s
                      Time elapsed: 01:04:38
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 1655/2000 [0m                     

                       Computation: 40238 steps/s (collection: 2.346s, learning 0.097s)
             Mean action noise std: 3.80
          Mean value_function loss: 178.7669
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 84.8223
                       Mean reward: 874.92
               Mean episode length: 233.55
    Episode_Reward/reaching_object: 1.9941
     Episode_Reward/lifting_object: 168.4998
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0941
          Episode_Reward/joint_vel: -0.0907
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 2.44s
                      Time elapsed: 01:04:40
                               ETA: 00:13:28

################################################################################
                     [1m Learning iteration 1656/2000 [0m                     

                       Computation: 40333 steps/s (collection: 2.346s, learning 0.091s)
             Mean action noise std: 3.81
          Mean value_function loss: 192.2956
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 84.8374
                       Mean reward: 820.62
               Mean episode length: 220.41
    Episode_Reward/reaching_object: 1.9227
     Episode_Reward/lifting_object: 161.9147
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0914
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 2.44s
                      Time elapsed: 01:04:43
                               ETA: 00:13:26

################################################################################
                     [1m Learning iteration 1657/2000 [0m                     

                       Computation: 40547 steps/s (collection: 2.312s, learning 0.113s)
             Mean action noise std: 3.81
          Mean value_function loss: 165.0048
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 84.8565
                       Mean reward: 835.87
               Mean episode length: 224.32
    Episode_Reward/reaching_object: 2.0097
     Episode_Reward/lifting_object: 169.1394
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0951
          Episode_Reward/joint_vel: -0.0921
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 2.42s
                      Time elapsed: 01:04:45
                               ETA: 00:13:23

################################################################################
                     [1m Learning iteration 1658/2000 [0m                     

                       Computation: 39868 steps/s (collection: 2.333s, learning 0.133s)
             Mean action noise std: 3.81
          Mean value_function loss: 135.1680
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 84.8681
                       Mean reward: 845.08
               Mean episode length: 226.41
    Episode_Reward/reaching_object: 1.9937
     Episode_Reward/lifting_object: 168.5199
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0944
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 2.47s
                      Time elapsed: 01:04:48
                               ETA: 00:13:21

################################################################################
                     [1m Learning iteration 1659/2000 [0m                     

                       Computation: 38315 steps/s (collection: 2.441s, learning 0.124s)
             Mean action noise std: 3.81
          Mean value_function loss: 125.9858
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 84.8726
                       Mean reward: 881.83
               Mean episode length: 236.53
    Episode_Reward/reaching_object: 1.9982
     Episode_Reward/lifting_object: 168.8106
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0946
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 2.57s
                      Time elapsed: 01:04:50
                               ETA: 00:13:19

################################################################################
                     [1m Learning iteration 1660/2000 [0m                     

                       Computation: 40055 steps/s (collection: 2.347s, learning 0.108s)
             Mean action noise std: 3.81
          Mean value_function loss: 157.9486
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 84.8817
                       Mean reward: 835.27
               Mean episode length: 224.32
    Episode_Reward/reaching_object: 1.9808
     Episode_Reward/lifting_object: 167.7122
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0938
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 2.45s
                      Time elapsed: 01:04:53
                               ETA: 00:13:16

################################################################################
                     [1m Learning iteration 1661/2000 [0m                     

                       Computation: 39272 steps/s (collection: 2.382s, learning 0.121s)
             Mean action noise std: 3.81
          Mean value_function loss: 145.5834
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 84.8931
                       Mean reward: 873.53
               Mean episode length: 234.11
    Episode_Reward/reaching_object: 1.9925
     Episode_Reward/lifting_object: 168.3408
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0945
          Episode_Reward/joint_vel: -0.0898
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 2.50s
                      Time elapsed: 01:04:55
                               ETA: 00:13:14

################################################################################
                     [1m Learning iteration 1662/2000 [0m                     

                       Computation: 40138 steps/s (collection: 2.339s, learning 0.111s)
             Mean action noise std: 3.82
          Mean value_function loss: 149.2791
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 84.9075
                       Mean reward: 870.60
               Mean episode length: 232.74
    Episode_Reward/reaching_object: 1.9919
     Episode_Reward/lifting_object: 168.7325
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0947
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 2.45s
                      Time elapsed: 01:04:58
                               ETA: 00:13:12

################################################################################
                     [1m Learning iteration 1663/2000 [0m                     

                       Computation: 41114 steps/s (collection: 2.286s, learning 0.105s)
             Mean action noise std: 3.82
          Mean value_function loss: 153.5731
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 84.9245
                       Mean reward: 840.56
               Mean episode length: 224.00
    Episode_Reward/reaching_object: 1.9760
     Episode_Reward/lifting_object: 167.3016
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0939
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 2.39s
                      Time elapsed: 01:05:00
                               ETA: 00:13:09

################################################################################
                     [1m Learning iteration 1664/2000 [0m                     

                       Computation: 41132 steps/s (collection: 2.280s, learning 0.110s)
             Mean action noise std: 3.82
          Mean value_function loss: 160.0624
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 84.9423
                       Mean reward: 875.36
               Mean episode length: 232.86
    Episode_Reward/reaching_object: 1.9795
     Episode_Reward/lifting_object: 167.9909
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0942
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 2.39s
                      Time elapsed: 01:05:02
                               ETA: 00:13:07

################################################################################
                     [1m Learning iteration 1665/2000 [0m                     

                       Computation: 41079 steps/s (collection: 2.302s, learning 0.091s)
             Mean action noise std: 3.82
          Mean value_function loss: 155.2334
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 84.9581
                       Mean reward: 885.15
               Mean episode length: 235.02
    Episode_Reward/reaching_object: 1.9547
     Episode_Reward/lifting_object: 165.4679
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.0868
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 2.39s
                      Time elapsed: 01:05:05
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 1666/2000 [0m                     

                       Computation: 24898 steps/s (collection: 3.851s, learning 0.098s)
             Mean action noise std: 3.82
          Mean value_function loss: 166.6715
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 84.9701
                       Mean reward: 817.24
               Mean episode length: 220.48
    Episode_Reward/reaching_object: 1.9584
     Episode_Reward/lifting_object: 165.3094
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0933
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 3.95s
                      Time elapsed: 01:05:09
                               ETA: 00:13:03

################################################################################
                     [1m Learning iteration 1667/2000 [0m                     

                       Computation: 13754 steps/s (collection: 7.016s, learning 0.131s)
             Mean action noise std: 3.83
          Mean value_function loss: 177.2203
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 84.9859
                       Mean reward: 754.80
               Mean episode length: 206.92
    Episode_Reward/reaching_object: 1.9527
     Episode_Reward/lifting_object: 164.6236
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0932
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 7.15s
                      Time elapsed: 01:05:16
                               ETA: 00:13:01

################################################################################
                     [1m Learning iteration 1668/2000 [0m                     

                       Computation: 13698 steps/s (collection: 7.048s, learning 0.128s)
             Mean action noise std: 3.83
          Mean value_function loss: 210.4505
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 85.0032
                       Mean reward: 833.29
               Mean episode length: 224.48
    Episode_Reward/reaching_object: 1.9618
     Episode_Reward/lifting_object: 165.8142
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0935
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 7.18s
                      Time elapsed: 01:05:23
                               ETA: 00:13:00

################################################################################
                     [1m Learning iteration 1669/2000 [0m                     

                       Computation: 13742 steps/s (collection: 7.029s, learning 0.124s)
             Mean action noise std: 3.83
          Mean value_function loss: 190.9158
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 85.0163
                       Mean reward: 850.25
               Mean episode length: 227.48
    Episode_Reward/reaching_object: 1.8863
     Episode_Reward/lifting_object: 158.4264
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0905
          Episode_Reward/joint_vel: -0.0852
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 7.15s
                      Time elapsed: 01:05:30
                               ETA: 00:12:59

################################################################################
                     [1m Learning iteration 1670/2000 [0m                     

                       Computation: 13365 steps/s (collection: 7.211s, learning 0.144s)
             Mean action noise std: 3.83
          Mean value_function loss: 167.1960
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 85.0244
                       Mean reward: 796.95
               Mean episode length: 214.72
    Episode_Reward/reaching_object: 1.9167
     Episode_Reward/lifting_object: 161.7505
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0918
          Episode_Reward/joint_vel: -0.0857
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 7.35s
                      Time elapsed: 01:05:38
                               ETA: 00:12:57

################################################################################
                     [1m Learning iteration 1671/2000 [0m                     

                       Computation: 13540 steps/s (collection: 7.136s, learning 0.124s)
             Mean action noise std: 3.83
          Mean value_function loss: 161.4969
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 85.0346
                       Mean reward: 875.16
               Mean episode length: 233.84
    Episode_Reward/reaching_object: 1.9710
     Episode_Reward/lifting_object: 166.4474
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0945
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 7.26s
                      Time elapsed: 01:05:45
                               ETA: 00:12:56

################################################################################
                     [1m Learning iteration 1672/2000 [0m                     

                       Computation: 13546 steps/s (collection: 7.121s, learning 0.136s)
             Mean action noise std: 3.83
          Mean value_function loss: 186.6734
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 85.0412
                       Mean reward: 839.60
               Mean episode length: 224.33
    Episode_Reward/reaching_object: 1.9379
     Episode_Reward/lifting_object: 163.7457
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.0874
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 7.26s
                      Time elapsed: 01:05:52
                               ETA: 00:12:54

################################################################################
                     [1m Learning iteration 1673/2000 [0m                     

                       Computation: 13690 steps/s (collection: 7.062s, learning 0.119s)
             Mean action noise std: 3.83
          Mean value_function loss: 179.5502
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 85.0437
                       Mean reward: 812.43
               Mean episode length: 218.89
    Episode_Reward/reaching_object: 1.9423
     Episode_Reward/lifting_object: 163.5868
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0933
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 7.18s
                      Time elapsed: 01:05:59
                               ETA: 00:12:53

################################################################################
                     [1m Learning iteration 1674/2000 [0m                     

                       Computation: 13861 steps/s (collection: 6.974s, learning 0.117s)
             Mean action noise std: 3.83
          Mean value_function loss: 173.2335
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 85.0488
                       Mean reward: 873.06
               Mean episode length: 232.49
    Episode_Reward/reaching_object: 1.9708
     Episode_Reward/lifting_object: 166.3915
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0947
          Episode_Reward/joint_vel: -0.0891
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 7.09s
                      Time elapsed: 01:06:06
                               ETA: 00:12:52

################################################################################
                     [1m Learning iteration 1675/2000 [0m                     

                       Computation: 20765 steps/s (collection: 4.644s, learning 0.090s)
             Mean action noise std: 3.84
          Mean value_function loss: 169.7705
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 85.0592
                       Mean reward: 851.13
               Mean episode length: 227.27
    Episode_Reward/reaching_object: 1.9662
     Episode_Reward/lifting_object: 166.3319
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0942
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 4.73s
                      Time elapsed: 01:06:11
                               ETA: 00:12:50

################################################################################
                     [1m Learning iteration 1676/2000 [0m                     

                       Computation: 43355 steps/s (collection: 2.180s, learning 0.088s)
             Mean action noise std: 3.84
          Mean value_function loss: 190.0270
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 85.0748
                       Mean reward: 828.40
               Mean episode length: 221.30
    Episode_Reward/reaching_object: 1.9570
     Episode_Reward/lifting_object: 165.5867
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0940
          Episode_Reward/joint_vel: -0.0887
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 2.27s
                      Time elapsed: 01:06:13
                               ETA: 00:12:47

################################################################################
                     [1m Learning iteration 1677/2000 [0m                     

                       Computation: 43574 steps/s (collection: 2.165s, learning 0.091s)
             Mean action noise std: 3.84
          Mean value_function loss: 175.9409
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 85.0854
                       Mean reward: 793.18
               Mean episode length: 215.26
    Episode_Reward/reaching_object: 1.9612
     Episode_Reward/lifting_object: 165.2105
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0944
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 2.26s
                      Time elapsed: 01:06:16
                               ETA: 00:12:45

################################################################################
                     [1m Learning iteration 1678/2000 [0m                     

                       Computation: 43326 steps/s (collection: 2.156s, learning 0.112s)
             Mean action noise std: 3.84
          Mean value_function loss: 194.7775
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 85.1019
                       Mean reward: 867.22
               Mean episode length: 231.71
    Episode_Reward/reaching_object: 1.9462
     Episode_Reward/lifting_object: 163.8431
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0939
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 2.27s
                      Time elapsed: 01:06:18
                               ETA: 00:12:42

################################################################################
                     [1m Learning iteration 1679/2000 [0m                     

                       Computation: 43537 steps/s (collection: 2.169s, learning 0.089s)
             Mean action noise std: 3.84
          Mean value_function loss: 179.4352
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 85.1269
                       Mean reward: 818.38
               Mean episode length: 223.03
    Episode_Reward/reaching_object: 1.9546
     Episode_Reward/lifting_object: 163.8544
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0945
          Episode_Reward/joint_vel: -0.0906
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 2.26s
                      Time elapsed: 01:06:20
                               ETA: 00:12:40

################################################################################
                     [1m Learning iteration 1680/2000 [0m                     

                       Computation: 42269 steps/s (collection: 2.207s, learning 0.119s)
             Mean action noise std: 3.85
          Mean value_function loss: 154.9925
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 85.1465
                       Mean reward: 881.24
               Mean episode length: 234.47
    Episode_Reward/reaching_object: 1.9612
     Episode_Reward/lifting_object: 165.9310
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0950
          Episode_Reward/joint_vel: -0.0907
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 2.33s
                      Time elapsed: 01:06:22
                               ETA: 00:12:38

################################################################################
                     [1m Learning iteration 1681/2000 [0m                     

                       Computation: 42089 steps/s (collection: 2.240s, learning 0.096s)
             Mean action noise std: 3.85
          Mean value_function loss: 159.0279
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 85.1613
                       Mean reward: 863.71
               Mean episode length: 232.00
    Episode_Reward/reaching_object: 1.9728
     Episode_Reward/lifting_object: 166.6073
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0953
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 2.34s
                      Time elapsed: 01:06:25
                               ETA: 00:12:35

################################################################################
                     [1m Learning iteration 1682/2000 [0m                     

                       Computation: 42242 steps/s (collection: 2.186s, learning 0.141s)
             Mean action noise std: 3.85
          Mean value_function loss: 187.8712
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 85.1786
                       Mean reward: 827.63
               Mean episode length: 221.21
    Episode_Reward/reaching_object: 1.9663
     Episode_Reward/lifting_object: 166.4951
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0955
          Episode_Reward/joint_vel: -0.0898
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 2.33s
                      Time elapsed: 01:06:27
                               ETA: 00:12:33

################################################################################
                     [1m Learning iteration 1683/2000 [0m                     

                       Computation: 40859 steps/s (collection: 2.308s, learning 0.098s)
             Mean action noise std: 3.85
          Mean value_function loss: 175.6306
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 85.1936
                       Mean reward: 837.09
               Mean episode length: 224.66
    Episode_Reward/reaching_object: 1.9850
     Episode_Reward/lifting_object: 167.5151
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0962
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 2.41s
                      Time elapsed: 01:06:30
                               ETA: 00:12:31

################################################################################
                     [1m Learning iteration 1684/2000 [0m                     

                       Computation: 41987 steps/s (collection: 2.243s, learning 0.098s)
             Mean action noise std: 3.85
          Mean value_function loss: 171.4338
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 85.2048
                       Mean reward: 835.86
               Mean episode length: 223.23
    Episode_Reward/reaching_object: 1.9653
     Episode_Reward/lifting_object: 166.9673
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0949
          Episode_Reward/joint_vel: -0.0875
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 2.34s
                      Time elapsed: 01:06:32
                               ETA: 00:12:28

################################################################################
                     [1m Learning iteration 1685/2000 [0m                     

                       Computation: 42548 steps/s (collection: 2.189s, learning 0.121s)
             Mean action noise std: 3.85
          Mean value_function loss: 163.0729
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 85.2121
                       Mean reward: 815.56
               Mean episode length: 219.83
    Episode_Reward/reaching_object: 1.9724
     Episode_Reward/lifting_object: 167.3949
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0957
          Episode_Reward/joint_vel: -0.0882
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 2.31s
                      Time elapsed: 01:06:34
                               ETA: 00:12:26

################################################################################
                     [1m Learning iteration 1686/2000 [0m                     

                       Computation: 42557 steps/s (collection: 2.211s, learning 0.099s)
             Mean action noise std: 3.86
          Mean value_function loss: 119.4613
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 85.2243
                       Mean reward: 910.33
               Mean episode length: 239.84
    Episode_Reward/reaching_object: 2.0324
     Episode_Reward/lifting_object: 172.7087
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0982
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 2.31s
                      Time elapsed: 01:06:36
                               ETA: 00:12:23

################################################################################
                     [1m Learning iteration 1687/2000 [0m                     

                       Computation: 42562 steps/s (collection: 2.204s, learning 0.106s)
             Mean action noise std: 3.86
          Mean value_function loss: 143.5519
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 85.2438
                       Mean reward: 854.84
               Mean episode length: 228.10
    Episode_Reward/reaching_object: 1.9739
     Episode_Reward/lifting_object: 167.7552
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0961
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 2.31s
                      Time elapsed: 01:06:39
                               ETA: 00:12:21

################################################################################
                     [1m Learning iteration 1688/2000 [0m                     

                       Computation: 42470 steps/s (collection: 2.225s, learning 0.090s)
             Mean action noise std: 3.86
          Mean value_function loss: 165.4549
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 85.2583
                       Mean reward: 845.62
               Mean episode length: 227.25
    Episode_Reward/reaching_object: 1.9434
     Episode_Reward/lifting_object: 164.9426
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0949
          Episode_Reward/joint_vel: -0.0870
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 2.31s
                      Time elapsed: 01:06:41
                               ETA: 00:12:19

################################################################################
                     [1m Learning iteration 1689/2000 [0m                     

                       Computation: 42683 steps/s (collection: 2.196s, learning 0.108s)
             Mean action noise std: 3.86
          Mean value_function loss: 178.7920
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 85.2767
                       Mean reward: 836.77
               Mean episode length: 224.82
    Episode_Reward/reaching_object: 1.9607
     Episode_Reward/lifting_object: 166.6287
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0959
          Episode_Reward/joint_vel: -0.0868
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 2.30s
                      Time elapsed: 01:06:43
                               ETA: 00:12:16

################################################################################
                     [1m Learning iteration 1690/2000 [0m                     

                       Computation: 42666 steps/s (collection: 2.202s, learning 0.102s)
             Mean action noise std: 3.87
          Mean value_function loss: 140.3740
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 85.2924
                       Mean reward: 855.63
               Mean episode length: 230.10
    Episode_Reward/reaching_object: 1.9455
     Episode_Reward/lifting_object: 164.7338
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0950
          Episode_Reward/joint_vel: -0.0859
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 2.30s
                      Time elapsed: 01:06:46
                               ETA: 00:12:14

################################################################################
                     [1m Learning iteration 1691/2000 [0m                     

                       Computation: 43440 steps/s (collection: 2.173s, learning 0.090s)
             Mean action noise std: 3.87
          Mean value_function loss: 166.2678
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 85.3068
                       Mean reward: 810.83
               Mean episode length: 218.68
    Episode_Reward/reaching_object: 1.9281
     Episode_Reward/lifting_object: 163.5481
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0941
          Episode_Reward/joint_vel: -0.0853
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 2.26s
                      Time elapsed: 01:06:48
                               ETA: 00:12:12

################################################################################
                     [1m Learning iteration 1692/2000 [0m                     

                       Computation: 41465 steps/s (collection: 2.263s, learning 0.108s)
             Mean action noise std: 3.87
          Mean value_function loss: 146.3639
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 85.3218
                       Mean reward: 874.51
               Mean episode length: 233.74
    Episode_Reward/reaching_object: 1.9433
     Episode_Reward/lifting_object: 164.4044
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0951
          Episode_Reward/joint_vel: -0.0862
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 2.37s
                      Time elapsed: 01:06:50
                               ETA: 00:12:09

################################################################################
                     [1m Learning iteration 1693/2000 [0m                     

                       Computation: 42600 steps/s (collection: 2.212s, learning 0.096s)
             Mean action noise std: 3.87
          Mean value_function loss: 169.8595
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 85.3374
                       Mean reward: 814.55
               Mean episode length: 217.77
    Episode_Reward/reaching_object: 1.9350
     Episode_Reward/lifting_object: 163.7648
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0947
          Episode_Reward/joint_vel: -0.0856
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 2.31s
                      Time elapsed: 01:06:53
                               ETA: 00:12:07

################################################################################
                     [1m Learning iteration 1694/2000 [0m                     

                       Computation: 42934 steps/s (collection: 2.181s, learning 0.109s)
             Mean action noise std: 3.87
          Mean value_function loss: 174.5722
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 85.3518
                       Mean reward: 834.60
               Mean episode length: 224.77
    Episode_Reward/reaching_object: 1.9548
     Episode_Reward/lifting_object: 165.4180
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0954
          Episode_Reward/joint_vel: -0.0856
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 2.29s
                      Time elapsed: 01:06:55
                               ETA: 00:12:04

################################################################################
                     [1m Learning iteration 1695/2000 [0m                     

                       Computation: 39690 steps/s (collection: 2.365s, learning 0.112s)
             Mean action noise std: 3.87
          Mean value_function loss: 133.1982
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 85.3658
                       Mean reward: 849.59
               Mean episode length: 228.00
    Episode_Reward/reaching_object: 2.0109
     Episode_Reward/lifting_object: 170.6608
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0979
          Episode_Reward/joint_vel: -0.0874
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 2.48s
                      Time elapsed: 01:06:57
                               ETA: 00:12:02

################################################################################
                     [1m Learning iteration 1696/2000 [0m                     

                       Computation: 39889 steps/s (collection: 2.323s, learning 0.141s)
             Mean action noise std: 3.88
          Mean value_function loss: 161.2884
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 85.3821
                       Mean reward: 807.74
               Mean episode length: 217.60
    Episode_Reward/reaching_object: 1.9773
     Episode_Reward/lifting_object: 167.6935
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0967
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 2.46s
                      Time elapsed: 01:07:00
                               ETA: 00:12:00

################################################################################
                     [1m Learning iteration 1697/2000 [0m                     

                       Computation: 39405 steps/s (collection: 2.384s, learning 0.111s)
             Mean action noise std: 3.88
          Mean value_function loss: 177.9413
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 85.3992
                       Mean reward: 821.46
               Mean episode length: 221.22
    Episode_Reward/reaching_object: 1.9176
     Episode_Reward/lifting_object: 161.6647
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0939
          Episode_Reward/joint_vel: -0.0842
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 2.49s
                      Time elapsed: 01:07:02
                               ETA: 00:11:57

################################################################################
                     [1m Learning iteration 1698/2000 [0m                     

                       Computation: 42086 steps/s (collection: 2.228s, learning 0.108s)
             Mean action noise std: 3.88
          Mean value_function loss: 163.6132
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 85.4110
                       Mean reward: 827.95
               Mean episode length: 222.50
    Episode_Reward/reaching_object: 1.9643
     Episode_Reward/lifting_object: 165.9059
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0963
          Episode_Reward/joint_vel: -0.0862
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 2.34s
                      Time elapsed: 01:07:05
                               ETA: 00:11:55

################################################################################
                     [1m Learning iteration 1699/2000 [0m                     

                       Computation: 42454 steps/s (collection: 2.224s, learning 0.092s)
             Mean action noise std: 3.88
          Mean value_function loss: 204.8724
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 85.4289
                       Mean reward: 801.81
               Mean episode length: 215.18
    Episode_Reward/reaching_object: 1.9026
     Episode_Reward/lifting_object: 160.3383
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0931
          Episode_Reward/joint_vel: -0.0830
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 2.32s
                      Time elapsed: 01:07:07
                               ETA: 00:11:53

################################################################################
                     [1m Learning iteration 1700/2000 [0m                     

                       Computation: 42106 steps/s (collection: 2.236s, learning 0.099s)
             Mean action noise std: 3.88
          Mean value_function loss: 165.4757
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 85.4474
                       Mean reward: 831.76
               Mean episode length: 222.12
    Episode_Reward/reaching_object: 1.9295
     Episode_Reward/lifting_object: 162.4912
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0946
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 2.33s
                      Time elapsed: 01:07:09
                               ETA: 00:11:50

################################################################################
                     [1m Learning iteration 1701/2000 [0m                     

                       Computation: 40051 steps/s (collection: 2.327s, learning 0.127s)
             Mean action noise std: 3.89
          Mean value_function loss: 183.5747
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 85.4636
                       Mean reward: 820.47
               Mean episode length: 222.21
    Episode_Reward/reaching_object: 1.9054
     Episode_Reward/lifting_object: 159.8840
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0937
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 2.45s
                      Time elapsed: 01:07:12
                               ETA: 00:11:48

################################################################################
                     [1m Learning iteration 1702/2000 [0m                     

                       Computation: 40700 steps/s (collection: 2.267s, learning 0.149s)
             Mean action noise std: 3.89
          Mean value_function loss: 177.5706
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 85.4739
                       Mean reward: 848.93
               Mean episode length: 225.91
    Episode_Reward/reaching_object: 1.9617
     Episode_Reward/lifting_object: 165.5219
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0959
          Episode_Reward/joint_vel: -0.0849
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 2.42s
                      Time elapsed: 01:07:14
                               ETA: 00:11:46

################################################################################
                     [1m Learning iteration 1703/2000 [0m                     

                       Computation: 42869 steps/s (collection: 2.204s, learning 0.089s)
             Mean action noise std: 3.89
          Mean value_function loss: 261.7786
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 85.4890
                       Mean reward: 833.53
               Mean episode length: 225.40
    Episode_Reward/reaching_object: 1.8938
     Episode_Reward/lifting_object: 157.8453
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0933
          Episode_Reward/joint_vel: -0.0842
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 2.29s
                      Time elapsed: 01:07:17
                               ETA: 00:11:43

################################################################################
                     [1m Learning iteration 1704/2000 [0m                     

                       Computation: 43137 steps/s (collection: 2.191s, learning 0.088s)
             Mean action noise std: 3.89
          Mean value_function loss: 247.1117
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 85.5119
                       Mean reward: 732.31
               Mean episode length: 198.50
    Episode_Reward/reaching_object: 1.9088
     Episode_Reward/lifting_object: 160.0680
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0939
          Episode_Reward/joint_vel: -0.0829
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 2.28s
                      Time elapsed: 01:07:19
                               ETA: 00:11:41

################################################################################
                     [1m Learning iteration 1705/2000 [0m                     

                       Computation: 42575 steps/s (collection: 2.215s, learning 0.094s)
             Mean action noise std: 3.89
          Mean value_function loss: 223.2955
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 85.5269
                       Mean reward: 856.59
               Mean episode length: 227.97
    Episode_Reward/reaching_object: 1.9343
     Episode_Reward/lifting_object: 162.3652
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0950
          Episode_Reward/joint_vel: -0.0836
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 2.31s
                      Time elapsed: 01:07:21
                               ETA: 00:11:38

################################################################################
                     [1m Learning iteration 1706/2000 [0m                     

                       Computation: 43102 steps/s (collection: 2.186s, learning 0.095s)
             Mean action noise std: 3.90
          Mean value_function loss: 201.3476
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 85.5336
                       Mean reward: 857.60
               Mean episode length: 227.91
    Episode_Reward/reaching_object: 1.9414
     Episode_Reward/lifting_object: 162.8923
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0956
          Episode_Reward/joint_vel: -0.0856
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 2.28s
                      Time elapsed: 01:07:23
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 1707/2000 [0m                     

                       Computation: 43105 steps/s (collection: 2.191s, learning 0.090s)
             Mean action noise std: 3.90
          Mean value_function loss: 259.6194
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 85.5426
                       Mean reward: 782.25
               Mean episode length: 210.17
    Episode_Reward/reaching_object: 1.8377
     Episode_Reward/lifting_object: 153.6088
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0909
          Episode_Reward/joint_vel: -0.0809
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 2.28s
                      Time elapsed: 01:07:26
                               ETA: 00:11:34

################################################################################
                     [1m Learning iteration 1708/2000 [0m                     

                       Computation: 42876 steps/s (collection: 2.199s, learning 0.093s)
             Mean action noise std: 3.90
          Mean value_function loss: 272.3377
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 85.5525
                       Mean reward: 741.24
               Mean episode length: 204.99
    Episode_Reward/reaching_object: 1.8787
     Episode_Reward/lifting_object: 156.9830
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0929
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 2.29s
                      Time elapsed: 01:07:28
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 1709/2000 [0m                     

                       Computation: 42404 steps/s (collection: 2.196s, learning 0.122s)
             Mean action noise std: 3.90
          Mean value_function loss: 236.6384
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 85.5584
                       Mean reward: 795.01
               Mean episode length: 214.45
    Episode_Reward/reaching_object: 1.8684
     Episode_Reward/lifting_object: 156.2355
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0923
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 2.32s
                      Time elapsed: 01:07:30
                               ETA: 00:11:29

################################################################################
                     [1m Learning iteration 1710/2000 [0m                     

                       Computation: 40089 steps/s (collection: 2.365s, learning 0.087s)
             Mean action noise std: 3.90
          Mean value_function loss: 197.9546
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 85.5648
                       Mean reward: 851.10
               Mean episode length: 228.60
    Episode_Reward/reaching_object: 1.9350
     Episode_Reward/lifting_object: 161.2303
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0959
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 2.45s
                      Time elapsed: 01:07:33
                               ETA: 00:11:26

################################################################################
                     [1m Learning iteration 1711/2000 [0m                     

                       Computation: 39800 steps/s (collection: 2.343s, learning 0.127s)
             Mean action noise std: 3.90
          Mean value_function loss: 186.9978
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 85.5739
                       Mean reward: 812.00
               Mean episode length: 218.55
    Episode_Reward/reaching_object: 1.9170
     Episode_Reward/lifting_object: 160.1377
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0947
          Episode_Reward/joint_vel: -0.0862
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 2.47s
                      Time elapsed: 01:07:35
                               ETA: 00:11:24

################################################################################
                     [1m Learning iteration 1712/2000 [0m                     

                       Computation: 36849 steps/s (collection: 2.505s, learning 0.163s)
             Mean action noise std: 3.90
          Mean value_function loss: 199.1574
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 85.5854
                       Mean reward: 823.90
               Mean episode length: 221.61
    Episode_Reward/reaching_object: 1.9169
     Episode_Reward/lifting_object: 159.6842
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0951
          Episode_Reward/joint_vel: -0.0871
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 2.67s
                      Time elapsed: 01:07:38
                               ETA: 00:11:22

################################################################################
                     [1m Learning iteration 1713/2000 [0m                     

                       Computation: 40068 steps/s (collection: 2.318s, learning 0.136s)
             Mean action noise std: 3.90
          Mean value_function loss: 219.5127
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 85.5962
                       Mean reward: 807.31
               Mean episode length: 217.26
    Episode_Reward/reaching_object: 1.9528
     Episode_Reward/lifting_object: 163.3977
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0965
          Episode_Reward/joint_vel: -0.0886
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 2.45s
                      Time elapsed: 01:07:40
                               ETA: 00:11:19

################################################################################
                     [1m Learning iteration 1714/2000 [0m                     

                       Computation: 40898 steps/s (collection: 2.301s, learning 0.103s)
             Mean action noise std: 3.91
          Mean value_function loss: 206.9651
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 85.6106
                       Mean reward: 823.49
               Mean episode length: 220.31
    Episode_Reward/reaching_object: 1.9137
     Episode_Reward/lifting_object: 159.4181
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0951
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 2.40s
                      Time elapsed: 01:07:43
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 1715/2000 [0m                     

                       Computation: 39963 steps/s (collection: 2.295s, learning 0.165s)
             Mean action noise std: 3.91
          Mean value_function loss: 236.2321
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 85.6237
                       Mean reward: 783.71
               Mean episode length: 212.35
    Episode_Reward/reaching_object: 1.9475
     Episode_Reward/lifting_object: 162.4495
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0965
          Episode_Reward/joint_vel: -0.0888
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 2.46s
                      Time elapsed: 01:07:45
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 1716/2000 [0m                     

                       Computation: 42104 steps/s (collection: 2.239s, learning 0.096s)
             Mean action noise std: 3.91
          Mean value_function loss: 212.5414
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 85.6432
                       Mean reward: 841.52
               Mean episode length: 226.76
    Episode_Reward/reaching_object: 1.9009
     Episode_Reward/lifting_object: 158.2509
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0944
          Episode_Reward/joint_vel: -0.0868
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 2.33s
                      Time elapsed: 01:07:48
                               ETA: 00:11:12

################################################################################
                     [1m Learning iteration 1717/2000 [0m                     

                       Computation: 42693 steps/s (collection: 2.203s, learning 0.100s)
             Mean action noise std: 3.91
          Mean value_function loss: 162.8773
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 85.6558
                       Mean reward: 851.25
               Mean episode length: 227.02
    Episode_Reward/reaching_object: 1.9558
     Episode_Reward/lifting_object: 163.4511
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0971
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 2.30s
                      Time elapsed: 01:07:50
                               ETA: 00:11:10

################################################################################
                     [1m Learning iteration 1718/2000 [0m                     

                       Computation: 42320 steps/s (collection: 2.228s, learning 0.095s)
             Mean action noise std: 3.91
          Mean value_function loss: 145.3490
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 85.6677
                       Mean reward: 863.21
               Mean episode length: 232.81
    Episode_Reward/reaching_object: 2.0247
     Episode_Reward/lifting_object: 169.3180
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.1004
          Episode_Reward/joint_vel: -0.0920
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 2.32s
                      Time elapsed: 01:07:52
                               ETA: 00:11:08

################################################################################
                     [1m Learning iteration 1719/2000 [0m                     

                       Computation: 42093 steps/s (collection: 2.194s, learning 0.141s)
             Mean action noise std: 3.92
          Mean value_function loss: 159.6232
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 85.6771
                       Mean reward: 843.24
               Mean episode length: 224.20
    Episode_Reward/reaching_object: 1.9755
     Episode_Reward/lifting_object: 165.5758
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0980
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 2.34s
                      Time elapsed: 01:07:54
                               ETA: 00:11:05

################################################################################
                     [1m Learning iteration 1720/2000 [0m                     

                       Computation: 42779 steps/s (collection: 2.207s, learning 0.091s)
             Mean action noise std: 3.92
          Mean value_function loss: 170.3498
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 85.6930
                       Mean reward: 864.87
               Mean episode length: 229.39
    Episode_Reward/reaching_object: 2.0133
     Episode_Reward/lifting_object: 169.6449
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0996
          Episode_Reward/joint_vel: -0.0907
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 2.30s
                      Time elapsed: 01:07:57
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 1721/2000 [0m                     

                       Computation: 41762 steps/s (collection: 2.244s, learning 0.110s)
             Mean action noise std: 3.92
          Mean value_function loss: 199.2951
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 85.7067
                       Mean reward: 808.20
               Mean episode length: 216.68
    Episode_Reward/reaching_object: 1.9194
     Episode_Reward/lifting_object: 160.6339
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0960
          Episode_Reward/joint_vel: -0.0879
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 2.35s
                      Time elapsed: 01:07:59
                               ETA: 00:11:00

################################################################################
                     [1m Learning iteration 1722/2000 [0m                     

                       Computation: 41786 steps/s (collection: 2.245s, learning 0.108s)
             Mean action noise std: 3.92
          Mean value_function loss: 133.4869
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 85.7225
                       Mean reward: 879.00
               Mean episode length: 235.46
    Episode_Reward/reaching_object: 1.9704
     Episode_Reward/lifting_object: 165.2413
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0983
          Episode_Reward/joint_vel: -0.0888
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 2.35s
                      Time elapsed: 01:08:02
                               ETA: 00:10:58

################################################################################
                     [1m Learning iteration 1723/2000 [0m                     

                       Computation: 40942 steps/s (collection: 2.278s, learning 0.124s)
             Mean action noise std: 3.92
          Mean value_function loss: 158.3657
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 85.7433
                       Mean reward: 837.73
               Mean episode length: 224.09
    Episode_Reward/reaching_object: 1.9706
     Episode_Reward/lifting_object: 165.4118
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0985
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 2.40s
                      Time elapsed: 01:08:04
                               ETA: 00:10:56

################################################################################
                     [1m Learning iteration 1724/2000 [0m                     

                       Computation: 41716 steps/s (collection: 2.264s, learning 0.093s)
             Mean action noise std: 3.93
          Mean value_function loss: 176.2063
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 85.7572
                       Mean reward: 833.61
               Mean episode length: 222.96
    Episode_Reward/reaching_object: 1.9666
     Episode_Reward/lifting_object: 165.7997
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0979
          Episode_Reward/joint_vel: -0.0886
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 2.36s
                      Time elapsed: 01:08:06
                               ETA: 00:10:53

################################################################################
                     [1m Learning iteration 1725/2000 [0m                     

                       Computation: 42581 steps/s (collection: 2.220s, learning 0.089s)
             Mean action noise std: 3.93
          Mean value_function loss: 195.5507
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 85.7701
                       Mean reward: 819.29
               Mean episode length: 219.58
    Episode_Reward/reaching_object: 1.9849
     Episode_Reward/lifting_object: 167.0866
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0993
          Episode_Reward/joint_vel: -0.0898
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 2.31s
                      Time elapsed: 01:08:09
                               ETA: 00:10:51

################################################################################
                     [1m Learning iteration 1726/2000 [0m                     

                       Computation: 41404 steps/s (collection: 2.273s, learning 0.102s)
             Mean action noise std: 3.93
          Mean value_function loss: 175.5829
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 85.7781
                       Mean reward: 801.21
               Mean episode length: 219.24
    Episode_Reward/reaching_object: 1.9398
     Episode_Reward/lifting_object: 162.9009
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0980
          Episode_Reward/joint_vel: -0.0888
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 2.37s
                      Time elapsed: 01:08:11
                               ETA: 00:10:49

################################################################################
                     [1m Learning iteration 1727/2000 [0m                     

                       Computation: 41194 steps/s (collection: 2.271s, learning 0.115s)
             Mean action noise std: 3.93
          Mean value_function loss: 174.8563
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 85.7882
                       Mean reward: 875.63
               Mean episode length: 232.73
    Episode_Reward/reaching_object: 1.9323
     Episode_Reward/lifting_object: 162.7028
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0968
          Episode_Reward/joint_vel: -0.0874
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 2.39s
                      Time elapsed: 01:08:13
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 1728/2000 [0m                     

                       Computation: 42408 steps/s (collection: 2.212s, learning 0.107s)
             Mean action noise std: 3.93
          Mean value_function loss: 190.7622
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 85.8037
                       Mean reward: 817.75
               Mean episode length: 218.46
    Episode_Reward/reaching_object: 1.9170
     Episode_Reward/lifting_object: 161.2312
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0963
          Episode_Reward/joint_vel: -0.0865
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 2.32s
                      Time elapsed: 01:08:16
                               ETA: 00:10:44

################################################################################
                     [1m Learning iteration 1729/2000 [0m                     

                       Computation: 41469 steps/s (collection: 2.259s, learning 0.111s)
             Mean action noise std: 3.93
          Mean value_function loss: 138.5116
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 85.8178
                       Mean reward: 832.84
               Mean episode length: 222.79
    Episode_Reward/reaching_object: 1.9804
     Episode_Reward/lifting_object: 167.2464
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0993
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 2.37s
                      Time elapsed: 01:08:18
                               ETA: 00:10:42

################################################################################
                     [1m Learning iteration 1730/2000 [0m                     

                       Computation: 41948 steps/s (collection: 2.241s, learning 0.103s)
             Mean action noise std: 3.93
          Mean value_function loss: 191.5593
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 85.8279
                       Mean reward: 829.01
               Mean episode length: 221.74
    Episode_Reward/reaching_object: 1.9680
     Episode_Reward/lifting_object: 166.0085
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0986
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 2.34s
                      Time elapsed: 01:08:20
                               ETA: 00:10:39

################################################################################
                     [1m Learning iteration 1731/2000 [0m                     

                       Computation: 42629 steps/s (collection: 2.209s, learning 0.097s)
             Mean action noise std: 3.94
          Mean value_function loss: 186.8206
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 85.8360
                       Mean reward: 856.03
               Mean episode length: 227.07
    Episode_Reward/reaching_object: 1.9542
     Episode_Reward/lifting_object: 165.0630
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0981
          Episode_Reward/joint_vel: -0.0882
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 2.31s
                      Time elapsed: 01:08:23
                               ETA: 00:10:37

################################################################################
                     [1m Learning iteration 1732/2000 [0m                     

                       Computation: 41831 steps/s (collection: 2.236s, learning 0.115s)
             Mean action noise std: 3.94
          Mean value_function loss: 165.1518
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 85.8477
                       Mean reward: 875.56
               Mean episode length: 232.24
    Episode_Reward/reaching_object: 1.9754
     Episode_Reward/lifting_object: 167.1378
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0993
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 2.35s
                      Time elapsed: 01:08:25
                               ETA: 00:10:34

################################################################################
                     [1m Learning iteration 1733/2000 [0m                     

                       Computation: 41940 steps/s (collection: 2.249s, learning 0.095s)
             Mean action noise std: 3.94
          Mean value_function loss: 137.8329
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 85.8620
                       Mean reward: 864.17
               Mean episode length: 230.79
    Episode_Reward/reaching_object: 1.9903
     Episode_Reward/lifting_object: 167.8730
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.1004
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 2.34s
                      Time elapsed: 01:08:27
                               ETA: 00:10:32

################################################################################
                     [1m Learning iteration 1734/2000 [0m                     

                       Computation: 42441 steps/s (collection: 2.220s, learning 0.096s)
             Mean action noise std: 3.94
          Mean value_function loss: 134.3459
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 85.8762
                       Mean reward: 895.82
               Mean episode length: 237.81
    Episode_Reward/reaching_object: 2.0349
     Episode_Reward/lifting_object: 171.8491
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.1025
          Episode_Reward/joint_vel: -0.0927
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 2.32s
                      Time elapsed: 01:08:30
                               ETA: 00:10:30

################################################################################
                     [1m Learning iteration 1735/2000 [0m                     

                       Computation: 42807 steps/s (collection: 2.197s, learning 0.100s)
             Mean action noise std: 3.94
          Mean value_function loss: 184.8168
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 85.8864
                       Mean reward: 862.07
               Mean episode length: 229.07
    Episode_Reward/reaching_object: 1.9908
     Episode_Reward/lifting_object: 167.8897
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.1003
          Episode_Reward/joint_vel: -0.0908
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 2.30s
                      Time elapsed: 01:08:32
                               ETA: 00:10:27

################################################################################
                     [1m Learning iteration 1736/2000 [0m                     

                       Computation: 41951 steps/s (collection: 2.221s, learning 0.122s)
             Mean action noise std: 3.94
          Mean value_function loss: 141.8806
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 85.8910
                       Mean reward: 861.88
               Mean episode length: 230.41
    Episode_Reward/reaching_object: 1.9981
     Episode_Reward/lifting_object: 168.5566
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.1009
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 2.34s
                      Time elapsed: 01:08:34
                               ETA: 00:10:25

################################################################################
                     [1m Learning iteration 1737/2000 [0m                     

                       Computation: 42008 steps/s (collection: 2.250s, learning 0.090s)
             Mean action noise std: 3.94
          Mean value_function loss: 189.3050
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 85.8978
                       Mean reward: 801.87
               Mean episode length: 215.53
    Episode_Reward/reaching_object: 1.9419
     Episode_Reward/lifting_object: 164.4696
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0985
          Episode_Reward/joint_vel: -0.0884
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 2.34s
                      Time elapsed: 01:08:37
                               ETA: 00:10:23

################################################################################
                     [1m Learning iteration 1738/2000 [0m                     

                       Computation: 41584 steps/s (collection: 2.257s, learning 0.107s)
             Mean action noise std: 3.95
          Mean value_function loss: 139.2899
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 85.9093
                       Mean reward: 872.80
               Mean episode length: 232.24
    Episode_Reward/reaching_object: 1.9698
     Episode_Reward/lifting_object: 166.7467
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0997
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 2.36s
                      Time elapsed: 01:08:39
                               ETA: 00:10:20

################################################################################
                     [1m Learning iteration 1739/2000 [0m                     

                       Computation: 40744 steps/s (collection: 2.235s, learning 0.178s)
             Mean action noise std: 3.95
          Mean value_function loss: 148.2164
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 85.9205
                       Mean reward: 843.64
               Mean episode length: 225.12
    Episode_Reward/reaching_object: 1.9943
     Episode_Reward/lifting_object: 169.1179
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.1012
          Episode_Reward/joint_vel: -0.0914
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 2.41s
                      Time elapsed: 01:08:41
                               ETA: 00:10:18

################################################################################
                     [1m Learning iteration 1740/2000 [0m                     

                       Computation: 42741 steps/s (collection: 2.209s, learning 0.091s)
             Mean action noise std: 3.95
          Mean value_function loss: 156.0536
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 85.9247
                       Mean reward: 858.47
               Mean episode length: 229.22
    Episode_Reward/reaching_object: 1.9692
     Episode_Reward/lifting_object: 166.5035
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.1000
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 2.30s
                      Time elapsed: 01:08:44
                               ETA: 00:10:15

################################################################################
                     [1m Learning iteration 1741/2000 [0m                     

                       Computation: 42157 steps/s (collection: 2.221s, learning 0.111s)
             Mean action noise std: 3.95
          Mean value_function loss: 180.6292
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 85.9341
                       Mean reward: 798.46
               Mean episode length: 213.83
    Episode_Reward/reaching_object: 1.9250
     Episode_Reward/lifting_object: 161.9558
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0981
          Episode_Reward/joint_vel: -0.0888
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 2.33s
                      Time elapsed: 01:08:46
                               ETA: 00:10:13

################################################################################
                     [1m Learning iteration 1742/2000 [0m                     

                       Computation: 40922 steps/s (collection: 2.276s, learning 0.126s)
             Mean action noise std: 3.95
          Mean value_function loss: 186.7643
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 85.9442
                       Mean reward: 813.86
               Mean episode length: 217.50
    Episode_Reward/reaching_object: 1.9565
     Episode_Reward/lifting_object: 165.2058
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0995
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 2.40s
                      Time elapsed: 01:08:48
                               ETA: 00:10:11

################################################################################
                     [1m Learning iteration 1743/2000 [0m                     

                       Computation: 41840 steps/s (collection: 2.244s, learning 0.105s)
             Mean action noise std: 3.95
          Mean value_function loss: 177.6078
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 85.9539
                       Mean reward: 815.89
               Mean episode length: 218.25
    Episode_Reward/reaching_object: 1.8944
     Episode_Reward/lifting_object: 159.8638
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0963
          Episode_Reward/joint_vel: -0.0863
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 2.35s
                      Time elapsed: 01:08:51
                               ETA: 00:10:08

################################################################################
                     [1m Learning iteration 1744/2000 [0m                     

                       Computation: 42256 steps/s (collection: 2.240s, learning 0.087s)
             Mean action noise std: 3.95
          Mean value_function loss: 148.3184
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 85.9671
                       Mean reward: 881.81
               Mean episode length: 234.58
    Episode_Reward/reaching_object: 1.9991
     Episode_Reward/lifting_object: 169.3770
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.1016
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 2.33s
                      Time elapsed: 01:08:53
                               ETA: 00:10:06

################################################################################
                     [1m Learning iteration 1745/2000 [0m                     

                       Computation: 41573 steps/s (collection: 2.237s, learning 0.128s)
             Mean action noise std: 3.96
          Mean value_function loss: 150.0851
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 85.9815
                       Mean reward: 853.07
               Mean episode length: 227.09
    Episode_Reward/reaching_object: 1.9934
     Episode_Reward/lifting_object: 168.4672
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.1012
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 2.36s
                      Time elapsed: 01:08:56
                               ETA: 00:10:04

################################################################################
                     [1m Learning iteration 1746/2000 [0m                     

                       Computation: 42391 steps/s (collection: 2.223s, learning 0.096s)
             Mean action noise std: 3.96
          Mean value_function loss: 152.3591
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 85.9955
                       Mean reward: 833.93
               Mean episode length: 223.34
    Episode_Reward/reaching_object: 2.0070
     Episode_Reward/lifting_object: 169.6695
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.1019
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 2.32s
                      Time elapsed: 01:08:58
                               ETA: 00:10:01

################################################################################
                     [1m Learning iteration 1747/2000 [0m                     

                       Computation: 42505 steps/s (collection: 2.215s, learning 0.098s)
             Mean action noise std: 3.96
          Mean value_function loss: 149.4745
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 86.0124
                       Mean reward: 865.68
               Mean episode length: 228.52
    Episode_Reward/reaching_object: 1.9623
     Episode_Reward/lifting_object: 165.2095
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.1002
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 2.31s
                      Time elapsed: 01:09:00
                               ETA: 00:09:59

################################################################################
                     [1m Learning iteration 1748/2000 [0m                     

                       Computation: 40083 steps/s (collection: 2.294s, learning 0.158s)
             Mean action noise std: 3.96
          Mean value_function loss: 163.9001
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 86.0252
                       Mean reward: 804.75
               Mean episode length: 219.12
    Episode_Reward/reaching_object: 1.9705
     Episode_Reward/lifting_object: 166.1116
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.1008
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 2.45s
                      Time elapsed: 01:09:03
                               ETA: 00:09:56

################################################################################
                     [1m Learning iteration 1749/2000 [0m                     

                       Computation: 42105 steps/s (collection: 2.243s, learning 0.092s)
             Mean action noise std: 3.96
          Mean value_function loss: 137.8361
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 86.0358
                       Mean reward: 872.23
               Mean episode length: 232.91
    Episode_Reward/reaching_object: 1.9877
     Episode_Reward/lifting_object: 167.6497
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.1018
          Episode_Reward/joint_vel: -0.0913
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 2.33s
                      Time elapsed: 01:09:05
                               ETA: 00:09:54

################################################################################
                     [1m Learning iteration 1750/2000 [0m                     

                       Computation: 41459 steps/s (collection: 2.235s, learning 0.136s)
             Mean action noise std: 3.96
          Mean value_function loss: 186.2622
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 86.0483
                       Mean reward: 811.16
               Mean episode length: 217.69
    Episode_Reward/reaching_object: 1.9646
     Episode_Reward/lifting_object: 165.4638
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.1003
          Episode_Reward/joint_vel: -0.0891
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 2.37s
                      Time elapsed: 01:09:07
                               ETA: 00:09:52

################################################################################
                     [1m Learning iteration 1751/2000 [0m                     

                       Computation: 40921 steps/s (collection: 2.306s, learning 0.097s)
             Mean action noise std: 3.97
          Mean value_function loss: 191.1478
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 86.0598
                       Mean reward: 806.39
               Mean episode length: 215.57
    Episode_Reward/reaching_object: 1.9326
     Episode_Reward/lifting_object: 162.8460
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0986
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 2.40s
                      Time elapsed: 01:09:10
                               ETA: 00:09:49

################################################################################
                     [1m Learning iteration 1752/2000 [0m                     

                       Computation: 42937 steps/s (collection: 2.198s, learning 0.091s)
             Mean action noise std: 3.97
          Mean value_function loss: 145.1308
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 86.0710
                       Mean reward: 861.87
               Mean episode length: 227.95
    Episode_Reward/reaching_object: 1.9517
     Episode_Reward/lifting_object: 164.4018
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0997
          Episode_Reward/joint_vel: -0.0872
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 2.29s
                      Time elapsed: 01:09:12
                               ETA: 00:09:47

################################################################################
                     [1m Learning iteration 1753/2000 [0m                     

                       Computation: 42411 steps/s (collection: 2.211s, learning 0.107s)
             Mean action noise std: 3.97
          Mean value_function loss: 119.2836
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 86.0794
                       Mean reward: 877.35
               Mean episode length: 232.81
    Episode_Reward/reaching_object: 2.0242
     Episode_Reward/lifting_object: 171.2086
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.1031
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 2.32s
                      Time elapsed: 01:09:14
                               ETA: 00:09:45

################################################################################
                     [1m Learning iteration 1754/2000 [0m                     

                       Computation: 41839 steps/s (collection: 2.252s, learning 0.098s)
             Mean action noise std: 3.97
          Mean value_function loss: 148.0580
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 86.0893
                       Mean reward: 859.39
               Mean episode length: 229.72
    Episode_Reward/reaching_object: 2.0030
     Episode_Reward/lifting_object: 168.9463
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.1026
          Episode_Reward/joint_vel: -0.0906
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 2.35s
                      Time elapsed: 01:09:17
                               ETA: 00:09:42

################################################################################
                     [1m Learning iteration 1755/2000 [0m                     

                       Computation: 41166 steps/s (collection: 2.279s, learning 0.109s)
             Mean action noise std: 3.97
          Mean value_function loss: 163.5800
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 86.1024
                       Mean reward: 876.50
               Mean episode length: 233.74
    Episode_Reward/reaching_object: 1.9220
     Episode_Reward/lifting_object: 161.6090
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0986
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 2.39s
                      Time elapsed: 01:09:19
                               ETA: 00:09:40

################################################################################
                     [1m Learning iteration 1756/2000 [0m                     

                       Computation: 41476 steps/s (collection: 2.218s, learning 0.152s)
             Mean action noise std: 3.97
          Mean value_function loss: 207.5038
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 86.1175
                       Mean reward: 841.69
               Mean episode length: 225.21
    Episode_Reward/reaching_object: 1.8803
     Episode_Reward/lifting_object: 158.1598
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0967
          Episode_Reward/joint_vel: -0.0861
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 2.37s
                      Time elapsed: 01:09:21
                               ETA: 00:09:37

################################################################################
                     [1m Learning iteration 1757/2000 [0m                     

                       Computation: 38653 steps/s (collection: 2.366s, learning 0.178s)
             Mean action noise std: 3.98
          Mean value_function loss: 178.6834
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 86.1347
                       Mean reward: 843.81
               Mean episode length: 225.61
    Episode_Reward/reaching_object: 1.9895
     Episode_Reward/lifting_object: 168.0493
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1018
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 2.54s
                      Time elapsed: 01:09:24
                               ETA: 00:09:35

################################################################################
                     [1m Learning iteration 1758/2000 [0m                     

                       Computation: 37882 steps/s (collection: 2.446s, learning 0.149s)
             Mean action noise std: 3.98
          Mean value_function loss: 199.3352
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 86.1510
                       Mean reward: 796.77
               Mean episode length: 214.58
    Episode_Reward/reaching_object: 1.9603
     Episode_Reward/lifting_object: 165.1875
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1007
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 2.59s
                      Time elapsed: 01:09:27
                               ETA: 00:09:33

################################################################################
                     [1m Learning iteration 1759/2000 [0m                     

                       Computation: 40768 steps/s (collection: 2.305s, learning 0.107s)
             Mean action noise std: 3.98
          Mean value_function loss: 161.5103
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 86.1690
                       Mean reward: 852.37
               Mean episode length: 228.26
    Episode_Reward/reaching_object: 1.9845
     Episode_Reward/lifting_object: 167.4929
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.1022
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 2.41s
                      Time elapsed: 01:09:29
                               ETA: 00:09:30

################################################################################
                     [1m Learning iteration 1760/2000 [0m                     

                       Computation: 41644 steps/s (collection: 2.255s, learning 0.106s)
             Mean action noise std: 3.98
          Mean value_function loss: 173.5194
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 86.1821
                       Mean reward: 839.45
               Mean episode length: 223.35
    Episode_Reward/reaching_object: 1.9487
     Episode_Reward/lifting_object: 164.7707
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.1002
          Episode_Reward/joint_vel: -0.0874
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 2.36s
                      Time elapsed: 01:09:31
                               ETA: 00:09:28

################################################################################
                     [1m Learning iteration 1761/2000 [0m                     

                       Computation: 39416 steps/s (collection: 2.407s, learning 0.087s)
             Mean action noise std: 3.98
          Mean value_function loss: 160.0373
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 86.1982
                       Mean reward: 866.61
               Mean episode length: 231.12
    Episode_Reward/reaching_object: 1.9755
     Episode_Reward/lifting_object: 166.5876
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.1019
          Episode_Reward/joint_vel: -0.0889
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 2.49s
                      Time elapsed: 01:09:34
                               ETA: 00:09:26

################################################################################
                     [1m Learning iteration 1762/2000 [0m                     

                       Computation: 40431 steps/s (collection: 2.341s, learning 0.091s)
             Mean action noise std: 3.99
          Mean value_function loss: 164.4005
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 86.2155
                       Mean reward: 863.50
               Mean episode length: 227.74
    Episode_Reward/reaching_object: 1.9845
     Episode_Reward/lifting_object: 168.0422
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.1025
          Episode_Reward/joint_vel: -0.0891
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 2.43s
                      Time elapsed: 01:09:36
                               ETA: 00:09:23

################################################################################
                     [1m Learning iteration 1763/2000 [0m                     

                       Computation: 41076 steps/s (collection: 2.293s, learning 0.101s)
             Mean action noise std: 3.99
          Mean value_function loss: 167.2402
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 86.2323
                       Mean reward: 813.25
               Mean episode length: 219.42
    Episode_Reward/reaching_object: 1.9844
     Episode_Reward/lifting_object: 167.7843
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.1027
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 2.39s
                      Time elapsed: 01:09:39
                               ETA: 00:09:21

################################################################################
                     [1m Learning iteration 1764/2000 [0m                     

                       Computation: 41051 steps/s (collection: 2.270s, learning 0.125s)
             Mean action noise std: 3.99
          Mean value_function loss: 150.2480
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 86.2478
                       Mean reward: 845.00
               Mean episode length: 226.43
    Episode_Reward/reaching_object: 1.9382
     Episode_Reward/lifting_object: 164.1293
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.1004
          Episode_Reward/joint_vel: -0.0869
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 2.39s
                      Time elapsed: 01:09:41
                               ETA: 00:09:19

################################################################################
                     [1m Learning iteration 1765/2000 [0m                     

                       Computation: 42441 steps/s (collection: 2.216s, learning 0.100s)
             Mean action noise std: 3.99
          Mean value_function loss: 175.3930
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 86.2643
                       Mean reward: 819.45
               Mean episode length: 219.52
    Episode_Reward/reaching_object: 1.9549
     Episode_Reward/lifting_object: 165.5845
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.1012
          Episode_Reward/joint_vel: -0.0879
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 2.32s
                      Time elapsed: 01:09:43
                               ETA: 00:09:16

################################################################################
                     [1m Learning iteration 1766/2000 [0m                     

                       Computation: 42642 steps/s (collection: 2.218s, learning 0.088s)
             Mean action noise std: 3.99
          Mean value_function loss: 134.4087
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 86.2743
                       Mean reward: 882.94
               Mean episode length: 235.59
    Episode_Reward/reaching_object: 1.9757
     Episode_Reward/lifting_object: 168.2705
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.1022
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 2.31s
                      Time elapsed: 01:09:46
                               ETA: 00:09:14

################################################################################
                     [1m Learning iteration 1767/2000 [0m                     

                       Computation: 41858 steps/s (collection: 2.233s, learning 0.116s)
             Mean action noise std: 4.00
          Mean value_function loss: 146.2446
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 86.2839
                       Mean reward: 841.47
               Mean episode length: 225.93
    Episode_Reward/reaching_object: 1.9862
     Episode_Reward/lifting_object: 168.6533
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1031
          Episode_Reward/joint_vel: -0.0882
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 2.35s
                      Time elapsed: 01:09:48
                               ETA: 00:09:11

################################################################################
                     [1m Learning iteration 1768/2000 [0m                     

                       Computation: 40874 steps/s (collection: 2.279s, learning 0.127s)
             Mean action noise std: 4.00
          Mean value_function loss: 141.2514
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 86.2961
                       Mean reward: 871.58
               Mean episode length: 233.00
    Episode_Reward/reaching_object: 2.0080
     Episode_Reward/lifting_object: 170.5680
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.1043
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 2.41s
                      Time elapsed: 01:09:50
                               ETA: 00:09:09

################################################################################
                     [1m Learning iteration 1769/2000 [0m                     

                       Computation: 41198 steps/s (collection: 2.290s, learning 0.097s)
             Mean action noise std: 4.00
          Mean value_function loss: 135.7775
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 86.3165
                       Mean reward: 803.25
               Mean episode length: 216.53
    Episode_Reward/reaching_object: 1.9857
     Episode_Reward/lifting_object: 168.8022
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1031
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 2.39s
                      Time elapsed: 01:09:53
                               ETA: 00:09:07

################################################################################
                     [1m Learning iteration 1770/2000 [0m                     

                       Computation: 42499 steps/s (collection: 2.220s, learning 0.093s)
             Mean action noise std: 4.00
          Mean value_function loss: 158.1386
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 86.3305
                       Mean reward: 851.12
               Mean episode length: 227.68
    Episode_Reward/reaching_object: 1.9749
     Episode_Reward/lifting_object: 167.3118
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1031
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 2.31s
                      Time elapsed: 01:09:55
                               ETA: 00:09:04

################################################################################
                     [1m Learning iteration 1771/2000 [0m                     

                       Computation: 38834 steps/s (collection: 2.341s, learning 0.190s)
             Mean action noise std: 4.00
          Mean value_function loss: 175.1948
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 86.3408
                       Mean reward: 853.14
               Mean episode length: 227.83
    Episode_Reward/reaching_object: 1.9585
     Episode_Reward/lifting_object: 166.2699
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.1018
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 2.53s
                      Time elapsed: 01:09:58
                               ETA: 00:09:02

################################################################################
                     [1m Learning iteration 1772/2000 [0m                     

                       Computation: 39136 steps/s (collection: 2.391s, learning 0.120s)
             Mean action noise std: 4.01
          Mean value_function loss: 155.1880
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 86.3533
                       Mean reward: 840.29
               Mean episode length: 225.06
    Episode_Reward/reaching_object: 1.9881
     Episode_Reward/lifting_object: 169.1050
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.1036
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 2.51s
                      Time elapsed: 01:10:00
                               ETA: 00:09:00

################################################################################
                     [1m Learning iteration 1773/2000 [0m                     

                       Computation: 38865 steps/s (collection: 2.329s, learning 0.201s)
             Mean action noise std: 4.01
          Mean value_function loss: 163.2420
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 86.3694
                       Mean reward: 847.24
               Mean episode length: 227.39
    Episode_Reward/reaching_object: 1.9680
     Episode_Reward/lifting_object: 167.0101
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.1028
          Episode_Reward/joint_vel: -0.0877
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 2.53s
                      Time elapsed: 01:10:03
                               ETA: 00:08:57

################################################################################
                     [1m Learning iteration 1774/2000 [0m                     

                       Computation: 39611 steps/s (collection: 2.380s, learning 0.102s)
             Mean action noise std: 4.01
          Mean value_function loss: 158.7830
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 86.3829
                       Mean reward: 835.50
               Mean episode length: 224.58
    Episode_Reward/reaching_object: 1.9576
     Episode_Reward/lifting_object: 166.2725
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.1022
          Episode_Reward/joint_vel: -0.0870
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 2.48s
                      Time elapsed: 01:10:05
                               ETA: 00:08:55

################################################################################
                     [1m Learning iteration 1775/2000 [0m                     

                       Computation: 38392 steps/s (collection: 2.441s, learning 0.119s)
             Mean action noise std: 4.01
          Mean value_function loss: 173.1536
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 86.3962
                       Mean reward: 833.51
               Mean episode length: 223.62
    Episode_Reward/reaching_object: 1.9323
     Episode_Reward/lifting_object: 163.2126
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1014
          Episode_Reward/joint_vel: -0.0873
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 2.56s
                      Time elapsed: 01:10:08
                               ETA: 00:08:53

################################################################################
                     [1m Learning iteration 1776/2000 [0m                     

                       Computation: 41156 steps/s (collection: 2.285s, learning 0.103s)
             Mean action noise std: 4.01
          Mean value_function loss: 132.0742
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 86.4090
                       Mean reward: 890.73
               Mean episode length: 238.16
    Episode_Reward/reaching_object: 2.0264
     Episode_Reward/lifting_object: 172.0039
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.1062
          Episode_Reward/joint_vel: -0.0899
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 2.39s
                      Time elapsed: 01:10:10
                               ETA: 00:08:50

################################################################################
                     [1m Learning iteration 1777/2000 [0m                     

                       Computation: 40562 steps/s (collection: 2.323s, learning 0.101s)
             Mean action noise std: 4.02
          Mean value_function loss: 163.4454
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 86.4239
                       Mean reward: 842.45
               Mean episode length: 227.05
    Episode_Reward/reaching_object: 1.9605
     Episode_Reward/lifting_object: 166.6017
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.1032
          Episode_Reward/joint_vel: -0.0879
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 2.42s
                      Time elapsed: 01:10:13
                               ETA: 00:08:48

################################################################################
                     [1m Learning iteration 1778/2000 [0m                     

                       Computation: 39001 steps/s (collection: 2.368s, learning 0.153s)
             Mean action noise std: 4.02
          Mean value_function loss: 148.3630
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 86.4348
                       Mean reward: 853.75
               Mean episode length: 230.36
    Episode_Reward/reaching_object: 1.9859
     Episode_Reward/lifting_object: 169.2357
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.1042
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 2.52s
                      Time elapsed: 01:10:15
                               ETA: 00:08:46

################################################################################
                     [1m Learning iteration 1779/2000 [0m                     

                       Computation: 39885 steps/s (collection: 2.365s, learning 0.100s)
             Mean action noise std: 4.02
          Mean value_function loss: 219.5937
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 86.4429
                       Mean reward: 859.98
               Mean episode length: 231.56
    Episode_Reward/reaching_object: 1.9262
     Episode_Reward/lifting_object: 163.6358
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.1019
          Episode_Reward/joint_vel: -0.0864
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 2.46s
                      Time elapsed: 01:10:18
                               ETA: 00:08:43

################################################################################
                     [1m Learning iteration 1780/2000 [0m                     

                       Computation: 39360 steps/s (collection: 2.353s, learning 0.145s)
             Mean action noise std: 4.02
          Mean value_function loss: 177.9199
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 86.4528
                       Mean reward: 849.46
               Mean episode length: 228.02
    Episode_Reward/reaching_object: 1.9223
     Episode_Reward/lifting_object: 163.8381
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.1018
          Episode_Reward/joint_vel: -0.0856
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 2.50s
                      Time elapsed: 01:10:20
                               ETA: 00:08:41

################################################################################
                     [1m Learning iteration 1781/2000 [0m                     

                       Computation: 40406 steps/s (collection: 2.331s, learning 0.102s)
             Mean action noise std: 4.02
          Mean value_function loss: 173.9332
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 86.4629
                       Mean reward: 875.08
               Mean episode length: 231.97
    Episode_Reward/reaching_object: 1.9802
     Episode_Reward/lifting_object: 169.2132
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.1044
          Episode_Reward/joint_vel: -0.0875
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 2.43s
                      Time elapsed: 01:10:22
                               ETA: 00:08:38

################################################################################
                     [1m Learning iteration 1782/2000 [0m                     

                       Computation: 41684 steps/s (collection: 2.245s, learning 0.113s)
             Mean action noise std: 4.02
          Mean value_function loss: 155.5899
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 86.4702
                       Mean reward: 808.55
               Mean episode length: 219.60
    Episode_Reward/reaching_object: 1.9350
     Episode_Reward/lifting_object: 164.8366
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.1028
          Episode_Reward/joint_vel: -0.0872
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 2.36s
                      Time elapsed: 01:10:25
                               ETA: 00:08:36

################################################################################
                     [1m Learning iteration 1783/2000 [0m                     

                       Computation: 41620 steps/s (collection: 2.258s, learning 0.104s)
             Mean action noise std: 4.02
          Mean value_function loss: 185.1244
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 86.4838
                       Mean reward: 857.00
               Mean episode length: 228.29
    Episode_Reward/reaching_object: 1.9186
     Episode_Reward/lifting_object: 163.7753
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.1016
          Episode_Reward/joint_vel: -0.0856
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 2.36s
                      Time elapsed: 01:10:27
                               ETA: 00:08:34

################################################################################
                     [1m Learning iteration 1784/2000 [0m                     

                       Computation: 39720 steps/s (collection: 2.331s, learning 0.144s)
             Mean action noise std: 4.03
          Mean value_function loss: 145.6047
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 86.5025
                       Mean reward: 885.47
               Mean episode length: 235.83
    Episode_Reward/reaching_object: 1.9479
     Episode_Reward/lifting_object: 165.7564
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.1032
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 2.47s
                      Time elapsed: 01:10:30
                               ETA: 00:08:31

################################################################################
                     [1m Learning iteration 1785/2000 [0m                     

                       Computation: 41643 steps/s (collection: 2.266s, learning 0.095s)
             Mean action noise std: 4.03
          Mean value_function loss: 206.6265
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 86.5200
                       Mean reward: 784.54
               Mean episode length: 210.68
    Episode_Reward/reaching_object: 1.8827
     Episode_Reward/lifting_object: 160.1338
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.1004
          Episode_Reward/joint_vel: -0.0855
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 2.36s
                      Time elapsed: 01:10:32
                               ETA: 00:08:29

################################################################################
                     [1m Learning iteration 1786/2000 [0m                     

                       Computation: 41763 steps/s (collection: 2.263s, learning 0.091s)
             Mean action noise std: 4.03
          Mean value_function loss: 190.1425
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 86.5321
                       Mean reward: 811.13
               Mean episode length: 219.09
    Episode_Reward/reaching_object: 1.9267
     Episode_Reward/lifting_object: 164.0863
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.1022
          Episode_Reward/joint_vel: -0.0877
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 2.35s
                      Time elapsed: 01:10:34
                               ETA: 00:08:27

################################################################################
                     [1m Learning iteration 1787/2000 [0m                     

                       Computation: 38763 steps/s (collection: 2.357s, learning 0.179s)
             Mean action noise std: 4.03
          Mean value_function loss: 182.4557
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 86.5384
                       Mean reward: 839.23
               Mean episode length: 225.91
    Episode_Reward/reaching_object: 1.9179
     Episode_Reward/lifting_object: 162.4966
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.1018
          Episode_Reward/joint_vel: -0.0877
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 2.54s
                      Time elapsed: 01:10:37
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1788/2000 [0m                     

                       Computation: 38752 steps/s (collection: 2.400s, learning 0.137s)
             Mean action noise std: 4.03
          Mean value_function loss: 137.0310
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 86.5498
                       Mean reward: 857.68
               Mean episode length: 230.31
    Episode_Reward/reaching_object: 1.9887
     Episode_Reward/lifting_object: 168.7643
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.1056
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 2.54s
                      Time elapsed: 01:10:39
                               ETA: 00:08:22

################################################################################
                     [1m Learning iteration 1789/2000 [0m                     

                       Computation: 39383 steps/s (collection: 2.374s, learning 0.123s)
             Mean action noise std: 4.03
          Mean value_function loss: 134.0535
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 86.5662
                       Mean reward: 879.64
               Mean episode length: 234.45
    Episode_Reward/reaching_object: 2.0158
     Episode_Reward/lifting_object: 171.9208
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.1068
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 2.50s
                      Time elapsed: 01:10:42
                               ETA: 00:08:20

################################################################################
                     [1m Learning iteration 1790/2000 [0m                     

                       Computation: 40819 steps/s (collection: 2.313s, learning 0.096s)
             Mean action noise std: 4.04
          Mean value_function loss: 164.6687
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 86.5767
                       Mean reward: 801.98
               Mean episode length: 215.27
    Episode_Reward/reaching_object: 1.9398
     Episode_Reward/lifting_object: 165.0210
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.1032
          Episode_Reward/joint_vel: -0.0875
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 2.41s
                      Time elapsed: 01:10:44
                               ETA: 00:08:17

################################################################################
                     [1m Learning iteration 1791/2000 [0m                     

                       Computation: 40253 steps/s (collection: 2.310s, learning 0.132s)
             Mean action noise std: 4.04
          Mean value_function loss: 162.6528
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 86.5875
                       Mean reward: 820.52
               Mean episode length: 220.29
    Episode_Reward/reaching_object: 1.9877
     Episode_Reward/lifting_object: 168.3770
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.1052
          Episode_Reward/joint_vel: -0.0887
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 2.44s
                      Time elapsed: 01:10:47
                               ETA: 00:08:15

################################################################################
                     [1m Learning iteration 1792/2000 [0m                     

                       Computation: 40408 steps/s (collection: 2.334s, learning 0.099s)
             Mean action noise std: 4.04
          Mean value_function loss: 154.9301
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 86.6008
                       Mean reward: 818.26
               Mean episode length: 219.92
    Episode_Reward/reaching_object: 1.9561
     Episode_Reward/lifting_object: 165.6811
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.1038
          Episode_Reward/joint_vel: -0.0872
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 2.43s
                      Time elapsed: 01:10:49
                               ETA: 00:08:12

################################################################################
                     [1m Learning iteration 1793/2000 [0m                     

                       Computation: 40679 steps/s (collection: 2.310s, learning 0.107s)
             Mean action noise std: 4.04
          Mean value_function loss: 134.2357
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 86.6126
                       Mean reward: 884.53
               Mean episode length: 235.07
    Episode_Reward/reaching_object: 2.0280
     Episode_Reward/lifting_object: 172.3109
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.1070
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 2.42s
                      Time elapsed: 01:10:52
                               ETA: 00:08:10

################################################################################
                     [1m Learning iteration 1794/2000 [0m                     

                       Computation: 40291 steps/s (collection: 2.347s, learning 0.093s)
             Mean action noise std: 4.04
          Mean value_function loss: 182.2320
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 86.6235
                       Mean reward: 840.15
               Mean episode length: 223.94
    Episode_Reward/reaching_object: 1.9878
     Episode_Reward/lifting_object: 168.6290
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.1046
          Episode_Reward/joint_vel: -0.0870
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 2.44s
                      Time elapsed: 01:10:54
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 1795/2000 [0m                     

                       Computation: 40896 steps/s (collection: 2.317s, learning 0.087s)
             Mean action noise std: 4.04
          Mean value_function loss: 175.8679
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 86.6340
                       Mean reward: 808.78
               Mean episode length: 217.60
    Episode_Reward/reaching_object: 1.9145
     Episode_Reward/lifting_object: 161.3637
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1018
          Episode_Reward/joint_vel: -0.0859
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 2.40s
                      Time elapsed: 01:10:56
                               ETA: 00:08:05

################################################################################
                     [1m Learning iteration 1796/2000 [0m                     

                       Computation: 41298 steps/s (collection: 2.272s, learning 0.109s)
             Mean action noise std: 4.04
          Mean value_function loss: 160.6501
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 86.6440
                       Mean reward: 867.99
               Mean episode length: 232.60
    Episode_Reward/reaching_object: 1.9654
     Episode_Reward/lifting_object: 165.7237
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.1039
          Episode_Reward/joint_vel: -0.0872
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 2.38s
                      Time elapsed: 01:10:59
                               ETA: 00:08:03

################################################################################
                     [1m Learning iteration 1797/2000 [0m                     

                       Computation: 38418 steps/s (collection: 2.416s, learning 0.143s)
             Mean action noise std: 4.05
          Mean value_function loss: 158.6823
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 86.6574
                       Mean reward: 837.53
               Mean episode length: 224.77
    Episode_Reward/reaching_object: 2.0114
     Episode_Reward/lifting_object: 169.7205
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.1062
          Episode_Reward/joint_vel: -0.0887
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 2.56s
                      Time elapsed: 01:11:01
                               ETA: 00:08:01

################################################################################
                     [1m Learning iteration 1798/2000 [0m                     

                       Computation: 38255 steps/s (collection: 2.425s, learning 0.145s)
             Mean action noise std: 4.05
          Mean value_function loss: 188.7772
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 86.6738
                       Mean reward: 807.06
               Mean episode length: 218.79
    Episode_Reward/reaching_object: 1.9716
     Episode_Reward/lifting_object: 165.7917
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.1048
          Episode_Reward/joint_vel: -0.0884
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 2.57s
                      Time elapsed: 01:11:04
                               ETA: 00:07:58

################################################################################
                     [1m Learning iteration 1799/2000 [0m                     

                       Computation: 38418 steps/s (collection: 2.419s, learning 0.140s)
             Mean action noise std: 4.05
          Mean value_function loss: 181.4256
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 86.6866
                       Mean reward: 853.03
               Mean episode length: 227.75
    Episode_Reward/reaching_object: 1.9628
     Episode_Reward/lifting_object: 165.2210
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.1045
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 2.56s
                      Time elapsed: 01:11:07
                               ETA: 00:07:56

################################################################################
                     [1m Learning iteration 1800/2000 [0m                     

                       Computation: 39000 steps/s (collection: 2.375s, learning 0.145s)
             Mean action noise std: 4.05
          Mean value_function loss: 196.7048
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 86.6912
                       Mean reward: 830.10
               Mean episode length: 223.63
    Episode_Reward/reaching_object: 1.9800
     Episode_Reward/lifting_object: 167.0845
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1052
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 2.52s
                      Time elapsed: 01:11:09
                               ETA: 00:07:54

################################################################################
                     [1m Learning iteration 1801/2000 [0m                     

                       Computation: 37818 steps/s (collection: 2.456s, learning 0.143s)
             Mean action noise std: 4.05
          Mean value_function loss: 176.9157
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 86.7031
                       Mean reward: 813.18
               Mean episode length: 220.15
    Episode_Reward/reaching_object: 1.9156
     Episode_Reward/lifting_object: 160.6811
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.1025
          Episode_Reward/joint_vel: -0.0864
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 2.60s
                      Time elapsed: 01:11:12
                               ETA: 00:07:51

################################################################################
                     [1m Learning iteration 1802/2000 [0m                     

                       Computation: 37860 steps/s (collection: 2.429s, learning 0.168s)
             Mean action noise std: 4.06
          Mean value_function loss: 184.3259
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 86.7170
                       Mean reward: 843.32
               Mean episode length: 224.75
    Episode_Reward/reaching_object: 1.9611
     Episode_Reward/lifting_object: 165.0669
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1042
          Episode_Reward/joint_vel: -0.0879
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 2.60s
                      Time elapsed: 01:11:14
                               ETA: 00:07:49

################################################################################
                     [1m Learning iteration 1803/2000 [0m                     

                       Computation: 38574 steps/s (collection: 2.395s, learning 0.154s)
             Mean action noise std: 4.06
          Mean value_function loss: 171.3574
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 86.7259
                       Mean reward: 846.43
               Mean episode length: 227.03
    Episode_Reward/reaching_object: 1.9817
     Episode_Reward/lifting_object: 166.9418
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.1058
          Episode_Reward/joint_vel: -0.0888
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 2.55s
                      Time elapsed: 01:11:17
                               ETA: 00:07:47

################################################################################
                     [1m Learning iteration 1804/2000 [0m                     

                       Computation: 37704 steps/s (collection: 2.422s, learning 0.185s)
             Mean action noise std: 4.06
          Mean value_function loss: 174.8524
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 86.7338
                       Mean reward: 850.41
               Mean episode length: 230.45
    Episode_Reward/reaching_object: 1.9761
     Episode_Reward/lifting_object: 166.3185
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.1053
          Episode_Reward/joint_vel: -0.0882
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 2.61s
                      Time elapsed: 01:11:19
                               ETA: 00:07:44

################################################################################
                     [1m Learning iteration 1805/2000 [0m                     

                       Computation: 38760 steps/s (collection: 2.408s, learning 0.128s)
             Mean action noise std: 4.06
          Mean value_function loss: 197.8942
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 86.7436
                       Mean reward: 837.70
               Mean episode length: 222.95
    Episode_Reward/reaching_object: 1.9502
     Episode_Reward/lifting_object: 164.4660
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.1040
          Episode_Reward/joint_vel: -0.0873
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 2.54s
                      Time elapsed: 01:11:22
                               ETA: 00:07:42

################################################################################
                     [1m Learning iteration 1806/2000 [0m                     

                       Computation: 37648 steps/s (collection: 2.476s, learning 0.136s)
             Mean action noise std: 4.06
          Mean value_function loss: 189.2808
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 86.7529
                       Mean reward: 860.15
               Mean episode length: 229.61
    Episode_Reward/reaching_object: 1.9445
     Episode_Reward/lifting_object: 163.6213
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.1040
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 2.61s
                      Time elapsed: 01:11:25
                               ETA: 00:07:40

################################################################################
                     [1m Learning iteration 1807/2000 [0m                     

                       Computation: 36909 steps/s (collection: 2.517s, learning 0.146s)
             Mean action noise std: 4.06
          Mean value_function loss: 198.2283
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 86.7630
                       Mean reward: 849.33
               Mean episode length: 227.07
    Episode_Reward/reaching_object: 1.9227
     Episode_Reward/lifting_object: 161.5551
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1032
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 2.66s
                      Time elapsed: 01:11:27
                               ETA: 00:07:37

################################################################################
                     [1m Learning iteration 1808/2000 [0m                     

                       Computation: 39260 steps/s (collection: 2.385s, learning 0.119s)
             Mean action noise std: 4.07
          Mean value_function loss: 200.9521
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 86.7841
                       Mean reward: 864.18
               Mean episode length: 230.27
    Episode_Reward/reaching_object: 1.9678
     Episode_Reward/lifting_object: 165.6221
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.1054
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 2.50s
                      Time elapsed: 01:11:30
                               ETA: 00:07:35

################################################################################
                     [1m Learning iteration 1809/2000 [0m                     

                       Computation: 39367 steps/s (collection: 2.358s, learning 0.139s)
             Mean action noise std: 4.07
          Mean value_function loss: 232.1815
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 86.8005
                       Mean reward: 746.26
               Mean episode length: 203.31
    Episode_Reward/reaching_object: 1.8911
     Episode_Reward/lifting_object: 158.3018
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.1018
          Episode_Reward/joint_vel: -0.0865
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 2.50s
                      Time elapsed: 01:11:32
                               ETA: 00:07:32

################################################################################
                     [1m Learning iteration 1810/2000 [0m                     

                       Computation: 37838 steps/s (collection: 2.416s, learning 0.182s)
             Mean action noise std: 4.07
          Mean value_function loss: 160.0400
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 86.8110
                       Mean reward: 833.12
               Mean episode length: 222.99
    Episode_Reward/reaching_object: 1.9351
     Episode_Reward/lifting_object: 163.3146
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.1042
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 2.60s
                      Time elapsed: 01:11:35
                               ETA: 00:07:30

################################################################################
                     [1m Learning iteration 1811/2000 [0m                     

                       Computation: 37426 steps/s (collection: 2.496s, learning 0.131s)
             Mean action noise std: 4.07
          Mean value_function loss: 215.1058
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 86.8249
                       Mean reward: 850.54
               Mean episode length: 226.85
    Episode_Reward/reaching_object: 1.9422
     Episode_Reward/lifting_object: 163.7747
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.1047
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 2.63s
                      Time elapsed: 01:11:37
                               ETA: 00:07:28

################################################################################
                     [1m Learning iteration 1812/2000 [0m                     

                       Computation: 38696 steps/s (collection: 2.418s, learning 0.123s)
             Mean action noise std: 4.07
          Mean value_function loss: 184.2185
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 86.8362
                       Mean reward: 793.83
               Mean episode length: 213.66
    Episode_Reward/reaching_object: 1.9492
     Episode_Reward/lifting_object: 164.6397
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.1051
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 2.54s
                      Time elapsed: 01:11:40
                               ETA: 00:07:25

################################################################################
                     [1m Learning iteration 1813/2000 [0m                     

                       Computation: 37916 steps/s (collection: 2.452s, learning 0.141s)
             Mean action noise std: 4.07
          Mean value_function loss: 159.3809
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 86.8427
                       Mean reward: 866.99
               Mean episode length: 230.59
    Episode_Reward/reaching_object: 1.9739
     Episode_Reward/lifting_object: 166.7695
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.1066
          Episode_Reward/joint_vel: -0.0905
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 2.59s
                      Time elapsed: 01:11:43
                               ETA: 00:07:23

################################################################################
                     [1m Learning iteration 1814/2000 [0m                     

                       Computation: 39053 steps/s (collection: 2.407s, learning 0.111s)
             Mean action noise std: 4.07
          Mean value_function loss: 172.5838
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 86.8465
                       Mean reward: 837.27
               Mean episode length: 223.21
    Episode_Reward/reaching_object: 1.9646
     Episode_Reward/lifting_object: 166.3641
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.1063
          Episode_Reward/joint_vel: -0.0921
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 2.52s
                      Time elapsed: 01:11:45
                               ETA: 00:07:21

################################################################################
                     [1m Learning iteration 1815/2000 [0m                     

                       Computation: 40367 steps/s (collection: 2.335s, learning 0.101s)
             Mean action noise std: 4.07
          Mean value_function loss: 188.5675
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 86.8548
                       Mean reward: 834.95
               Mean episode length: 224.08
    Episode_Reward/reaching_object: 1.9736
     Episode_Reward/lifting_object: 167.5148
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.1071
          Episode_Reward/joint_vel: -0.0919
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 2.44s
                      Time elapsed: 01:11:48
                               ETA: 00:07:18

################################################################################
                     [1m Learning iteration 1816/2000 [0m                     

                       Computation: 41140 steps/s (collection: 2.274s, learning 0.115s)
             Mean action noise std: 4.08
          Mean value_function loss: 192.9031
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 86.8673
                       Mean reward: 800.02
               Mean episode length: 217.34
    Episode_Reward/reaching_object: 1.9182
     Episode_Reward/lifting_object: 161.5428
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.1047
          Episode_Reward/joint_vel: -0.0909
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 2.39s
                      Time elapsed: 01:11:50
                               ETA: 00:07:16

################################################################################
                     [1m Learning iteration 1817/2000 [0m                     

                       Computation: 41059 steps/s (collection: 2.280s, learning 0.115s)
             Mean action noise std: 4.08
          Mean value_function loss: 238.9023
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 86.8837
                       Mean reward: 807.26
               Mean episode length: 217.68
    Episode_Reward/reaching_object: 1.9498
     Episode_Reward/lifting_object: 165.5259
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.1058
          Episode_Reward/joint_vel: -0.0908
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 2.39s
                      Time elapsed: 01:11:52
                               ETA: 00:07:14

################################################################################
                     [1m Learning iteration 1818/2000 [0m                     

                       Computation: 41615 steps/s (collection: 2.273s, learning 0.090s)
             Mean action noise std: 4.08
          Mean value_function loss: 167.8257
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 86.8999
                       Mean reward: 851.36
               Mean episode length: 226.80
    Episode_Reward/reaching_object: 1.9807
     Episode_Reward/lifting_object: 168.0956
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.1078
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 2.36s
                      Time elapsed: 01:11:55
                               ETA: 00:07:11

################################################################################
                     [1m Learning iteration 1819/2000 [0m                     

                       Computation: 41056 steps/s (collection: 2.305s, learning 0.090s)
             Mean action noise std: 4.08
          Mean value_function loss: 186.6572
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 86.9091
                       Mean reward: 805.85
               Mean episode length: 216.53
    Episode_Reward/reaching_object: 1.9426
     Episode_Reward/lifting_object: 164.4893
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.1061
          Episode_Reward/joint_vel: -0.0918
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 2.39s
                      Time elapsed: 01:11:57
                               ETA: 00:07:09

################################################################################
                     [1m Learning iteration 1820/2000 [0m                     

                       Computation: 41737 steps/s (collection: 2.231s, learning 0.125s)
             Mean action noise std: 4.08
          Mean value_function loss: 178.2981
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 86.9181
                       Mean reward: 874.93
               Mean episode length: 233.13
    Episode_Reward/reaching_object: 1.9967
     Episode_Reward/lifting_object: 169.9173
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.1088
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 2.36s
                      Time elapsed: 01:11:59
                               ETA: 00:07:07

################################################################################
                     [1m Learning iteration 1821/2000 [0m                     

                       Computation: 41485 steps/s (collection: 2.274s, learning 0.096s)
             Mean action noise std: 4.08
          Mean value_function loss: 174.0094
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 86.9267
                       Mean reward: 830.53
               Mean episode length: 222.67
    Episode_Reward/reaching_object: 1.9666
     Episode_Reward/lifting_object: 167.3618
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.1075
          Episode_Reward/joint_vel: -0.0915
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 2.37s
                      Time elapsed: 01:12:02
                               ETA: 00:07:04

################################################################################
                     [1m Learning iteration 1822/2000 [0m                     

                       Computation: 41667 steps/s (collection: 2.251s, learning 0.109s)
             Mean action noise std: 4.09
          Mean value_function loss: 143.6706
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 86.9350
                       Mean reward: 860.49
               Mean episode length: 229.79
    Episode_Reward/reaching_object: 1.9802
     Episode_Reward/lifting_object: 168.1606
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.1083
          Episode_Reward/joint_vel: -0.0929
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 2.36s
                      Time elapsed: 01:12:04
                               ETA: 00:07:02

################################################################################
                     [1m Learning iteration 1823/2000 [0m                     

                       Computation: 40772 steps/s (collection: 2.282s, learning 0.130s)
             Mean action noise std: 4.09
          Mean value_function loss: 166.6266
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 86.9474
                       Mean reward: 851.24
               Mean episode length: 225.60
    Episode_Reward/reaching_object: 2.0005
     Episode_Reward/lifting_object: 170.4698
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.1090
          Episode_Reward/joint_vel: -0.0933
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 2.41s
                      Time elapsed: 01:12:07
                               ETA: 00:06:59

################################################################################
                     [1m Learning iteration 1824/2000 [0m                     

                       Computation: 42110 steps/s (collection: 2.244s, learning 0.091s)
             Mean action noise std: 4.09
          Mean value_function loss: 170.6293
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 86.9672
                       Mean reward: 827.46
               Mean episode length: 223.35
    Episode_Reward/reaching_object: 1.9115
     Episode_Reward/lifting_object: 161.8089
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.1050
          Episode_Reward/joint_vel: -0.0905
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 2.33s
                      Time elapsed: 01:12:09
                               ETA: 00:06:57

################################################################################
                     [1m Learning iteration 1825/2000 [0m                     

                       Computation: 41733 steps/s (collection: 2.262s, learning 0.093s)
             Mean action noise std: 4.09
          Mean value_function loss: 151.7265
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 86.9833
                       Mean reward: 873.18
               Mean episode length: 231.78
    Episode_Reward/reaching_object: 1.9789
     Episode_Reward/lifting_object: 167.8733
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.1083
          Episode_Reward/joint_vel: -0.0931
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 2.36s
                      Time elapsed: 01:12:11
                               ETA: 00:06:55

################################################################################
                     [1m Learning iteration 1826/2000 [0m                     

                       Computation: 41267 steps/s (collection: 2.272s, learning 0.110s)
             Mean action noise std: 4.09
          Mean value_function loss: 158.8578
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 86.9987
                       Mean reward: 840.28
               Mean episode length: 226.10
    Episode_Reward/reaching_object: 1.9579
     Episode_Reward/lifting_object: 166.5620
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.1072
          Episode_Reward/joint_vel: -0.0914
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 2.38s
                      Time elapsed: 01:12:14
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1827/2000 [0m                     

                       Computation: 41512 steps/s (collection: 2.260s, learning 0.108s)
             Mean action noise std: 4.10
          Mean value_function loss: 149.6858
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 87.0139
                       Mean reward: 838.60
               Mean episode length: 228.19
    Episode_Reward/reaching_object: 1.9730
     Episode_Reward/lifting_object: 167.0853
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.1085
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 2.37s
                      Time elapsed: 01:12:16
                               ETA: 00:06:50

################################################################################
                     [1m Learning iteration 1828/2000 [0m                     

                       Computation: 40613 steps/s (collection: 2.313s, learning 0.108s)
             Mean action noise std: 4.10
          Mean value_function loss: 184.4959
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 87.0373
                       Mean reward: 870.19
               Mean episode length: 230.62
    Episode_Reward/reaching_object: 1.9632
     Episode_Reward/lifting_object: 167.0846
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.1078
          Episode_Reward/joint_vel: -0.0927
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 2.42s
                      Time elapsed: 01:12:18
                               ETA: 00:06:48

################################################################################
                     [1m Learning iteration 1829/2000 [0m                     

                       Computation: 41810 steps/s (collection: 2.239s, learning 0.112s)
             Mean action noise std: 4.10
          Mean value_function loss: 200.0585
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 87.0551
                       Mean reward: 821.94
               Mean episode length: 221.12
    Episode_Reward/reaching_object: 1.9071
     Episode_Reward/lifting_object: 160.8873
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.1052
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 2.35s
                      Time elapsed: 01:12:21
                               ETA: 00:06:45

################################################################################
                     [1m Learning iteration 1830/2000 [0m                     

                       Computation: 42335 steps/s (collection: 2.232s, learning 0.090s)
             Mean action noise std: 4.10
          Mean value_function loss: 210.2417
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 87.0711
                       Mean reward: 830.05
               Mean episode length: 222.43
    Episode_Reward/reaching_object: 1.9274
     Episode_Reward/lifting_object: 163.3300
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.1059
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 2.32s
                      Time elapsed: 01:12:23
                               ETA: 00:06:43

################################################################################
                     [1m Learning iteration 1831/2000 [0m                     

                       Computation: 41684 steps/s (collection: 2.265s, learning 0.094s)
             Mean action noise std: 4.10
          Mean value_function loss: 141.6979
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 87.0858
                       Mean reward: 842.29
               Mean episode length: 228.28
    Episode_Reward/reaching_object: 1.9842
     Episode_Reward/lifting_object: 168.0013
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.1091
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 2.36s
                      Time elapsed: 01:12:25
                               ETA: 00:06:40

################################################################################
                     [1m Learning iteration 1832/2000 [0m                     

                       Computation: 42163 steps/s (collection: 2.230s, learning 0.102s)
             Mean action noise std: 4.11
          Mean value_function loss: 157.5165
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 87.0969
                       Mean reward: 835.29
               Mean episode length: 223.29
    Episode_Reward/reaching_object: 2.0198
     Episode_Reward/lifting_object: 171.6373
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.1103
          Episode_Reward/joint_vel: -0.0923
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 2.33s
                      Time elapsed: 01:12:28
                               ETA: 00:06:38

################################################################################
                     [1m Learning iteration 1833/2000 [0m                     

                       Computation: 42233 steps/s (collection: 2.230s, learning 0.098s)
             Mean action noise std: 4.11
          Mean value_function loss: 167.2934
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 87.1114
                       Mean reward: 851.91
               Mean episode length: 227.14
    Episode_Reward/reaching_object: 1.9379
     Episode_Reward/lifting_object: 163.6888
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.1069
          Episode_Reward/joint_vel: -0.0899
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 2.33s
                      Time elapsed: 01:12:30
                               ETA: 00:06:36

################################################################################
                     [1m Learning iteration 1834/2000 [0m                     

                       Computation: 41690 steps/s (collection: 2.263s, learning 0.095s)
             Mean action noise std: 4.11
          Mean value_function loss: 138.6071
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 87.1267
                       Mean reward: 896.86
               Mean episode length: 237.03
    Episode_Reward/reaching_object: 2.0093
     Episode_Reward/lifting_object: 170.7215
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.1105
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 2.36s
                      Time elapsed: 01:12:32
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 1835/2000 [0m                     

                       Computation: 41867 steps/s (collection: 2.220s, learning 0.128s)
             Mean action noise std: 4.11
          Mean value_function loss: 184.1170
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 87.1353
                       Mean reward: 850.92
               Mean episode length: 225.98
    Episode_Reward/reaching_object: 1.9245
     Episode_Reward/lifting_object: 162.9541
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.1064
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 2.35s
                      Time elapsed: 01:12:35
                               ETA: 00:06:31

################################################################################
                     [1m Learning iteration 1836/2000 [0m                     

                       Computation: 41867 steps/s (collection: 2.262s, learning 0.086s)
             Mean action noise std: 4.11
          Mean value_function loss: 163.2909
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 87.1472
                       Mean reward: 818.67
               Mean episode length: 220.96
    Episode_Reward/reaching_object: 1.9569
     Episode_Reward/lifting_object: 165.3266
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.1085
          Episode_Reward/joint_vel: -0.0913
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 2.35s
                      Time elapsed: 01:12:37
                               ETA: 00:06:29

################################################################################
                     [1m Learning iteration 1837/2000 [0m                     

                       Computation: 42392 steps/s (collection: 2.225s, learning 0.094s)
             Mean action noise std: 4.12
          Mean value_function loss: 182.5231
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 87.1672
                       Mean reward: 766.45
               Mean episode length: 210.51
    Episode_Reward/reaching_object: 1.9121
     Episode_Reward/lifting_object: 161.1031
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.1057
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 2.32s
                      Time elapsed: 01:12:39
                               ETA: 00:06:26

################################################################################
                     [1m Learning iteration 1838/2000 [0m                     

                       Computation: 41898 steps/s (collection: 2.230s, learning 0.117s)
             Mean action noise std: 4.12
          Mean value_function loss: 157.2711
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 87.1922
                       Mean reward: 866.36
               Mean episode length: 230.17
    Episode_Reward/reaching_object: 1.9871
     Episode_Reward/lifting_object: 168.9841
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.1100
          Episode_Reward/joint_vel: -0.0918
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 2.35s
                      Time elapsed: 01:12:42
                               ETA: 00:06:24

################################################################################
                     [1m Learning iteration 1839/2000 [0m                     

                       Computation: 41979 steps/s (collection: 2.235s, learning 0.107s)
             Mean action noise std: 4.12
          Mean value_function loss: 141.3644
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 87.2095
                       Mean reward: 871.33
               Mean episode length: 233.77
    Episode_Reward/reaching_object: 1.9924
     Episode_Reward/lifting_object: 168.9983
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.1105
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 2.34s
                      Time elapsed: 01:12:44
                               ETA: 00:06:21

################################################################################
                     [1m Learning iteration 1840/2000 [0m                     

                       Computation: 41805 steps/s (collection: 2.262s, learning 0.090s)
             Mean action noise std: 4.13
          Mean value_function loss: 168.4878
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 87.2318
                       Mean reward: 866.73
               Mean episode length: 230.23
    Episode_Reward/reaching_object: 1.9842
     Episode_Reward/lifting_object: 168.5661
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.1102
          Episode_Reward/joint_vel: -0.0922
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 2.35s
                      Time elapsed: 01:12:47
                               ETA: 00:06:19

################################################################################
                     [1m Learning iteration 1841/2000 [0m                     

                       Computation: 41348 steps/s (collection: 2.274s, learning 0.104s)
             Mean action noise std: 4.13
          Mean value_function loss: 188.5346
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 87.2421
                       Mean reward: 887.69
               Mean episode length: 236.48
    Episode_Reward/reaching_object: 1.9345
     Episode_Reward/lifting_object: 163.6382
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.1081
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 2.38s
                      Time elapsed: 01:12:49
                               ETA: 00:06:17

################################################################################
                     [1m Learning iteration 1842/2000 [0m                     

                       Computation: 42178 steps/s (collection: 2.241s, learning 0.090s)
             Mean action noise std: 4.13
          Mean value_function loss: 193.3993
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 87.2506
                       Mean reward: 826.25
               Mean episode length: 221.45
    Episode_Reward/reaching_object: 1.9135
     Episode_Reward/lifting_object: 162.8805
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1071
          Episode_Reward/joint_vel: -0.0887
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 2.33s
                      Time elapsed: 01:12:51
                               ETA: 00:06:14

################################################################################
                     [1m Learning iteration 1843/2000 [0m                     

                       Computation: 41703 steps/s (collection: 2.258s, learning 0.099s)
             Mean action noise std: 4.13
          Mean value_function loss: 192.9872
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 87.2669
                       Mean reward: 871.95
               Mean episode length: 230.84
    Episode_Reward/reaching_object: 1.9563
     Episode_Reward/lifting_object: 166.2840
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.1098
          Episode_Reward/joint_vel: -0.0905
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 2.36s
                      Time elapsed: 01:12:54
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 1844/2000 [0m                     

                       Computation: 40914 steps/s (collection: 2.294s, learning 0.109s)
             Mean action noise std: 4.13
          Mean value_function loss: 149.2216
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 87.2844
                       Mean reward: 868.92
               Mean episode length: 232.65
    Episode_Reward/reaching_object: 1.9445
     Episode_Reward/lifting_object: 165.0599
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.1094
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 2.40s
                      Time elapsed: 01:12:56
                               ETA: 00:06:10

################################################################################
                     [1m Learning iteration 1845/2000 [0m                     

                       Computation: 41657 steps/s (collection: 2.252s, learning 0.108s)
             Mean action noise std: 4.14
          Mean value_function loss: 199.4173
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 87.3054
                       Mean reward: 802.48
               Mean episode length: 217.49
    Episode_Reward/reaching_object: 1.8910
     Episode_Reward/lifting_object: 160.6680
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.1066
          Episode_Reward/joint_vel: -0.0888
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 2.36s
                      Time elapsed: 01:12:58
                               ETA: 00:06:07

################################################################################
                     [1m Learning iteration 1846/2000 [0m                     

                       Computation: 41348 steps/s (collection: 2.257s, learning 0.120s)
             Mean action noise std: 4.14
          Mean value_function loss: 132232422.1000
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 87.3136
                       Mean reward: 842.00
               Mean episode length: 224.44
    Episode_Reward/reaching_object: 1.9667
     Episode_Reward/lifting_object: 167.3155
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.1108
          Episode_Reward/joint_vel: -0.0916
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 2.38s
                      Time elapsed: 01:13:01
                               ETA: 00:06:05

################################################################################
                     [1m Learning iteration 1847/2000 [0m                     

                       Computation: 42207 steps/s (collection: 2.240s, learning 0.089s)
             Mean action noise std: 4.14
          Mean value_function loss: 1224565606842415.0000
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 87.3147
                       Mean reward: 830.99
               Mean episode length: 227.22
    Episode_Reward/reaching_object: 1.9314
     Episode_Reward/lifting_object: 163.7151
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -16869.7344
          Episode_Reward/joint_vel: -3325331.7500
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 2.33s
                      Time elapsed: 01:13:03
                               ETA: 00:06:02

################################################################################
                     [1m Learning iteration 1848/2000 [0m                     

                       Computation: 39526 steps/s (collection: 2.377s, learning 0.110s)
             Mean action noise std: 4.14
          Mean value_function loss: 133.9358
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 87.3215
                       Mean reward: 874.50
               Mean episode length: 231.14
    Episode_Reward/reaching_object: 1.9755
     Episode_Reward/lifting_object: 168.3789
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.1112
          Episode_Reward/joint_vel: -0.0908
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 2.49s
                      Time elapsed: 01:13:06
                               ETA: 00:06:00

################################################################################
                     [1m Learning iteration 1849/2000 [0m                     

                       Computation: 38779 steps/s (collection: 2.409s, learning 0.126s)
             Mean action noise std: 4.14
          Mean value_function loss: 172.6087
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 87.3377
                       Mean reward: 839.29
               Mean episode length: 224.47
    Episode_Reward/reaching_object: 1.8845
     Episode_Reward/lifting_object: 160.2526
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1068
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 2.53s
                      Time elapsed: 01:13:08
                               ETA: 00:05:58

################################################################################
                     [1m Learning iteration 1850/2000 [0m                     

                       Computation: 39687 steps/s (collection: 2.321s, learning 0.156s)
             Mean action noise std: 4.14
          Mean value_function loss: 164.9568
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 87.3509
                       Mean reward: 877.32
               Mean episode length: 233.80
    Episode_Reward/reaching_object: 1.9659
     Episode_Reward/lifting_object: 167.6843
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.1108
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 2.48s
                      Time elapsed: 01:13:11
                               ETA: 00:05:55

################################################################################
                     [1m Learning iteration 1851/2000 [0m                     

                       Computation: 37748 steps/s (collection: 2.496s, learning 0.108s)
             Mean action noise std: 4.14
          Mean value_function loss: 176.6424
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 87.3651
                       Mean reward: 816.58
               Mean episode length: 220.35
    Episode_Reward/reaching_object: 1.9470
     Episode_Reward/lifting_object: 165.2447
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1101
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 2.60s
                      Time elapsed: 01:13:13
                               ETA: 00:05:53

################################################################################
                     [1m Learning iteration 1852/2000 [0m                     

                       Computation: 41639 steps/s (collection: 2.263s, learning 0.098s)
             Mean action noise std: 4.15
          Mean value_function loss: 158.6268
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 87.3768
                       Mean reward: 858.47
               Mean episode length: 230.54
    Episode_Reward/reaching_object: 1.9650
     Episode_Reward/lifting_object: 167.1897
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.1109
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 2.36s
                      Time elapsed: 01:13:16
                               ETA: 00:05:51

################################################################################
                     [1m Learning iteration 1853/2000 [0m                     

                       Computation: 40279 steps/s (collection: 2.298s, learning 0.143s)
             Mean action noise std: 4.15
          Mean value_function loss: 171.6641
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 87.3880
                       Mean reward: 855.01
               Mean episode length: 228.50
    Episode_Reward/reaching_object: 1.9321
     Episode_Reward/lifting_object: 164.0336
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.1091
          Episode_Reward/joint_vel: -0.0886
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 2.44s
                      Time elapsed: 01:13:18
                               ETA: 00:05:48

################################################################################
                     [1m Learning iteration 1854/2000 [0m                     

                       Computation: 40483 steps/s (collection: 2.337s, learning 0.091s)
             Mean action noise std: 4.15
          Mean value_function loss: 186.9528
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 87.3990
                       Mean reward: 820.10
               Mean episode length: 221.66
    Episode_Reward/reaching_object: 1.9437
     Episode_Reward/lifting_object: 164.1135
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.1100
          Episode_Reward/joint_vel: -0.0891
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 2.43s
                      Time elapsed: 01:13:20
                               ETA: 00:05:46

################################################################################
                     [1m Learning iteration 1855/2000 [0m                     

                       Computation: 41237 steps/s (collection: 2.269s, learning 0.115s)
             Mean action noise std: 4.15
          Mean value_function loss: 200.1167
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 87.4105
                       Mean reward: 852.94
               Mean episode length: 229.23
    Episode_Reward/reaching_object: 1.9172
     Episode_Reward/lifting_object: 162.4383
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.1088
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 2.38s
                      Time elapsed: 01:13:23
                               ETA: 00:05:44

################################################################################
                     [1m Learning iteration 1856/2000 [0m                     

                       Computation: 40414 steps/s (collection: 2.301s, learning 0.131s)
             Mean action noise std: 4.15
          Mean value_function loss: 198.9966
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 87.4188
                       Mean reward: 788.44
               Mean episode length: 212.64
    Episode_Reward/reaching_object: 1.9148
     Episode_Reward/lifting_object: 161.9219
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.1085
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 2.43s
                      Time elapsed: 01:13:25
                               ETA: 00:05:41

################################################################################
                     [1m Learning iteration 1857/2000 [0m                     

                       Computation: 38041 steps/s (collection: 2.486s, learning 0.098s)
             Mean action noise std: 4.15
          Mean value_function loss: 188.9331
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 87.4341
                       Mean reward: 802.14
               Mean episode length: 215.81
    Episode_Reward/reaching_object: 1.9120
     Episode_Reward/lifting_object: 161.5147
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.1083
          Episode_Reward/joint_vel: -0.0879
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 2.58s
                      Time elapsed: 01:13:28
                               ETA: 00:05:39

################################################################################
                     [1m Learning iteration 1858/2000 [0m                     

                       Computation: 40342 steps/s (collection: 2.324s, learning 0.113s)
             Mean action noise std: 4.16
          Mean value_function loss: 230.8672
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 87.4505
                       Mean reward: 802.07
               Mean episode length: 215.46
    Episode_Reward/reaching_object: 1.8556
     Episode_Reward/lifting_object: 156.4814
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.1055
          Episode_Reward/joint_vel: -0.0862
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 2.44s
                      Time elapsed: 01:13:30
                               ETA: 00:05:36

################################################################################
                     [1m Learning iteration 1859/2000 [0m                     

                       Computation: 39158 steps/s (collection: 2.345s, learning 0.165s)
             Mean action noise std: 4.16
          Mean value_function loss: 178.4125
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 87.4637
                       Mean reward: 848.10
               Mean episode length: 225.71
    Episode_Reward/reaching_object: 1.9117
     Episode_Reward/lifting_object: 161.6908
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.1085
          Episode_Reward/joint_vel: -0.0882
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 2.51s
                      Time elapsed: 01:13:33
                               ETA: 00:05:34

################################################################################
                     [1m Learning iteration 1860/2000 [0m                     

                       Computation: 40934 steps/s (collection: 2.302s, learning 0.100s)
             Mean action noise std: 4.16
          Mean value_function loss: 212.3542
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 87.4725
                       Mean reward: 826.51
               Mean episode length: 221.05
    Episode_Reward/reaching_object: 1.9340
     Episode_Reward/lifting_object: 163.6786
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.1095
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 2.40s
                      Time elapsed: 01:13:35
                               ETA: 00:05:32

################################################################################
                     [1m Learning iteration 1861/2000 [0m                     

                       Computation: 41595 steps/s (collection: 2.268s, learning 0.095s)
             Mean action noise std: 4.16
          Mean value_function loss: 181.3656
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 87.4842
                       Mean reward: 834.21
               Mean episode length: 222.90
    Episode_Reward/reaching_object: 1.9633
     Episode_Reward/lifting_object: 166.4648
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.1118
          Episode_Reward/joint_vel: -0.0910
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 2.36s
                      Time elapsed: 01:13:38
                               ETA: 00:05:29

################################################################################
                     [1m Learning iteration 1862/2000 [0m                     

                       Computation: 40580 steps/s (collection: 2.305s, learning 0.117s)
             Mean action noise std: 4.16
          Mean value_function loss: 158.1388
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 87.4958
                       Mean reward: 835.13
               Mean episode length: 221.25
    Episode_Reward/reaching_object: 1.9765
     Episode_Reward/lifting_object: 168.3535
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1117
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 2.42s
                      Time elapsed: 01:13:40
                               ETA: 00:05:27

################################################################################
                     [1m Learning iteration 1863/2000 [0m                     

                       Computation: 38931 steps/s (collection: 2.430s, learning 0.095s)
             Mean action noise std: 4.16
          Mean value_function loss: 171.1331
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 87.5125
                       Mean reward: 831.89
               Mean episode length: 223.29
    Episode_Reward/reaching_object: 1.9181
     Episode_Reward/lifting_object: 162.3387
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.1098
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 2.53s
                      Time elapsed: 01:13:42
                               ETA: 00:05:25

################################################################################
                     [1m Learning iteration 1864/2000 [0m                     

                       Computation: 40353 steps/s (collection: 2.339s, learning 0.097s)
             Mean action noise std: 4.17
          Mean value_function loss: 165.1963
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 87.5249
                       Mean reward: 798.82
               Mean episode length: 216.51
    Episode_Reward/reaching_object: 1.9306
     Episode_Reward/lifting_object: 163.7019
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.1104
          Episode_Reward/joint_vel: -0.0891
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 2.44s
                      Time elapsed: 01:13:45
                               ETA: 00:05:22

################################################################################
                     [1m Learning iteration 1865/2000 [0m                     

                       Computation: 40624 steps/s (collection: 2.321s, learning 0.099s)
             Mean action noise std: 4.17
          Mean value_function loss: 149.5865
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 87.5414
                       Mean reward: 850.54
               Mean episode length: 227.95
    Episode_Reward/reaching_object: 1.9822
     Episode_Reward/lifting_object: 168.6435
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.1129
          Episode_Reward/joint_vel: -0.0913
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 2.42s
                      Time elapsed: 01:13:47
                               ETA: 00:05:20

################################################################################
                     [1m Learning iteration 1866/2000 [0m                     

                       Computation: 39994 steps/s (collection: 2.331s, learning 0.127s)
             Mean action noise std: 4.17
          Mean value_function loss: 142.5432
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 87.5564
                       Mean reward: 898.63
               Mean episode length: 238.05
    Episode_Reward/reaching_object: 2.0275
     Episode_Reward/lifting_object: 173.0732
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.1149
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 2.46s
                      Time elapsed: 01:13:50
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 1867/2000 [0m                     

                       Computation: 40247 steps/s (collection: 2.321s, learning 0.121s)
             Mean action noise std: 4.17
          Mean value_function loss: 156.3359
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 87.5689
                       Mean reward: 845.79
               Mean episode length: 226.09
    Episode_Reward/reaching_object: 1.9572
     Episode_Reward/lifting_object: 165.7815
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.1120
          Episode_Reward/joint_vel: -0.0908
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 2.44s
                      Time elapsed: 01:13:52
                               ETA: 00:05:15

################################################################################
                     [1m Learning iteration 1868/2000 [0m                     

                       Computation: 40314 steps/s (collection: 2.320s, learning 0.118s)
             Mean action noise std: 4.18
          Mean value_function loss: 167.2250
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 87.5928
                       Mean reward: 822.65
               Mean episode length: 220.78
    Episode_Reward/reaching_object: 1.9638
     Episode_Reward/lifting_object: 166.6303
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1121
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 2.44s
                      Time elapsed: 01:13:55
                               ETA: 00:05:13

################################################################################
                     [1m Learning iteration 1869/2000 [0m                     

                       Computation: 39898 steps/s (collection: 2.309s, learning 0.155s)
             Mean action noise std: 4.18
          Mean value_function loss: 174.9134
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 87.6056
                       Mean reward: 789.99
               Mean episode length: 214.24
    Episode_Reward/reaching_object: 1.9220
     Episode_Reward/lifting_object: 162.5996
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.1100
          Episode_Reward/joint_vel: -0.0885
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 2.46s
                      Time elapsed: 01:13:57
                               ETA: 00:05:10

################################################################################
                     [1m Learning iteration 1870/2000 [0m                     

                       Computation: 40581 steps/s (collection: 2.331s, learning 0.092s)
             Mean action noise std: 4.18
          Mean value_function loss: 146.9374
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 87.6180
                       Mean reward: 868.70
               Mean episode length: 233.23
    Episode_Reward/reaching_object: 1.9843
     Episode_Reward/lifting_object: 167.6030
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.1131
          Episode_Reward/joint_vel: -0.0909
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 2.42s
                      Time elapsed: 01:14:00
                               ETA: 00:05:08

################################################################################
                     [1m Learning iteration 1871/2000 [0m                     

                       Computation: 38819 steps/s (collection: 2.406s, learning 0.126s)
             Mean action noise std: 4.18
          Mean value_function loss: 189.4115
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 87.6334
                       Mean reward: 849.91
               Mean episode length: 226.37
    Episode_Reward/reaching_object: 1.9705
     Episode_Reward/lifting_object: 166.3086
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.1122
          Episode_Reward/joint_vel: -0.0898
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 2.53s
                      Time elapsed: 01:14:02
                               ETA: 00:05:06

################################################################################
                     [1m Learning iteration 1872/2000 [0m                     

                       Computation: 41017 steps/s (collection: 2.294s, learning 0.103s)
             Mean action noise std: 4.18
          Mean value_function loss: 182.7432
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 87.6432
                       Mean reward: 844.05
               Mean episode length: 224.28
    Episode_Reward/reaching_object: 1.9292
     Episode_Reward/lifting_object: 162.8193
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.1103
          Episode_Reward/joint_vel: -0.0886
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 2.40s
                      Time elapsed: 01:14:04
                               ETA: 00:05:03

################################################################################
                     [1m Learning iteration 1873/2000 [0m                     

                       Computation: 40661 steps/s (collection: 2.319s, learning 0.099s)
             Mean action noise std: 4.18
          Mean value_function loss: 155.7483
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 87.6587
                       Mean reward: 835.66
               Mean episode length: 223.42
    Episode_Reward/reaching_object: 2.0026
     Episode_Reward/lifting_object: 169.5447
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.1141
          Episode_Reward/joint_vel: -0.0907
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 2.42s
                      Time elapsed: 01:14:07
                               ETA: 00:05:01

################################################################################
                     [1m Learning iteration 1874/2000 [0m                     

                       Computation: 40880 steps/s (collection: 2.307s, learning 0.098s)
             Mean action noise std: 4.19
          Mean value_function loss: 186.7169
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 87.6686
                       Mean reward: 846.99
               Mean episode length: 226.49
    Episode_Reward/reaching_object: 1.9497
     Episode_Reward/lifting_object: 164.7752
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.1115
          Episode_Reward/joint_vel: -0.0886
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 2.40s
                      Time elapsed: 01:14:09
                               ETA: 00:04:59

################################################################################
                     [1m Learning iteration 1875/2000 [0m                     

                       Computation: 37769 steps/s (collection: 2.472s, learning 0.131s)
             Mean action noise std: 4.19
          Mean value_function loss: 170.1317
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 87.6792
                       Mean reward: 847.85
               Mean episode length: 226.26
    Episode_Reward/reaching_object: 1.9824
     Episode_Reward/lifting_object: 167.0690
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.1140
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 2.60s
                      Time elapsed: 01:14:12
                               ETA: 00:04:56

################################################################################
                     [1m Learning iteration 1876/2000 [0m                     

                       Computation: 40596 steps/s (collection: 2.330s, learning 0.092s)
             Mean action noise std: 4.19
          Mean value_function loss: 188.3272
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 87.6988
                       Mean reward: 861.99
               Mean episode length: 231.23
    Episode_Reward/reaching_object: 1.9474
     Episode_Reward/lifting_object: 164.5446
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.1119
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 2.42s
                      Time elapsed: 01:14:14
                               ETA: 00:04:54

################################################################################
                     [1m Learning iteration 1877/2000 [0m                     

                       Computation: 39890 steps/s (collection: 2.356s, learning 0.108s)
             Mean action noise std: 4.19
          Mean value_function loss: 175.1721
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 87.7172
                       Mean reward: 842.03
               Mean episode length: 224.71
    Episode_Reward/reaching_object: 1.9506
     Episode_Reward/lifting_object: 164.7640
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.1121
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 2.46s
                      Time elapsed: 01:14:17
                               ETA: 00:04:51

################################################################################
                     [1m Learning iteration 1878/2000 [0m                     

                       Computation: 39216 steps/s (collection: 2.358s, learning 0.149s)
             Mean action noise std: 4.19
          Mean value_function loss: 203.2212
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 87.7319
                       Mean reward: 820.20
               Mean episode length: 218.85
    Episode_Reward/reaching_object: 1.9294
     Episode_Reward/lifting_object: 163.2173
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.1111
          Episode_Reward/joint_vel: -0.0884
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 2.51s
                      Time elapsed: 01:14:19
                               ETA: 00:04:49

################################################################################
                     [1m Learning iteration 1879/2000 [0m                     

                       Computation: 40109 steps/s (collection: 2.354s, learning 0.097s)
             Mean action noise std: 4.20
          Mean value_function loss: 195.7249
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 87.7454
                       Mean reward: 837.78
               Mean episode length: 224.88
    Episode_Reward/reaching_object: 1.9058
     Episode_Reward/lifting_object: 160.5853
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.1101
          Episode_Reward/joint_vel: -0.0879
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 2.45s
                      Time elapsed: 01:14:22
                               ETA: 00:04:47

################################################################################
                     [1m Learning iteration 1880/2000 [0m                     

                       Computation: 40470 steps/s (collection: 2.324s, learning 0.105s)
             Mean action noise std: 4.20
          Mean value_function loss: 213.3189
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 87.7600
                       Mean reward: 832.40
               Mean episode length: 221.79
    Episode_Reward/reaching_object: 1.8884
     Episode_Reward/lifting_object: 159.2225
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.1094
          Episode_Reward/joint_vel: -0.0877
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 2.43s
                      Time elapsed: 01:14:24
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 1881/2000 [0m                     

                       Computation: 40801 steps/s (collection: 2.305s, learning 0.104s)
             Mean action noise std: 4.20
          Mean value_function loss: 191.0574
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 87.7714
                       Mean reward: 820.50
               Mean episode length: 220.22
    Episode_Reward/reaching_object: 1.9433
     Episode_Reward/lifting_object: 163.9834
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.1125
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 2.41s
                      Time elapsed: 01:14:27
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 1882/2000 [0m                     

                       Computation: 40257 steps/s (collection: 2.346s, learning 0.096s)
             Mean action noise std: 4.20
          Mean value_function loss: 175.6329
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 87.7787
                       Mean reward: 879.77
               Mean episode length: 234.04
    Episode_Reward/reaching_object: 1.9357
     Episode_Reward/lifting_object: 162.7808
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.1121
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 2.44s
                      Time elapsed: 01:14:29
                               ETA: 00:04:40

################################################################################
                     [1m Learning iteration 1883/2000 [0m                     

                       Computation: 40086 steps/s (collection: 2.343s, learning 0.109s)
             Mean action noise std: 4.20
          Mean value_function loss: 300.3119
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 87.7833
                       Mean reward: 765.97
               Mean episode length: 210.79
    Episode_Reward/reaching_object: 1.8607
     Episode_Reward/lifting_object: 155.1758
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.1089
          Episode_Reward/joint_vel: -0.0887
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 2.45s
                      Time elapsed: 01:14:31
                               ETA: 00:04:37

################################################################################
                     [1m Learning iteration 1884/2000 [0m                     

                       Computation: 40908 steps/s (collection: 2.303s, learning 0.100s)
             Mean action noise std: 4.20
          Mean value_function loss: 172.5235
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 87.7908
                       Mean reward: 856.82
               Mean episode length: 229.03
    Episode_Reward/reaching_object: 1.9846
     Episode_Reward/lifting_object: 167.6232
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.1152
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 2.40s
                      Time elapsed: 01:14:34
                               ETA: 00:04:35

################################################################################
                     [1m Learning iteration 1885/2000 [0m                     

                       Computation: 39834 steps/s (collection: 2.341s, learning 0.127s)
             Mean action noise std: 4.20
          Mean value_function loss: 186.5545
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 87.8008
                       Mean reward: 816.84
               Mean episode length: 220.40
    Episode_Reward/reaching_object: 1.9352
     Episode_Reward/lifting_object: 163.0832
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.1127
          Episode_Reward/joint_vel: -0.0914
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 2.47s
                      Time elapsed: 01:14:36
                               ETA: 00:04:32

################################################################################
                     [1m Learning iteration 1886/2000 [0m                     

                       Computation: 41030 steps/s (collection: 2.302s, learning 0.094s)
             Mean action noise std: 4.21
          Mean value_function loss: 210.4286
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 87.8144
                       Mean reward: 751.83
               Mean episode length: 205.66
    Episode_Reward/reaching_object: 1.8889
     Episode_Reward/lifting_object: 158.6482
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.1102
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 2.40s
                      Time elapsed: 01:14:39
                               ETA: 00:04:30

################################################################################
                     [1m Learning iteration 1887/2000 [0m                     

                       Computation: 40497 steps/s (collection: 2.329s, learning 0.099s)
             Mean action noise std: 4.21
          Mean value_function loss: 191.8322
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 87.8221
                       Mean reward: 804.63
               Mean episode length: 215.56
    Episode_Reward/reaching_object: 1.9263
     Episode_Reward/lifting_object: 162.3883
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.1124
          Episode_Reward/joint_vel: -0.0913
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 2.43s
                      Time elapsed: 01:14:41
                               ETA: 00:04:28

################################################################################
                     [1m Learning iteration 1888/2000 [0m                     

                       Computation: 40242 steps/s (collection: 2.339s, learning 0.104s)
             Mean action noise std: 4.21
          Mean value_function loss: 163.2269
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 87.8317
                       Mean reward: 854.68
               Mean episode length: 230.16
    Episode_Reward/reaching_object: 1.9518
     Episode_Reward/lifting_object: 164.8053
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1142
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 2.44s
                      Time elapsed: 01:14:44
                               ETA: 00:04:25

################################################################################
                     [1m Learning iteration 1889/2000 [0m                     

                       Computation: 40459 steps/s (collection: 2.338s, learning 0.092s)
             Mean action noise std: 4.21
          Mean value_function loss: 147.1245
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 87.8443
                       Mean reward: 814.17
               Mean episode length: 217.71
    Episode_Reward/reaching_object: 1.9490
     Episode_Reward/lifting_object: 165.2570
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.1138
          Episode_Reward/joint_vel: -0.0929
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 2.43s
                      Time elapsed: 01:14:46
                               ETA: 00:04:23

################################################################################
                     [1m Learning iteration 1890/2000 [0m                     

                       Computation: 40329 steps/s (collection: 2.344s, learning 0.093s)
             Mean action noise std: 4.21
          Mean value_function loss: 149.0284
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 87.8514
                       Mean reward: 852.08
               Mean episode length: 227.50
    Episode_Reward/reaching_object: 1.9919
     Episode_Reward/lifting_object: 168.9846
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.1166
          Episode_Reward/joint_vel: -0.0950
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 2.44s
                      Time elapsed: 01:14:48
                               ETA: 00:04:21

################################################################################
                     [1m Learning iteration 1891/2000 [0m                     

                       Computation: 40954 steps/s (collection: 2.301s, learning 0.099s)
             Mean action noise std: 4.21
          Mean value_function loss: 201.3145
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 87.8586
                       Mean reward: 858.80
               Mean episode length: 228.68
    Episode_Reward/reaching_object: 1.9709
     Episode_Reward/lifting_object: 167.3610
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1151
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 2.40s
                      Time elapsed: 01:14:51
                               ETA: 00:04:18

################################################################################
                     [1m Learning iteration 1892/2000 [0m                     

                       Computation: 39282 steps/s (collection: 2.373s, learning 0.130s)
             Mean action noise std: 4.21
          Mean value_function loss: 252.0371
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 87.8766
                       Mean reward: 894.35
               Mean episode length: 237.34
    Episode_Reward/reaching_object: 1.9527
     Episode_Reward/lifting_object: 165.8186
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.1144
          Episode_Reward/joint_vel: -0.0937
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 2.50s
                      Time elapsed: 01:14:53
                               ETA: 00:04:16

################################################################################
                     [1m Learning iteration 1893/2000 [0m                     

                       Computation: 38820 steps/s (collection: 2.405s, learning 0.127s)
             Mean action noise std: 4.22
          Mean value_function loss: 228.9863
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 87.8918
                       Mean reward: 849.62
               Mean episode length: 226.03
    Episode_Reward/reaching_object: 1.9621
     Episode_Reward/lifting_object: 166.1079
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1148
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 2.53s
                      Time elapsed: 01:14:56
                               ETA: 00:04:14

################################################################################
                     [1m Learning iteration 1894/2000 [0m                     

                       Computation: 38533 steps/s (collection: 2.418s, learning 0.133s)
             Mean action noise std: 4.22
          Mean value_function loss: 226.9187
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 87.9036
                       Mean reward: 823.23
               Mean episode length: 220.58
    Episode_Reward/reaching_object: 1.9117
     Episode_Reward/lifting_object: 162.4087
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.1123
          Episode_Reward/joint_vel: -0.0927
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 2.55s
                      Time elapsed: 01:14:58
                               ETA: 00:04:11

################################################################################
                     [1m Learning iteration 1895/2000 [0m                     

                       Computation: 37926 steps/s (collection: 2.422s, learning 0.170s)
             Mean action noise std: 4.22
          Mean value_function loss: 152.8557
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 87.9092
                       Mean reward: 847.36
               Mean episode length: 225.00
    Episode_Reward/reaching_object: 1.9502
     Episode_Reward/lifting_object: 166.4318
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.1146
          Episode_Reward/joint_vel: -0.0932
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 2.59s
                      Time elapsed: 01:15:01
                               ETA: 00:04:09

################################################################################
                     [1m Learning iteration 1896/2000 [0m                     

                       Computation: 40109 steps/s (collection: 2.353s, learning 0.098s)
             Mean action noise std: 4.22
          Mean value_function loss: 156.5444
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 87.9212
                       Mean reward: 820.73
               Mean episode length: 220.74
    Episode_Reward/reaching_object: 1.9661
     Episode_Reward/lifting_object: 167.4683
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1157
          Episode_Reward/joint_vel: -0.0944
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 2.45s
                      Time elapsed: 01:15:04
                               ETA: 00:04:06

################################################################################
                     [1m Learning iteration 1897/2000 [0m                     

                       Computation: 40044 steps/s (collection: 2.357s, learning 0.098s)
             Mean action noise std: 4.22
          Mean value_function loss: 166.8866
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 87.9379
                       Mean reward: 826.58
               Mean episode length: 222.48
    Episode_Reward/reaching_object: 1.9217
     Episode_Reward/lifting_object: 163.8616
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.1133
          Episode_Reward/joint_vel: -0.0925
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 2.45s
                      Time elapsed: 01:15:06
                               ETA: 00:04:04

################################################################################
                     [1m Learning iteration 1898/2000 [0m                     

                       Computation: 40574 steps/s (collection: 2.315s, learning 0.107s)
             Mean action noise std: 4.22
          Mean value_function loss: 153.0713
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 87.9468
                       Mean reward: 844.48
               Mean episode length: 225.21
    Episode_Reward/reaching_object: 1.9815
     Episode_Reward/lifting_object: 169.1126
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.1165
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 2.42s
                      Time elapsed: 01:15:08
                               ETA: 00:04:02

################################################################################
                     [1m Learning iteration 1899/2000 [0m                     

                       Computation: 39469 steps/s (collection: 2.390s, learning 0.101s)
             Mean action noise std: 4.23
          Mean value_function loss: 153.7295
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 87.9528
                       Mean reward: 839.13
               Mean episode length: 224.97
    Episode_Reward/reaching_object: 1.9814
     Episode_Reward/lifting_object: 168.7838
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1167
          Episode_Reward/joint_vel: -0.0953
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 2.49s
                      Time elapsed: 01:15:11
                               ETA: 00:03:59

################################################################################
                     [1m Learning iteration 1900/2000 [0m                     

                       Computation: 37785 steps/s (collection: 2.426s, learning 0.175s)
             Mean action noise std: 4.23
          Mean value_function loss: 215.6463
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 87.9653
                       Mean reward: 776.40
               Mean episode length: 209.30
    Episode_Reward/reaching_object: 1.9247
     Episode_Reward/lifting_object: 163.9241
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.1140
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 2.60s
                      Time elapsed: 01:15:13
                               ETA: 00:03:57

################################################################################
                     [1m Learning iteration 1901/2000 [0m                     

                       Computation: 39389 steps/s (collection: 2.401s, learning 0.095s)
             Mean action noise std: 4.23
          Mean value_function loss: 135.7382
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 87.9781
                       Mean reward: 843.38
               Mean episode length: 226.23
    Episode_Reward/reaching_object: 1.9948
     Episode_Reward/lifting_object: 169.7783
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.1184
          Episode_Reward/joint_vel: -0.1778
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 2.50s
                      Time elapsed: 01:15:16
                               ETA: 00:03:55

################################################################################
                     [1m Learning iteration 1902/2000 [0m                     

                       Computation: 39183 steps/s (collection: 2.366s, learning 0.143s)
             Mean action noise std: 4.23
          Mean value_function loss: 163.8275
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 87.9827
                       Mean reward: 829.93
               Mean episode length: 222.23
    Episode_Reward/reaching_object: 1.8993
     Episode_Reward/lifting_object: 161.3847
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.1132
          Episode_Reward/joint_vel: -0.0931
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 2.51s
                      Time elapsed: 01:15:18
                               ETA: 00:03:52

################################################################################
                     [1m Learning iteration 1903/2000 [0m                     

                       Computation: 39676 steps/s (collection: 2.372s, learning 0.105s)
             Mean action noise std: 4.23
          Mean value_function loss: 173.0741
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 87.9948
                       Mean reward: 841.77
               Mean episode length: 223.73
    Episode_Reward/reaching_object: 1.9515
     Episode_Reward/lifting_object: 165.8135
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.1158
          Episode_Reward/joint_vel: -0.0945
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 2.48s
                      Time elapsed: 01:15:21
                               ETA: 00:03:50

################################################################################
                     [1m Learning iteration 1904/2000 [0m                     

                       Computation: 39786 steps/s (collection: 2.361s, learning 0.110s)
             Mean action noise std: 4.24
          Mean value_function loss: 152.8991
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 88.0160
                       Mean reward: 891.41
               Mean episode length: 238.02
    Episode_Reward/reaching_object: 2.0134
     Episode_Reward/lifting_object: 170.8726
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.1191
          Episode_Reward/joint_vel: -0.0962
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 2.47s
                      Time elapsed: 01:15:23
                               ETA: 00:03:47

################################################################################
                     [1m Learning iteration 1905/2000 [0m                     

                       Computation: 39776 steps/s (collection: 2.350s, learning 0.121s)
             Mean action noise std: 4.24
          Mean value_function loss: 215.9790
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 88.0306
                       Mean reward: 781.96
               Mean episode length: 210.29
    Episode_Reward/reaching_object: 1.9237
     Episode_Reward/lifting_object: 163.4259
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.1137
          Episode_Reward/joint_vel: -0.0924
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 2.47s
                      Time elapsed: 01:15:26
                               ETA: 00:03:45

################################################################################
                     [1m Learning iteration 1906/2000 [0m                     

                       Computation: 38348 steps/s (collection: 2.358s, learning 0.206s)
             Mean action noise std: 4.24
          Mean value_function loss: 182.1559
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 88.0398
                       Mean reward: 853.24
               Mean episode length: 229.22
    Episode_Reward/reaching_object: 1.9023
     Episode_Reward/lifting_object: 161.0734
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.1131
          Episode_Reward/joint_vel: -0.0932
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 2.56s
                      Time elapsed: 01:15:28
                               ETA: 00:03:43

################################################################################
                     [1m Learning iteration 1907/2000 [0m                     

                       Computation: 37400 steps/s (collection: 2.520s, learning 0.109s)
             Mean action noise std: 4.24
          Mean value_function loss: 150.6324
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 88.0531
                       Mean reward: 843.37
               Mean episode length: 227.06
    Episode_Reward/reaching_object: 1.9719
     Episode_Reward/lifting_object: 167.9211
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.1166
          Episode_Reward/joint_vel: -0.0953
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 2.63s
                      Time elapsed: 01:15:31
                               ETA: 00:03:40

################################################################################
                     [1m Learning iteration 1908/2000 [0m                     

                       Computation: 39272 steps/s (collection: 2.389s, learning 0.115s)
             Mean action noise std: 4.24
          Mean value_function loss: 192.6673
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 88.0650
                       Mean reward: 819.96
               Mean episode length: 220.18
    Episode_Reward/reaching_object: 1.9251
     Episode_Reward/lifting_object: 163.3524
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.1147
          Episode_Reward/joint_vel: -0.0944
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 2.50s
                      Time elapsed: 01:15:34
                               ETA: 00:03:38

################################################################################
                     [1m Learning iteration 1909/2000 [0m                     

                       Computation: 36984 steps/s (collection: 2.491s, learning 0.167s)
             Mean action noise std: 4.24
          Mean value_function loss: 164.0551
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 88.0810
                       Mean reward: 880.24
               Mean episode length: 232.55
    Episode_Reward/reaching_object: 1.9207
     Episode_Reward/lifting_object: 163.3613
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.1149
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 2.66s
                      Time elapsed: 01:15:36
                               ETA: 00:03:36

################################################################################
                     [1m Learning iteration 1910/2000 [0m                     

                       Computation: 38686 steps/s (collection: 2.443s, learning 0.098s)
             Mean action noise std: 4.25
          Mean value_function loss: 122.0818
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 88.0968
                       Mean reward: 838.02
               Mean episode length: 222.95
    Episode_Reward/reaching_object: 1.9806
     Episode_Reward/lifting_object: 169.3094
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.1178
          Episode_Reward/joint_vel: -0.0948
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 2.54s
                      Time elapsed: 01:15:39
                               ETA: 00:03:33

################################################################################
                     [1m Learning iteration 1911/2000 [0m                     

                       Computation: 39787 steps/s (collection: 2.355s, learning 0.116s)
             Mean action noise std: 4.25
          Mean value_function loss: 153.7012
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 88.1192
                       Mean reward: 867.35
               Mean episode length: 231.80
    Episode_Reward/reaching_object: 1.9681
     Episode_Reward/lifting_object: 168.1636
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.1174
          Episode_Reward/joint_vel: -0.0951
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 2.47s
                      Time elapsed: 01:15:41
                               ETA: 00:03:31

################################################################################
                     [1m Learning iteration 1912/2000 [0m                     

                       Computation: 39249 steps/s (collection: 2.389s, learning 0.116s)
             Mean action noise std: 4.25
          Mean value_function loss: 134.0831
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 88.1361
                       Mean reward: 876.52
               Mean episode length: 233.32
    Episode_Reward/reaching_object: 1.9817
     Episode_Reward/lifting_object: 169.6573
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.1183
          Episode_Reward/joint_vel: -0.0953
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 2.50s
                      Time elapsed: 01:15:44
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 1913/2000 [0m                     

                       Computation: 39701 steps/s (collection: 2.352s, learning 0.124s)
             Mean action noise std: 4.25
          Mean value_function loss: 154.6349
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 88.1487
                       Mean reward: 818.60
               Mean episode length: 220.79
    Episode_Reward/reaching_object: 1.9763
     Episode_Reward/lifting_object: 169.3652
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.1180
          Episode_Reward/joint_vel: -0.0962
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 2.48s
                      Time elapsed: 01:15:46
                               ETA: 00:03:26

################################################################################
                     [1m Learning iteration 1914/2000 [0m                     

                       Computation: 38718 steps/s (collection: 2.431s, learning 0.108s)
             Mean action noise std: 4.26
          Mean value_function loss: 162.5781
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 88.1645
                       Mean reward: 829.94
               Mean episode length: 221.88
    Episode_Reward/reaching_object: 1.9553
     Episode_Reward/lifting_object: 167.1389
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.1172
          Episode_Reward/joint_vel: -0.0955
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 2.54s
                      Time elapsed: 01:15:49
                               ETA: 00:03:24

################################################################################
                     [1m Learning iteration 1915/2000 [0m                     

                       Computation: 39910 steps/s (collection: 2.333s, learning 0.130s)
             Mean action noise std: 4.26
          Mean value_function loss: 188.1949
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 88.1749
                       Mean reward: 805.98
               Mean episode length: 216.85
    Episode_Reward/reaching_object: 1.9393
     Episode_Reward/lifting_object: 165.7540
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.1162
          Episode_Reward/joint_vel: -0.0947
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 2.46s
                      Time elapsed: 01:15:51
                               ETA: 00:03:21

################################################################################
                     [1m Learning iteration 1916/2000 [0m                     

                       Computation: 39885 steps/s (collection: 2.370s, learning 0.095s)
             Mean action noise std: 4.26
          Mean value_function loss: 171.0888
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 88.1903
                       Mean reward: 819.72
               Mean episode length: 219.90
    Episode_Reward/reaching_object: 1.9526
     Episode_Reward/lifting_object: 167.0200
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.1171
          Episode_Reward/joint_vel: -0.0953
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 2.46s
                      Time elapsed: 01:15:54
                               ETA: 00:03:19

################################################################################
                     [1m Learning iteration 1917/2000 [0m                     

                       Computation: 39327 steps/s (collection: 2.390s, learning 0.110s)
             Mean action noise std: 4.26
          Mean value_function loss: 148.2767
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 88.2029
                       Mean reward: 795.29
               Mean episode length: 213.45
    Episode_Reward/reaching_object: 1.9722
     Episode_Reward/lifting_object: 168.5712
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.1186
          Episode_Reward/joint_vel: -0.0976
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 2.50s
                      Time elapsed: 01:15:56
                               ETA: 00:03:17

################################################################################
                     [1m Learning iteration 1918/2000 [0m                     

                       Computation: 40040 steps/s (collection: 2.361s, learning 0.094s)
             Mean action noise std: 4.26
          Mean value_function loss: 170.2377
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 88.2145
                       Mean reward: 822.36
               Mean episode length: 221.58
    Episode_Reward/reaching_object: 1.9376
     Episode_Reward/lifting_object: 165.0850
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.1168
          Episode_Reward/joint_vel: -0.0964
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 2.46s
                      Time elapsed: 01:15:59
                               ETA: 00:03:14

################################################################################
                     [1m Learning iteration 1919/2000 [0m                     

                       Computation: 40136 steps/s (collection: 2.346s, learning 0.103s)
             Mean action noise std: 4.26
          Mean value_function loss: 171.3845
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 88.2236
                       Mean reward: 839.85
               Mean episode length: 225.33
    Episode_Reward/reaching_object: 1.9615
     Episode_Reward/lifting_object: 167.9720
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.1178
          Episode_Reward/joint_vel: -0.0974
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 2.45s
                      Time elapsed: 01:16:01
                               ETA: 00:03:12

################################################################################
                     [1m Learning iteration 1920/2000 [0m                     

                       Computation: 39821 steps/s (collection: 2.349s, learning 0.120s)
             Mean action noise std: 4.27
          Mean value_function loss: 138.3543
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 88.2381
                       Mean reward: 875.19
               Mean episode length: 232.65
    Episode_Reward/reaching_object: 1.9931
     Episode_Reward/lifting_object: 170.5850
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1197
          Episode_Reward/joint_vel: -0.0982
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 2.47s
                      Time elapsed: 01:16:04
                               ETA: 00:03:10

################################################################################
                     [1m Learning iteration 1921/2000 [0m                     

                       Computation: 40165 steps/s (collection: 2.327s, learning 0.120s)
             Mean action noise std: 4.27
          Mean value_function loss: 121.2087
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 88.2493
                       Mean reward: 904.76
               Mean episode length: 239.31
    Episode_Reward/reaching_object: 1.9507
     Episode_Reward/lifting_object: 166.1815
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.1180
          Episode_Reward/joint_vel: -0.0984
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 2.45s
                      Time elapsed: 01:16:06
                               ETA: 00:03:07

################################################################################
                     [1m Learning iteration 1922/2000 [0m                     

                       Computation: 39503 steps/s (collection: 2.358s, learning 0.130s)
             Mean action noise std: 4.27
          Mean value_function loss: 185.8366
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 88.2663
                       Mean reward: 844.55
               Mean episode length: 227.50
    Episode_Reward/reaching_object: 1.9085
     Episode_Reward/lifting_object: 162.4473
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.1157
          Episode_Reward/joint_vel: -0.0963
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 2.49s
                      Time elapsed: 01:16:09
                               ETA: 00:03:05

################################################################################
                     [1m Learning iteration 1923/2000 [0m                     

                       Computation: 39781 steps/s (collection: 2.353s, learning 0.118s)
             Mean action noise std: 4.27
          Mean value_function loss: 184.9315
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 88.2841
                       Mean reward: 813.42
               Mean episode length: 218.59
    Episode_Reward/reaching_object: 1.9597
     Episode_Reward/lifting_object: 167.7592
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.1185
          Episode_Reward/joint_vel: -0.0980
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 2.47s
                      Time elapsed: 01:16:11
                               ETA: 00:03:02

################################################################################
                     [1m Learning iteration 1924/2000 [0m                     

                       Computation: 39276 steps/s (collection: 2.376s, learning 0.127s)
             Mean action noise std: 4.27
          Mean value_function loss: 131.3079
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 88.2901
                       Mean reward: 878.41
               Mean episode length: 232.93
    Episode_Reward/reaching_object: 1.9806
     Episode_Reward/lifting_object: 168.8566
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.1196
          Episode_Reward/joint_vel: -0.0989
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 2.50s
                      Time elapsed: 01:16:13
                               ETA: 00:03:00

################################################################################
                     [1m Learning iteration 1925/2000 [0m                     

                       Computation: 40016 steps/s (collection: 2.356s, learning 0.100s)
             Mean action noise std: 4.28
          Mean value_function loss: 146.2512
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 88.3009
                       Mean reward: 887.76
               Mean episode length: 236.32
    Episode_Reward/reaching_object: 1.9793
     Episode_Reward/lifting_object: 169.3257
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.1198
          Episode_Reward/joint_vel: -0.0989
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 2.46s
                      Time elapsed: 01:16:16
                               ETA: 00:02:58

################################################################################
                     [1m Learning iteration 1926/2000 [0m                     

                       Computation: 39627 steps/s (collection: 2.357s, learning 0.124s)
             Mean action noise std: 4.28
          Mean value_function loss: 168.4183
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 88.3076
                       Mean reward: 804.70
               Mean episode length: 215.63
    Episode_Reward/reaching_object: 1.9748
     Episode_Reward/lifting_object: 169.3092
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.1194
          Episode_Reward/joint_vel: -0.0970
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 2.48s
                      Time elapsed: 01:16:18
                               ETA: 00:02:55

################################################################################
                     [1m Learning iteration 1927/2000 [0m                     

                       Computation: 40071 steps/s (collection: 2.341s, learning 0.113s)
             Mean action noise std: 4.28
          Mean value_function loss: 160.8776
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 88.3132
                       Mean reward: 861.84
               Mean episode length: 230.89
    Episode_Reward/reaching_object: 1.9710
     Episode_Reward/lifting_object: 168.0564
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.1194
          Episode_Reward/joint_vel: -0.0980
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 2.45s
                      Time elapsed: 01:16:21
                               ETA: 00:02:53

################################################################################
                     [1m Learning iteration 1928/2000 [0m                     

                       Computation: 38899 steps/s (collection: 2.407s, learning 0.120s)
             Mean action noise std: 4.28
          Mean value_function loss: 180.8519
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 88.3243
                       Mean reward: 858.12
               Mean episode length: 230.51
    Episode_Reward/reaching_object: 1.9705
     Episode_Reward/lifting_object: 167.7939
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.1195
          Episode_Reward/joint_vel: -0.0984
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 2.53s
                      Time elapsed: 01:16:23
                               ETA: 00:02:51

################################################################################
                     [1m Learning iteration 1929/2000 [0m                     

                       Computation: 37609 steps/s (collection: 2.462s, learning 0.152s)
             Mean action noise std: 4.28
          Mean value_function loss: 170.0048
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 88.3436
                       Mean reward: 821.31
               Mean episode length: 220.77
    Episode_Reward/reaching_object: 1.9343
     Episode_Reward/lifting_object: 165.0784
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.1174
          Episode_Reward/joint_vel: -0.0967
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 2.61s
                      Time elapsed: 01:16:26
                               ETA: 00:02:48

################################################################################
                     [1m Learning iteration 1930/2000 [0m                     

                       Computation: 38163 steps/s (collection: 2.465s, learning 0.111s)
             Mean action noise std: 4.28
          Mean value_function loss: 150.1367
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 88.3559
                       Mean reward: 848.69
               Mean episode length: 226.94
    Episode_Reward/reaching_object: 1.9700
     Episode_Reward/lifting_object: 167.8426
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1197
          Episode_Reward/joint_vel: -0.0983
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 2.58s
                      Time elapsed: 01:16:29
                               ETA: 00:02:46

################################################################################
                     [1m Learning iteration 1931/2000 [0m                     

                       Computation: 37901 steps/s (collection: 2.481s, learning 0.113s)
             Mean action noise std: 4.29
          Mean value_function loss: 202.3541
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 88.3710
                       Mean reward: 803.37
               Mean episode length: 216.24
    Episode_Reward/reaching_object: 1.9252
     Episode_Reward/lifting_object: 164.1072
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.1168
          Episode_Reward/joint_vel: -0.0950
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 2.59s
                      Time elapsed: 01:16:31
                               ETA: 00:02:43

################################################################################
                     [1m Learning iteration 1932/2000 [0m                     

                       Computation: 39727 steps/s (collection: 2.366s, learning 0.108s)
             Mean action noise std: 4.29
          Mean value_function loss: 185.0187
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 88.3862
                       Mean reward: 823.79
               Mean episode length: 222.16
    Episode_Reward/reaching_object: 1.9515
     Episode_Reward/lifting_object: 165.5586
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.1186
          Episode_Reward/joint_vel: -0.0965
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 2.47s
                      Time elapsed: 01:16:34
                               ETA: 00:02:41

################################################################################
                     [1m Learning iteration 1933/2000 [0m                     

                       Computation: 40546 steps/s (collection: 2.330s, learning 0.094s)
             Mean action noise std: 4.29
          Mean value_function loss: 151.5339
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 88.4007
                       Mean reward: 872.01
               Mean episode length: 232.19
    Episode_Reward/reaching_object: 1.9977
     Episode_Reward/lifting_object: 170.4534
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.1208
          Episode_Reward/joint_vel: -0.0978
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 2.42s
                      Time elapsed: 01:16:36
                               ETA: 00:02:39

################################################################################
                     [1m Learning iteration 1934/2000 [0m                     

                       Computation: 39814 steps/s (collection: 2.341s, learning 0.128s)
             Mean action noise std: 4.29
          Mean value_function loss: 187.8962
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 88.4178
                       Mean reward: 826.36
               Mean episode length: 222.41
    Episode_Reward/reaching_object: 1.9319
     Episode_Reward/lifting_object: 163.8029
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.1176
          Episode_Reward/joint_vel: -0.0947
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 2.47s
                      Time elapsed: 01:16:39
                               ETA: 00:02:36

################################################################################
                     [1m Learning iteration 1935/2000 [0m                     

                       Computation: 40341 steps/s (collection: 2.342s, learning 0.095s)
             Mean action noise std: 4.29
          Mean value_function loss: 209.0372
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 88.4402
                       Mean reward: 852.81
               Mean episode length: 227.42
    Episode_Reward/reaching_object: 1.9518
     Episode_Reward/lifting_object: 166.0581
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.1185
          Episode_Reward/joint_vel: -0.0962
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 2.44s
                      Time elapsed: 01:16:41
                               ETA: 00:02:34

################################################################################
                     [1m Learning iteration 1936/2000 [0m                     

                       Computation: 40563 steps/s (collection: 2.329s, learning 0.094s)
             Mean action noise std: 4.30
          Mean value_function loss: 146.4333
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 88.4474
                       Mean reward: 871.93
               Mean episode length: 233.94
    Episode_Reward/reaching_object: 1.9940
     Episode_Reward/lifting_object: 169.6267
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.1213
          Episode_Reward/joint_vel: -0.0982
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 2.42s
                      Time elapsed: 01:16:43
                               ETA: 00:02:32

################################################################################
                     [1m Learning iteration 1937/2000 [0m                     

                       Computation: 40132 steps/s (collection: 2.322s, learning 0.128s)
             Mean action noise std: 4.30
          Mean value_function loss: 170.5652
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 88.4520
                       Mean reward: 827.54
               Mean episode length: 222.87
    Episode_Reward/reaching_object: 1.9792
     Episode_Reward/lifting_object: 168.2948
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.1203
          Episode_Reward/joint_vel: -0.0978
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 2.45s
                      Time elapsed: 01:16:46
                               ETA: 00:02:29

################################################################################
                     [1m Learning iteration 1938/2000 [0m                     

                       Computation: 39700 steps/s (collection: 2.354s, learning 0.122s)
             Mean action noise std: 4.30
          Mean value_function loss: 186.9632
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 88.4572
                       Mean reward: 835.23
               Mean episode length: 222.30
    Episode_Reward/reaching_object: 1.8905
     Episode_Reward/lifting_object: 160.1785
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.1154
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 2.48s
                      Time elapsed: 01:16:48
                               ETA: 00:02:27

################################################################################
                     [1m Learning iteration 1939/2000 [0m                     

                       Computation: 36758 steps/s (collection: 2.569s, learning 0.106s)
             Mean action noise std: 4.30
          Mean value_function loss: 175.0382
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 88.4642
                       Mean reward: 858.95
               Mean episode length: 228.68
    Episode_Reward/reaching_object: 1.9606
     Episode_Reward/lifting_object: 166.1809
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.1190
          Episode_Reward/joint_vel: -0.0957
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 2.67s
                      Time elapsed: 01:16:51
                               ETA: 00:02:25

################################################################################
                     [1m Learning iteration 1940/2000 [0m                     

                       Computation: 38119 steps/s (collection: 2.460s, learning 0.119s)
             Mean action noise std: 4.30
          Mean value_function loss: 167.0884
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 88.4741
                       Mean reward: 861.98
               Mean episode length: 229.24
    Episode_Reward/reaching_object: 1.9797
     Episode_Reward/lifting_object: 168.7199
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.1202
          Episode_Reward/joint_vel: -0.0966
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 2.58s
                      Time elapsed: 01:16:54
                               ETA: 00:02:22

################################################################################
                     [1m Learning iteration 1941/2000 [0m                     

                       Computation: 39430 steps/s (collection: 2.368s, learning 0.125s)
             Mean action noise std: 4.30
          Mean value_function loss: 164.4901
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 88.4864
                       Mean reward: 870.84
               Mean episode length: 231.00
    Episode_Reward/reaching_object: 1.9714
     Episode_Reward/lifting_object: 167.7117
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1197
          Episode_Reward/joint_vel: -0.0955
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 2.49s
                      Time elapsed: 01:16:56
                               ETA: 00:02:20

################################################################################
                     [1m Learning iteration 1942/2000 [0m                     

                       Computation: 39762 steps/s (collection: 2.369s, learning 0.103s)
             Mean action noise std: 4.30
          Mean value_function loss: 150.1806
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 88.5033
                       Mean reward: 899.32
               Mean episode length: 239.90
    Episode_Reward/reaching_object: 1.9791
     Episode_Reward/lifting_object: 167.3985
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1206
          Episode_Reward/joint_vel: -0.0960
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 2.47s
                      Time elapsed: 01:16:59
                               ETA: 00:02:17

################################################################################
                     [1m Learning iteration 1943/2000 [0m                     

                       Computation: 35544 steps/s (collection: 2.661s, learning 0.105s)
             Mean action noise std: 4.30
          Mean value_function loss: 4084127.9250
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 88.5086
                       Mean reward: 841.41
               Mean episode length: 225.48
    Episode_Reward/reaching_object: 1.9719
     Episode_Reward/lifting_object: 166.8319
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -3.7085
          Episode_Reward/joint_vel: -111.8945
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 2.77s
                      Time elapsed: 01:17:01
                               ETA: 00:02:15

################################################################################
                     [1m Learning iteration 1944/2000 [0m                     

                       Computation: 37987 steps/s (collection: 2.458s, learning 0.130s)
             Mean action noise std: 4.31
          Mean value_function loss: 164.0337
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 88.5124
                       Mean reward: 815.40
               Mean episode length: 220.88
    Episode_Reward/reaching_object: 1.9650
     Episode_Reward/lifting_object: 166.3772
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.1200
          Episode_Reward/joint_vel: -0.0953
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 2.59s
                      Time elapsed: 01:17:04
                               ETA: 00:02:13

################################################################################
                     [1m Learning iteration 1945/2000 [0m                     

                       Computation: 38158 steps/s (collection: 2.443s, learning 0.133s)
             Mean action noise std: 4.31
          Mean value_function loss: 166.1273
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 88.5253
                       Mean reward: 872.39
               Mean episode length: 231.48
    Episode_Reward/reaching_object: 1.9954
     Episode_Reward/lifting_object: 169.0734
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.1215
          Episode_Reward/joint_vel: -0.0964
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 2.58s
                      Time elapsed: 01:17:06
                               ETA: 00:02:10

################################################################################
                     [1m Learning iteration 1946/2000 [0m                     

                       Computation: 38641 steps/s (collection: 2.438s, learning 0.106s)
             Mean action noise std: 4.31
          Mean value_function loss: 185.7515
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 88.5416
                       Mean reward: 854.75
               Mean episode length: 228.44
    Episode_Reward/reaching_object: 1.9607
     Episode_Reward/lifting_object: 165.8497
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1199
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 2.54s
                      Time elapsed: 01:17:09
                               ETA: 00:02:08

################################################################################
                     [1m Learning iteration 1947/2000 [0m                     

                       Computation: 39240 steps/s (collection: 2.403s, learning 0.102s)
             Mean action noise std: 4.31
          Mean value_function loss: 188.1379
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 88.5539
                       Mean reward: 850.73
               Mean episode length: 226.81
    Episode_Reward/reaching_object: 1.9575
     Episode_Reward/lifting_object: 166.1277
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1198
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 2.51s
                      Time elapsed: 01:17:12
                               ETA: 00:02:06

################################################################################
                     [1m Learning iteration 1948/2000 [0m                     

                       Computation: 39327 steps/s (collection: 2.379s, learning 0.121s)
             Mean action noise std: 4.31
          Mean value_function loss: 232.8272
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 88.5626
                       Mean reward: 818.46
               Mean episode length: 221.89
    Episode_Reward/reaching_object: 1.9067
     Episode_Reward/lifting_object: 161.0190
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.1176
          Episode_Reward/joint_vel: -0.0931
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 2.50s
                      Time elapsed: 01:17:14
                               ETA: 00:02:03

################################################################################
                     [1m Learning iteration 1949/2000 [0m                     

                       Computation: 39583 steps/s (collection: 2.373s, learning 0.110s)
             Mean action noise std: 4.31
          Mean value_function loss: 161.7262
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 88.5698
                       Mean reward: 889.30
               Mean episode length: 236.08
    Episode_Reward/reaching_object: 2.0029
     Episode_Reward/lifting_object: 170.4294
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.1227
          Episode_Reward/joint_vel: -0.0960
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 2.48s
                      Time elapsed: 01:17:17
                               ETA: 00:02:01

################################################################################
                     [1m Learning iteration 1950/2000 [0m                     

                       Computation: 39798 steps/s (collection: 2.318s, learning 0.152s)
             Mean action noise std: 4.32
          Mean value_function loss: 199.0095
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 88.5763
                       Mean reward: 872.92
               Mean episode length: 232.90
    Episode_Reward/reaching_object: 1.9519
     Episode_Reward/lifting_object: 165.4821
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.1201
          Episode_Reward/joint_vel: -0.0938
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 2.47s
                      Time elapsed: 01:17:19
                               ETA: 00:01:58

################################################################################
                     [1m Learning iteration 1951/2000 [0m                     

                       Computation: 40513 steps/s (collection: 2.334s, learning 0.092s)
             Mean action noise std: 4.32
          Mean value_function loss: 166.3491
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 88.5871
                       Mean reward: 885.90
               Mean episode length: 235.17
    Episode_Reward/reaching_object: 1.9228
     Episode_Reward/lifting_object: 163.1036
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.1182
          Episode_Reward/joint_vel: -0.0919
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 2.43s
                      Time elapsed: 01:17:21
                               ETA: 00:01:56

################################################################################
                     [1m Learning iteration 1952/2000 [0m                     

                       Computation: 39910 steps/s (collection: 2.355s, learning 0.108s)
             Mean action noise std: 4.32
          Mean value_function loss: 160.9616
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 88.5978
                       Mean reward: 801.97
               Mean episode length: 216.66
    Episode_Reward/reaching_object: 1.9264
     Episode_Reward/lifting_object: 163.2270
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.1186
          Episode_Reward/joint_vel: -0.0921
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 2.46s
                      Time elapsed: 01:17:24
                               ETA: 00:01:54

################################################################################
                     [1m Learning iteration 1953/2000 [0m                     

                       Computation: 36970 steps/s (collection: 2.537s, learning 0.122s)
             Mean action noise std: 4.32
          Mean value_function loss: 147.9425
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 88.6080
                       Mean reward: 852.92
               Mean episode length: 228.62
    Episode_Reward/reaching_object: 1.9830
     Episode_Reward/lifting_object: 168.6178
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.1220
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 2.66s
                      Time elapsed: 01:17:27
                               ETA: 00:01:51

################################################################################
                     [1m Learning iteration 1954/2000 [0m                     

                       Computation: 40076 steps/s (collection: 2.318s, learning 0.135s)
             Mean action noise std: 4.32
          Mean value_function loss: 167.5614
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 88.6252
                       Mean reward: 839.16
               Mean episode length: 223.84
    Episode_Reward/reaching_object: 1.9550
     Episode_Reward/lifting_object: 166.4786
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.1203
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 2.45s
                      Time elapsed: 01:17:29
                               ETA: 00:01:49

################################################################################
                     [1m Learning iteration 1955/2000 [0m                     

                       Computation: 39710 steps/s (collection: 2.356s, learning 0.119s)
             Mean action noise std: 4.32
          Mean value_function loss: 182.0437
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 88.6370
                       Mean reward: 818.45
               Mean episode length: 220.80
    Episode_Reward/reaching_object: 1.9480
     Episode_Reward/lifting_object: 165.2850
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.1205
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 2.48s
                      Time elapsed: 01:17:31
                               ETA: 00:01:47

################################################################################
                     [1m Learning iteration 1956/2000 [0m                     

                       Computation: 38888 steps/s (collection: 2.352s, learning 0.176s)
             Mean action noise std: 4.33
          Mean value_function loss: 167.0545
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 88.6536
                       Mean reward: 818.81
               Mean episode length: 220.14
    Episode_Reward/reaching_object: 1.9109
     Episode_Reward/lifting_object: 161.4591
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.1183
          Episode_Reward/joint_vel: -0.0924
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 2.53s
                      Time elapsed: 01:17:34
                               ETA: 00:01:44

################################################################################
                     [1m Learning iteration 1957/2000 [0m                     

                       Computation: 38873 steps/s (collection: 2.422s, learning 0.107s)
             Mean action noise std: 4.33
          Mean value_function loss: 153.8316
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 88.6647
                       Mean reward: 866.36
               Mean episode length: 230.50
    Episode_Reward/reaching_object: 2.0167
     Episode_Reward/lifting_object: 171.8026
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.1237
          Episode_Reward/joint_vel: -0.0950
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 2.53s
                      Time elapsed: 01:17:37
                               ETA: 00:01:42

################################################################################
                     [1m Learning iteration 1958/2000 [0m                     

                       Computation: 40263 steps/s (collection: 2.325s, learning 0.116s)
             Mean action noise std: 4.33
          Mean value_function loss: 210.7924
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 88.6743
                       Mean reward: 793.53
               Mean episode length: 212.39
    Episode_Reward/reaching_object: 1.8752
     Episode_Reward/lifting_object: 158.7340
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.1164
          Episode_Reward/joint_vel: -0.0910
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 2.44s
                      Time elapsed: 01:17:39
                               ETA: 00:01:39

################################################################################
                     [1m Learning iteration 1959/2000 [0m                     

                       Computation: 40047 steps/s (collection: 2.347s, learning 0.108s)
             Mean action noise std: 4.33
          Mean value_function loss: 149.3064
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 88.6825
                       Mean reward: 895.52
               Mean episode length: 237.66
    Episode_Reward/reaching_object: 2.0117
     Episode_Reward/lifting_object: 171.0220
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.1238
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 2.45s
                      Time elapsed: 01:17:41
                               ETA: 00:01:37

################################################################################
                     [1m Learning iteration 1960/2000 [0m                     

                       Computation: 41787 steps/s (collection: 2.249s, learning 0.104s)
             Mean action noise std: 4.33
          Mean value_function loss: 181.1351
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 88.6894
                       Mean reward: 865.50
               Mean episode length: 231.25
    Episode_Reward/reaching_object: 1.9264
     Episode_Reward/lifting_object: 163.0548
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1190
          Episode_Reward/joint_vel: -0.0923
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 2.35s
                      Time elapsed: 01:17:44
                               ETA: 00:01:35

################################################################################
                     [1m Learning iteration 1961/2000 [0m                     

                       Computation: 40176 steps/s (collection: 2.340s, learning 0.107s)
             Mean action noise std: 4.33
          Mean value_function loss: 204.0396
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 88.7003
                       Mean reward: 821.95
               Mean episode length: 220.40
    Episode_Reward/reaching_object: 1.9511
     Episode_Reward/lifting_object: 164.9282
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.1205
          Episode_Reward/joint_vel: -0.0932
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 2.45s
                      Time elapsed: 01:17:46
                               ETA: 00:01:32

################################################################################
                     [1m Learning iteration 1962/2000 [0m                     

                       Computation: 41434 steps/s (collection: 2.280s, learning 0.093s)
             Mean action noise std: 4.33
          Mean value_function loss: 153.3704
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 88.7108
                       Mean reward: 866.05
               Mean episode length: 230.84
    Episode_Reward/reaching_object: 1.9702
     Episode_Reward/lifting_object: 166.4904
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.1214
          Episode_Reward/joint_vel: -0.0947
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 2.37s
                      Time elapsed: 01:17:49
                               ETA: 00:01:30

################################################################################
                     [1m Learning iteration 1963/2000 [0m                     

                       Computation: 42105 steps/s (collection: 2.230s, learning 0.105s)
             Mean action noise std: 4.34
          Mean value_function loss: 170.3228
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 88.7157
                       Mean reward: 822.89
               Mean episode length: 219.44
    Episode_Reward/reaching_object: 1.9635
     Episode_Reward/lifting_object: 166.3749
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.1210
          Episode_Reward/joint_vel: -0.0940
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 2.33s
                      Time elapsed: 01:17:51
                               ETA: 00:01:28

################################################################################
                     [1m Learning iteration 1964/2000 [0m                     

                       Computation: 42433 steps/s (collection: 2.218s, learning 0.099s)
             Mean action noise std: 4.34
          Mean value_function loss: 262.5024
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 88.7253
                       Mean reward: 826.35
               Mean episode length: 222.12
    Episode_Reward/reaching_object: 1.9393
     Episode_Reward/lifting_object: 164.0791
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.1199
          Episode_Reward/joint_vel: -0.0933
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 2.32s
                      Time elapsed: 01:17:53
                               ETA: 00:01:25

################################################################################
                     [1m Learning iteration 1965/2000 [0m                     

                       Computation: 42444 steps/s (collection: 2.228s, learning 0.088s)
             Mean action noise std: 4.34
          Mean value_function loss: 227.5143
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 88.7355
                       Mean reward: 858.74
               Mean episode length: 229.58
    Episode_Reward/reaching_object: 1.9555
     Episode_Reward/lifting_object: 165.8287
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1209
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 2.32s
                      Time elapsed: 01:17:56
                               ETA: 00:01:23

################################################################################
                     [1m Learning iteration 1966/2000 [0m                     

                       Computation: 41973 steps/s (collection: 2.213s, learning 0.129s)
             Mean action noise std: 4.34
          Mean value_function loss: 168.7631
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 88.7441
                       Mean reward: 849.53
               Mean episode length: 224.38
    Episode_Reward/reaching_object: 1.9815
     Episode_Reward/lifting_object: 168.5462
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.1222
          Episode_Reward/joint_vel: -0.0948
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 2.34s
                      Time elapsed: 01:17:58
                               ETA: 00:01:20

################################################################################
                     [1m Learning iteration 1967/2000 [0m                     

                       Computation: 41512 steps/s (collection: 2.248s, learning 0.120s)
             Mean action noise std: 4.34
          Mean value_function loss: 174.0181
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 88.7566
                       Mean reward: 830.25
               Mean episode length: 221.81
    Episode_Reward/reaching_object: 1.9388
     Episode_Reward/lifting_object: 164.4644
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1202
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 2.37s
                      Time elapsed: 01:18:00
                               ETA: 00:01:18

################################################################################
                     [1m Learning iteration 1968/2000 [0m                     

                       Computation: 42645 steps/s (collection: 2.198s, learning 0.108s)
             Mean action noise std: 4.34
          Mean value_function loss: 188.3885
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 88.7693
                       Mean reward: 854.27
               Mean episode length: 226.69
    Episode_Reward/reaching_object: 1.9453
     Episode_Reward/lifting_object: 164.6873
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.1210
          Episode_Reward/joint_vel: -0.0953
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 2.31s
                      Time elapsed: 01:18:03
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 1969/2000 [0m                     

                       Computation: 41835 steps/s (collection: 2.202s, learning 0.148s)
             Mean action noise std: 4.35
          Mean value_function loss: 149.8301
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 88.7842
                       Mean reward: 834.18
               Mean episode length: 223.43
    Episode_Reward/reaching_object: 1.9442
     Episode_Reward/lifting_object: 165.3694
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.1207
          Episode_Reward/joint_vel: -0.0949
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 2.35s
                      Time elapsed: 01:18:05
                               ETA: 00:01:13

################################################################################
                     [1m Learning iteration 1970/2000 [0m                     

                       Computation: 42033 steps/s (collection: 2.215s, learning 0.124s)
             Mean action noise std: 4.35
          Mean value_function loss: 203.3878
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 88.7929
                       Mean reward: 800.72
               Mean episode length: 216.10
    Episode_Reward/reaching_object: 1.9613
     Episode_Reward/lifting_object: 166.4223
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.1221
          Episode_Reward/joint_vel: -0.0967
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 2.34s
                      Time elapsed: 01:18:07
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 1971/2000 [0m                     

                       Computation: 43359 steps/s (collection: 2.169s, learning 0.099s)
             Mean action noise std: 4.35
          Mean value_function loss: 155.5389
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 88.8056
                       Mean reward: 846.56
               Mean episode length: 225.38
    Episode_Reward/reaching_object: 1.9435
     Episode_Reward/lifting_object: 164.8897
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.1214
          Episode_Reward/joint_vel: -0.0959
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 2.27s
                      Time elapsed: 01:18:10
                               ETA: 00:01:08

################################################################################
                     [1m Learning iteration 1972/2000 [0m                     

                       Computation: 42346 steps/s (collection: 2.235s, learning 0.086s)
             Mean action noise std: 4.35
          Mean value_function loss: 188.1059
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 88.8184
                       Mean reward: 812.61
               Mean episode length: 219.43
    Episode_Reward/reaching_object: 1.9354
     Episode_Reward/lifting_object: 164.2619
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.1210
          Episode_Reward/joint_vel: -0.0959
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 2.32s
                      Time elapsed: 01:18:12
                               ETA: 00:01:06

################################################################################
                     [1m Learning iteration 1973/2000 [0m                     

                       Computation: 42831 steps/s (collection: 2.182s, learning 0.114s)
             Mean action noise std: 4.35
          Mean value_function loss: 171.0447
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 88.8283
                       Mean reward: 840.34
               Mean episode length: 224.81
    Episode_Reward/reaching_object: 1.9544
     Episode_Reward/lifting_object: 166.2673
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1220
          Episode_Reward/joint_vel: -0.0961
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 2.30s
                      Time elapsed: 01:18:14
                               ETA: 00:01:04

################################################################################
                     [1m Learning iteration 1974/2000 [0m                     

                       Computation: 42137 steps/s (collection: 2.213s, learning 0.120s)
             Mean action noise std: 4.35
          Mean value_function loss: 169.7491
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 88.8364
                       Mean reward: 850.98
               Mean episode length: 227.85
    Episode_Reward/reaching_object: 1.9388
     Episode_Reward/lifting_object: 164.6025
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.1213
          Episode_Reward/joint_vel: -0.0961
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 2.33s
                      Time elapsed: 01:18:16
                               ETA: 00:01:01

################################################################################
                     [1m Learning iteration 1975/2000 [0m                     

                       Computation: 42185 steps/s (collection: 2.217s, learning 0.113s)
             Mean action noise std: 4.35
          Mean value_function loss: 175.6117
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 88.8477
                       Mean reward: 859.08
               Mean episode length: 229.96
    Episode_Reward/reaching_object: 1.9282
     Episode_Reward/lifting_object: 163.6411
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.1214
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 2.33s
                      Time elapsed: 01:18:19
                               ETA: 00:00:59

################################################################################
                     [1m Learning iteration 1976/2000 [0m                     

                       Computation: 41065 steps/s (collection: 2.290s, learning 0.104s)
             Mean action noise std: 4.36
          Mean value_function loss: 187.4122
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 88.8571
                       Mean reward: 822.64
               Mean episode length: 220.59
    Episode_Reward/reaching_object: 1.9674
     Episode_Reward/lifting_object: 167.9174
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.1230
          Episode_Reward/joint_vel: -0.0955
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 2.39s
                      Time elapsed: 01:18:21
                               ETA: 00:00:57

################################################################################
                     [1m Learning iteration 1977/2000 [0m                     

                       Computation: 41484 steps/s (collection: 2.253s, learning 0.117s)
             Mean action noise std: 4.36
          Mean value_function loss: 148.2892
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 88.8678
                       Mean reward: 856.04
               Mean episode length: 227.98
    Episode_Reward/reaching_object: 1.9897
     Episode_Reward/lifting_object: 170.0370
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.1246
          Episode_Reward/joint_vel: -0.0973
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 2.37s
                      Time elapsed: 01:18:24
                               ETA: 00:00:54

################################################################################
                     [1m Learning iteration 1978/2000 [0m                     

                       Computation: 38457 steps/s (collection: 2.421s, learning 0.136s)
             Mean action noise std: 4.36
          Mean value_function loss: 196.4171
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 88.8777
                       Mean reward: 857.17
               Mean episode length: 228.73
    Episode_Reward/reaching_object: 1.9261
     Episode_Reward/lifting_object: 164.0203
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.1212
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 2.56s
                      Time elapsed: 01:18:26
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1979/2000 [0m                     

                       Computation: 41257 steps/s (collection: 2.290s, learning 0.093s)
             Mean action noise std: 4.36
          Mean value_function loss: 160.0510
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 88.8944
                       Mean reward: 798.55
               Mean episode length: 213.40
    Episode_Reward/reaching_object: 1.9318
     Episode_Reward/lifting_object: 164.4752
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.1210
          Episode_Reward/joint_vel: -0.0933
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 2.38s
                      Time elapsed: 01:18:29
                               ETA: 00:00:49

################################################################################
                     [1m Learning iteration 1980/2000 [0m                     

                       Computation: 40320 steps/s (collection: 2.348s, learning 0.091s)
             Mean action noise std: 4.36
          Mean value_function loss: 156.9479
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 88.9068
                       Mean reward: 843.12
               Mean episode length: 225.58
    Episode_Reward/reaching_object: 1.9738
     Episode_Reward/lifting_object: 168.2001
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.1239
          Episode_Reward/joint_vel: -0.0954
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 2.44s
                      Time elapsed: 01:18:31
                               ETA: 00:00:47

################################################################################
                     [1m Learning iteration 1981/2000 [0m                     

                       Computation: 41004 steps/s (collection: 2.253s, learning 0.144s)
             Mean action noise std: 4.37
          Mean value_function loss: 218.4873
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 88.9162
                       Mean reward: 811.97
               Mean episode length: 217.76
    Episode_Reward/reaching_object: 1.9276
     Episode_Reward/lifting_object: 164.0103
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.1213
          Episode_Reward/joint_vel: -0.0937
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 2.40s
                      Time elapsed: 01:18:33
                               ETA: 00:00:45

################################################################################
                     [1m Learning iteration 1982/2000 [0m                     

                       Computation: 40655 steps/s (collection: 2.309s, learning 0.109s)
             Mean action noise std: 4.37
          Mean value_function loss: 212.2575
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 88.9320
                       Mean reward: 820.06
               Mean episode length: 220.80
    Episode_Reward/reaching_object: 1.9153
     Episode_Reward/lifting_object: 162.3151
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.1206
          Episode_Reward/joint_vel: -0.0930
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 2.42s
                      Time elapsed: 01:18:36
                               ETA: 00:00:42

################################################################################
                     [1m Learning iteration 1983/2000 [0m                     

                       Computation: 41091 steps/s (collection: 2.267s, learning 0.125s)
             Mean action noise std: 4.37
          Mean value_function loss: 195.0816
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 88.9438
                       Mean reward: 820.75
               Mean episode length: 220.43
    Episode_Reward/reaching_object: 1.9152
     Episode_Reward/lifting_object: 162.5778
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.1207
          Episode_Reward/joint_vel: -0.0938
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 2.39s
                      Time elapsed: 01:18:38
                               ETA: 00:00:40

################################################################################
                     [1m Learning iteration 1984/2000 [0m                     

                       Computation: 39039 steps/s (collection: 2.407s, learning 0.111s)
             Mean action noise std: 4.37
          Mean value_function loss: 180.1338
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 88.9597
                       Mean reward: 825.06
               Mean episode length: 220.63
    Episode_Reward/reaching_object: 1.9022
     Episode_Reward/lifting_object: 162.2492
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.1200
          Episode_Reward/joint_vel: -0.0919
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 2.52s
                      Time elapsed: 01:18:41
                               ETA: 00:00:38

################################################################################
                     [1m Learning iteration 1985/2000 [0m                     

                       Computation: 39426 steps/s (collection: 2.349s, learning 0.144s)
             Mean action noise std: 4.37
          Mean value_function loss: 177.3265
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 88.9717
                       Mean reward: 804.72
               Mean episode length: 218.14
    Episode_Reward/reaching_object: 1.9329
     Episode_Reward/lifting_object: 163.9043
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.1215
          Episode_Reward/joint_vel: -0.0932
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 2.49s
                      Time elapsed: 01:18:43
                               ETA: 00:00:35

################################################################################
                     [1m Learning iteration 1986/2000 [0m                     

                       Computation: 38566 steps/s (collection: 2.404s, learning 0.145s)
             Mean action noise std: 4.38
          Mean value_function loss: 149.9697
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 88.9890
                       Mean reward: 859.49
               Mean episode length: 229.69
    Episode_Reward/reaching_object: 1.9879
     Episode_Reward/lifting_object: 169.1229
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.1249
          Episode_Reward/joint_vel: -0.0947
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 2.55s
                      Time elapsed: 01:18:46
                               ETA: 00:00:33

################################################################################
                     [1m Learning iteration 1987/2000 [0m                     

                       Computation: 40229 steps/s (collection: 2.337s, learning 0.107s)
             Mean action noise std: 4.38
          Mean value_function loss: 162.3142
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 89.0034
                       Mean reward: 858.83
               Mean episode length: 227.57
    Episode_Reward/reaching_object: 1.9462
     Episode_Reward/lifting_object: 165.8772
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.1227
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 2.44s
                      Time elapsed: 01:18:48
                               ETA: 00:00:30

################################################################################
                     [1m Learning iteration 1988/2000 [0m                     

                       Computation: 41068 steps/s (collection: 2.286s, learning 0.108s)
             Mean action noise std: 4.38
          Mean value_function loss: 162.1753
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 89.0146
                       Mean reward: 834.06
               Mean episode length: 222.64
    Episode_Reward/reaching_object: 1.9565
     Episode_Reward/lifting_object: 166.3929
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.1236
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 2.39s
                      Time elapsed: 01:18:51
                               ETA: 00:00:28

################################################################################
                     [1m Learning iteration 1989/2000 [0m                     

                       Computation: 39662 steps/s (collection: 2.380s, learning 0.099s)
             Mean action noise std: 4.38
          Mean value_function loss: 191.6155
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 89.0276
                       Mean reward: 822.78
               Mean episode length: 219.57
    Episode_Reward/reaching_object: 1.9331
     Episode_Reward/lifting_object: 164.5459
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.1221
          Episode_Reward/joint_vel: -0.0922
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 2.48s
                      Time elapsed: 01:18:53
                               ETA: 00:00:26

################################################################################
                     [1m Learning iteration 1990/2000 [0m                     

                       Computation: 40727 steps/s (collection: 2.270s, learning 0.144s)
             Mean action noise std: 4.38
          Mean value_function loss: 184.6661
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 89.0408
                       Mean reward: 855.99
               Mean episode length: 229.01
    Episode_Reward/reaching_object: 1.9185
     Episode_Reward/lifting_object: 162.9846
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.1218
          Episode_Reward/joint_vel: -0.0936
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 2.41s
                      Time elapsed: 01:18:55
                               ETA: 00:00:23

################################################################################
                     [1m Learning iteration 1991/2000 [0m                     

                       Computation: 41398 steps/s (collection: 2.266s, learning 0.109s)
             Mean action noise std: 4.39
          Mean value_function loss: 194.1998
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 89.0558
                       Mean reward: 810.56
               Mean episode length: 216.77
    Episode_Reward/reaching_object: 1.9162
     Episode_Reward/lifting_object: 163.1319
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.1218
          Episode_Reward/joint_vel: -0.0926
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 2.37s
                      Time elapsed: 01:18:58
                               ETA: 00:00:21

################################################################################
                     [1m Learning iteration 1992/2000 [0m                     

                       Computation: 39664 steps/s (collection: 2.379s, learning 0.100s)
             Mean action noise std: 4.39
          Mean value_function loss: 182.4304
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 89.0732
                       Mean reward: 829.57
               Mean episode length: 222.32
    Episode_Reward/reaching_object: 1.9513
     Episode_Reward/lifting_object: 165.3471
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.1240
          Episode_Reward/joint_vel: -0.0937
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 2.48s
                      Time elapsed: 01:19:00
                               ETA: 00:00:19

################################################################################
                     [1m Learning iteration 1993/2000 [0m                     

                       Computation: 40939 steps/s (collection: 2.280s, learning 0.121s)
             Mean action noise std: 4.39
          Mean value_function loss: 208.4496
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 89.0811
                       Mean reward: 797.06
               Mean episode length: 213.65
    Episode_Reward/reaching_object: 1.8969
     Episode_Reward/lifting_object: 160.7430
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.1209
          Episode_Reward/joint_vel: -0.0924
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 2.40s
                      Time elapsed: 01:19:03
                               ETA: 00:00:16

################################################################################
                     [1m Learning iteration 1994/2000 [0m                     

                       Computation: 40231 steps/s (collection: 2.352s, learning 0.092s)
             Mean action noise std: 4.39
          Mean value_function loss: 191.9112
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 89.0890
                       Mean reward: 783.64
               Mean episode length: 211.93
    Episode_Reward/reaching_object: 1.8662
     Episode_Reward/lifting_object: 157.8167
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.1198
          Episode_Reward/joint_vel: -0.0924
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 2.44s
                      Time elapsed: 01:19:05
                               ETA: 00:00:14

################################################################################
                     [1m Learning iteration 1995/2000 [0m                     

                       Computation: 41957 steps/s (collection: 2.242s, learning 0.101s)
             Mean action noise std: 4.39
          Mean value_function loss: 150.0433
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 89.0991
                       Mean reward: 848.40
               Mean episode length: 227.98
    Episode_Reward/reaching_object: 1.9687
     Episode_Reward/lifting_object: 167.1351
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.1255
          Episode_Reward/joint_vel: -0.0961
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 2.34s
                      Time elapsed: 01:19:07
                               ETA: 00:00:11

################################################################################
                     [1m Learning iteration 1996/2000 [0m                     

                       Computation: 40994 steps/s (collection: 2.295s, learning 0.103s)
             Mean action noise std: 4.39
          Mean value_function loss: 146.8982
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 89.1075
                       Mean reward: 855.90
               Mean episode length: 228.61
    Episode_Reward/reaching_object: 1.9702
     Episode_Reward/lifting_object: 167.4730
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.1256
          Episode_Reward/joint_vel: -0.0956
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 2.40s
                      Time elapsed: 01:19:10
                               ETA: 00:00:09

################################################################################
                     [1m Learning iteration 1997/2000 [0m                     

                       Computation: 40925 steps/s (collection: 2.310s, learning 0.092s)
             Mean action noise std: 4.40
          Mean value_function loss: 152.5021
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 89.1210
                       Mean reward: 862.13
               Mean episode length: 229.99
    Episode_Reward/reaching_object: 1.9609
     Episode_Reward/lifting_object: 166.6301
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.1252
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 2.40s
                      Time elapsed: 01:19:12
                               ETA: 00:00:07

################################################################################
                     [1m Learning iteration 1998/2000 [0m                     

                       Computation: 40366 steps/s (collection: 2.317s, learning 0.119s)
             Mean action noise std: 4.40
          Mean value_function loss: 158.9737
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 89.1346
                       Mean reward: 844.00
               Mean episode length: 226.18
    Episode_Reward/reaching_object: 1.9609
     Episode_Reward/lifting_object: 166.8724
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.1245
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 2.44s
                      Time elapsed: 01:19:15
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1999/2000 [0m                     

                       Computation: 40142 steps/s (collection: 2.359s, learning 0.090s)
             Mean action noise std: 4.40
          Mean value_function loss: 191.7892
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 89.1466
                       Mean reward: 853.19
               Mean episode length: 230.37
    Episode_Reward/reaching_object: 1.9423
     Episode_Reward/lifting_object: 164.1269
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.1248
          Episode_Reward/joint_vel: -0.0953
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 2.45s
                      Time elapsed: 01:19:17
                               ETA: 00:00:02

